WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
    check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
  File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
        raise ImportError(check_min_version("4.38.0.dev0")

ImportError  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
Traceback (most recent call last):
  File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
Traceback (most recent call last):
      File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
    check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
    check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
    raise ImportError(
ImportErrorraise ImportError(
: ImportErrorThis example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
[2024-03-13 12:53:06,365] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3385) of binary: /ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/bin/python
Traceback (most recent call last):
  File "/ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_12:53:06
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3386)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_12:53:06
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3387)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-13_12:53:06
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3388)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_12:53:06
  host      : v016.pvt.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3385)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
Traceback (most recent call last):
    check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
      File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.check_min_version("4.38.0.dev0")

  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
      File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
    raise ImportError(
ImportError  File "/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/run_glue.py", line 52, in <module>
:     This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
check_min_version("4.38.0.dev0")
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.37.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
[2024-03-13 12:54:21,001] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3466) of binary: /ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/bin/python
Traceback (most recent call last):
  File "/ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ocean/projects/cis220031p/mmisra/miniconda3/envs/llama/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_12:54:21
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3467)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_12:54:21
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3468)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-13_12:54:21
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3474)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_12:54:21
  host      : v016.pvt.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3466)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
03/13/2024 12:55:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/13/2024 12:55:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/runs/Mar13_12-55-26_v016.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
  File "run_glue.py", line 656, in <module>
        main()
  File "run_glue.py", line 236, in main
      File "run_glue.py", line 656, in <module>
    main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "<string>", line 121, in __init__
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
        and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    and (self.device.type != "cuda")
      File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
            cached = self.fget(obj)cached = self.fget(obj)cached = self.fget(obj)


  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
            self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
        torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
        torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
torch._C._cuda_setDevice(device)

RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Overwrite dataset info from restored data version if exists.
03/13/2024 12:55:29 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 12:55:29 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
03/13/2024 12:55:29 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 12:55:29 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-03-13 12:55:31,590 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-03-13 12:55:31,594 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-03-13 12:55:31,654 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-03-13 12:55:31,654 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-03-13 12:55:31,654 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-03-13 12:55:31,654 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-03-13 12:55:31,654 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-03-13 12:55:31,747 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-03-13 12:55:31,757] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3536 closing signal SIGTERM
[2024-03-13 12:55:31,821] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 3537) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_12:55:31
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3538)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_12:55:31
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3539)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_12:55:31
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3537)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "run_glue.py", line 52, in <module>
  File "run_glue.py", line 52, in <module>
        check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
        raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.0.dev0.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
raise ImportError(
ImportError  File "run_glue.py", line 52, in <module>
:     This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.0.dev0.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
check_min_version("4.39.0.dev0")  File "run_glue.py", line 52, in <module>

  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
        check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 244, in check_min_version
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.0.dev0.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.0.dev0.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
[2024-03-13 12:57:19,241] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 4176) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_12:57:19
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 4177)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_12:57:19
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 4178)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-13_12:57:19
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4179)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_12:57:19
  host      : v016.pvt.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4176)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "run_glue.py", line 52, in <module>
    Traceback (most recent call last):
  File "run_glue.py", line 52, in <module>
    check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 247, in check_min_version
    check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 247, in check_min_version
      File "run_glue.py", line 52, in <module>
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
  File "run_glue.py", line 52, in <module>
check_min_version("4.39.0.dev0")    
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 247, in check_min_version
raise ImportError(    
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
check_min_version("4.39.0.dev0")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/__init__.py", line 247, in check_min_version
    raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
raise ImportError(
ImportError: This example requires a source install from HuggingFace Transformers (see `https://huggingface.co/docs/transformers/installation#install-from-source`), but the version found is 4.38.2.
Check out https://github.com/huggingface/transformers/tree/main/examples#important-note for the examples corresponding to other versions of HuggingFace Transformers.
[2024-03-13 13:02:47,436] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 5531) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_13:02:47
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 5532)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_13:02:47
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 5533)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-03-13_13:02:47
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 5534)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_13:02:47
  host      : v016.pvt.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 5531)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 234, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 234, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses

  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 122, in __init__
      File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1532, in __post_init__
obj = dtype(**inputs)
  File "<string>", line 122, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1532, in __post_init__
    and (self.device.type != "cuda")    
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 2008, in device
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 2008, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 63, in __get__
return self._setup_devices    
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 63, in __get__
    cached = self.fget(obj)cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1944, in _setup_devices

  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1944, in _setup_devices
        self.distributed_state = PartialState(self.distributed_state = PartialState(

  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
        torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: torch._C._cuda_setDevice(device)CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

03/13/2024 13:06:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/13/2024 13:06:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/runs/Mar13_13-06-36_v016.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 234, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 122, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1532, in __post_init__
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 2008, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 63, in __get__
    cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1944, in _setup_devices
    self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Overwrite dataset info from restored data version if exists.
03/13/2024 13:06:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 13:06:39 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
03/13/2024 13:06:39 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 13:06:39 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:726] 2024-03-13 13:06:39,695 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:789] 2024-03-13 13:06:39,698 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:06:39,743 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:06:39,744 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:06:39,744 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:06:39,744 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:06:39,744 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[2024-03-13 13:06:39,794] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 7673 closing signal SIGTERM
[2024-03-13 13:06:39,859] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 7674) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-13_13:06:39
  host      : v016.pvt.bridges2.psc.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 7675)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-13_13:06:39
  host      : v016.pvt.bridges2.psc.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 7676)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_13:06:39
  host      : v016.pvt.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 7674)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v016: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
03/13/2024 13:09:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/13/2024 13:09:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/runs/Mar13_13-09-35_v016.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
03/13/2024 13:09:37 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 13:09:37 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
03/13/2024 13:09:37 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
03/13/2024 13:09:37 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:726] 2024-03-13 13:09:37,954 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:789] 2024-03-13 13:09:37,956 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:09:37,990 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:09:37,990 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:09:37,990 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:09:37,991 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2057] 2024-03-13 13:09:37,991 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-03-13 13:09:38,049 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3279] 2024-03-13 13:09:38,135 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:4007] 2024-03-13 13:10:35,814 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:4019] 2024-03-13 13:10:35,814 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/13/2024 13:10:35 - WARNING - __main__ - The max_seq_length passed (4096) is larger than the maximum length for the model (2048). Using max_seq_length=2048.
Running tokenizer on dataset:   0%|          | 0/392702 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-fc004e8353e27e22.arrow
03/13/2024 13:10:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-fc004e8353e27e22.arrow
Running tokenizer on dataset:   0%|          | 1000/392702 [00:00<05:53, 1108.22 examples/s]Running tokenizer on dataset:   1%|          | 2000/392702 [00:01<05:45, 1131.76 examples/s]Running tokenizer on dataset:   1%|          | 3000/392702 [00:02<05:26, 1194.19 examples/s]Running tokenizer on dataset:   1%|          | 4000/392702 [00:03<05:22, 1207.09 examples/s]Running tokenizer on dataset:   1%|         | 5000/392702 [00:04<05:20, 1209.36 examples/s]Running tokenizer on dataset:   2%|         | 6000/392702 [00:04<05:14, 1228.09 examples/s]Running tokenizer on dataset:   2%|         | 7000/392702 [00:05<05:14, 1225.11 examples/s]Running tokenizer on dataset:   2%|         | 8000/392702 [00:06<05:14, 1224.45 examples/s]Running tokenizer on dataset:   2%|         | 9000/392702 [00:07<05:11, 1233.53 examples/s]Running tokenizer on dataset:   3%|         | 10000/392702 [00:08<05:08, 1238.72 examples/s]Running tokenizer on dataset:   3%|         | 11000/392702 [00:09<05:09, 1231.55 examples/s]Running tokenizer on dataset:   3%|         | 12000/392702 [00:09<05:06, 1240.15 examples/s]Running tokenizer on dataset:   3%|         | 13000/392702 [00:10<05:05, 1240.95 examples/s]Running tokenizer on dataset:   4%|         | 14000/392702 [00:11<05:06, 1234.70 examples/s]Running tokenizer on dataset:   4%|         | 15000/392702 [00:12<05:04, 1241.37 examples/s]Running tokenizer on dataset:   4%|         | 16000/392702 [00:13<05:03, 1242.70 examples/s]Running tokenizer on dataset:   4%|         | 17000/392702 [00:13<05:04, 1235.02 examples/s]Running tokenizer on dataset:   5%|         | 18000/392702 [00:14<05:01, 1242.34 examples/s]Running tokenizer on dataset:   5%|         | 19000/392702 [00:15<05:00, 1242.57 examples/s]Running tokenizer on dataset:   5%|         | 20000/392702 [00:16<05:02, 1233.67 examples/s]Running tokenizer on dataset:   5%|         | 21000/392702 [00:17<04:59, 1241.02 examples/s]Running tokenizer on dataset:   6%|         | 22000/392702 [00:17<04:58, 1241.89 examples/s]Running tokenizer on dataset:   6%|         | 23000/392702 [00:18<04:59, 1236.16 examples/s]Running tokenizer on dataset:   6%|         | 24000/392702 [00:19<04:56, 1242.21 examples/s]Running tokenizer on dataset:   6%|         | 25000/392702 [00:20<04:56, 1242.06 examples/s]Running tokenizer on dataset:   7%|         | 26000/392702 [00:21<04:57, 1233.57 examples/s]Running tokenizer on dataset:   7%|         | 27000/392702 [00:21<04:55, 1239.19 examples/s]Running tokenizer on dataset:   7%|         | 28000/392702 [00:22<05:08, 1183.75 examples/s]Running tokenizer on dataset:   7%|         | 29000/392702 [00:23<05:04, 1193.18 examples/s]Running tokenizer on dataset:   8%|         | 30000/392702 [00:24<05:00, 1207.54 examples/s]Running tokenizer on dataset:   8%|         | 31000/392702 [00:25<04:56, 1219.02 examples/s]Running tokenizer on dataset:   8%|         | 32000/392702 [00:26<04:56, 1217.08 examples/s]Running tokenizer on dataset:   8%|         | 33000/392702 [00:26<04:53, 1226.68 examples/s]Running tokenizer on dataset:   9%|         | 34000/392702 [00:27<04:51, 1230.74 examples/s]Running tokenizer on dataset:   9%|         | 35000/392702 [00:28<04:52, 1224.88 examples/s]Running tokenizer on dataset:   9%|         | 36000/392702 [00:29<04:48, 1236.75 examples/s]Running tokenizer on dataset:   9%|         | 37000/392702 [00:30<04:47, 1238.55 examples/s]Running tokenizer on dataset:  10%|         | 38000/392702 [00:30<04:48, 1229.55 examples/s]Running tokenizer on dataset:  10%|         | 39000/392702 [00:31<04:45, 1237.88 examples/s]Running tokenizer on dataset:  10%|         | 40000/392702 [00:32<04:44, 1240.36 examples/s]Running tokenizer on dataset:  10%|         | 41000/392702 [00:33<04:45, 1230.82 examples/s]Running tokenizer on dataset:  11%|         | 42000/392702 [00:34<04:44, 1234.25 examples/s]Running tokenizer on dataset:  11%|         | 43000/392702 [00:35<04:42, 1236.23 examples/s]Running tokenizer on dataset:  11%|         | 44000/392702 [00:35<04:44, 1227.76 examples/s]Running tokenizer on dataset:  11%|        | 45000/392702 [00:36<04:41, 1234.27 examples/s]Running tokenizer on dataset:  12%|        | 46000/392702 [00:37<04:40, 1236.80 examples/s]Running tokenizer on dataset:  12%|        | 47000/392702 [00:38<04:41, 1229.64 examples/s]Running tokenizer on dataset:  12%|        | 48000/392702 [00:39<04:38, 1236.15 examples/s]Running tokenizer on dataset:  12%|        | 49000/392702 [00:39<04:37, 1237.12 examples/s]Running tokenizer on dataset:  13%|        | 50000/392702 [00:40<04:38, 1228.89 examples/s]Running tokenizer on dataset:  13%|        | 51000/392702 [00:41<04:36, 1235.84 examples/s]Running tokenizer on dataset:  13%|        | 52000/392702 [00:42<04:34, 1243.33 examples/s]Running tokenizer on dataset:  13%|        | 53000/392702 [00:43<04:36, 1227.43 examples/s]Running tokenizer on dataset:  14%|        | 54000/392702 [00:43<04:33, 1236.32 examples/s]Running tokenizer on dataset:  14%|        | 55000/392702 [00:44<04:32, 1237.08 examples/s]Running tokenizer on dataset:  14%|        | 56000/392702 [00:45<04:33, 1230.19 examples/s]Running tokenizer on dataset:  15%|        | 57000/392702 [00:46<04:31, 1237.82 examples/s]Running tokenizer on dataset:  15%|        | 58000/392702 [00:47<04:43, 1179.40 examples/s]Running tokenizer on dataset:  15%|        | 59000/392702 [00:48<04:40, 1188.26 examples/s]Running tokenizer on dataset:  15%|        | 60000/392702 [00:48<04:35, 1208.75 examples/s]Running tokenizer on dataset:  16%|        | 61000/392702 [00:49<04:32, 1216.19 examples/s]Running tokenizer on dataset:  16%|        | 62000/392702 [00:50<04:32, 1212.72 examples/s]Running tokenizer on dataset:  16%|        | 63000/392702 [00:51<04:29, 1224.14 examples/s]Running tokenizer on dataset:  16%|        | 64000/392702 [00:52<04:28, 1226.45 examples/s]Running tokenizer on dataset:  17%|        | 65000/392702 [00:52<04:27, 1223.10 examples/s]Running tokenizer on dataset:  17%|        | 66000/392702 [00:53<04:26, 1225.71 examples/s]Running tokenizer on dataset:  17%|        | 67000/392702 [00:54<04:25, 1228.34 examples/s]Running tokenizer on dataset:  17%|        | 68000/392702 [00:55<04:26, 1220.16 examples/s]Running tokenizer on dataset:  18%|        | 69000/392702 [00:56<04:24, 1223.71 examples/s]Running tokenizer on dataset:  18%|        | 70000/392702 [00:57<04:25, 1215.96 examples/s]Running tokenizer on dataset:  18%|        | 71000/392702 [00:57<04:24, 1215.62 examples/s]Running tokenizer on dataset:  18%|        | 72000/392702 [00:58<04:21, 1227.66 examples/s]Running tokenizer on dataset:  19%|        | 73000/392702 [00:59<04:20, 1227.91 examples/s]Running tokenizer on dataset:  19%|        | 74000/392702 [01:00<04:21, 1217.03 examples/s]Running tokenizer on dataset:  19%|        | 75000/392702 [01:01<04:19, 1224.43 examples/s]Running tokenizer on dataset:  19%|        | 76000/392702 [01:01<04:18, 1226.93 examples/s]Running tokenizer on dataset:  20%|        | 77000/392702 [01:02<04:19, 1218.58 examples/s]Running tokenizer on dataset:  20%|        | 78000/392702 [01:03<04:16, 1225.44 examples/s]Running tokenizer on dataset:  20%|        | 79000/392702 [01:04<04:15, 1230.09 examples/s]Running tokenizer on dataset:  20%|        | 80000/392702 [01:05<04:16, 1219.68 examples/s]Running tokenizer on dataset:  21%|        | 81000/392702 [01:06<04:12, 1232.08 examples/s]Running tokenizer on dataset:  21%|        | 82000/392702 [01:06<04:13, 1228.00 examples/s]Running tokenizer on dataset:  21%|        | 83000/392702 [01:07<04:13, 1223.42 examples/s]Running tokenizer on dataset:  21%|       | 84000/392702 [01:08<04:10, 1232.69 examples/s]Running tokenizer on dataset:  22%|       | 85000/392702 [01:09<04:09, 1232.40 examples/s]Running tokenizer on dataset:  22%|       | 86000/392702 [01:10<04:10, 1224.77 examples/s]Running tokenizer on dataset:  22%|       | 87000/392702 [01:10<04:07, 1232.75 examples/s]Running tokenizer on dataset:  22%|       | 88000/392702 [01:11<04:22, 1162.55 examples/s]Running tokenizer on dataset:  23%|       | 89000/392702 [01:12<04:18, 1174.46 examples/s]Running tokenizer on dataset:  23%|       | 90000/392702 [01:13<04:13, 1193.86 examples/s]Running tokenizer on dataset:  23%|       | 91000/392702 [01:14<04:10, 1205.59 examples/s]Running tokenizer on dataset:  23%|       | 92000/392702 [01:15<04:08, 1207.94 examples/s]Running tokenizer on dataset:  24%|       | 93000/392702 [01:15<04:05, 1222.32 examples/s]Running tokenizer on dataset:  24%|       | 94000/392702 [01:16<04:03, 1226.76 examples/s]Running tokenizer on dataset:  24%|       | 95000/392702 [01:17<04:04, 1220.04 examples/s]Running tokenizer on dataset:  24%|       | 96000/392702 [01:18<04:02, 1225.41 examples/s]Running tokenizer on dataset:  25%|       | 97000/392702 [01:19<04:00, 1230.88 examples/s]Running tokenizer on dataset:  25%|       | 98000/392702 [01:20<04:00, 1225.22 examples/s]Running tokenizer on dataset:  25%|       | 99000/392702 [01:20<03:57, 1234.79 examples/s]Running tokenizer on dataset:  25%|       | 100000/392702 [01:21<03:57, 1233.65 examples/s]Running tokenizer on dataset:  26%|       | 101000/392702 [01:22<03:57, 1226.47 examples/s]Running tokenizer on dataset:  26%|       | 102000/392702 [01:23<03:58, 1217.58 examples/s]Running tokenizer on dataset:  26%|       | 103000/392702 [01:24<03:56, 1224.18 examples/s]Running tokenizer on dataset:  26%|       | 104000/392702 [01:24<03:57, 1217.17 examples/s]Running tokenizer on dataset:  27%|       | 105000/392702 [01:25<03:54, 1228.27 examples/s]Running tokenizer on dataset:  27%|       | 106000/392702 [01:26<03:53, 1226.59 examples/s]Running tokenizer on dataset:  27%|       | 107000/392702 [01:27<03:55, 1213.93 examples/s]Running tokenizer on dataset:  28%|       | 108000/392702 [01:28<03:52, 1225.15 examples/s]Running tokenizer on dataset:  28%|       | 109000/392702 [01:29<03:51, 1227.58 examples/s]Running tokenizer on dataset:  28%|       | 110000/392702 [01:29<03:52, 1217.06 examples/s]Running tokenizer on dataset:  28%|       | 111000/392702 [01:30<03:49, 1224.82 examples/s]Running tokenizer on dataset:  29%|       | 112000/392702 [01:31<03:48, 1229.22 examples/s]Running tokenizer on dataset:  29%|       | 113000/392702 [01:32<03:48, 1224.08 examples/s]Running tokenizer on dataset:  29%|       | 114000/392702 [01:33<03:46, 1229.69 examples/s]Running tokenizer on dataset:  29%|       | 115000/392702 [01:33<03:45, 1229.29 examples/s]Running tokenizer on dataset:  30%|       | 116000/392702 [01:34<03:45, 1225.25 examples/s]Running tokenizer on dataset:  30%|       | 117000/392702 [01:35<03:43, 1231.81 examples/s]Running tokenizer on dataset:  30%|       | 118000/392702 [01:36<03:55, 1164.79 examples/s]Running tokenizer on dataset:  30%|       | 119000/392702 [01:37<03:52, 1179.16 examples/s]Running tokenizer on dataset:  31%|       | 120000/392702 [01:38<03:46, 1203.13 examples/s]Running tokenizer on dataset:  31%|       | 121000/392702 [01:38<03:45, 1206.07 examples/s]Running tokenizer on dataset:  31%|       | 122000/392702 [01:39<03:44, 1205.33 examples/s]Running tokenizer on dataset:  31%|      | 123000/392702 [01:40<03:41, 1215.46 examples/s]Running tokenizer on dataset:  32%|      | 124000/392702 [01:41<03:39, 1225.25 examples/s]Running tokenizer on dataset:  32%|      | 125000/392702 [01:42<03:39, 1217.69 examples/s]Running tokenizer on dataset:  32%|      | 126000/392702 [01:43<03:37, 1225.67 examples/s]Running tokenizer on dataset:  32%|      | 127000/392702 [01:43<03:37, 1222.45 examples/s]Running tokenizer on dataset:  33%|      | 128000/392702 [01:44<03:40, 1202.95 examples/s]Running tokenizer on dataset:  33%|      | 129000/392702 [01:45<03:36, 1215.73 examples/s]Running tokenizer on dataset:  33%|      | 130000/392702 [01:46<03:34, 1222.63 examples/s]Running tokenizer on dataset:  33%|      | 131000/392702 [01:47<03:35, 1214.27 examples/s]Running tokenizer on dataset:  34%|      | 132000/392702 [01:47<03:33, 1218.67 examples/s]Running tokenizer on dataset:  34%|      | 133000/392702 [01:48<03:32, 1221.63 examples/s]Running tokenizer on dataset:  34%|      | 134000/392702 [01:49<03:31, 1220.44 examples/s]Running tokenizer on dataset:  34%|      | 135000/392702 [01:50<03:29, 1229.97 examples/s]Running tokenizer on dataset:  35%|      | 136000/392702 [01:51<03:28, 1231.98 examples/s]Running tokenizer on dataset:  35%|      | 137000/392702 [01:52<03:29, 1223.29 examples/s]Running tokenizer on dataset:  35%|      | 138000/392702 [01:52<03:27, 1229.84 examples/s]Running tokenizer on dataset:  35%|      | 139000/392702 [01:53<03:25, 1231.77 examples/s]Running tokenizer on dataset:  36%|      | 140000/392702 [01:54<03:27, 1220.34 examples/s]Running tokenizer on dataset:  36%|      | 141000/392702 [01:55<03:24, 1228.10 examples/s]Running tokenizer on dataset:  36%|      | 142000/392702 [01:56<03:23, 1231.91 examples/s]Running tokenizer on dataset:  36%|      | 143000/392702 [01:56<03:23, 1225.01 examples/s]Running tokenizer on dataset:  37%|      | 144000/392702 [01:57<03:21, 1232.45 examples/s]Running tokenizer on dataset:  37%|      | 145000/392702 [01:58<03:20, 1232.83 examples/s]Running tokenizer on dataset:  37%|      | 146000/392702 [01:59<03:20, 1230.96 examples/s]Running tokenizer on dataset:  37%|      | 147000/392702 [02:00<03:19, 1230.09 examples/s]Running tokenizer on dataset:  38%|      | 148000/392702 [02:01<03:30, 1162.03 examples/s]Running tokenizer on dataset:  38%|      | 149000/392702 [02:01<03:27, 1174.98 examples/s]Running tokenizer on dataset:  38%|      | 150000/392702 [02:02<03:22, 1198.37 examples/s]Running tokenizer on dataset:  38%|      | 151000/392702 [02:03<03:19, 1209.00 examples/s]Running tokenizer on dataset:  39%|      | 152000/392702 [02:04<03:20, 1201.56 examples/s]Running tokenizer on dataset:  39%|      | 153000/392702 [02:05<03:19, 1202.60 examples/s]Running tokenizer on dataset:  39%|      | 154000/392702 [02:06<03:16, 1212.56 examples/s]Running tokenizer on dataset:  39%|      | 155000/392702 [02:06<03:16, 1209.16 examples/s]Running tokenizer on dataset:  40%|      | 156000/392702 [02:07<03:14, 1218.63 examples/s]Running tokenizer on dataset:  40%|      | 157000/392702 [02:08<03:13, 1216.35 examples/s]Running tokenizer on dataset:  40%|      | 158000/392702 [02:09<03:13, 1214.83 examples/s]Running tokenizer on dataset:  40%|      | 159000/392702 [02:10<03:10, 1225.97 examples/s]Running tokenizer on dataset:  41%|      | 160000/392702 [02:10<03:09, 1226.47 examples/s]Running tokenizer on dataset:  41%|      | 161000/392702 [02:11<03:10, 1217.85 examples/s]Running tokenizer on dataset:  41%|     | 162000/392702 [02:12<03:08, 1224.79 examples/s]Running tokenizer on dataset:  42%|     | 163000/392702 [02:13<03:06, 1230.56 examples/s]Running tokenizer on dataset:  42%|     | 164000/392702 [02:14<03:06, 1223.19 examples/s]Running tokenizer on dataset:  42%|     | 165000/392702 [02:15<03:05, 1226.51 examples/s]Running tokenizer on dataset:  42%|     | 166000/392702 [02:15<03:05, 1225.37 examples/s]Running tokenizer on dataset:  43%|     | 167000/392702 [02:16<03:05, 1219.44 examples/s]Running tokenizer on dataset:  43%|     | 168000/392702 [02:17<03:02, 1229.17 examples/s]Running tokenizer on dataset:  43%|     | 169000/392702 [02:18<03:01, 1229.68 examples/s]Running tokenizer on dataset:  43%|     | 170000/392702 [02:19<03:01, 1227.36 examples/s]Running tokenizer on dataset:  44%|     | 171000/392702 [02:19<03:00, 1226.58 examples/s]Running tokenizer on dataset:  44%|     | 172000/392702 [02:20<02:59, 1228.84 examples/s]Running tokenizer on dataset:  44%|     | 173000/392702 [02:21<02:59, 1225.09 examples/s]Running tokenizer on dataset:  44%|     | 174000/392702 [02:22<02:57, 1231.20 examples/s]Running tokenizer on dataset:  45%|     | 175000/392702 [02:23<02:56, 1230.80 examples/s]Running tokenizer on dataset:  45%|     | 176000/392702 [02:24<02:57, 1222.06 examples/s]Running tokenizer on dataset:  45%|     | 177000/392702 [02:24<02:55, 1231.69 examples/s]Running tokenizer on dataset:  45%|     | 178000/392702 [02:25<03:06, 1149.43 examples/s]Running tokenizer on dataset:  46%|     | 179000/392702 [02:26<03:03, 1162.69 examples/s]Running tokenizer on dataset:  46%|     | 180000/392702 [02:27<02:59, 1186.59 examples/s]Running tokenizer on dataset:  46%|     | 181000/392702 [02:28<02:56, 1198.87 examples/s]Running tokenizer on dataset:  46%|     | 182000/392702 [02:29<02:56, 1194.96 examples/s]Running tokenizer on dataset:  47%|     | 183000/392702 [02:29<02:53, 1207.36 examples/s]Running tokenizer on dataset:  47%|     | 184000/392702 [02:30<02:50, 1223.39 examples/s]Running tokenizer on dataset:  47%|     | 185000/392702 [02:31<02:51, 1211.64 examples/s]Running tokenizer on dataset:  47%|     | 186000/392702 [02:32<02:49, 1219.47 examples/s]Running tokenizer on dataset:  48%|     | 187000/392702 [02:33<02:48, 1223.36 examples/s]Running tokenizer on dataset:  48%|     | 188000/392702 [02:34<02:48, 1218.06 examples/s]Running tokenizer on dataset:  48%|     | 189000/392702 [02:34<02:46, 1224.99 examples/s]Running tokenizer on dataset:  48%|     | 190000/392702 [02:35<02:45, 1227.08 examples/s]Running tokenizer on dataset:  49%|     | 191000/392702 [02:36<02:45, 1222.38 examples/s]Running tokenizer on dataset:  49%|     | 192000/392702 [02:37<02:42, 1231.36 examples/s]Running tokenizer on dataset:  49%|     | 193000/392702 [02:38<02:41, 1235.03 examples/s]Running tokenizer on dataset:  49%|     | 194000/392702 [02:38<02:42, 1223.03 examples/s]Running tokenizer on dataset:  50%|     | 195000/392702 [02:39<02:40, 1230.46 examples/s]Running tokenizer on dataset:  50%|     | 196000/392702 [02:40<02:40, 1228.29 examples/s]Running tokenizer on dataset:  50%|     | 197000/392702 [02:41<02:40, 1222.97 examples/s]Running tokenizer on dataset:  50%|     | 198000/392702 [02:42<02:38, 1228.28 examples/s]Running tokenizer on dataset:  51%|     | 199000/392702 [02:42<02:37, 1228.63 examples/s]Running tokenizer on dataset:  51%|     | 200000/392702 [02:43<02:38, 1218.69 examples/s]Running tokenizer on dataset:  51%|     | 201000/392702 [02:44<02:36, 1228.77 examples/s]Running tokenizer on dataset:  51%|    | 202000/392702 [02:45<02:35, 1229.92 examples/s]Running tokenizer on dataset:  52%|    | 203000/392702 [02:46<02:36, 1214.21 examples/s]Running tokenizer on dataset:  52%|    | 204000/392702 [02:47<02:35, 1214.83 examples/s]Running tokenizer on dataset:  52%|    | 205000/392702 [02:47<02:33, 1219.19 examples/s]Running tokenizer on dataset:  52%|    | 206000/392702 [02:48<02:33, 1213.80 examples/s]Running tokenizer on dataset:  53%|    | 207000/392702 [02:49<02:32, 1220.53 examples/s]Running tokenizer on dataset:  53%|    | 208000/392702 [02:50<02:40, 1150.35 examples/s]Running tokenizer on dataset:  53%|    | 209000/392702 [02:51<02:37, 1164.14 examples/s]Running tokenizer on dataset:  53%|    | 210000/392702 [02:52<02:33, 1190.07 examples/s]Running tokenizer on dataset:  54%|    | 211000/392702 [02:52<02:31, 1201.74 examples/s]Running tokenizer on dataset:  54%|    | 212000/392702 [02:53<02:29, 1205.57 examples/s]Running tokenizer on dataset:  54%|    | 213000/392702 [02:54<02:28, 1210.04 examples/s]Running tokenizer on dataset:  54%|    | 214000/392702 [02:55<02:26, 1217.15 examples/s]Running tokenizer on dataset:  55%|    | 215000/392702 [02:56<02:26, 1213.71 examples/s]Running tokenizer on dataset:  55%|    | 216000/392702 [02:57<02:24, 1222.21 examples/s]Running tokenizer on dataset:  55%|    | 217000/392702 [02:57<02:23, 1225.50 examples/s]Running tokenizer on dataset:  56%|    | 218000/392702 [02:58<02:23, 1218.65 examples/s]Running tokenizer on dataset:  56%|    | 219000/392702 [02:59<02:21, 1226.04 examples/s]Running tokenizer on dataset:  56%|    | 220000/392702 [03:00<02:20, 1229.60 examples/s]Running tokenizer on dataset:  56%|    | 221000/392702 [03:01<02:20, 1221.45 examples/s]Running tokenizer on dataset:  57%|    | 222000/392702 [03:01<02:19, 1226.63 examples/s]Running tokenizer on dataset:  57%|    | 223000/392702 [03:02<02:18, 1227.29 examples/s]Running tokenizer on dataset:  57%|    | 224000/392702 [03:03<02:18, 1221.63 examples/s]Running tokenizer on dataset:  57%|    | 225000/392702 [03:04<02:16, 1227.65 examples/s]Running tokenizer on dataset:  58%|    | 226000/392702 [03:05<02:15, 1228.45 examples/s]Running tokenizer on dataset:  58%|    | 227000/392702 [03:06<02:15, 1219.71 examples/s]Running tokenizer on dataset:  58%|    | 228000/392702 [03:06<02:14, 1222.65 examples/s]Running tokenizer on dataset:  58%|    | 229000/392702 [03:07<02:15, 1211.52 examples/s]Running tokenizer on dataset:  59%|    | 230000/392702 [03:08<02:14, 1206.57 examples/s]Running tokenizer on dataset:  59%|    | 231000/392702 [03:09<02:12, 1218.53 examples/s]Running tokenizer on dataset:  59%|    | 232000/392702 [03:10<02:11, 1220.96 examples/s]Running tokenizer on dataset:  59%|    | 233000/392702 [03:10<02:12, 1205.42 examples/s]Running tokenizer on dataset:  60%|    | 234000/392702 [03:11<02:10, 1216.89 examples/s]Running tokenizer on dataset:  60%|    | 235000/392702 [03:12<02:09, 1221.12 examples/s]Running tokenizer on dataset:  60%|    | 236000/392702 [03:13<02:08, 1215.13 examples/s]Running tokenizer on dataset:  60%|    | 237000/392702 [03:14<02:07, 1222.42 examples/s]Running tokenizer on dataset:  61%|    | 238000/392702 [03:15<02:13, 1158.83 examples/s]Running tokenizer on dataset:  61%|    | 239000/392702 [03:16<02:11, 1169.96 examples/s]Running tokenizer on dataset:  61%|    | 240000/392702 [03:16<02:08, 1190.23 examples/s]Running tokenizer on dataset:  61%|   | 241000/392702 [03:17<02:06, 1200.02 examples/s]Running tokenizer on dataset:  62%|   | 242000/392702 [03:18<02:05, 1200.01 examples/s]Running tokenizer on dataset:  62%|   | 243000/392702 [03:19<02:03, 1213.08 examples/s]Running tokenizer on dataset:  62%|   | 244000/392702 [03:20<02:01, 1221.48 examples/s]Running tokenizer on dataset:  62%|   | 245000/392702 [03:20<02:01, 1214.00 examples/s]Running tokenizer on dataset:  63%|   | 246000/392702 [03:21<02:00, 1220.96 examples/s]Running tokenizer on dataset:  63%|   | 247000/392702 [03:22<01:59, 1223.94 examples/s]Running tokenizer on dataset:  63%|   | 248000/392702 [03:23<01:58, 1219.45 examples/s]Running tokenizer on dataset:  63%|   | 249000/392702 [03:24<01:57, 1223.46 examples/s]Running tokenizer on dataset:  64%|   | 250000/392702 [03:25<01:56, 1225.03 examples/s]Running tokenizer on dataset:  64%|   | 251000/392702 [03:25<01:56, 1217.09 examples/s]Running tokenizer on dataset:  64%|   | 252000/392702 [03:26<01:54, 1224.17 examples/s]Running tokenizer on dataset:  64%|   | 253000/392702 [03:27<01:54, 1220.33 examples/s]Running tokenizer on dataset:  65%|   | 254000/392702 [03:28<01:55, 1201.79 examples/s]Running tokenizer on dataset:  65%|   | 255000/392702 [03:29<01:53, 1214.84 examples/s]Running tokenizer on dataset:  65%|   | 256000/392702 [03:29<01:52, 1219.93 examples/s]Running tokenizer on dataset:  65%|   | 257000/392702 [03:30<01:52, 1210.03 examples/s]Running tokenizer on dataset:  66%|   | 258000/392702 [03:31<01:51, 1212.16 examples/s]Running tokenizer on dataset:  66%|   | 259000/392702 [03:32<01:49, 1216.39 examples/s]Running tokenizer on dataset:  66%|   | 260000/392702 [03:33<01:49, 1210.58 examples/s]Running tokenizer on dataset:  66%|   | 261000/392702 [03:34<01:47, 1220.81 examples/s]Running tokenizer on dataset:  67%|   | 262000/392702 [03:34<01:47, 1218.91 examples/s]Running tokenizer on dataset:  67%|   | 263000/392702 [03:35<01:46, 1214.14 examples/s]Running tokenizer on dataset:  67%|   | 264000/392702 [03:36<01:45, 1222.94 examples/s]Running tokenizer on dataset:  67%|   | 265000/392702 [03:37<01:44, 1226.24 examples/s]Running tokenizer on dataset:  68%|   | 266000/392702 [03:38<01:44, 1214.63 examples/s]Running tokenizer on dataset:  68%|   | 267000/392702 [03:38<01:42, 1220.94 examples/s]Running tokenizer on dataset:  68%|   | 268000/392702 [03:39<01:47, 1156.02 examples/s]Running tokenizer on dataset:  68%|   | 269000/392702 [03:40<01:51, 1108.12 examples/s]Running tokenizer on dataset:  69%|   | 270000/392702 [03:41<01:47, 1145.80 examples/s]Running tokenizer on dataset:  69%|   | 271000/392702 [03:42<01:43, 1173.37 examples/s]Running tokenizer on dataset:  69%|   | 272000/392702 [03:43<01:42, 1181.29 examples/s]Running tokenizer on dataset:  70%|   | 273000/392702 [03:44<01:39, 1198.86 examples/s]Running tokenizer on dataset:  70%|   | 274000/392702 [03:45<01:38, 1209.35 examples/s]Running tokenizer on dataset:  70%|   | 275000/392702 [03:45<01:37, 1207.73 examples/s]Running tokenizer on dataset:  70%|   | 276000/392702 [03:46<01:35, 1217.67 examples/s]Running tokenizer on dataset:  71%|   | 277000/392702 [03:47<01:34, 1220.50 examples/s]Running tokenizer on dataset:  71%|   | 278000/392702 [03:48<01:34, 1212.47 examples/s]Running tokenizer on dataset:  71%|   | 279000/392702 [03:49<01:34, 1204.81 examples/s]Running tokenizer on dataset:  71%|  | 280000/392702 [03:49<01:32, 1212.17 examples/s]Running tokenizer on dataset:  72%|  | 281000/392702 [03:50<01:32, 1210.79 examples/s]Running tokenizer on dataset:  72%|  | 282000/392702 [03:51<01:30, 1220.53 examples/s]Running tokenizer on dataset:  72%|  | 283000/392702 [03:52<01:30, 1212.72 examples/s]Running tokenizer on dataset:  72%|  | 284000/392702 [03:53<01:29, 1208.76 examples/s]Running tokenizer on dataset:  73%|  | 285000/392702 [03:54<01:28, 1216.04 examples/s]Running tokenizer on dataset:  73%|  | 286000/392702 [03:54<01:27, 1223.77 examples/s]Running tokenizer on dataset:  73%|  | 287000/392702 [03:55<01:27, 1212.16 examples/s]Running tokenizer on dataset:  73%|  | 288000/392702 [03:56<01:25, 1222.88 examples/s]Running tokenizer on dataset:  74%|  | 289000/392702 [03:57<01:24, 1222.07 examples/s]Running tokenizer on dataset:  74%|  | 290000/392702 [03:58<01:24, 1217.80 examples/s]Running tokenizer on dataset:  74%|  | 291000/392702 [03:58<01:22, 1225.41 examples/s]Running tokenizer on dataset:  74%|  | 292000/392702 [03:59<01:22, 1221.47 examples/s]Running tokenizer on dataset:  75%|  | 293000/392702 [04:00<01:21, 1217.19 examples/s]Running tokenizer on dataset:  75%|  | 294000/392702 [04:01<01:20, 1224.20 examples/s]Running tokenizer on dataset:  75%|  | 295000/392702 [04:02<01:19, 1223.34 examples/s]Running tokenizer on dataset:  75%|  | 296000/392702 [04:03<01:19, 1217.16 examples/s]Running tokenizer on dataset:  76%|  | 297000/392702 [04:03<01:18, 1225.66 examples/s]Running tokenizer on dataset:  76%|  | 298000/392702 [04:04<01:21, 1156.38 examples/s]Running tokenizer on dataset:  76%|  | 299000/392702 [04:05<01:20, 1167.11 examples/s]Running tokenizer on dataset:  76%|  | 300000/392702 [04:06<01:18, 1186.77 examples/s]Running tokenizer on dataset:  77%|  | 301000/392702 [04:07<01:16, 1197.88 examples/s]Running tokenizer on dataset:  77%|  | 302000/392702 [04:08<01:15, 1196.65 examples/s]Running tokenizer on dataset:  77%|  | 303000/392702 [04:08<01:14, 1207.31 examples/s]Running tokenizer on dataset:  77%|  | 304000/392702 [04:09<01:13, 1200.85 examples/s]Running tokenizer on dataset:  78%|  | 305000/392702 [04:10<01:13, 1199.85 examples/s]Running tokenizer on dataset:  78%|  | 306000/392702 [04:11<01:11, 1212.48 examples/s]Running tokenizer on dataset:  78%|  | 307000/392702 [04:12<01:10, 1212.97 examples/s]Running tokenizer on dataset:  78%|  | 308000/392702 [04:13<01:10, 1199.39 examples/s]Running tokenizer on dataset:  79%|  | 309000/392702 [04:13<01:09, 1211.40 examples/s]Running tokenizer on dataset:  79%|  | 310000/392702 [04:14<01:08, 1215.77 examples/s]Running tokenizer on dataset:  79%|  | 311000/392702 [04:15<01:07, 1209.81 examples/s]Running tokenizer on dataset:  79%|  | 312000/392702 [04:16<01:06, 1214.95 examples/s]Running tokenizer on dataset:  80%|  | 313000/392702 [04:17<01:05, 1216.46 examples/s]Running tokenizer on dataset:  80%|  | 314000/392702 [04:18<01:05, 1209.83 examples/s]Running tokenizer on dataset:  80%|  | 315000/392702 [04:18<01:03, 1218.56 examples/s]Running tokenizer on dataset:  80%|  | 316000/392702 [04:19<01:03, 1216.98 examples/s]Running tokenizer on dataset:  81%|  | 317000/392702 [04:20<01:02, 1211.56 examples/s]Running tokenizer on dataset:  81%|  | 318000/392702 [04:21<01:01, 1217.81 examples/s]Running tokenizer on dataset:  81%|  | 319000/392702 [04:22<01:00, 1218.86 examples/s]Running tokenizer on dataset:  81%| | 320000/392702 [04:23<01:00, 1211.46 examples/s]Running tokenizer on dataset:  82%| | 321000/392702 [04:23<00:58, 1220.05 examples/s]Running tokenizer on dataset:  82%| | 322000/392702 [04:24<00:57, 1222.03 examples/s]Running tokenizer on dataset:  82%| | 323000/392702 [04:25<00:57, 1214.20 examples/s]Running tokenizer on dataset:  83%| | 324000/392702 [04:26<00:56, 1221.29 examples/s]Running tokenizer on dataset:  83%| | 325000/392702 [04:27<00:55, 1220.75 examples/s]Running tokenizer on dataset:  83%| | 326000/392702 [04:27<00:54, 1212.96 examples/s]Running tokenizer on dataset:  83%| | 327000/392702 [04:28<00:53, 1222.56 examples/s]Running tokenizer on dataset:  84%| | 328000/392702 [04:29<00:56, 1152.37 examples/s]Running tokenizer on dataset:  84%| | 329000/392702 [04:30<00:55, 1154.08 examples/s]Running tokenizer on dataset:  84%| | 330000/392702 [04:31<00:53, 1181.16 examples/s]Running tokenizer on dataset:  84%| | 331000/392702 [04:32<00:51, 1194.64 examples/s]Running tokenizer on dataset:  85%| | 332000/392702 [04:33<00:50, 1196.94 examples/s]Running tokenizer on dataset:  85%| | 333000/392702 [04:33<00:49, 1201.28 examples/s]Running tokenizer on dataset:  85%| | 334000/392702 [04:34<00:48, 1206.82 examples/s]Running tokenizer on dataset:  85%| | 335000/392702 [04:35<00:47, 1206.07 examples/s]Running tokenizer on dataset:  86%| | 336000/392702 [04:36<00:46, 1216.26 examples/s]Running tokenizer on dataset:  86%| | 337000/392702 [04:37<00:45, 1217.05 examples/s]Running tokenizer on dataset:  86%| | 338000/392702 [04:37<00:45, 1210.46 examples/s]Running tokenizer on dataset:  86%| | 339000/392702 [04:38<00:44, 1217.62 examples/s]Running tokenizer on dataset:  87%| | 340000/392702 [04:39<00:43, 1218.43 examples/s]Running tokenizer on dataset:  87%| | 341000/392702 [04:40<00:42, 1208.13 examples/s]Running tokenizer on dataset:  87%| | 342000/392702 [04:41<00:41, 1220.68 examples/s]Running tokenizer on dataset:  87%| | 343000/392702 [04:42<00:40, 1222.82 examples/s]Running tokenizer on dataset:  88%| | 344000/392702 [04:42<00:40, 1216.01 examples/s]Running tokenizer on dataset:  88%| | 345000/392702 [04:43<00:39, 1221.94 examples/s]Running tokenizer on dataset:  88%| | 346000/392702 [04:44<00:38, 1224.98 examples/s]Running tokenizer on dataset:  88%| | 347000/392702 [04:45<00:37, 1217.85 examples/s]Running tokenizer on dataset:  89%| | 348000/392702 [04:46<00:36, 1225.65 examples/s]Running tokenizer on dataset:  89%| | 349000/392702 [04:46<00:35, 1222.80 examples/s]Running tokenizer on dataset:  89%| | 350000/392702 [04:47<00:35, 1214.36 examples/s]Running tokenizer on dataset:  89%| | 351000/392702 [04:48<00:34, 1222.29 examples/s]Running tokenizer on dataset:  90%| | 352000/392702 [04:49<00:33, 1226.24 examples/s]Running tokenizer on dataset:  90%| | 353000/392702 [04:50<00:33, 1202.13 examples/s]Running tokenizer on dataset:  90%| | 354000/392702 [04:51<00:31, 1211.02 examples/s]Running tokenizer on dataset:  90%| | 355000/392702 [04:51<00:31, 1212.49 examples/s]Running tokenizer on dataset:  91%| | 356000/392702 [04:52<00:30, 1205.48 examples/s]Running tokenizer on dataset:  91%| | 357000/392702 [04:53<00:29, 1212.19 examples/s]Running tokenizer on dataset:  91%| | 358000/392702 [04:54<00:30, 1142.69 examples/s]Running tokenizer on dataset:  91%|| 359000/392702 [04:55<00:29, 1160.77 examples/s]Running tokenizer on dataset:  92%|| 360000/392702 [04:56<00:27, 1183.89 examples/s]Running tokenizer on dataset:  92%|| 361000/392702 [04:57<00:26, 1197.82 examples/s]Running tokenizer on dataset:  92%|| 362000/392702 [04:57<00:25, 1196.96 examples/s]Running tokenizer on dataset:  92%|| 363000/392702 [04:58<00:24, 1212.19 examples/s]Running tokenizer on dataset:  93%|| 364000/392702 [04:59<00:23, 1217.80 examples/s]Running tokenizer on dataset:  93%|| 365000/392702 [05:00<00:22, 1213.45 examples/s]Running tokenizer on dataset:  93%|| 366000/392702 [05:01<00:21, 1223.83 examples/s]Running tokenizer on dataset:  93%|| 367000/392702 [05:01<00:20, 1227.81 examples/s]Running tokenizer on dataset:  94%|| 368000/392702 [05:02<00:20, 1219.33 examples/s]Running tokenizer on dataset:  94%|| 369000/392702 [05:03<00:19, 1225.47 examples/s]Running tokenizer on dataset:  94%|| 370000/392702 [05:04<00:18, 1226.27 examples/s]Running tokenizer on dataset:  94%|| 371000/392702 [05:05<00:17, 1218.44 examples/s]Running tokenizer on dataset:  95%|| 372000/392702 [05:05<00:16, 1226.45 examples/s]Running tokenizer on dataset:  95%|| 373000/392702 [05:06<00:16, 1228.07 examples/s]Running tokenizer on dataset:  95%|| 374000/392702 [05:07<00:15, 1218.54 examples/s]Running tokenizer on dataset:  95%|| 375000/392702 [05:08<00:14, 1211.00 examples/s]Running tokenizer on dataset:  96%|| 376000/392702 [05:09<00:13, 1214.22 examples/s]Running tokenizer on dataset:  96%|| 377000/392702 [05:10<00:12, 1212.68 examples/s]Running tokenizer on dataset:  96%|| 378000/392702 [05:10<00:12, 1220.38 examples/s]Running tokenizer on dataset:  97%|| 379000/392702 [05:11<00:11, 1217.64 examples/s]Running tokenizer on dataset:  97%|| 380000/392702 [05:12<00:10, 1212.75 examples/s]Running tokenizer on dataset:  97%|| 381000/392702 [05:13<00:09, 1221.46 examples/s]Running tokenizer on dataset:  97%|| 382000/392702 [05:14<00:08, 1226.86 examples/s]Running tokenizer on dataset:  98%|| 383000/392702 [05:15<00:07, 1216.61 examples/s]Running tokenizer on dataset:  98%|| 384000/392702 [05:15<00:07, 1225.64 examples/s]Running tokenizer on dataset:  98%|| 385000/392702 [05:16<00:06, 1227.79 examples/s]Running tokenizer on dataset:  98%|| 386000/392702 [05:17<00:05, 1218.44 examples/s]Running tokenizer on dataset:  99%|| 387000/392702 [05:18<00:04, 1223.63 examples/s]Running tokenizer on dataset:  99%|| 388000/392702 [05:19<00:04, 1155.45 examples/s]Running tokenizer on dataset:  99%|| 389000/392702 [05:20<00:03, 1167.88 examples/s]Running tokenizer on dataset:  99%|| 390000/392702 [05:20<00:02, 1189.44 examples/s]Running tokenizer on dataset: 100%|| 391000/392702 [05:21<00:01, 1200.96 examples/s]Running tokenizer on dataset: 100%|| 392000/392702 [05:22<00:00, 1200.97 examples/s]Running tokenizer on dataset: 100%|| 392702/392702 [05:23<00:00, 1207.18 examples/s]Running tokenizer on dataset: 100%|| 392702/392702 [05:23<00:00, 1214.50 examples/s]
Running tokenizer on dataset:   0%|          | 0/9815 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c39f160d79c39f1.arrow
03/13/2024 13:15:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c39f160d79c39f1.arrow
Running tokenizer on dataset:  10%|         | 1000/9815 [00:00<00:07, 1186.15 examples/s]Running tokenizer on dataset:  20%|        | 2000/9815 [00:01<00:06, 1205.65 examples/s]Running tokenizer on dataset:  31%|       | 3000/9815 [00:02<00:05, 1196.16 examples/s]Running tokenizer on dataset:  41%|      | 4000/9815 [00:03<00:04, 1195.23 examples/s]Running tokenizer on dataset:  51%|     | 5000/9815 [00:04<00:04, 1203.57 examples/s]Running tokenizer on dataset:  61%|    | 6000/9815 [00:04<00:03, 1208.62 examples/s]Running tokenizer on dataset:  71%|  | 7000/9815 [00:05<00:02, 1193.25 examples/s]Running tokenizer on dataset:  82%| | 8000/9815 [00:06<00:01, 1208.88 examples/s]Running tokenizer on dataset:  92%|| 9000/9815 [00:07<00:00, 1220.37 examples/s]Running tokenizer on dataset: 100%|| 9815/9815 [00:08<00:00, 1211.68 examples/s]Running tokenizer on dataset: 100%|| 9815/9815 [00:08<00:00, 1201.87 examples/s]
Running tokenizer on dataset:   0%|          | 0/9832 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-960a82f5ba85f3b3.arrow
03/13/2024 13:16:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-960a82f5ba85f3b3.arrow
Running tokenizer on dataset:  10%|         | 1000/9832 [00:00<00:07, 1219.13 examples/s]Running tokenizer on dataset:  20%|        | 2000/9832 [00:01<00:06, 1182.79 examples/s]Running tokenizer on dataset:  31%|       | 3000/9832 [00:02<00:05, 1183.36 examples/s]Running tokenizer on dataset:  41%|      | 4000/9832 [00:03<00:04, 1192.57 examples/s]Running tokenizer on dataset:  51%|     | 5000/9832 [00:04<00:04, 1178.10 examples/s]Running tokenizer on dataset:  61%|    | 6000/9832 [00:05<00:03, 1183.62 examples/s]Running tokenizer on dataset:  71%|   | 7000/9832 [00:05<00:02, 1198.30 examples/s]Running tokenizer on dataset:  81%| | 8000/9832 [00:06<00:01, 1206.72 examples/s]Running tokenizer on dataset:  92%|| 9000/9832 [00:07<00:00, 1200.99 examples/s]Running tokenizer on dataset: 100%|| 9832/9832 [00:08<00:00, 1202.18 examples/s]Running tokenizer on dataset: 100%|| 9832/9832 [00:08<00:00, 1191.50 examples/s]
Running tokenizer on dataset:   0%|          | 0/9796 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f245bb6568efb52f.arrow
03/13/2024 13:16:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f245bb6568efb52f.arrow
Running tokenizer on dataset:  10%|         | 1000/9796 [00:00<00:07, 1219.15 examples/s]Running tokenizer on dataset:  20%|        | 2000/9796 [00:01<00:06, 1182.63 examples/s]Running tokenizer on dataset:  31%|       | 3000/9796 [00:02<00:05, 1167.87 examples/s]Running tokenizer on dataset:  41%|      | 4000/9796 [00:03<00:04, 1179.56 examples/s]Running tokenizer on dataset:  51%|     | 5000/9796 [00:04<00:04, 1177.40 examples/s]Running tokenizer on dataset:  61%|    | 6000/9796 [00:05<00:03, 1192.52 examples/s]Running tokenizer on dataset:  71%|  | 7000/9796 [00:06<00:02, 1128.96 examples/s]Running tokenizer on dataset:  82%| | 8000/9796 [00:06<00:01, 1151.44 examples/s]Running tokenizer on dataset:  92%|| 9000/9796 [00:07<00:00, 1175.31 examples/s]Running tokenizer on dataset: 100%|| 9796/9796 [00:08<00:00, 1180.69 examples/s]Running tokenizer on dataset: 100%|| 9796/9796 [00:08<00:00, 1169.32 examples/s]
Running tokenizer on dataset:   0%|          | 0/9847 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8cc6bd44a4318dfe.arrow
03/13/2024 13:16:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8cc6bd44a4318dfe.arrow
Running tokenizer on dataset:  10%|         | 1000/9847 [00:00<00:07, 1224.51 examples/s]Running tokenizer on dataset:  20%|        | 2000/9847 [00:01<00:06, 1176.84 examples/s]Running tokenizer on dataset:  30%|       | 3000/9847 [00:02<00:06, 1138.80 examples/s]Running tokenizer on dataset:  41%|      | 4000/9847 [00:03<00:05, 1160.66 examples/s]Running tokenizer on dataset:  51%|     | 5000/9847 [00:04<00:04, 1126.98 examples/s]Running tokenizer on dataset:  61%|    | 6000/9847 [00:05<00:03, 1150.43 examples/s]Running tokenizer on dataset:  71%|   | 7000/9847 [00:06<00:02, 1167.94 examples/s]Running tokenizer on dataset:  81%|  | 8000/9847 [00:06<00:01, 1185.99 examples/s]Running tokenizer on dataset:  91%|| 9000/9847 [00:07<00:00, 1189.60 examples/s]Running tokenizer on dataset: 100%|| 9847/9847 [00:08<00:00, 1194.87 examples/s]Running tokenizer on dataset: 100%|| 9847/9847 [00:08<00:00, 1170.14 examples/s]
03/13/2024 13:16:32 - INFO - __main__ - Sample 81 of the training set: {'premise': 'The man should have died instantly.', 'hypothesis': 'The man was perfectly fine. ', 'label': 2, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 767, 881, 505, 6423, 26232, 29889, 1, 450, 767, 471, 7970, 2691, 29889, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
03/13/2024 13:16:32 - INFO - __main__ - Sample 14 of the training set: {'premise': "I don't mean to be glib about your concerns, but if I were you, I might be more concerned about the near-term rate implications of this $1.", 'hypothesis': 'I am concerned more about your issues than the near-term rate implications.', 'label': 2, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 1016, 29915, 29873, 2099, 304, 367, 330, 1982, 1048, 596, 21838, 29892, 541, 565, 306, 892, 366, 29892, 306, 1795, 367, 901, 15041, 1048, 278, 2978, 29899, 8489, 6554, 2411, 5795, 310, 445, 395, 29896, 29889, 1, 306, 626, 15041, 901, 1048, 596, 5626, 1135, 278, 2978, 29899, 8489, 6554, 2411, 5795, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
03/13/2024 13:16:32 - INFO - __main__ - Sample 3 of the training set: {'premise': 'How do you know? All this is their information again.', 'hypothesis': 'This information belongs to them.', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1128, 437, 366, 1073, 29973, 2178, 445, 338, 1009, 2472, 1449, 29889, 1, 910, 2472, 14393, 304, 963, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
03/13/2024 13:16:33 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:761] 2024-03-13 13:16:35,163 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1826] 2024-03-13 13:16:35,595 >> ***** Running training *****
[INFO|trainer.py:1827] 2024-03-13 13:16:35,596 >>   Num examples = 100
[INFO|trainer.py:1828] 2024-03-13 13:16:35,596 >>   Num Epochs = 3
[INFO|trainer.py:1829] 2024-03-13 13:16:35,596 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1832] 2024-03-13 13:16:35,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 1
[INFO|trainer.py:1833] 2024-03-13 13:16:35,596 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1834] 2024-03-13 13:16:35,596 >>   Total optimization steps = 300
[INFO|trainer.py:1835] 2024-03-13 13:16:35,598 >>   Number of trainable parameters = 1,280,155,648
  0%|          | 0/300 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/300 [00:02<14:25,  2.89s/it]  1%|          | 2/300 [00:04<10:49,  2.18s/it]  1%|          | 3/300 [00:06<09:50,  1.99s/it]  1%|         | 4/300 [00:08<09:22,  1.90s/it]  2%|         | 5/300 [00:09<09:06,  1.85s/it]  2%|         | 6/300 [00:11<08:55,  1.82s/it]  2%|         | 7/300 [00:13<08:49,  1.81s/it]  3%|         | 8/300 [00:15<08:43,  1.79s/it]  3%|         | 9/300 [00:16<08:39,  1.79s/it]  3%|         | 10/300 [00:18<08:36,  1.78s/it]  4%|         | 11/300 [00:20<08:33,  1.78s/it]  4%|         | 12/300 [00:22<08:31,  1.78s/it]  4%|         | 13/300 [00:24<08:28,  1.77s/it]  5%|         | 14/300 [00:25<08:26,  1.77s/it]  5%|         | 15/300 [00:27<08:24,  1.77s/it]  5%|         | 16/300 [00:29<08:22,  1.77s/it]  6%|         | 17/300 [00:31<08:20,  1.77s/it]  6%|         | 18/300 [00:32<08:18,  1.77s/it]  6%|         | 19/300 [00:34<08:17,  1.77s/it]  7%|         | 20/300 [00:36<08:15,  1.77s/it]  7%|         | 21/300 [00:38<08:13,  1.77s/it]  7%|         | 22/300 [00:39<08:12,  1.77s/it]  8%|         | 23/300 [00:41<08:10,  1.77s/it]  8%|         | 24/300 [00:43<08:08,  1.77s/it]  8%|         | 25/300 [00:45<08:06,  1.77s/it]  9%|         | 26/300 [00:47<08:04,  1.77s/it]  9%|         | 27/300 [00:48<08:03,  1.77s/it]  9%|         | 28/300 [00:50<08:01,  1.77s/it] 10%|         | 29/300 [00:52<07:59,  1.77s/it] 10%|         | 30/300 [00:54<07:57,  1.77s/it] 10%|         | 31/300 [00:55<07:56,  1.77s/it] 11%|         | 32/300 [00:57<07:54,  1.77s/it] 11%|         | 33/300 [00:59<07:52,  1.77s/it] 11%|        | 34/300 [01:01<07:50,  1.77s/it] 12%|        | 35/300 [01:02<07:49,  1.77s/it] 12%|        | 36/300 [01:04<07:46,  1.77s/it] 12%|        | 37/300 [01:06<07:45,  1.77s/it] 13%|        | 38/300 [01:08<07:43,  1.77s/it] 13%|        | 39/300 [01:10<07:42,  1.77s/it] 13%|        | 40/300 [01:11<07:40,  1.77s/it] 14%|        | 41/300 [01:13<07:38,  1.77s/it] 14%|        | 42/300 [01:15<07:36,  1.77s/it] 14%|        | 43/300 [01:17<07:34,  1.77s/it] 15%|        | 44/300 [01:18<07:32,  1.77s/it] 15%|        | 45/300 [01:20<07:31,  1.77s/it] 15%|        | 46/300 [01:22<07:29,  1.77s/it] 16%|        | 47/300 [01:24<07:27,  1.77s/it] 16%|        | 48/300 [01:25<07:25,  1.77s/it] 16%|        | 49/300 [01:27<07:23,  1.77s/it] 17%|        | 50/300 [01:29<07:22,  1.77s/it] 17%|        | 51/300 [01:31<07:20,  1.77s/it] 17%|        | 52/300 [01:33<07:18,  1.77s/it] 18%|        | 53/300 [01:34<07:17,  1.77s/it] 18%|        | 54/300 [01:36<07:15,  1.77s/it] 18%|        | 55/300 [01:38<07:13,  1.77s/it] 19%|        | 56/300 [01:40<07:11,  1.77s/it] 19%|        | 57/300 [01:41<07:10,  1.77s/it] 19%|        | 58/300 [01:43<07:07,  1.77s/it] 20%|        | 59/300 [01:45<07:06,  1.77s/it] 20%|        | 60/300 [01:47<07:04,  1.77s/it] 20%|        | 61/300 [01:48<07:02,  1.77s/it] 21%|        | 62/300 [01:50<07:01,  1.77s/it] 21%|        | 63/300 [01:52<06:59,  1.77s/it] 21%|       | 64/300 [01:54<06:57,  1.77s/it] 22%|       | 65/300 [01:56<06:56,  1.77s/it] 22%|       | 66/300 [01:57<06:54,  1.77s/it] 22%|       | 67/300 [01:59<06:52,  1.77s/it] 23%|       | 68/300 [02:01<06:50,  1.77s/it] 23%|       | 69/300 [02:03<06:49,  1.77s/it] 23%|       | 70/300 [02:04<06:47,  1.77s/it] 24%|       | 71/300 [02:06<06:45,  1.77s/it] 24%|       | 72/300 [02:08<06:43,  1.77s/it] 24%|       | 73/300 [02:10<06:42,  1.77s/it] 25%|       | 74/300 [02:11<06:40,  1.77s/it] 25%|       | 75/300 [02:13<06:38,  1.77s/it] 25%|       | 76/300 [02:15<06:36,  1.77s/it] 26%|       | 77/300 [02:17<06:35,  1.77s/it] 26%|       | 78/300 [02:19<06:33,  1.77s/it] 26%|       | 79/300 [02:20<06:31,  1.77s/it] 27%|       | 80/300 [02:22<06:30,  1.77s/it] 27%|       | 81/300 [02:24<06:28,  1.77s/it] 27%|       | 82/300 [02:26<06:26,  1.77s/it] 28%|       | 83/300 [02:27<06:24,  1.77s/it] 28%|       | 84/300 [02:29<06:23,  1.77s/it] 28%|       | 85/300 [02:31<06:21,  1.77s/it] 29%|       | 86/300 [02:33<06:19,  1.77s/it] 29%|       | 87/300 [02:35<06:17,  1.77s/it] 29%|       | 88/300 [02:36<06:16,  1.77s/it] 30%|       | 89/300 [02:38<06:14,  1.77s/it] 30%|       | 90/300 [02:40<06:12,  1.77s/it] 30%|       | 91/300 [02:42<06:10,  1.77s/it] 31%|       | 92/300 [02:43<06:09,  1.77s/it] 31%|       | 93/300 [02:45<06:07,  1.77s/it] 31%|      | 94/300 [02:47<06:05,  1.77s/it] 32%|      | 95/300 [02:49<06:03,  1.77s/it] 32%|      | 96/300 [02:50<06:01,  1.77s/it] 32%|      | 97/300 [02:52<06:00,  1.77s/it] 33%|      | 98/300 [02:54<05:58,  1.77s/it] 33%|      | 99/300 [02:56<05:56,  1.77s/it] 33%|      | 100/300 [02:58<05:54,  1.77s/it] 34%|      | 101/300 [02:59<05:52,  1.77s/it] 34%|      | 102/300 [03:01<05:51,  1.77s/it] 34%|      | 103/300 [03:03<05:49,  1.77s/it] 35%|      | 104/300 [03:05<05:47,  1.77s/it] 35%|      | 105/300 [03:06<05:45,  1.77s/it] 35%|      | 106/300 [03:08<05:43,  1.77s/it] 36%|      | 107/300 [03:10<05:42,  1.77s/it] 36%|      | 108/300 [03:12<05:40,  1.77s/it] 36%|      | 109/300 [03:14<05:38,  1.77s/it] 37%|      | 110/300 [03:15<05:37,  1.77s/it] 37%|      | 111/300 [03:17<05:35,  1.77s/it] 37%|      | 112/300 [03:19<05:33,  1.77s/it] 38%|      | 113/300 [03:21<05:31,  1.77s/it] 38%|      | 114/300 [03:22<05:29,  1.77s/it] 38%|      | 115/300 [03:24<05:28,  1.77s/it] 39%|      | 116/300 [03:26<05:26,  1.77s/it] 39%|      | 117/300 [03:28<05:24,  1.77s/it] 39%|      | 118/300 [03:30<05:22,  1.77s/it] 40%|      | 119/300 [03:31<05:20,  1.77s/it] 40%|      | 120/300 [03:33<05:19,  1.77s/it] 40%|      | 121/300 [03:35<05:17,  1.77s/it] 41%|      | 122/300 [03:37<05:15,  1.77s/it] 41%|      | 123/300 [03:38<05:14,  1.77s/it] 41%|     | 124/300 [03:40<05:12,  1.77s/it] 42%|     | 125/300 [03:42<05:10,  1.77s/it] 42%|     | 126/300 [03:44<05:08,  1.77s/it] 42%|     | 127/300 [03:45<05:06,  1.77s/it] 43%|     | 128/300 [03:47<05:05,  1.77s/it] 43%|     | 129/300 [03:49<05:03,  1.77s/it] 43%|     | 130/300 [03:51<05:01,  1.77s/it] 44%|     | 131/300 [03:53<04:59,  1.77s/it] 44%|     | 132/300 [03:54<04:57,  1.77s/it] 44%|     | 133/300 [03:56<04:56,  1.77s/it] 45%|     | 134/300 [03:58<04:54,  1.77s/it] 45%|     | 135/300 [04:00<04:52,  1.77s/it] 45%|     | 136/300 [04:01<04:50,  1.77s/it] 46%|     | 137/300 [04:03<04:48,  1.77s/it] 46%|     | 138/300 [04:05<04:46,  1.77s/it] 46%|     | 139/300 [04:07<04:45,  1.77s/it] 47%|     | 140/300 [04:09<04:43,  1.77s/it] 47%|     | 141/300 [04:10<04:41,  1.77s/it] 47%|     | 142/300 [04:12<04:40,  1.77s/it] 48%|     | 143/300 [04:14<04:38,  1.77s/it] 48%|     | 144/300 [04:16<04:36,  1.77s/it] 48%|     | 145/300 [04:17<04:34,  1.77s/it] 49%|     | 146/300 [04:19<04:32,  1.77s/it] 49%|     | 147/300 [04:21<04:31,  1.77s/it] 49%|     | 148/300 [04:23<04:29,  1.77s/it] 50%|     | 149/300 [04:24<04:27,  1.77s/it] 50%|     | 150/300 [04:26<04:25,  1.77s/it] 50%|     | 151/300 [04:28<04:24,  1.77s/it] 51%|     | 152/300 [04:30<04:22,  1.77s/it] 51%|     | 153/300 [04:32<04:20,  1.77s/it] 51%|    | 154/300 [04:33<04:18,  1.77s/it] 52%|    | 155/300 [04:35<04:16,  1.77s/it] 52%|    | 156/300 [04:37<04:15,  1.77s/it] 52%|    | 157/300 [04:39<04:13,  1.77s/it] 53%|    | 158/300 [04:40<04:11,  1.77s/it] 53%|    | 159/300 [04:42<04:09,  1.77s/it] 53%|    | 160/300 [04:44<04:08,  1.77s/it] 54%|    | 161/300 [04:46<04:06,  1.77s/it] 54%|    | 162/300 [04:48<04:04,  1.77s/it] 54%|    | 163/300 [04:49<04:02,  1.77s/it] 55%|    | 164/300 [04:51<04:01,  1.77s/it] 55%|    | 165/300 [04:53<03:59,  1.77s/it] 55%|    | 166/300 [04:55<03:57,  1.77s/it] 56%|    | 167/300 [04:56<03:55,  1.77s/it] 56%|    | 168/300 [04:58<03:54,  1.77s/it] 56%|    | 169/300 [05:00<03:52,  1.77s/it] 57%|    | 170/300 [05:02<03:50,  1.77s/it] 57%|    | 171/300 [05:03<03:48,  1.77s/it] 57%|    | 172/300 [05:05<03:46,  1.77s/it] 58%|    | 173/300 [05:07<03:45,  1.77s/it] 58%|    | 174/300 [05:09<03:43,  1.77s/it] 58%|    | 175/300 [05:11<03:41,  1.77s/it] 59%|    | 176/300 [05:12<03:39,  1.77s/it] 59%|    | 177/300 [05:14<03:38,  1.77s/it] 59%|    | 178/300 [05:16<03:36,  1.77s/it] 60%|    | 179/300 [05:18<03:34,  1.77s/it] 60%|    | 180/300 [05:19<03:32,  1.77s/it] 60%|    | 181/300 [05:21<03:31,  1.77s/it] 61%|    | 182/300 [05:23<03:29,  1.77s/it] 61%|    | 183/300 [05:25<03:27,  1.78s/it] 61%|   | 184/300 [05:27<03:25,  1.78s/it] 62%|   | 185/300 [05:28<03:24,  1.78s/it] 62%|   | 186/300 [05:30<03:22,  1.77s/it] 62%|   | 187/300 [05:32<03:20,  1.77s/it] 63%|   | 188/300 [05:34<03:18,  1.78s/it] 63%|   | 189/300 [05:35<03:16,  1.77s/it] 63%|   | 190/300 [05:37<03:15,  1.78s/it] 64%|   | 191/300 [05:39<03:13,  1.77s/it] 64%|   | 192/300 [05:41<03:11,  1.77s/it] 64%|   | 193/300 [05:43<03:09,  1.77s/it] 65%|   | 194/300 [05:44<03:08,  1.77s/it] 65%|   | 195/300 [05:46<03:06,  1.77s/it] 65%|   | 196/300 [05:48<03:04,  1.77s/it] 66%|   | 197/300 [05:50<03:02,  1.78s/it] 66%|   | 198/300 [05:51<03:00,  1.77s/it] 66%|   | 199/300 [05:53<02:59,  1.77s/it] 67%|   | 200/300 [05:55<02:57,  1.77s/it] 67%|   | 201/300 [05:57<02:55,  1.77s/it] 67%|   | 202/300 [05:58<02:53,  1.77s/it] 68%|   | 203/300 [06:00<02:52,  1.77s/it] 68%|   | 204/300 [06:02<02:50,  1.77s/it] 68%|   | 205/300 [06:04<02:48,  1.77s/it] 69%|   | 206/300 [06:06<02:46,  1.77s/it] 69%|   | 207/300 [06:07<02:44,  1.77s/it] 69%|   | 208/300 [06:09<02:43,  1.77s/it] 70%|   | 209/300 [06:11<02:41,  1.77s/it] 70%|   | 210/300 [06:13<02:39,  1.77s/it] 70%|   | 211/300 [06:14<02:37,  1.77s/it] 71%|   | 212/300 [06:16<02:36,  1.77s/it] 71%|   | 213/300 [06:18<02:34,  1.77s/it] 71%|  | 214/300 [06:20<02:32,  1.77s/it] 72%|  | 215/300 [06:22<02:30,  1.77s/it] 72%|  | 216/300 [06:23<02:28,  1.77s/it] 72%|  | 217/300 [06:25<02:27,  1.77s/it] 73%|  | 218/300 [06:27<02:25,  1.77s/it] 73%|  | 219/300 [06:29<02:23,  1.77s/it] 73%|  | 220/300 [06:30<02:21,  1.77s/it] 74%|  | 221/300 [06:32<02:20,  1.77s/it] 74%|  | 222/300 [06:34<02:18,  1.77s/it] 74%|  | 223/300 [06:36<02:16,  1.77s/it] 75%|  | 224/300 [06:37<02:14,  1.78s/it] 75%|  | 225/300 [06:39<02:13,  1.77s/it] 75%|  | 226/300 [06:41<02:11,  1.77s/it] 76%|  | 227/300 [06:43<02:09,  1.77s/it] 76%|  | 228/300 [06:45<02:07,  1.77s/it] 76%|  | 229/300 [06:46<02:05,  1.77s/it] 77%|  | 230/300 [06:48<02:04,  1.77s/it] 77%|  | 231/300 [06:50<02:02,  1.78s/it] 77%|  | 232/300 [06:52<02:00,  1.78s/it] 78%|  | 233/300 [06:53<01:58,  1.78s/it] 78%|  | 234/300 [06:55<01:57,  1.77s/it] 78%|  | 235/300 [06:57<01:55,  1.77s/it] 79%|  | 236/300 [06:59<01:53,  1.78s/it] 79%|  | 237/300 [07:01<01:51,  1.77s/it] 79%|  | 238/300 [07:02<01:50,  1.78s/it] 80%|  | 239/300 [07:04<01:48,  1.77s/it] 80%|  | 240/300 [07:06<01:46,  1.78s/it] 80%|  | 241/300 [07:08<01:44,  1.78s/it] 81%|  | 242/300 [07:09<01:42,  1.78s/it] 81%|  | 243/300 [07:11<01:41,  1.78s/it] 81%| | 244/300 [07:13<01:39,  1.78s/it] 82%| | 245/300 [07:15<01:37,  1.78s/it] 82%| | 246/300 [07:17<01:35,  1.78s/it] 82%| | 247/300 [07:18<01:34,  1.78s/it] 83%| | 248/300 [07:20<01:32,  1.78s/it] 83%| | 249/300 [07:22<01:30,  1.78s/it] 83%| | 250/300 [07:24<01:28,  1.78s/it] 84%| | 251/300 [07:25<01:27,  1.78s/it] 84%| | 252/300 [07:27<01:25,  1.78s/it] 84%| | 253/300 [07:29<01:23,  1.78s/it] 85%| | 254/300 [07:31<01:21,  1.78s/it] 85%| | 255/300 [07:33<01:19,  1.78s/it] 85%| | 256/300 [07:34<01:18,  1.78s/it] 86%| | 257/300 [07:36<01:16,  1.78s/it] 86%| | 258/300 [07:38<01:14,  1.78s/it] 86%| | 259/300 [07:40<01:12,  1.78s/it] 87%| | 260/300 [07:41<01:11,  1.78s/it] 87%| | 261/300 [07:43<01:09,  1.78s/it] 87%| | 262/300 [07:45<01:07,  1.78s/it] 88%| | 263/300 [07:47<01:05,  1.78s/it] 88%| | 264/300 [07:49<01:03,  1.78s/it] 88%| | 265/300 [07:50<01:02,  1.78s/it] 89%| | 266/300 [07:52<01:00,  1.78s/it] 89%| | 267/300 [07:54<00:58,  1.78s/it] 89%| | 268/300 [07:56<00:56,  1.78s/it] 90%| | 269/300 [07:57<00:55,  1.78s/it] 90%| | 270/300 [07:59<00:53,  1.78s/it] 90%| | 271/300 [08:01<00:51,  1.78s/it] 91%| | 272/300 [08:03<00:49,  1.78s/it] 91%| | 273/300 [08:05<00:47,  1.78s/it] 91%|| 274/300 [08:06<00:46,  1.78s/it] 92%|| 275/300 [08:08<00:44,  1.78s/it] 92%|| 276/300 [08:10<00:42,  1.78s/it] 92%|| 277/300 [08:12<00:40,  1.78s/it] 93%|| 278/300 [08:13<00:39,  1.78s/it] 93%|| 279/300 [08:15<00:37,  1.78s/it] 93%|| 280/300 [08:17<00:35,  1.78s/it] 94%|| 281/300 [08:19<00:33,  1.78s/it] 94%|| 282/300 [08:20<00:31,  1.78s/it] 94%|| 283/300 [08:22<00:30,  1.78s/it] 95%|| 284/300 [08:24<00:28,  1.78s/it] 95%|| 285/300 [08:26<00:26,  1.78s/it] 95%|| 286/300 [08:28<00:24,  1.77s/it] 96%|| 287/300 [08:29<00:23,  1.78s/it] 96%|| 288/300 [08:31<00:21,  1.78s/it] 96%|| 289/300 [08:33<00:19,  1.78s/it] 97%|| 290/300 [08:35<00:17,  1.78s/it] 97%|| 291/300 [08:36<00:15,  1.78s/it] 97%|| 292/300 [08:38<00:14,  1.78s/it] 98%|| 293/300 [08:40<00:12,  1.78s/it] 98%|| 294/300 [08:42<00:10,  1.78s/it] 98%|| 295/300 [08:44<00:08,  1.78s/it] 99%|| 296/300 [08:45<00:07,  1.78s/it] 99%|| 297/300 [08:47<00:05,  1.78s/it] 99%|| 298/300 [08:49<00:03,  1.78s/it]100%|| 299/300 [08:51<00:01,  1.78s/it]100%|| 300/300 [08:52<00:00,  1.78s/it][INFO|trainer.py:2084] 2024-03-13 13:25:28,560 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 533.0936, 'train_samples_per_second': 0.563, 'train_steps_per_second': 0.563, 'train_loss': 2.139360148111979, 'epoch': 3.0}
                                                 100%|| 300/300 [08:53<00:00,  1.78s/it]100%|| 300/300 [08:53<00:00,  1.78s/it]
[INFO|trainer.py:3054] 2024-03-13 13:25:28,696 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli
[INFO|configuration_utils.py:471] 2024-03-13 13:25:28,698 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/config.json
[INFO|modeling_utils.py:2478] 2024-03-13 13:25:49,969 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2470] 2024-03-13 13:25:49,971 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/tokenizer_config.json
[INFO|tokenization_utils_base.py:2479] 2024-03-13 13:25:49,973 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.1394
  train_runtime            = 0:08:53.09
  train_samples            =        100
  train_samples_per_second =      0.563
  train_steps_per_second   =      0.563
03/13/2024 13:25:50 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:761] 2024-03-13 13:25:50,045 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3363] 2024-03-13 13:25:50,047 >> ***** Running Evaluation *****
[INFO|trainer.py:3365] 2024-03-13 13:25:50,047 >>   Num examples = 100
[INFO|trainer.py:3368] 2024-03-13 13:25:50,047 >>   Batch size = 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|         | 2/100 [00:00<00:24,  4.01it/s]  3%|         | 3/100 [00:00<00:34,  2.82it/s]  4%|         | 4/100 [00:01<00:39,  2.45it/s]  5%|         | 5/100 [00:01<00:41,  2.27it/s]  6%|         | 6/100 [00:02<00:43,  2.17it/s]  7%|         | 7/100 [00:03<00:44,  2.11it/s]  8%|         | 8/100 [00:03<00:44,  2.07it/s]  9%|         | 9/100 [00:04<00:44,  2.05it/s] 10%|         | 10/100 [00:04<00:44,  2.03it/s] 11%|         | 11/100 [00:05<00:44,  2.02it/s] 12%|        | 12/100 [00:05<00:43,  2.02it/s] 13%|        | 13/100 [00:06<00:43,  2.01it/s] 14%|        | 14/100 [00:06<00:42,  2.00it/s] 15%|        | 15/100 [00:07<00:42,  2.00it/s] 16%|        | 16/100 [00:07<00:42,  2.00it/s] 17%|        | 17/100 [00:08<00:41,  2.00it/s] 18%|        | 18/100 [00:08<00:41,  2.00it/s] 19%|        | 19/100 [00:09<00:40,  2.00it/s] 20%|        | 20/100 [00:09<00:40,  1.99it/s] 21%|        | 21/100 [00:10<00:39,  1.99it/s] 22%|       | 22/100 [00:10<00:39,  1.99it/s] 23%|       | 23/100 [00:11<00:38,  1.99it/s] 24%|       | 24/100 [00:11<00:38,  1.99it/s] 25%|       | 25/100 [00:12<00:37,  1.99it/s] 26%|       | 26/100 [00:12<00:37,  1.99it/s] 27%|       | 27/100 [00:13<00:36,  1.99it/s] 28%|       | 28/100 [00:13<00:36,  1.99it/s] 29%|       | 29/100 [00:14<00:35,  1.99it/s] 30%|       | 30/100 [00:14<00:35,  1.99it/s] 31%|       | 31/100 [00:15<00:34,  1.99it/s] 32%|      | 32/100 [00:15<00:34,  1.99it/s] 33%|      | 33/100 [00:16<00:33,  1.99it/s] 34%|      | 34/100 [00:16<00:33,  1.99it/s] 35%|      | 35/100 [00:17<00:32,  1.99it/s] 36%|      | 36/100 [00:17<00:32,  1.99it/s] 37%|      | 37/100 [00:18<00:31,  1.99it/s] 38%|      | 38/100 [00:18<00:31,  1.99it/s] 39%|      | 39/100 [00:19<00:30,  1.99it/s] 40%|      | 40/100 [00:19<00:30,  1.99it/s] 41%|      | 41/100 [00:20<00:29,  1.99it/s] 42%|     | 42/100 [00:20<00:29,  1.99it/s] 43%|     | 43/100 [00:21<00:28,  1.99it/s] 44%|     | 44/100 [00:21<00:28,  1.99it/s] 45%|     | 45/100 [00:22<00:27,  1.99it/s] 46%|     | 46/100 [00:22<00:27,  1.99it/s] 47%|     | 47/100 [00:23<00:26,  1.99it/s] 48%|     | 48/100 [00:23<00:26,  1.99it/s] 49%|     | 49/100 [00:24<00:25,  1.99it/s] 50%|     | 50/100 [00:24<00:25,  1.99it/s] 51%|     | 51/100 [00:25<00:24,  1.99it/s] 52%|    | 52/100 [00:25<00:24,  1.99it/s] 53%|    | 53/100 [00:26<00:23,  1.99it/s] 54%|    | 54/100 [00:26<00:23,  1.99it/s] 55%|    | 55/100 [00:27<00:22,  1.99it/s] 56%|    | 56/100 [00:27<00:22,  1.99it/s] 57%|    | 57/100 [00:28<00:21,  1.99it/s] 58%|    | 58/100 [00:28<00:21,  1.99it/s] 59%|    | 59/100 [00:29<00:20,  1.98it/s] 60%|    | 60/100 [00:29<00:20,  1.98it/s] 61%|    | 61/100 [00:30<00:19,  1.98it/s] 62%|   | 62/100 [00:30<00:19,  1.98it/s] 63%|   | 63/100 [00:31<00:18,  1.98it/s] 64%|   | 64/100 [00:31<00:18,  1.99it/s] 65%|   | 65/100 [00:32<00:17,  1.99it/s] 66%|   | 66/100 [00:32<00:17,  1.99it/s] 67%|   | 67/100 [00:33<00:16,  1.99it/s] 68%|   | 68/100 [00:33<00:16,  1.99it/s] 69%|   | 69/100 [00:34<00:15,  1.99it/s] 70%|   | 70/100 [00:34<00:15,  1.99it/s] 71%|   | 71/100 [00:35<00:14,  1.99it/s] 72%|  | 72/100 [00:35<00:14,  1.99it/s] 73%|  | 73/100 [00:36<00:13,  1.99it/s] 74%|  | 74/100 [00:36<00:13,  1.98it/s] 75%|  | 75/100 [00:37<00:12,  1.98it/s] 76%|  | 76/100 [00:37<00:12,  1.98it/s] 77%|  | 77/100 [00:38<00:11,  1.98it/s] 78%|  | 78/100 [00:38<00:11,  1.98it/s] 79%|  | 79/100 [00:39<00:10,  1.98it/s] 80%|  | 80/100 [00:39<00:10,  1.98it/s] 81%|  | 81/100 [00:40<00:09,  1.98it/s] 82%| | 82/100 [00:40<00:09,  1.98it/s] 83%| | 83/100 [00:41<00:08,  1.98it/s] 84%| | 84/100 [00:41<00:08,  1.98it/s] 85%| | 85/100 [00:42<00:07,  1.98it/s] 86%| | 86/100 [00:42<00:07,  1.98it/s] 87%| | 87/100 [00:43<00:06,  1.98it/s] 88%| | 88/100 [00:43<00:06,  1.98it/s] 89%| | 89/100 [00:44<00:05,  1.98it/s] 90%| | 90/100 [00:44<00:05,  1.98it/s] 91%| | 91/100 [00:45<00:04,  1.98it/s] 92%|| 92/100 [00:45<00:04,  1.99it/s] 93%|| 93/100 [00:46<00:03,  1.99it/s] 94%|| 94/100 [00:46<00:03,  1.99it/s] 95%|| 95/100 [00:47<00:02,  1.99it/s] 96%|| 96/100 [00:47<00:02,  1.98it/s] 97%|| 97/100 [00:48<00:01,  1.98it/s] 98%|| 98/100 [00:48<00:01,  1.98it/s] 99%|| 99/100 [00:49<00:00,  1.98it/s]100%|| 100/100 [00:49<00:00,  1.99it/s]100%|| 100/100 [00:49<00:00,  2.01it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.43
  eval_loss               =     2.7932
  eval_runtime            = 0:00:50.33
  eval_samples            =        100
  eval_samples_per_second =      1.987
  eval_steps_per_second   =      1.987
[INFO|trainer.py:761] 2024-03-13 13:26:40,381 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3363] 2024-03-13 13:26:40,383 >> ***** Running Evaluation *****
[INFO|trainer.py:3365] 2024-03-13 13:26:40,383 >>   Num examples = 100
[INFO|trainer.py:3368] 2024-03-13 13:26:40,383 >>   Batch size = 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|         | 2/100 [00:00<00:24,  3.96it/s]  3%|         | 3/100 [00:01<00:34,  2.80it/s]  4%|         | 4/100 [00:01<00:39,  2.42it/s]  5%|         | 5/100 [00:02<00:42,  2.25it/s]  6%|         | 6/100 [00:02<00:43,  2.16it/s]  7%|         | 7/100 [00:03<00:44,  2.10it/s]  8%|         | 8/100 [00:03<00:44,  2.06it/s]  9%|         | 9/100 [00:04<00:44,  2.04it/s] 10%|         | 10/100 [00:04<00:44,  2.02it/s] 11%|         | 11/100 [00:05<00:44,  2.01it/s] 12%|        | 12/100 [00:05<00:43,  2.00it/s] 13%|        | 13/100 [00:06<00:43,  2.00it/s] 14%|        | 14/100 [00:06<00:43,  1.99it/s] 15%|        | 15/100 [00:07<00:42,  1.99it/s] 16%|        | 16/100 [00:07<00:42,  1.99it/s] 17%|        | 17/100 [00:08<00:41,  1.99it/s] 18%|        | 18/100 [00:08<00:41,  1.99it/s] 19%|        | 19/100 [00:09<00:40,  1.99it/s] 20%|        | 20/100 [00:09<00:40,  1.99it/s] 21%|        | 21/100 [00:10<00:39,  1.99it/s] 22%|       | 22/100 [00:10<00:39,  1.99it/s] 23%|       | 23/100 [00:11<00:38,  1.99it/s] 24%|       | 24/100 [00:11<00:38,  1.98it/s] 25%|       | 25/100 [00:12<00:37,  1.99it/s] 26%|       | 26/100 [00:12<00:37,  1.99it/s] 27%|       | 27/100 [00:13<00:36,  1.99it/s] 28%|       | 28/100 [00:13<00:36,  1.99it/s] 29%|       | 29/100 [00:14<00:35,  1.98it/s] 30%|       | 30/100 [00:14<00:35,  1.98it/s] 31%|       | 31/100 [00:15<00:34,  1.98it/s] 32%|      | 32/100 [00:15<00:34,  1.98it/s] 33%|      | 33/100 [00:16<00:33,  1.98it/s] 34%|      | 34/100 [00:16<00:33,  1.98it/s] 35%|      | 35/100 [00:17<00:32,  1.98it/s] 36%|      | 36/100 [00:17<00:32,  1.98it/s] 37%|      | 37/100 [00:18<00:31,  1.98it/s] 38%|      | 38/100 [00:18<00:31,  1.98it/s] 39%|      | 39/100 [00:19<00:30,  1.98it/s] 40%|      | 40/100 [00:19<00:30,  1.98it/s] 41%|      | 41/100 [00:20<00:29,  1.98it/s] 42%|     | 42/100 [00:20<00:29,  1.98it/s] 43%|     | 43/100 [00:21<00:28,  1.98it/s] 44%|     | 44/100 [00:21<00:28,  1.98it/s] 45%|     | 45/100 [00:22<00:27,  1.98it/s] 46%|     | 46/100 [00:22<00:27,  1.98it/s] 47%|     | 47/100 [00:23<00:26,  1.98it/s] 48%|     | 48/100 [00:23<00:26,  1.98it/s] 49%|     | 49/100 [00:24<00:25,  1.98it/s] 50%|     | 50/100 [00:24<00:25,  1.98it/s] 51%|     | 51/100 [00:25<00:24,  1.98it/s] 52%|    | 52/100 [00:25<00:24,  1.98it/s] 53%|    | 53/100 [00:26<00:23,  1.98it/s] 54%|    | 54/100 [00:26<00:23,  1.98it/s] 55%|    | 55/100 [00:27<00:22,  1.98it/s] 56%|    | 56/100 [00:27<00:22,  1.98it/s] 57%|    | 57/100 [00:28<00:21,  1.98it/s] 58%|    | 58/100 [00:28<00:21,  1.98it/s] 59%|    | 59/100 [00:29<00:20,  1.98it/s] 60%|    | 60/100 [00:29<00:20,  1.98it/s] 61%|    | 61/100 [00:30<00:19,  1.98it/s] 62%|   | 62/100 [00:30<00:19,  1.98it/s] 63%|   | 63/100 [00:31<00:18,  1.98it/s] 64%|   | 64/100 [00:31<00:18,  1.98it/s] 65%|   | 65/100 [00:32<00:17,  1.98it/s] 66%|   | 66/100 [00:32<00:17,  1.98it/s] 67%|   | 67/100 [00:33<00:16,  1.98it/s] 68%|   | 68/100 [00:33<00:16,  1.99it/s] 69%|   | 69/100 [00:34<00:15,  1.99it/s] 70%|   | 70/100 [00:34<00:15,  1.99it/s] 71%|   | 71/100 [00:35<00:14,  1.98it/s] 72%|  | 72/100 [00:35<00:14,  1.98it/s] 73%|  | 73/100 [00:36<00:13,  1.99it/s] 74%|  | 74/100 [00:36<00:13,  1.99it/s] 75%|  | 75/100 [00:37<00:12,  1.99it/s] 76%|  | 76/100 [00:37<00:12,  1.99it/s] 77%|  | 77/100 [00:38<00:11,  1.98it/s] 78%|  | 78/100 [00:38<00:11,  1.98it/s] 79%|  | 79/100 [00:39<00:10,  1.98it/s] 80%|  | 80/100 [00:39<00:10,  1.98it/s] 81%|  | 81/100 [00:40<00:09,  1.98it/s] 82%| | 82/100 [00:40<00:09,  1.98it/s] 83%| | 83/100 [00:41<00:08,  1.98it/s] 84%| | 84/100 [00:41<00:08,  1.98it/s] 85%| | 85/100 [00:42<00:07,  1.98it/s] 86%| | 86/100 [00:42<00:07,  1.98it/s] 87%| | 87/100 [00:43<00:06,  1.98it/s] 88%| | 88/100 [00:43<00:06,  1.98it/s] 89%| | 89/100 [00:44<00:05,  1.98it/s] 90%| | 90/100 [00:44<00:05,  1.98it/s] 91%| | 91/100 [00:45<00:04,  1.98it/s] 92%|| 92/100 [00:45<00:04,  1.98it/s] 93%|| 93/100 [00:46<00:03,  1.98it/s] 94%|| 94/100 [00:46<00:03,  1.98it/s] 95%|| 95/100 [00:47<00:02,  1.98it/s] 96%|| 96/100 [00:47<00:02,  1.98it/s] 97%|| 97/100 [00:48<00:01,  1.98it/s] 98%|| 98/100 [00:48<00:01,  1.98it/s] 99%|| 99/100 [00:49<00:00,  1.98it/s]100%|| 100/100 [00:49<00:00,  1.99it/s]100%|| 100/100 [00:49<00:00,  2.00it/s]
***** eval metrics *****
  epoch_mm                   =        3.0
  eval_accuracy_mm           =       0.49
  eval_loss_mm               =     2.9145
  eval_runtime_mm            = 0:00:50.42
  eval_samples_mm            =        100
  eval_samples_per_second_mm =      1.983
  eval_steps_per_second_mm   =      1.983
