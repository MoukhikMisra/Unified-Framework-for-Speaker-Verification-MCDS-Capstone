/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:17:52 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 4387.34 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 5145.00 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 5698.43 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 5914.98 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 5560.95 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3766.78 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3655.01 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 4283.56 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 4956.22 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 2208.14 examples/s]
03/13/2024 20:19:39 - INFO - __main__ - Sample 362 of the training set: {'input_ids': [1, 379, 948, 861, 14645, 29949, 382, 29889, 29967, 29889, 399, 3634, 24905, 9571, 278, 10608, 5432, 278, 364, 19478, 385, 376, 714, 6617, 681, 1044, 12242, 287, 472, 263, 7934, 946, 8395, 869, 376, 1, 25065, 1919, 379, 948, 861, 9444, 4144, 2199, 943, 992, 7612, 4549, 29035, 310, 278, 3303, 3900, 1919, 5432, 967, 10608, 385, 376, 714, 6617, 681, 260, 17911, 12242, 287, 472, 263, 7934, 946, 8395, 869, 376], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:19:39 - INFO - __main__ - Sample 3601 of the training set: {'input_ids': [1, 13863, 561, 2722, 1919, 322, 670, 4783, 4667, 1919, 892, 373, 278, 28668, 24165, 15050, 4515, 3250, 17724, 322, 498, 1295, 3250, 7250, 869, 1, 13863, 561, 2722, 1919, 322, 670, 4783, 1919, 4667, 1919, 11977, 472, 28668, 297, 278, 4688, 17724, 322, 10398, 278, 2462, 411, 28668, 11182, 12828, 15225, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:19:39 - INFO - __main__ - Sample 2061 of the training set: {'input_ids': [1, 7634, 278, 3814, 1641, 9326, 15050, 4515, 3250, 297, 7660, 1919, 402, 29924, 674, 3867, 26028, 12012, 3864, 18577, 411, 534, 2707, 29879, 6943, 26413, 3038, 11301, 21083, 869, 1, 7634, 278, 3814, 1919, 607, 402, 29924, 322, 26028, 892, 304, 7475, 346, 15050, 4515, 3250, 297, 7660, 1919, 402, 29924, 674, 3867, 26028, 12012, 3864, 18577, 411, 534, 2707, 29879, 6943, 26413, 3038, 11301, 21083, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:19:41 - INFO - __main__ - ***** Running training *****
03/13/2024 20:19:41 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:19:41 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:19:41 - INFO - __main__ -   Instantaneous batch size per device = 24
03/13/2024 20:19:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 24
03/13/2024 20:19:41 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:19:41 - INFO - __main__ -   Total optimization steps = 459
  0%|          | 0/459 [00:00<?, ?it/s]  0%|          | 1/459 [00:02<21:07,  2.77s/it]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 549, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 755, in forward
    hidden_states = self.mlp(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 43.31 MiB is free. Including non-PyTorch memory, this process has 31.69 GiB memory in use. Of the allocated memory 29.88 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/459 [00:03<24:33,  3.22s/it]
[2024-03-13 20:19:47,557] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 4024) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:19:47
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 4024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:20:23 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6687.51 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 6841.69 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6876.05 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6729.05 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6105.16 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 6493.89 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6585.94 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6470.52 examples/s]
03/13/2024 20:20:31 - INFO - __main__ - Sample 3657 of the training set: {'input_ids': [1, 5806, 3237, 29889, 660, 545, 29875, 338, 17644, 3390, 287, 322, 756, 263, 1472, 4955, 310, 27214, 1218, 411, 278, 11996, 275, 1919, 540, 2609, 2149, 1316, 263, 14294, 12853, 869, 1, 5806, 660, 545, 423, 338, 3390, 287, 322, 756, 263, 4955, 310, 27214, 1218, 411, 278, 11996, 275, 1919, 263, 14294, 12853, 338, 451, 3806, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:20:31 - INFO - __main__ - Sample 1000 of the training set: {'input_ids': [1, 940, 3614, 8957, 292, 6944, 373, 278, 1746, 363, 278, 1473, 931, 323, 1041, 3250, 1951, 670, 8718, 29899, 3250, 24092, 869, 1, 435, 1308, 1919, 1058, 766, 28809, 670, 2175, 23468, 297, 263, 22369, 4779, 29871, 29941, 29896, 1919, 3614, 8957, 292, 6944, 373, 278, 1746, 363, 278, 937, 931, 27822, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:20:31 - INFO - __main__ - Sample 1599 of the training set: {'input_ids': [1, 376, 450, 7613, 1258, 577, 322, 9259, 393, 620, 25072, 1919, 376, 1497, 4052, 5594, 1919, 1058, 10329, 2859, 278, 11781, 869, 1, 376, 450, 7613, 1258, 577, 1919, 322, 9259, 393, 620, 25072, 1919, 376, 1497, 278, 28942, 310, 278, 14523, 525, 29879, 22874, 362, 21118, 1919, 379, 29889, 8965, 4052, 5594, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:20:32 - INFO - __main__ - ***** Running training *****
03/13/2024 20:20:32 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:20:32 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:20:32 - INFO - __main__ -   Instantaneous batch size per device = 20
03/13/2024 20:20:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 20
03/13/2024 20:20:32 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:20:32 - INFO - __main__ -   Total optimization steps = 552
  0%|          | 0/552 [00:00<?, ?it/s]  0%|          | 1/552 [00:02<18:35,  2.02s/it]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 549, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 737, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 90, in forward
    return self.weight * hidden_states.to(input_dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/552 [00:02<22:27,  2.45s/it]
[2024-03-13 20:20:38,300] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 7739) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:20:38
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 7739)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:21:05 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 4644.73 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 5717.96 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6180.67 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 5999.43 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6275.26 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7039.66 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6939.31 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6843.20 examples/s]
03/13/2024 20:21:13 - INFO - __main__ - Sample 2367 of the training set: {'input_ids': [1, 14440, 7455, 495, 1919, 806, 1740, 29891, 525, 29879, 1098, 25252, 1919, 750, 263, 1422, 1776, 869, 1, 806, 1740, 29891, 525, 29879, 1098, 25252, 1919, 14440, 7455, 495, 1919, 2000, 278, 364, 19478, 376, 263, 12176, 15354, 869, 376], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:21:13 - INFO - __main__ - Sample 1904 of the training set: {'input_ids': [1, 20893, 29875, 424, 756, 1063, 373, 278, 2908, 1951, 372, 5714, 278, 1820, 838, 2957, 360, 608, 29883, 29939, 3633, 297, 3786, 869, 1, 20893, 29875, 424, 756, 1063, 263, 3646, 1951, 372, 5714, 263, 7618, 1455, 3132, 1919, 838, 2957, 360, 608, 29939, 1919, 297, 3786, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:21:13 - INFO - __main__ - Sample 387 of the training set: {'input_ids': [1, 450, 14582, 1497, 376, 372, 471, 451, 1749, 16392, 304, 3646, 470, 1283, 355, 738, 2318, 470, 12407, 470, 304, 297, 2036, 3056, 1127, 470, 21448, 869, 376, 1, 376, 512, 4969, 278, 3748, 1919, 372, 471, 451, 1749, 16392, 304, 3646, 470, 1283, 355, 738, 2318, 470, 12407, 470, 304, 297, 2036, 3056, 1127, 470, 21448, 2750, 1316, 6471, 12407, 869, 376], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:21:14 - INFO - __main__ - ***** Running training *****
03/13/2024 20:21:14 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:21:14 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:21:14 - INFO - __main__ -   Instantaneous batch size per device = 18
03/13/2024 20:21:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 18
03/13/2024 20:21:14 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:21:14 - INFO - __main__ -   Total optimization steps = 612
  0%|          | 0/612 [00:00<?, ?it/s]  0%|          | 1/612 [00:01<19:19,  1.90s/it]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 549, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 740, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 638, in forward
    query_states = self.q_proj(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 9.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.89 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/612 [00:02<22:01,  2.16s/it]
[2024-03-13 20:21:20,741] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 9565) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:21:20
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 9565)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:21:47 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 5784.87 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 6367.04 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6598.51 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6614.06 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6452.42 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6051.59 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7004.25 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7029.48 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6602.66 examples/s]
03/13/2024 20:21:54 - INFO - __main__ - Sample 3056 of the training set: {'input_ids': [1, 450, 15483, 29899, 433, 1133, 22318, 1388, 29939, 24497, 568, 11374, 529, 869, 6415, 2965, 1405, 4845, 1312, 29871, 29896, 29953, 29889, 29953, 29947, 3291, 1919, 470, 29871, 29896, 29889, 29900, 29896, 10151, 1919, 472, 29871, 29896, 29892, 29953, 29941, 29953, 29889, 29929, 29946, 869, 1, 450, 2545, 1664, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 869, 5550, 29990, 321, 1463, 29871, 29955, 29889, 29945, 29955, 3291, 1919, 470, 29871, 29900, 29889, 29955, 29953, 10151, 1919, 472, 29871, 29929, 29929, 29900, 29889, 29929, 29946, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:21:54 - INFO - __main__ - Sample 1492 of the training set: {'input_ids': [1, 1920, 305, 10961, 29879, 892, 21682, 491, 263, 269, 473, 29821, 579, 515, 8991, 20279, 1883, 973, 29879, 1919, 607, 471, 24774, 408, 263, 4319, 288, 1527, 363, 278, 701, 11506, 12616, 368, 2326, 11753, 4259, 869, 1, 319, 269, 473, 29821, 579, 515, 8991, 20279, 1883, 973, 29879, 9266, 869, 317, 3904, 29956, 29889, 29949, 1925, 901, 12959, 373, 14406, 7103, 1434, 278, 12616, 368, 2326, 11753, 4259, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:21:54 - INFO - __main__ - Sample 2440 of the training set: {'input_ids': [1, 319, 16336, 4307, 24555, 13561, 6221, 1919, 5353, 292, 278, 1206, 373, 25502, 310, 385, 4735, 537, 1919, 15659, 278, 12326, 408, 9070, 2168, 2191, 4077, 2291, 869, 1, 450, 6221, 1919, 20766, 278, 623, 9003, 2673, 472, 12115, 525, 29879, 4522, 273, 4623, 18117, 1919, 15659, 278, 12326, 408, 9070, 2168, 2191, 4077, 2291, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:21:56 - INFO - __main__ - ***** Running training *****
03/13/2024 20:21:56 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:21:56 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:21:56 - INFO - __main__ -   Instantaneous batch size per device = 16
03/13/2024 20:21:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
03/13/2024 20:21:56 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:21:56 - INFO - __main__ -   Total optimization steps = 690
  0%|          | 0/690 [00:00<?, ?it/s]  0%|          | 1/690 [00:01<22:15,  1.94s/it]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 549, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 737, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 90, in forward
    return self.weight * hidden_states.to(input_dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.86 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/690 [00:02<25:07,  2.19s/it]
[2024-03-13 20:22:02,645] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 10551) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:22:02
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10551)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:22:30 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6813.74 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 7038.10 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 7184.43 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 7054.41 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6484.19 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7360.66 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7234.01 examples/s]
03/13/2024 20:22:37 - INFO - __main__ - Sample 2126 of the training set: {'input_ids': [1, 5310, 7075, 1919, 263, 15754, 653, 9638, 391, 472, 278, 3014, 310, 23716, 1919, 11981, 278, 395, 29871, 29941, 29906, 29945, 7284, 29715, 10655, 869, 1, 323, 11787, 29909, 338, 278, 3234, 310, 3014, 310, 23716, 15754, 653, 9638, 391, 4667, 1952, 3903, 265, 1919, 1302, 29899, 11569, 5286, 1061, 373, 278, 29715, 10655, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:22:37 - INFO - __main__ - Sample 491 of the training set: {'input_ids': [1, 12886, 713, 620, 6972, 2879, 884, 1033, 367, 2665, 975, 344, 294, 363, 17643, 1316, 408, 278, 17789, 4080, 310, 27135, 9777, 322, 21375, 29939, 869, 1, 12886, 713, 620, 6972, 2879, 884, 1033, 367, 2665, 975, 344, 294, 363, 17643, 1316, 408, 17789, 4080, 297, 27135, 9777, 322, 21375, 29939, 1919, 1985, 297, 11909, 515, 5578, 1061, 304, 534, 2707, 7156, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:22:37 - INFO - __main__ - Sample 2461 of the training set: {'input_ids': [1, 739, 756, 263, 2298, 470, 26134, 29871, 29946, 29889, 29929, 10151, 5906, 310, 1059, 869, 1, 739, 750, 263, 5906, 310, 1059, 310, 2298, 470, 26134, 5320, 19649, 3291, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:22:39 - INFO - __main__ - ***** Running training *****
03/13/2024 20:22:39 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:22:39 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:22:39 - INFO - __main__ -   Instantaneous batch size per device = 14
03/13/2024 20:22:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 14
03/13/2024 20:22:39 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:22:39 - INFO - __main__ -   Total optimization steps = 786
  0%|          | 0/786 [00:00<?, ?it/s]  0%|          | 1/786 [00:01<24:42,  1.89s/it]  0%|          | 2/786 [00:02<17:11,  1.32s/it]  0%|          | 3/786 [00:03<13:42,  1.05s/it]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 549, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 755, in forward
    hidden_states = self.mlp(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.76 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 3/786 [00:03<17:00,  1.30s/it]
[2024-03-13 20:22:46,158] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 12379) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:22:46
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 12379)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:23:24 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6207.90 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 6290.73 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6386.88 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6381.08 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6305.28 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 5940.14 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 6630.45 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6721.93 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6603.53 examples/s]
03/13/2024 20:23:31 - INFO - __main__ - Sample 560 of the training set: {'input_ids': [1, 360, 28778, 1925, 670, 3114, 373, 2479, 16340, 17724, 297, 15613, 278, 379, 14287, 1894, 29891, 29871, 29906, 29906, 29945, 472, 349, 29379, 3938, 557, 4623, 23613, 1582, 869, 1, 8075, 360, 28778, 10201, 1754, 15613, 278, 379, 14287, 1894, 29891, 29871, 29906, 29906, 29945, 1106, 4780, 16340, 472, 349, 29379, 3938, 557, 4623, 23613, 1582, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:23:31 - INFO - __main__ - Sample 1670 of the training set: {'input_ids': [1, 1551, 278, 24081, 680, 1919, 315, 6400, 29899, 1625, 29874, 3189, 869, 313, 7098, 344, 584, 476, 29949, 448, 9763, 448, 2305, 1723, 8967, 6133, 21665, 363, 278, 12616, 4688, 373, 498, 1295, 3250, 1919, 9213, 491, 7824, 9667, 869, 1, 315, 6400, 29899, 1625, 29874, 3189, 869, 313, 476, 29949, 1723, 8967, 6133, 21665, 363, 278, 12616, 1919, 9213, 491, 7824, 9667, 1919, 4688, 373, 498, 1295, 3250, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:23:31 - INFO - __main__ - Sample 856 of the training set: {'input_ids': [1, 6853, 630, 5254, 399, 5385, 317, 2518, 476, 688, 1358, 26869, 304, 445, 3461, 515, 1570, 3088, 4412, 869, 1, 6853, 630, 5254, 399, 5385, 5765, 402, 555, 2330, 26869, 304, 445, 3461, 515, 16197, 1384, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:23:33 - INFO - __main__ - ***** Running training *****
03/13/2024 20:23:33 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:23:33 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:23:33 - INFO - __main__ -   Instantaneous batch size per device = 12
03/13/2024 20:23:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 12
03/13/2024 20:23:33 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:23:33 - INFO - __main__ -   Total optimization steps = 918
  0%|          | 0/918 [00:00<?, ?it/s]  0%|          | 1/918 [00:01<24:36,  1.61s/it]  0%|          | 2/918 [00:02<16:09,  1.06s/it]  0%|          | 3/918 [00:03<13:52,  1.10it/s]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 557, in main
    optimizer.step()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 187, in step
    adamw(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 339, in adamw
    func(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 608, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 35.31 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 29.16 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 3/918 [00:04<20:55,  1.37s/it]
[2024-03-13 20:23:40,532] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 14344) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:23:40
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 14344)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:23:58 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6203.94 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 6532.90 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6694.56 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6583.01 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6397.44 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7098.16 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7077.71 examples/s]
03/13/2024 20:24:05 - INFO - __main__ - Sample 1747 of the training set: {'input_ids': [1, 739, 508, 3787, 901, 1135, 263, 19340, 10798, 371, 310, 2472, 7126, 304, 2820, 29871, 29896, 29906, 6199, 310, 4696, 297, 697, 13630, 293, 1644, 17528, 276, 869, 1, 1126, 372, 508, 3787, 901, 1135, 263, 19340, 10798, 371, 310, 2472, 448, 7126, 304, 29871, 29896, 29892, 29900, 29900, 29900, 1880, 11029, 4558, 470, 2820, 29871, 29896, 29906, 6199, 310, 4696, 448, 297, 925, 697, 13630, 293, 1644, 17528, 276, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:24:05 - INFO - __main__ - Sample 1460 of the training set: {'input_ids': [1, 1205, 278, 716, 325, 5753, 457, 674, 451, 1584, 3380, 304, 367, 9528, 297, 2305, 2745, 29871, 29906, 29900, 29900, 29945, 322, 674, 451, 367, 23454, 363, 671, 363, 3196, 2440, 1156, 393, 869, 1, 2398, 1919, 278, 716, 325, 5753, 457, 2113, 525, 29873, 3380, 304, 367, 9528, 297, 2305, 2745, 29871, 29906, 29900, 29900, 29945, 322, 2113, 525, 29873, 367, 23454, 363, 671, 363, 3196, 2440, 1156, 393, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:24:05 - INFO - __main__ - Sample 1099 of the training set: {'input_ids': [1, 7634, 278, 501, 29889, 29903, 29889, 20063, 1919, 278, 916, 24171, 310, 11559, 1919, 278, 5619, 310, 28757, 1919, 947, 451, 11719, 373, 9117, 394, 492, 2925, 869, 1, 450, 501, 29889, 29903, 29889, 5619, 310, 28757, 1919, 278, 916, 24171, 310, 11559, 1919, 947, 451, 11719, 373, 7539, 583, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:24:07 - INFO - __main__ - ***** Running training *****
03/13/2024 20:24:07 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:24:07 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:24:07 - INFO - __main__ -   Instantaneous batch size per device = 8
03/13/2024 20:24:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
03/13/2024 20:24:07 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:24:07 - INFO - __main__ -   Total optimization steps = 1377
  0%|          | 0/1377 [00:00<?, ?it/s]  0%|          | 1/1377 [00:01<30:16,  1.32s/it]  0%|          | 2/1377 [00:01<19:38,  1.17it/s]  0%|          | 3/1377 [00:02<16:53,  1.36it/s]  0%|          | 4/1377 [00:03<15:56,  1.44it/s]  0%|          | 5/1377 [00:03<15:27,  1.48it/s]  0%|          | 6/1377 [00:04<14:55,  1.53it/s]  1%|          | 7/1377 [00:04<15:00,  1.52it/s]  1%|          | 8/1377 [00:05<14:32,  1.57it/s]  1%|          | 9/1377 [00:06<14:00,  1.63it/s]  1%|          | 10/1377 [00:06<14:12,  1.60it/s]  1%|          | 11/1377 [00:07<13:47,  1.65it/s]  1%|          | 12/1377 [00:07<13:24,  1.70it/s]  1%|          | 13/1377 [00:08<13:42,  1.66it/s]  1%|          | 14/1377 [00:09<14:12,  1.60it/s]  1%|          | 15/1377 [00:09<13:59,  1.62it/s]  1%|          | 16/1377 [00:10<13:35,  1.67it/s]  1%|          | 17/1377 [00:11<13:53,  1.63it/s]  1%|▏         | 18/1377 [00:11<14:17,  1.58it/s]  1%|▏         | 19/1377 [00:12<13:44,  1.65it/s]  1%|▏         | 20/1377 [00:12<14:11,  1.59it/s]  2%|▏         | 21/1377 [00:13<14:30,  1.56it/s]  2%|▏         | 22/1377 [00:14<14:11,  1.59it/s]  2%|▏         | 23/1377 [00:14<14:02,  1.61it/s]  2%|▏         | 24/1377 [00:15<13:24,  1.68it/s]  2%|▏         | 25/1377 [00:15<12:58,  1.74it/s]  2%|▏         | 26/1377 [00:16<12:40,  1.78it/s]  2%|▏         | 27/1377 [00:16<12:38,  1.78it/s]  2%|▏         | 28/1377 [00:17<12:34,  1.79it/s]  2%|▏         | 29/1377 [00:18<12:23,  1.81it/s]  2%|▏         | 30/1377 [00:18<12:27,  1.80it/s]  2%|▏         | 31/1377 [00:19<12:43,  1.76it/s]  2%|▏         | 32/1377 [00:19<13:00,  1.72it/s]  2%|▏         | 33/1377 [00:20<13:06,  1.71it/s]  2%|▏         | 34/1377 [00:21<13:16,  1.69it/s]  3%|▎         | 35/1377 [00:21<14:00,  1.60it/s]  3%|▎         | 36/1377 [00:22<13:34,  1.65it/s]  3%|▎         | 37/1377 [00:22<13:29,  1.66it/s]  3%|▎         | 38/1377 [00:23<13:05,  1.70it/s]  3%|▎         | 39/1377 [00:23<12:53,  1.73it/s]  3%|▎         | 40/1377 [00:24<12:33,  1.77it/s]  3%|▎         | 41/1377 [00:25<12:52,  1.73it/s]  3%|▎         | 42/1377 [00:25<13:04,  1.70it/s]  3%|▎         | 43/1377 [00:26<13:36,  1.63it/s]  3%|▎         | 44/1377 [00:27<13:50,  1.61it/s]  3%|▎         | 45/1377 [00:27<13:23,  1.66it/s]  3%|▎         | 46/1377 [00:28<13:24,  1.66it/s]  3%|▎         | 47/1377 [00:28<13:05,  1.69it/s]  3%|▎         | 48/1377 [00:29<13:12,  1.68it/s]  4%|▎         | 49/1377 [00:30<13:45,  1.61it/s]  4%|▎         | 50/1377 [00:30<13:22,  1.65it/s]  4%|▎         | 51/1377 [00:31<13:16,  1.66it/s]  4%|▍         | 52/1377 [00:31<13:01,  1.70it/s]  4%|▍         | 53/1377 [00:32<13:20,  1.65it/s]  4%|▍         | 54/1377 [00:32<12:59,  1.70it/s]  4%|▍         | 55/1377 [00:33<12:59,  1.70it/s]  4%|▍         | 56/1377 [00:34<12:37,  1.74it/s]  4%|▍         | 57/1377 [00:34<12:21,  1.78it/s]  4%|▍         | 58/1377 [00:35<12:52,  1.71it/s]  4%|▍         | 59/1377 [00:35<12:45,  1.72it/s]  4%|▍         | 60/1377 [00:36<12:56,  1.70it/s]  4%|▍         | 61/1377 [00:37<13:30,  1.62it/s]  5%|▍         | 62/1377 [00:37<13:28,  1.63it/s]  5%|▍         | 63/1377 [00:38<13:38,  1.60it/s]  5%|▍         | 64/1377 [00:39<13:33,  1.61it/s]  5%|▍         | 65/1377 [00:39<13:22,  1.64it/s]  5%|▍         | 66/1377 [00:40<13:20,  1.64it/s]  5%|▍         | 67/1377 [00:40<13:19,  1.64it/s]  5%|▍         | 68/1377 [00:41<13:31,  1.61it/s]  5%|▌         | 69/1377 [00:42<13:42,  1.59it/s]  5%|▌         | 70/1377 [00:42<13:37,  1.60it/s]  5%|▌         | 71/1377 [00:43<13:13,  1.65it/s]  5%|▌         | 72/1377 [00:43<13:48,  1.58it/s]  5%|▌         | 73/1377 [00:44<14:04,  1.54it/s]  5%|▌         | 74/1377 [00:45<14:03,  1.54it/s]  5%|▌         | 75/1377 [00:45<13:25,  1.62it/s]  6%|▌         | 76/1377 [00:46<13:44,  1.58it/s]  6%|▌         | 77/1377 [00:47<13:13,  1.64it/s]  6%|▌         | 78/1377 [00:47<13:14,  1.64it/s]  6%|▌         | 79/1377 [00:48<13:38,  1.59it/s]  6%|▌         | 80/1377 [00:48<13:08,  1.65it/s]  6%|▌         | 81/1377 [00:49<13:21,  1.62it/s]  6%|▌         | 82/1377 [00:50<13:12,  1.63it/s]  6%|▌         | 83/1377 [00:50<12:50,  1.68it/s]  6%|▌         | 84/1377 [00:51<13:45,  1.57it/s]  6%|▌         | 85/1377 [00:52<13:49,  1.56it/s]  6%|▌         | 86/1377 [00:52<13:49,  1.56it/s]  6%|▋         | 87/1377 [00:53<13:32,  1.59it/s]  6%|▋         | 88/1377 [00:53<13:17,  1.62it/s]  6%|▋         | 89/1377 [00:54<12:50,  1.67it/s]  7%|▋         | 90/1377 [00:55<12:48,  1.68it/s]  7%|▋         | 91/1377 [00:55<12:34,  1.70it/s]  7%|▋         | 92/1377 [00:56<12:43,  1.68it/s]  7%|▋         | 93/1377 [00:56<12:43,  1.68it/s]  7%|▋         | 94/1377 [00:57<12:58,  1.65it/s]  7%|▋         | 95/1377 [00:58<12:41,  1.68it/s]  7%|▋         | 96/1377 [00:58<12:45,  1.67it/s]  7%|▋         | 97/1377 [00:59<12:46,  1.67it/s]  7%|▋         | 98/1377 [00:59<13:02,  1.63it/s]  7%|▋         | 99/1377 [01:00<12:44,  1.67it/s]  7%|▋         | 100/1377 [01:01<12:40,  1.68it/s]  7%|▋         | 101/1377 [01:01<12:27,  1.71it/s]  7%|▋         | 102/1377 [01:02<12:07,  1.75it/s]  7%|▋         | 103/1377 [01:02<12:17,  1.73it/s]  8%|▊         | 104/1377 [01:03<12:09,  1.74it/s]  8%|▊         | 105/1377 [01:03<12:34,  1.68it/s]  8%|▊         | 106/1377 [01:04<12:35,  1.68it/s]  8%|▊         | 107/1377 [01:05<12:42,  1.67it/s]  8%|▊         | 108/1377 [01:05<12:39,  1.67it/s]  8%|▊         | 109/1377 [01:06<12:36,  1.68it/s]  8%|▊         | 110/1377 [01:06<12:40,  1.67it/s]  8%|▊         | 111/1377 [01:07<12:56,  1.63it/s]  8%|▊         | 112/1377 [01:08<13:07,  1.61it/s]  8%|▊         | 113/1377 [01:08<13:29,  1.56it/s]  8%|▊         | 114/1377 [01:09<12:50,  1.64it/s]  8%|▊         | 115/1377 [01:10<13:00,  1.62it/s]  8%|▊         | 116/1377 [01:10<12:51,  1.63it/s]  8%|▊         | 117/1377 [01:11<13:15,  1.58it/s]  9%|▊         | 118/1377 [01:12<13:18,  1.58it/s]  9%|▊         | 119/1377 [01:12<13:33,  1.55it/s]  9%|▊         | 120/1377 [01:13<13:32,  1.55it/s]  9%|▉         | 121/1377 [01:14<13:32,  1.55it/s]  9%|▉         | 122/1377 [01:14<13:18,  1.57it/s]  9%|▉         | 123/1377 [01:15<12:46,  1.64it/s]  9%|▉         | 124/1377 [01:15<12:15,  1.70it/s]  9%|▉         | 125/1377 [01:16<11:55,  1.75it/s]  9%|▉         | 126/1377 [01:16<11:49,  1.76it/s]  9%|▉         | 127/1377 [01:17<11:46,  1.77it/s]  9%|▉         | 128/1377 [01:18<12:27,  1.67it/s]  9%|▉         | 129/1377 [01:18<12:42,  1.64it/s]  9%|▉         | 130/1377 [01:19<13:19,  1.56it/s] 10%|▉         | 131/1377 [01:19<12:19,  1.69it/s] 10%|▉         | 132/1377 [01:20<12:47,  1.62it/s] 10%|▉         | 133/1377 [01:21<12:59,  1.60it/s] 10%|▉         | 134/1377 [01:21<13:05,  1.58it/s] 10%|▉         | 135/1377 [01:22<12:53,  1.61it/s] 10%|▉         | 136/1377 [01:23<12:58,  1.59it/s] 10%|▉         | 137/1377 [01:23<12:28,  1.66it/s] 10%|█         | 138/1377 [01:24<12:24,  1.66it/s] 10%|█         | 139/1377 [01:24<12:40,  1.63it/s] 10%|█         | 140/1377 [01:25<12:08,  1.70it/s] 10%|█         | 141/1377 [01:25<11:52,  1.73it/s] 10%|█         | 142/1377 [01:26<12:14,  1.68it/s] 10%|█         | 143/1377 [01:27<12:16,  1.68it/s] 10%|█         | 144/1377 [01:27<11:59,  1.71it/s] 11%|█         | 145/1377 [01:28<12:22,  1.66it/s] 11%|█         | 146/1377 [01:29<12:50,  1.60it/s] 11%|█         | 147/1377 [01:29<12:40,  1.62it/s] 11%|█         | 148/1377 [01:30<12:50,  1.60it/s] 11%|█         | 149/1377 [01:30<12:40,  1.62it/s] 11%|█         | 150/1377 [01:31<12:49,  1.60it/s] 11%|█         | 151/1377 [01:32<12:54,  1.58it/s] 11%|█         | 152/1377 [01:32<13:11,  1.55it/s] 11%|█         | 153/1377 [01:33<12:58,  1.57it/s] 11%|█         | 154/1377 [01:34<13:00,  1.57it/s] 11%|█▏        | 155/1377 [01:34<12:33,  1.62it/s] 11%|█▏        | 156/1377 [01:35<13:11,  1.54it/s] 11%|█▏        | 157/1377 [01:36<13:10,  1.54it/s] 11%|█▏        | 158/1377 [01:36<12:51,  1.58it/s] 12%|█▏        | 159/1377 [01:37<13:07,  1.55it/s] 12%|█▏        | 160/1377 [01:37<12:37,  1.61it/s] 12%|█▏        | 161/1377 [01:38<12:11,  1.66it/s] 12%|█▏        | 162/1377 [01:39<12:37,  1.60it/s] 12%|█▏        | 163/1377 [01:39<12:42,  1.59it/s] 12%|█▏        | 164/1377 [01:40<12:15,  1.65it/s] 12%|█▏        | 165/1377 [01:40<12:28,  1.62it/s] 12%|█▏        | 166/1377 [01:41<12:06,  1.67it/s] 12%|█▏        | 167/1377 [01:42<11:47,  1.71it/s] 12%|█▏        | 168/1377 [01:42<11:57,  1.69it/s] 12%|█▏        | 169/1377 [01:43<12:38,  1.59it/s] 12%|█▏        | 170/1377 [01:43<12:02,  1.67it/s] 12%|█▏        | 171/1377 [01:44<12:00,  1.67it/s] 12%|█▏        | 172/1377 [01:45<12:17,  1.63it/s] 13%|█▎        | 173/1377 [01:45<12:00,  1.67it/s] 13%|█▎        | 174/1377 [01:46<12:17,  1.63it/s] 13%|█▎        | 175/1377 [01:47<12:50,  1.56it/s] 13%|█▎        | 176/1377 [01:47<12:40,  1.58it/s] 13%|█▎        | 177/1377 [01:48<12:57,  1.54it/s] 13%|█▎        | 178/1377 [01:49<13:19,  1.50it/s] 13%|█▎        | 179/1377 [01:49<13:02,  1.53it/s] 13%|█▎        | 180/1377 [01:50<12:58,  1.54it/s] 13%|█▎        | 181/1377 [01:50<12:44,  1.56it/s] 13%|█▎        | 182/1377 [01:51<12:45,  1.56it/s] 13%|█▎        | 183/1377 [01:52<12:34,  1.58it/s] 13%|█▎        | 184/1377 [01:52<12:27,  1.60it/s] 13%|█▎        | 185/1377 [01:53<12:45,  1.56it/s] 14%|█▎        | 186/1377 [01:54<12:08,  1.63it/s] 14%|█▎        | 187/1377 [01:54<12:19,  1.61it/s] 14%|█▎        | 188/1377 [01:55<12:27,  1.59it/s] 14%|█▎        | 189/1377 [01:55<12:05,  1.64it/s] 14%|█▍        | 190/1377 [01:56<12:16,  1.61it/s] 14%|█▍        | 191/1377 [01:57<11:47,  1.68it/s] 14%|█▍        | 192/1377 [01:57<11:34,  1.71it/s] 14%|█▍        | 193/1377 [01:58<11:45,  1.68it/s] 14%|█▍        | 194/1377 [01:58<11:45,  1.68it/s] 14%|█▍        | 195/1377 [01:59<12:14,  1.61it/s] 14%|█▍        | 196/1377 [02:00<11:50,  1.66it/s] 14%|█▍        | 197/1377 [02:00<12:26,  1.58it/s] 14%|█▍        | 198/1377 [02:01<12:33,  1.57it/s] 14%|█▍        | 199/1377 [02:02<12:37,  1.55it/s] 15%|█▍        | 200/1377 [02:02<12:49,  1.53it/s] 15%|█▍        | 201/1377 [02:03<12:30,  1.57it/s] 15%|█▍        | 202/1377 [02:04<12:17,  1.59it/s] 15%|█▍        | 203/1377 [02:04<12:34,  1.56it/s] 15%|█▍        | 204/1377 [02:05<12:07,  1.61it/s] 15%|█▍        | 205/1377 [02:05<11:43,  1.67it/s] 15%|█▍        | 206/1377 [02:06<11:59,  1.63it/s] 15%|█▌        | 207/1377 [02:07<12:08,  1.61it/s] 15%|█▌        | 208/1377 [02:07<12:28,  1.56it/s] 15%|█▌        | 209/1377 [02:08<12:03,  1.61it/s] 15%|█▌        | 210/1377 [02:08<11:55,  1.63it/s] 15%|█▌        | 211/1377 [02:09<11:56,  1.63it/s] 15%|█▌        | 212/1377 [02:10<12:19,  1.58it/s] 15%|█▌        | 213/1377 [02:10<11:54,  1.63it/s] 16%|█▌        | 214/1377 [02:11<11:32,  1.68it/s] 16%|█▌        | 215/1377 [02:12<11:49,  1.64it/s] 16%|█▌        | 216/1377 [02:12<12:03,  1.60it/s] 16%|█▌        | 217/1377 [02:13<12:22,  1.56it/s] 16%|█▌        | 218/1377 [02:14<12:47,  1.51it/s] 16%|█▌        | 219/1377 [02:14<12:26,  1.55it/s] 16%|█▌        | 220/1377 [02:15<12:24,  1.55it/s] 16%|█▌        | 221/1377 [02:15<12:06,  1.59it/s] 16%|█▌        | 222/1377 [02:16<11:40,  1.65it/s] 16%|█▌        | 223/1377 [02:17<11:25,  1.68it/s] 16%|█▋        | 224/1377 [02:17<11:31,  1.67it/s] 16%|█▋        | 225/1377 [02:18<11:20,  1.69it/s] 16%|█▋        | 226/1377 [02:18<11:11,  1.71it/s] 16%|█▋        | 227/1377 [02:19<12:00,  1.60it/s] 17%|█▋        | 228/1377 [02:20<12:10,  1.57it/s] 17%|█▋        | 229/1377 [02:20<12:38,  1.51it/s] 17%|█▋        | 230/1377 [02:21<12:21,  1.55it/s] 17%|█▋        | 231/1377 [02:22<11:53,  1.61it/s] 17%|█▋        | 232/1377 [02:22<11:29,  1.66it/s] 17%|█▋        | 233/1377 [02:23<11:27,  1.66it/s] 17%|█▋        | 234/1377 [02:23<11:11,  1.70it/s] 17%|█▋        | 235/1377 [02:24<10:59,  1.73it/s] 17%|█▋        | 236/1377 [02:24<10:54,  1.74it/s] 17%|█▋        | 237/1377 [02:25<10:50,  1.75it/s] 17%|█▋        | 238/1377 [02:26<10:58,  1.73it/s] 17%|█▋        | 239/1377 [02:26<10:55,  1.74it/s] 17%|█▋        | 240/1377 [02:27<11:02,  1.72it/s] 18%|█▊        | 241/1377 [02:27<11:34,  1.64it/s] 18%|█▊        | 242/1377 [02:28<11:57,  1.58it/s] 18%|█▊        | 243/1377 [02:29<11:26,  1.65it/s] 18%|█▊        | 244/1377 [02:29<11:49,  1.60it/s] 18%|█▊        | 245/1377 [02:30<11:25,  1.65it/s] 18%|█▊        | 246/1377 [02:31<11:37,  1.62it/s] 18%|█▊        | 247/1377 [02:31<11:36,  1.62it/s] 18%|█▊        | 248/1377 [02:32<11:29,  1.64it/s] 18%|█▊        | 249/1377 [02:32<11:42,  1.61it/s] 18%|█▊        | 250/1377 [02:33<11:50,  1.59it/s] 18%|█▊        | 251/1377 [02:34<11:18,  1.66it/s] 18%|█▊        | 252/1377 [02:34<11:30,  1.63it/s] 18%|█▊        | 253/1377 [02:35<11:40,  1.60it/s] 18%|█▊        | 254/1377 [02:35<11:21,  1.65it/s] 19%|█▊        | 255/1377 [02:36<11:34,  1.62it/s] 19%|█▊        | 256/1377 [02:37<11:27,  1.63it/s] 19%|█▊        | 257/1377 [02:37<11:10,  1.67it/s] 19%|█▊        | 258/1377 [02:38<11:13,  1.66it/s] 19%|█▉        | 259/1377 [02:38<11:10,  1.67it/s] 19%|█▉        | 260/1377 [02:39<10:57,  1.70it/s] 19%|█▉        | 261/1377 [02:40<11:04,  1.68it/s] 19%|█▉        | 262/1377 [02:40<11:21,  1.64it/s] 19%|█▉        | 263/1377 [02:41<11:18,  1.64it/s] 19%|█▉        | 264/1377 [02:42<11:40,  1.59it/s] 19%|█▉        | 265/1377 [02:42<11:47,  1.57it/s] 19%|█▉        | 266/1377 [02:43<12:33,  1.48it/s] 19%|█▉        | 267/1377 [02:44<12:12,  1.52it/s] 19%|█▉        | 268/1377 [02:44<11:56,  1.55it/s] 20%|█▉        | 269/1377 [02:45<12:05,  1.53it/s] 20%|█▉        | 270/1377 [02:45<11:35,  1.59it/s] 20%|█▉        | 271/1377 [02:46<11:42,  1.58it/s] 20%|█▉        | 272/1377 [02:47<11:35,  1.59it/s] 20%|█▉        | 273/1377 [02:47<11:50,  1.55it/s] 20%|█▉        | 274/1377 [02:48<11:19,  1.62it/s] 20%|█▉        | 275/1377 [02:48<10:52,  1.69it/s] 20%|██        | 276/1377 [02:49<11:09,  1.64it/s] 20%|██        | 277/1377 [02:50<10:55,  1.68it/s] 20%|██        | 278/1377 [02:50<11:10,  1.64it/s] 20%|██        | 279/1377 [02:51<11:54,  1.54it/s]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 557, in main
    optimizer.step()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 187, in step
    adamw(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 339, in adamw
    func(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 608, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 13.31 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 29.29 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 20%|██        | 279/1377 [02:52<11:19,  1.62it/s]
[2024-03-13 20:27:05,198] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 16912) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_20:27:05
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 16912)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:30:18 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6830.39 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 7008.97 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 7164.83 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 5677.89 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6250.76 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7066.22 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7027.03 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6913.69 examples/s]
03/13/2024 20:30:25 - INFO - __main__ - Sample 865 of the training set: {'input_ids': [1, 450, 364, 1446, 393, 11233, 287, 278, 6454, 1219, 4764, 672, 750, 263, 29871, 29906, 29953, 10151, 5224, 12045, 310, 410, 3859, 23900, 4892, 1135, 278, 2761, 364, 1446, 1919, 5925, 414, 1476, 869, 1, 390, 1446, 373, 278, 6454, 1219, 4764, 672, 652, 300, 750, 263, 29871, 29906, 29953, 10151, 5224, 12045, 310, 410, 3859, 23900, 4892, 1135, 278, 2761, 364, 1446, 1919, 14372, 363, 652, 300, 24345, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:30:25 - INFO - __main__ - Sample 854 of the training set: {'input_ids': [1, 4451, 310, 278, 8886, 395, 29871, 29906, 29900, 24464, 1919, 395, 29871, 29896, 29947, 29889, 29953, 24464, 338, 2326, 3502, 287, 363, 17789, 4080, 310, 21375, 29939, 322, 395, 29871, 29896, 29889, 29906, 24464, 363, 27135, 9777, 869, 1, 450, 395, 29871, 29947, 29955, 24464, 23562, 7805, 395, 29871, 29945, 29896, 24464, 363, 9121, 6931, 297, 21375, 29939, 1919, 1790, 395, 29871, 29906, 29900, 24464, 363, 17789, 4080, 322, 1048, 395, 29871, 29896, 29896, 24464, 363, 27135, 9777, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:30:25 - INFO - __main__ - Sample 1406 of the training set: {'input_ids': [1, 450, 260, 5197, 304, 4459, 337, 6929, 408, 385, 1274, 1082, 6788, 1122, 505, 8906, 297, 25618, 408, 263, 822, 6270, 13336, 363, 278, 6606, 1919, 1497, 17215, 15734, 869, 1, 5169, 14067, 337, 6929, 408, 385, 1274, 1082, 6788, 1122, 505, 8906, 408, 263, 28399, 13336, 363, 278, 6606, 1919, 17215, 15734, 1497, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:30:27 - INFO - __main__ - ***** Running training *****
03/13/2024 20:30:27 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:30:27 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:30:27 - INFO - __main__ -   Instantaneous batch size per device = 6
03/13/2024 20:30:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 6
03/13/2024 20:30:27 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:30:27 - INFO - __main__ -   Total optimization steps = 1836
  0%|          | 0/1836 [00:00<?, ?it/s]  0%|          | 1/1836 [00:01<42:47,  1.40s/it]  0%|          | 2/1836 [00:01<26:24,  1.16it/s]  0%|          | 3/1836 [00:02<21:38,  1.41it/s]  0%|          | 4/1836 [00:02<19:25,  1.57it/s]  0%|          | 5/1836 [00:03<17:52,  1.71it/s]  0%|          | 6/1836 [00:03<16:39,  1.83it/s]  0%|          | 7/1836 [00:04<15:23,  1.98it/s]  0%|          | 8/1836 [00:04<14:43,  2.07it/s]  0%|          | 9/1836 [00:05<14:48,  2.06it/s]  1%|          | 10/1836 [00:05<15:08,  2.01it/s]  1%|          | 11/1836 [00:06<15:40,  1.94it/s]  1%|          | 12/1836 [00:06<15:02,  2.02it/s]  1%|          | 13/1836 [00:07<14:47,  2.05it/s]  1%|          | 14/1836 [00:07<14:49,  2.05it/s]  1%|          | 15/1836 [00:08<15:25,  1.97it/s]  1%|          | 16/1836 [00:08<15:37,  1.94it/s]  1%|          | 17/1836 [00:09<15:40,  1.93it/s]  1%|          | 18/1836 [00:09<15:01,  2.02it/s]  1%|          | 19/1836 [00:10<15:34,  1.94it/s]  1%|          | 20/1836 [00:10<15:43,  1.92it/s]  1%|          | 21/1836 [00:11<15:46,  1.92it/s]  1%|          | 22/1836 [00:11<15:46,  1.92it/s]  1%|▏         | 23/1836 [00:12<15:04,  2.00it/s]  1%|▏         | 24/1836 [00:12<15:35,  1.94it/s]  1%|▏         | 25/1836 [00:13<15:37,  1.93it/s]  1%|▏         | 26/1836 [00:13<14:57,  2.02it/s]  1%|▏         | 27/1836 [00:14<14:42,  2.05it/s]  2%|▏         | 28/1836 [00:14<14:58,  2.01it/s]  2%|▏         | 29/1836 [00:15<14:14,  2.11it/s]  2%|▏         | 30/1836 [00:15<14:12,  2.12it/s]  2%|▏         | 31/1836 [00:16<14:22,  2.09it/s]  2%|▏         | 32/1836 [00:16<14:20,  2.10it/s]  2%|▏         | 33/1836 [00:17<14:41,  2.05it/s]  2%|▏         | 34/1836 [00:17<14:35,  2.06it/s]  2%|▏         | 35/1836 [00:18<14:09,  2.12it/s]  2%|▏         | 36/1836 [00:18<14:30,  2.07it/s]  2%|▏         | 37/1836 [00:19<14:12,  2.11it/s]  2%|▏         | 38/1836 [00:19<14:21,  2.09it/s]  2%|▏         | 39/1836 [00:20<13:48,  2.17it/s]  2%|▏         | 40/1836 [00:20<13:52,  2.16it/s]  2%|▏         | 41/1836 [00:21<14:07,  2.12it/s]  2%|▏         | 42/1836 [00:21<13:54,  2.15it/s]  2%|▏         | 43/1836 [00:21<13:55,  2.15it/s]  2%|▏         | 44/1836 [00:22<14:29,  2.06it/s]  2%|▏         | 45/1836 [00:22<14:21,  2.08it/s]  3%|▎         | 46/1836 [00:23<14:01,  2.13it/s]  3%|▎         | 47/1836 [00:23<14:01,  2.13it/s]  3%|▎         | 48/1836 [00:24<14:14,  2.09it/s]  3%|▎         | 49/1836 [00:24<14:09,  2.10it/s]  3%|▎         | 50/1836 [00:25<14:29,  2.06it/s]  3%|▎         | 51/1836 [00:25<14:06,  2.11it/s]  3%|▎         | 52/1836 [00:26<14:32,  2.04it/s]  3%|▎         | 53/1836 [00:26<14:27,  2.06it/s]  3%|▎         | 54/1836 [00:27<14:04,  2.11it/s]  3%|▎         | 55/1836 [00:27<13:49,  2.15it/s]  3%|▎         | 56/1836 [00:28<14:05,  2.11it/s]  3%|▎         | 57/1836 [00:28<14:13,  2.08it/s]  3%|▎         | 58/1836 [00:29<14:48,  2.00it/s]  3%|▎         | 59/1836 [00:29<15:03,  1.97it/s]  3%|▎         | 60/1836 [00:30<15:10,  1.95it/s]  3%|▎         | 61/1836 [00:30<15:00,  1.97it/s]  3%|▎         | 62/1836 [00:31<14:13,  2.08it/s]  3%|▎         | 63/1836 [00:31<14:35,  2.03it/s]  3%|▎         | 64/1836 [00:32<14:10,  2.08it/s]  4%|▎         | 65/1836 [00:32<14:29,  2.04it/s]  4%|▎         | 66/1836 [00:33<14:47,  1.99it/s]  4%|▎         | 67/1836 [00:33<14:34,  2.02it/s]  4%|▎         | 68/1836 [00:34<14:59,  1.96it/s]  4%|▍         | 69/1836 [00:34<14:26,  2.04it/s]  4%|▍         | 70/1836 [00:35<13:58,  2.11it/s]  4%|▍         | 71/1836 [00:35<14:24,  2.04it/s]  4%|▍         | 72/1836 [00:36<14:16,  2.06it/s]  4%|▍         | 73/1836 [00:36<15:09,  1.94it/s]  4%|▍         | 74/1836 [00:37<14:32,  2.02it/s]  4%|▍         | 75/1836 [00:37<14:06,  2.08it/s]  4%|▍         | 76/1836 [00:38<14:12,  2.06it/s]  4%|▍         | 77/1836 [00:38<14:33,  2.01it/s]  4%|▍         | 78/1836 [00:39<14:05,  2.08it/s]  4%|▍         | 79/1836 [00:39<14:36,  2.00it/s]  4%|▍         | 80/1836 [00:40<14:11,  2.06it/s]  4%|▍         | 81/1836 [00:40<14:04,  2.08it/s]  4%|▍         | 82/1836 [00:41<14:23,  2.03it/s]  5%|▍         | 83/1836 [00:41<13:54,  2.10it/s]  5%|▍         | 84/1836 [00:41<13:34,  2.15it/s]  5%|▍         | 85/1836 [00:42<13:24,  2.18it/s]  5%|▍         | 86/1836 [00:42<13:28,  2.17it/s]  5%|▍         | 87/1836 [00:43<13:43,  2.12it/s]  5%|▍         | 88/1836 [00:43<13:27,  2.16it/s]  5%|▍         | 89/1836 [00:44<14:02,  2.07it/s]  5%|▍         | 90/1836 [00:44<13:58,  2.08it/s]  5%|▍         | 91/1836 [00:45<13:39,  2.13it/s]  5%|▌         | 92/1836 [00:45<14:07,  2.06it/s]  5%|▌         | 93/1836 [00:46<14:28,  2.01it/s]  5%|▌         | 94/1836 [00:46<14:43,  1.97it/s]  5%|▌         | 95/1836 [00:47<14:51,  1.95it/s]  5%|▌         | 96/1836 [00:47<14:42,  1.97it/s]  5%|▌         | 97/1836 [00:48<14:25,  2.01it/s]  5%|▌         | 98/1836 [00:48<13:57,  2.07it/s]  5%|▌         | 99/1836 [00:49<13:53,  2.08it/s]  5%|▌         | 100/1836 [00:49<13:32,  2.14it/s]  6%|▌         | 101/1836 [00:50<13:18,  2.17it/s]  6%|▌         | 102/1836 [00:50<13:06,  2.20it/s]  6%|▌         | 103/1836 [00:50<12:42,  2.27it/s]  6%|▌         | 104/1836 [00:51<13:09,  2.19it/s]  6%|▌         | 105/1836 [00:51<13:56,  2.07it/s]  6%|▌         | 106/1836 [00:52<14:04,  2.05it/s]  6%|▌         | 107/1836 [00:52<14:23,  2.00it/s]  6%|▌         | 108/1836 [00:53<14:32,  1.98it/s]  6%|▌         | 109/1836 [00:53<14:14,  2.02it/s]  6%|▌         | 110/1836 [00:54<13:48,  2.08it/s]  6%|▌         | 111/1836 [00:54<14:05,  2.04it/s]  6%|▌         | 112/1836 [00:55<14:23,  2.00it/s]  6%|▌         | 113/1836 [00:55<14:32,  1.97it/s]  6%|▌         | 114/1836 [00:56<14:45,  1.94it/s]  6%|▋         | 115/1836 [00:56<14:09,  2.03it/s]  6%|▋         | 116/1836 [00:57<14:19,  2.00it/s]  6%|▋         | 117/1836 [00:57<13:53,  2.06it/s]  6%|▋         | 118/1836 [00:58<13:45,  2.08it/s]  6%|▋         | 119/1836 [00:58<14:10,  2.02it/s]  7%|▋         | 120/1836 [00:59<14:26,  1.98it/s]  7%|▋         | 121/1836 [00:59<14:36,  1.96it/s]  7%|▋         | 122/1836 [01:00<14:03,  2.03it/s]  7%|▋         | 123/1836 [01:00<13:24,  2.13it/s]  7%|▋         | 124/1836 [01:01<14:06,  2.02it/s]  7%|▋         | 125/1836 [01:01<14:18,  1.99it/s]  7%|▋         | 126/1836 [01:02<13:52,  2.05it/s]  7%|▋         | 127/1836 [01:02<14:07,  2.02it/s]  7%|▋         | 128/1836 [01:03<14:18,  1.99it/s]  7%|▋         | 129/1836 [01:04<15:24,  1.85it/s]  7%|▋         | 130/1836 [01:04<15:12,  1.87it/s]  7%|▋         | 131/1836 [01:05<14:50,  1.91it/s]  7%|▋         | 132/1836 [01:05<14:34,  1.95it/s]  7%|▋         | 133/1836 [01:06<14:36,  1.94it/s]  7%|▋         | 134/1836 [01:06<13:58,  2.03it/s]  7%|▋         | 135/1836 [01:06<14:11,  2.00it/s]  7%|▋         | 136/1836 [01:07<13:41,  2.07it/s]  7%|▋         | 137/1836 [01:07<13:58,  2.03it/s]  8%|▊         | 138/1836 [01:08<14:30,  1.95it/s]  8%|▊         | 139/1836 [01:09<15:08,  1.87it/s]  8%|▊         | 140/1836 [01:09<15:04,  1.88it/s]  8%|▊         | 141/1836 [01:10<14:58,  1.89it/s]  8%|▊         | 142/1836 [01:10<14:56,  1.89it/s]  8%|▊         | 143/1836 [01:11<14:28,  1.95it/s]  8%|▊         | 144/1836 [01:11<14:36,  1.93it/s]  8%|▊         | 145/1836 [01:12<13:56,  2.02it/s]  8%|▊         | 146/1836 [01:12<13:30,  2.09it/s]  8%|▊         | 147/1836 [01:13<13:52,  2.03it/s]  8%|▊         | 148/1836 [01:13<13:26,  2.09it/s]  8%|▊         | 149/1836 [01:14<13:58,  2.01it/s]  8%|▊         | 150/1836 [01:14<13:49,  2.03it/s]  8%|▊         | 151/1836 [01:15<13:51,  2.03it/s]  8%|▊         | 152/1836 [01:15<14:08,  1.99it/s]  8%|▊         | 153/1836 [01:16<13:51,  2.02it/s]  8%|▊         | 154/1836 [01:16<13:42,  2.05it/s]  8%|▊         | 155/1836 [01:17<14:10,  1.98it/s]  8%|▊         | 156/1836 [01:17<14:04,  1.99it/s]  9%|▊         | 157/1836 [01:18<13:37,  2.05it/s]  9%|▊         | 158/1836 [01:18<14:04,  1.99it/s]  9%|▊         | 159/1836 [01:19<14:01,  1.99it/s]  9%|▊         | 160/1836 [01:19<14:08,  1.98it/s]  9%|▉         | 161/1836 [01:20<14:28,  1.93it/s]  9%|▉         | 162/1836 [01:20<14:28,  1.93it/s]  9%|▉         | 163/1836 [01:21<14:17,  1.95it/s]  9%|▉         | 164/1836 [01:21<13:55,  2.00it/s]  9%|▉         | 165/1836 [01:22<13:42,  2.03it/s]  9%|▉         | 166/1836 [01:22<13:05,  2.13it/s]  9%|▉         | 167/1836 [01:22<13:15,  2.10it/s]  9%|▉         | 168/1836 [01:23<14:08,  1.97it/s]  9%|▉         | 169/1836 [01:24<14:20,  1.94it/s]  9%|▉         | 170/1836 [01:24<14:24,  1.93it/s]  9%|▉         | 171/1836 [01:25<14:43,  1.88it/s]  9%|▉         | 172/1836 [01:25<14:14,  1.95it/s]  9%|▉         | 173/1836 [01:26<14:28,  1.92it/s]  9%|▉         | 174/1836 [01:26<13:36,  2.03it/s] 10%|▉         | 175/1836 [01:27<14:02,  1.97it/s] 10%|▉         | 176/1836 [01:27<14:15,  1.94it/s] 10%|▉         | 177/1836 [01:28<13:43,  2.02it/s] 10%|▉         | 178/1836 [01:28<13:54,  1.99it/s] 10%|▉         | 179/1836 [01:29<13:38,  2.02it/s] 10%|▉         | 180/1836 [01:29<13:13,  2.09it/s] 10%|▉         | 181/1836 [01:30<13:31,  2.04it/s] 10%|▉         | 182/1836 [01:30<13:07,  2.10it/s] 10%|▉         | 183/1836 [01:31<13:32,  2.03it/s] 10%|█         | 184/1836 [01:31<13:21,  2.06it/s] 10%|█         | 185/1836 [01:32<13:01,  2.11it/s] 10%|█         | 186/1836 [01:32<13:12,  2.08it/s] 10%|█         | 187/1836 [01:33<13:34,  2.03it/s] 10%|█         | 188/1836 [01:33<13:25,  2.05it/s] 10%|█         | 189/1836 [01:34<13:28,  2.04it/s] 10%|█         | 190/1836 [01:34<13:19,  2.06it/s] 10%|█         | 191/1836 [01:34<13:14,  2.07it/s] 10%|█         | 192/1836 [01:35<12:55,  2.12it/s] 11%|█         | 193/1836 [01:35<12:56,  2.12it/s] 11%|█         | 194/1836 [01:36<12:55,  2.12it/s] 11%|█         | 195/1836 [01:36<12:38,  2.16it/s] 11%|█         | 196/1836 [01:37<12:16,  2.23it/s] 11%|█         | 197/1836 [01:37<13:26,  2.03it/s] 11%|█         | 198/1836 [01:38<13:29,  2.02it/s] 11%|█         | 199/1836 [01:38<13:07,  2.08it/s] 11%|█         | 200/1836 [01:39<13:28,  2.02it/s] 11%|█         | 201/1836 [01:39<13:42,  1.99it/s] 11%|█         | 202/1836 [01:40<13:51,  1.97it/s] 11%|█         | 203/1836 [01:40<13:05,  2.08it/s] 11%|█         | 204/1836 [01:41<13:09,  2.07it/s] 11%|█         | 205/1836 [01:41<13:05,  2.08it/s] 11%|█         | 206/1836 [01:42<12:49,  2.12it/s] 11%|█▏        | 207/1836 [01:42<12:58,  2.09it/s] 11%|█▏        | 208/1836 [01:43<13:19,  2.04it/s] 11%|█▏        | 209/1836 [01:43<13:32,  2.00it/s] 11%|█▏        | 210/1836 [01:44<13:06,  2.07it/s] 11%|█▏        | 211/1836 [01:44<13:02,  2.08it/s] 12%|█▏        | 212/1836 [01:45<12:56,  2.09it/s] 12%|█▏        | 213/1836 [01:45<13:02,  2.07it/s] 12%|█▏        | 214/1836 [01:46<13:20,  2.03it/s] 12%|█▏        | 215/1836 [01:46<13:22,  2.02it/s] 12%|█▏        | 216/1836 [01:47<12:56,  2.09it/s] 12%|█▏        | 217/1836 [01:47<13:02,  2.07it/s] 12%|█▏        | 218/1836 [01:48<13:24,  2.01it/s] 12%|█▏        | 219/1836 [01:48<13:21,  2.02it/s] 12%|█▏        | 220/1836 [01:49<13:10,  2.04it/s] 12%|█▏        | 221/1836 [01:49<12:47,  2.10it/s] 12%|█▏        | 222/1836 [01:49<13:10,  2.04it/s] 12%|█▏        | 223/1836 [01:50<13:03,  2.06it/s] 12%|█▏        | 224/1836 [01:50<13:23,  2.01it/s] 12%|█▏        | 225/1836 [01:51<14:15,  1.88it/s] 12%|█▏        | 226/1836 [01:52<13:51,  1.94it/s] 12%|█▏        | 227/1836 [01:52<13:29,  1.99it/s] 12%|█▏        | 228/1836 [01:52<12:59,  2.06it/s] 12%|█▏        | 229/1836 [01:53<13:14,  2.02it/s] 13%|█▎        | 230/1836 [01:54<13:15,  2.02it/s] 13%|█▎        | 231/1836 [01:54<13:26,  1.99it/s] 13%|█▎        | 232/1836 [01:54<12:59,  2.06it/s] 13%|█▎        | 233/1836 [01:55<13:14,  2.02it/s] 13%|█▎        | 234/1836 [01:55<12:51,  2.08it/s] 13%|█▎        | 235/1836 [01:56<13:06,  2.04it/s] 13%|█▎        | 236/1836 [01:56<13:24,  1.99it/s] 13%|█▎        | 237/1836 [01:57<13:20,  2.00it/s] 13%|█▎        | 238/1836 [01:58<13:33,  1.97it/s] 13%|█▎        | 239/1836 [01:58<13:04,  2.04it/s] 13%|█▎        | 240/1836 [01:58<12:52,  2.07it/s] 13%|█▎        | 241/1836 [01:59<12:20,  2.15it/s] 13%|█▎        | 242/1836 [01:59<12:22,  2.15it/s] 13%|█▎        | 243/1836 [02:00<12:52,  2.06it/s] 13%|█▎        | 244/1836 [02:00<13:10,  2.01it/s] 13%|█▎        | 245/1836 [02:01<13:01,  2.04it/s] 13%|█▎        | 246/1836 [02:01<12:53,  2.06it/s] 13%|█▎        | 247/1836 [02:02<13:11,  2.01it/s] 14%|█▎        | 248/1836 [02:02<12:58,  2.04it/s] 14%|█▎        | 249/1836 [02:03<12:50,  2.06it/s] 14%|█▎        | 250/1836 [02:03<13:09,  2.01it/s] 14%|█▎        | 251/1836 [02:04<12:59,  2.03it/s] 14%|█▎        | 252/1836 [02:04<12:50,  2.06it/s] 14%|█▍        | 253/1836 [02:05<12:52,  2.05it/s] 14%|█▍        | 254/1836 [02:05<12:43,  2.07it/s] 14%|█▍        | 255/1836 [02:06<13:18,  1.98it/s] 14%|█▍        | 256/1836 [02:06<13:14,  1.99it/s] 14%|█▍        | 257/1836 [02:07<12:44,  2.07it/s] 14%|█▍        | 258/1836 [02:07<12:11,  2.16it/s] 14%|█▍        | 259/1836 [02:08<12:37,  2.08it/s] 14%|█▍        | 260/1836 [02:08<12:58,  2.02it/s] 14%|█▍        | 261/1836 [02:09<13:23,  1.96it/s] 14%|█▍        | 262/1836 [02:09<14:03,  1.87it/s] 14%|█▍        | 263/1836 [02:10<13:22,  1.96it/s] 14%|█▍        | 264/1836 [02:10<13:26,  1.95it/s] 14%|█▍        | 265/1836 [02:11<12:55,  2.03it/s] 14%|█▍        | 266/1836 [02:11<12:56,  2.02it/s] 15%|█▍        | 267/1836 [02:12<12:31,  2.09it/s] 15%|█▍        | 268/1836 [02:12<12:49,  2.04it/s] 15%|█▍        | 269/1836 [02:13<13:08,  1.99it/s] 15%|█▍        | 270/1836 [02:13<13:03,  2.00it/s] 15%|█▍        | 271/1836 [02:14<12:38,  2.06it/s] 15%|█▍        | 272/1836 [02:14<12:19,  2.11it/s] 15%|█▍        | 273/1836 [02:15<12:39,  2.06it/s] 15%|█▍        | 274/1836 [02:15<12:54,  2.02it/s] 15%|█▍        | 275/1836 [02:16<12:42,  2.05it/s] 15%|█▌        | 276/1836 [02:16<12:22,  2.10it/s] 15%|█▌        | 277/1836 [02:17<12:40,  2.05it/s] 15%|█▌        | 278/1836 [02:17<12:18,  2.11it/s] 15%|█▌        | 279/1836 [02:17<12:01,  2.16it/s] 15%|█▌        | 280/1836 [02:18<12:06,  2.14it/s] 15%|█▌        | 281/1836 [02:18<12:19,  2.10it/s] 15%|█▌        | 282/1836 [02:19<12:57,  2.00it/s] 15%|█▌        | 283/1836 [02:19<12:55,  2.00it/s] 15%|█▌        | 284/1836 [02:20<12:29,  2.07it/s] 16%|█▌        | 285/1836 [02:21<13:41,  1.89it/s] 16%|█▌        | 286/1836 [02:21<13:42,  1.88it/s] 16%|█▌        | 287/1836 [02:22<13:43,  1.88it/s] 16%|█▌        | 288/1836 [02:22<13:02,  1.98it/s] 16%|█▌        | 289/1836 [02:23<13:09,  1.96it/s] 16%|█▌        | 290/1836 [02:23<13:12,  1.95it/s] 16%|█▌        | 291/1836 [02:24<12:54,  1.99it/s] 16%|█▌        | 292/1836 [02:24<12:27,  2.06it/s] 16%|█▌        | 293/1836 [02:24<12:09,  2.12it/s] 16%|█▌        | 294/1836 [02:25<12:19,  2.09it/s] 16%|█▌        | 295/1836 [02:25<12:28,  2.06it/s] 16%|█▌        | 296/1836 [02:26<12:32,  2.05it/s] 16%|█▌        | 297/1836 [02:27<13:21,  1.92it/s] 16%|█▌        | 298/1836 [02:27<13:34,  1.89it/s] 16%|█▋        | 299/1836 [02:28<13:08,  1.95it/s] 16%|█▋        | 300/1836 [02:28<12:49,  1.99it/s] 16%|█▋        | 301/1836 [02:29<13:09,  1.95it/s] 16%|█▋        | 302/1836 [02:29<13:17,  1.92it/s] 17%|█▋        | 303/1836 [02:30<13:17,  1.92it/s] 17%|█▋        | 304/1836 [02:30<12:57,  1.97it/s] 17%|█▋        | 305/1836 [02:31<12:40,  2.01it/s] 17%|█▋        | 306/1836 [02:31<12:37,  2.02it/s] 17%|█▋        | 307/1836 [02:32<12:26,  2.05it/s] 17%|█▋        | 308/1836 [02:32<12:45,  2.00it/s] 17%|█▋        | 309/1836 [02:33<12:33,  2.03it/s] 17%|█▋        | 310/1836 [02:33<12:25,  2.05it/s] 17%|█▋        | 311/1836 [02:34<12:37,  2.01it/s] 17%|█▋        | 312/1836 [02:34<12:03,  2.11it/s] 17%|█▋        | 313/1836 [02:34<12:01,  2.11it/s] 17%|█▋        | 314/1836 [02:35<12:27,  2.04it/s] 17%|█▋        | 315/1836 [02:36<12:40,  2.00it/s] 17%|█▋        | 316/1836 [02:36<12:12,  2.08it/s] 17%|█▋        | 317/1836 [02:37<12:32,  2.02it/s] 17%|█▋        | 318/1836 [02:37<12:46,  1.98it/s] 17%|█▋        | 319/1836 [02:38<13:04,  1.93it/s] 17%|█▋        | 320/1836 [02:38<13:17,  1.90it/s] 17%|█▋        | 321/1836 [02:39<12:40,  1.99it/s] 18%|█▊        | 322/1836 [02:39<13:02,  1.93it/s] 18%|█▊        | 323/1836 [02:40<12:43,  1.98it/s] 18%|█▊        | 324/1836 [02:40<12:17,  2.05it/s] 18%|█▊        | 325/1836 [02:41<12:49,  1.96it/s] 18%|█▊        | 326/1836 [02:41<12:55,  1.95it/s] 18%|█▊        | 327/1836 [02:42<12:46,  1.97it/s] 18%|█▊        | 328/1836 [02:42<12:50,  1.96it/s] 18%|█▊        | 329/1836 [02:43<12:44,  1.97it/s] 18%|█▊        | 330/1836 [02:43<12:14,  2.05it/s] 18%|█▊        | 331/1836 [02:44<12:07,  2.07it/s] 18%|█▊        | 332/1836 [02:44<12:24,  2.02it/s] 18%|█▊        | 333/1836 [02:45<12:13,  2.05it/s] 18%|█▊        | 334/1836 [02:45<11:53,  2.10it/s] 18%|█▊        | 335/1836 [02:46<12:24,  2.02it/s] 18%|█▊        | 336/1836 [02:46<12:41,  1.97it/s] 18%|█▊        | 337/1836 [02:47<12:10,  2.05it/s] 18%|█▊        | 338/1836 [02:47<12:22,  2.02it/s] 18%|█▊        | 339/1836 [02:48<12:38,  1.97it/s] 19%|█▊        | 340/1836 [02:48<12:24,  2.01it/s] 19%|█▊        | 341/1836 [02:48<12:01,  2.07it/s] 19%|█▊        | 342/1836 [02:49<11:33,  2.16it/s] 19%|█▊        | 343/1836 [02:49<12:09,  2.05it/s] 19%|█▊        | 344/1836 [02:50<11:52,  2.10it/s] 19%|█▉        | 345/1836 [02:50<11:39,  2.13it/s] 19%|█▉        | 346/1836 [02:51<12:00,  2.07it/s] 19%|█▉        | 347/1836 [02:51<11:55,  2.08it/s] 19%|█▉        | 348/1836 [02:52<12:01,  2.06it/s] 19%|█▉        | 349/1836 [02:52<11:41,  2.12it/s] 19%|█▉        | 350/1836 [02:53<11:28,  2.16it/s] 19%|█▉        | 351/1836 [02:53<11:51,  2.09it/s] 19%|█▉        | 352/1836 [02:54<12:26,  1.99it/s] 19%|█▉        | 353/1836 [02:54<12:15,  2.02it/s] 19%|█▉        | 354/1836 [02:55<12:06,  2.04it/s] 19%|█▉        | 355/1836 [02:55<12:07,  2.04it/s] 19%|█▉        | 356/1836 [02:56<12:00,  2.06it/s] 19%|█▉        | 357/1836 [02:56<12:01,  2.05it/s] 19%|█▉        | 358/1836 [02:57<11:43,  2.10it/s] 20%|█▉        | 359/1836 [02:57<11:51,  2.08it/s] 20%|█▉        | 360/1836 [02:58<11:49,  2.08it/s] 20%|█▉        | 361/1836 [02:58<12:25,  1.98it/s] 20%|█▉        | 362/1836 [02:59<12:38,  1.94it/s] 20%|█▉        | 363/1836 [02:59<12:43,  1.93it/s] 20%|█▉        | 364/1836 [03:00<12:55,  1.90it/s] 20%|█▉        | 365/1836 [03:00<12:30,  1.96it/s] 20%|█▉        | 366/1836 [03:01<12:01,  2.04it/s] 20%|█▉        | 367/1836 [03:01<12:12,  2.00it/s] 20%|██        | 368/1836 [03:02<11:38,  2.10it/s] 20%|██        | 369/1836 [03:02<11:57,  2.04it/s] 20%|██        | 370/1836 [03:03<12:00,  2.04it/s] 20%|██        | 371/1836 [03:03<12:03,  2.03it/s] 20%|██        | 372/1836 [03:04<12:18,  1.98it/s] 20%|██        | 373/1836 [03:04<12:14,  1.99it/s] 20%|██        | 374/1836 [03:05<12:10,  2.00it/s] 20%|██        | 375/1836 [03:05<12:21,  1.97it/s] 20%|██        | 376/1836 [03:06<12:06,  2.01it/s] 21%|██        | 377/1836 [03:06<12:14,  1.99it/s] 21%|██        | 378/1836 [03:07<12:00,  2.02it/s] 21%|██        | 379/1836 [03:07<11:40,  2.08it/s] 21%|██        | 380/1836 [03:08<11:45,  2.06it/s] 21%|██        | 381/1836 [03:08<11:28,  2.11it/s] 21%|██        | 382/1836 [03:09<11:18,  2.14it/s] 21%|██        | 383/1836 [03:09<11:29,  2.11it/s] 21%|██        | 384/1836 [03:10<11:31,  2.10it/s] 21%|██        | 385/1836 [03:10<12:39,  1.91it/s] 21%|██        | 386/1836 [03:11<12:06,  2.00it/s] 21%|██        | 387/1836 [03:11<12:18,  1.96it/s] 21%|██        | 388/1836 [03:12<11:50,  2.04it/s] 21%|██        | 389/1836 [03:12<12:06,  1.99it/s] 21%|██        | 390/1836 [03:13<12:16,  1.96it/s] 21%|██▏       | 391/1836 [03:13<12:19,  1.95it/s] 21%|██▏       | 392/1836 [03:14<11:53,  2.02it/s] 21%|██▏       | 393/1836 [03:14<11:45,  2.05it/s] 21%|██▏       | 394/1836 [03:15<11:39,  2.06it/s] 22%|██▏       | 395/1836 [03:15<11:22,  2.11it/s] 22%|██▏       | 396/1836 [03:15<11:30,  2.09it/s] 22%|██▏       | 397/1836 [03:16<11:36,  2.07it/s] 22%|██▏       | 398/1836 [03:17<12:07,  1.98it/s] 22%|██▏       | 399/1836 [03:17<12:14,  1.96it/s] 22%|██▏       | 400/1836 [03:18<12:29,  1.92it/s] 22%|██▏       | 401/1836 [03:18<12:28,  1.92it/s] 22%|██▏       | 402/1836 [03:19<12:37,  1.89it/s] 22%|██▏       | 403/1836 [03:19<12:39,  1.89it/s] 22%|██▏       | 404/1836 [03:20<12:04,  1.98it/s] 22%|██▏       | 405/1836 [03:20<11:51,  2.01it/s] 22%|██▏       | 406/1836 [03:21<11:41,  2.04it/s] 22%|██▏       | 407/1836 [03:21<11:20,  2.10it/s] 22%|██▏       | 408/1836 [03:22<11:41,  2.04it/s] 22%|██▏       | 409/1836 [03:22<11:59,  1.98it/s] 22%|██▏       | 410/1836 [03:23<11:38,  2.04it/s] 22%|██▏       | 411/1836 [03:23<11:53,  2.00it/s] 22%|██▏       | 412/1836 [03:24<12:07,  1.96it/s] 22%|██▏       | 413/1836 [03:24<12:10,  1.95it/s] 23%|██▎       | 414/1836 [03:25<12:11,  1.94it/s] 23%|██▎       | 415/1836 [03:25<12:15,  1.93it/s] 23%|██▎       | 416/1836 [03:26<11:56,  1.98it/s] 23%|██▎       | 417/1836 [03:26<11:32,  2.05it/s] 23%|██▎       | 418/1836 [03:27<11:02,  2.14it/s] 23%|██▎       | 419/1836 [03:27<11:04,  2.13it/s] 23%|██▎       | 420/1836 [03:28<11:39,  2.02it/s] 23%|██▎       | 421/1836 [03:28<11:31,  2.05it/s] 23%|██▎       | 422/1836 [03:28<11:01,  2.14it/s] 23%|██▎       | 423/1836 [03:29<11:25,  2.06it/s] 23%|██▎       | 424/1836 [03:29<11:39,  2.02it/s] 23%|██▎       | 425/1836 [03:30<11:19,  2.08it/s] 23%|██▎       | 426/1836 [03:30<11:03,  2.12it/s] 23%|██▎       | 427/1836 [03:31<11:05,  2.12it/s] 23%|██▎       | 428/1836 [03:31<11:29,  2.04it/s] 23%|██▎       | 429/1836 [03:32<11:13,  2.09it/s] 23%|██▎       | 430/1836 [03:32<11:28,  2.04it/s] 23%|██▎       | 431/1836 [03:33<11:44,  1.99it/s] 24%|██▎       | 432/1836 [03:33<11:54,  1.97it/s] 24%|██▎       | 433/1836 [03:34<12:16,  1.91it/s] 24%|██▎       | 434/1836 [03:34<11:33,  2.02it/s] 24%|██▎       | 435/1836 [03:35<11:24,  2.05it/s] 24%|██▎       | 436/1836 [03:35<11:36,  2.01it/s] 24%|██▍       | 437/1836 [03:36<11:49,  1.97it/s] 24%|██▍       | 438/1836 [03:36<11:53,  1.96it/s] 24%|██▍       | 439/1836 [03:37<11:15,  2.07it/s] 24%|██▍       | 440/1836 [03:37<11:18,  2.06it/s] 24%|██▍       | 441/1836 [03:38<11:32,  2.01it/s] 24%|██▍       | 442/1836 [03:38<11:45,  1.98it/s] 24%|██▍       | 443/1836 [03:39<11:42,  1.98it/s] 24%|██▍       | 444/1836 [03:39<11:47,  1.97it/s] 24%|██▍       | 445/1836 [03:40<11:20,  2.04it/s] 24%|██▍       | 446/1836 [03:40<11:15,  2.06it/s] 24%|██▍       | 447/1836 [03:41<11:40,  1.98it/s] 24%|██▍       | 448/1836 [03:41<11:30,  2.01it/s] 24%|██▍       | 449/1836 [03:42<11:52,  1.95it/s] 25%|██▍       | 450/1836 [03:42<11:55,  1.94it/s] 25%|██▍       | 451/1836 [03:43<11:58,  1.93it/s] 25%|██▍       | 452/1836 [03:44<12:03,  1.91it/s] 25%|██▍       | 453/1836 [03:44<11:21,  2.03it/s] 25%|██▍       | 454/1836 [03:44<11:01,  2.09it/s] 25%|██▍       | 455/1836 [03:45<10:49,  2.13it/s] 25%|██▍       | 456/1836 [03:45<10:48,  2.13it/s] 25%|██▍       | 457/1836 [03:46<10:36,  2.17it/s] 25%|██▍       | 458/1836 [03:46<10:31,  2.18it/s] 25%|██▌       | 459/1836 [03:47<11:11,  2.05it/s] 25%|██▌       | 460/1836 [03:47<11:14,  2.04it/s] 25%|██▌       | 461/1836 [03:48<11:43,  1.96it/s] 25%|██▌       | 462/1836 [03:48<11:17,  2.03it/s] 25%|██▌       | 463/1836 [03:49<11:16,  2.03it/s] 25%|██▌       | 464/1836 [03:49<11:26,  2.00it/s] 25%|██▌       | 465/1836 [03:50<11:17,  2.02it/s] 25%|██▌       | 466/1836 [03:50<11:30,  1.98it/s] 25%|██▌       | 467/1836 [03:51<11:36,  1.97it/s] 25%|██▌       | 468/1836 [03:51<11:44,  1.94it/s] 26%|██▌       | 469/1836 [03:52<11:59,  1.90it/s] 26%|██▌       | 470/1836 [03:52<11:39,  1.95it/s] 26%|██▌       | 471/1836 [03:53<11:24,  1.99it/s] 26%|██▌       | 472/1836 [03:53<11:11,  2.03it/s] 26%|██▌       | 473/1836 [03:54<11:39,  1.95it/s] 26%|██▌       | 474/1836 [03:54<11:44,  1.93it/s] 26%|██▌       | 475/1836 [03:55<11:27,  1.98it/s] 26%|██▌       | 476/1836 [03:55<11:04,  2.05it/s] 26%|██▌       | 477/1836 [03:56<11:20,  2.00it/s] 26%|██▌       | 478/1836 [03:56<10:59,  2.06it/s] 26%|██▌       | 479/1836 [03:57<10:55,  2.07it/s] 26%|██▌       | 480/1836 [03:57<11:56,  1.89it/s] 26%|██▌       | 481/1836 [03:58<11:25,  1.98it/s] 26%|██▋       | 482/1836 [03:58<11:33,  1.95it/s] 26%|██▋       | 483/1836 [03:59<11:07,  2.03it/s] 26%|██▋       | 484/1836 [03:59<11:29,  1.96it/s] 26%|██▋       | 485/1836 [04:00<11:01,  2.04it/s] 26%|██▋       | 486/1836 [04:00<10:54,  2.06it/s] 27%|██▋       | 487/1836 [04:01<10:50,  2.07it/s] 27%|██▋       | 488/1836 [04:01<10:36,  2.12it/s] 27%|██▋       | 489/1836 [04:02<10:13,  2.20it/s] 27%|██▋       | 490/1836 [04:02<10:39,  2.10it/s] 27%|██▋       | 491/1836 [04:03<11:31,  1.94it/s] 27%|██▋       | 492/1836 [04:03<11:08,  2.01it/s] 27%|██▋       | 493/1836 [04:04<11:19,  1.98it/s] 27%|██▋       | 494/1836 [04:04<11:29,  1.95it/s] 27%|██▋       | 495/1836 [04:05<11:14,  1.99it/s] 27%|██▋       | 496/1836 [04:05<10:51,  2.06it/s] 27%|██▋       | 497/1836 [04:06<10:55,  2.04it/s] 27%|██▋       | 498/1836 [04:06<10:56,  2.04it/s] 27%|██▋       | 499/1836 [04:07<10:59,  2.03it/s] 27%|██▋       | 500/1836 [04:07<10:42,  2.08it/s] 27%|██▋       | 501/1836 [04:08<10:37,  2.09it/s] 27%|██▋       | 502/1836 [04:08<10:35,  2.10it/s] 27%|██▋       | 503/1836 [04:09<10:42,  2.07it/s] 27%|██▋       | 504/1836 [04:09<11:13,  1.98it/s] 28%|██▊       | 505/1836 [04:10<11:18,  1.96it/s] 28%|██▊       | 506/1836 [04:10<11:22,  1.95it/s] 28%|██▊       | 507/1836 [04:11<10:54,  2.03it/s] 28%|██▊       | 508/1836 [04:11<11:02,  2.00it/s] 28%|██▊       | 509/1836 [04:12<10:59,  2.01it/s] 28%|██▊       | 510/1836 [04:12<11:18,  1.95it/s] 28%|██▊       | 511/1836 [04:13<10:53,  2.03it/s] 28%|██▊       | 512/1836 [04:13<10:35,  2.08it/s] 28%|██▊       | 513/1836 [04:14<10:21,  2.13it/s] 28%|██▊       | 514/1836 [04:14<10:31,  2.09it/s] 28%|██▊       | 515/1836 [04:15<10:39,  2.07it/s] 28%|██▊       | 516/1836 [04:15<10:52,  2.02it/s] 28%|██▊       | 517/1836 [04:16<10:54,  2.02it/s] 28%|██▊       | 518/1836 [04:16<10:35,  2.07it/s] 28%|██▊       | 519/1836 [04:17<10:51,  2.02it/s] 28%|██▊       | 520/1836 [04:17<10:44,  2.04it/s] 28%|██▊       | 521/1836 [04:18<11:01,  1.99it/s] 28%|██▊       | 522/1836 [04:18<10:38,  2.06it/s] 28%|██▊       | 523/1836 [04:18<10:49,  2.02it/s] 29%|██▊       | 524/1836 [04:19<11:03,  1.98it/s] 29%|██▊       | 525/1836 [04:19<10:52,  2.01it/s] 29%|██▊       | 526/1836 [04:20<11:03,  1.97it/s] 29%|██▊       | 527/1836 [04:21<11:20,  1.92it/s] 29%|██▉       | 528/1836 [04:21<11:22,  1.92it/s] 29%|██▉       | 529/1836 [04:22<11:24,  1.91it/s] 29%|██▉       | 530/1836 [04:22<11:23,  1.91it/s] 29%|██▉       | 531/1836 [04:23<11:12,  1.94it/s] 29%|██▉       | 532/1836 [04:23<10:55,  1.99it/s] 29%|██▉       | 533/1836 [04:24<11:02,  1.97it/s] 29%|██▉       | 534/1836 [04:24<10:36,  2.05it/s] 29%|██▉       | 535/1836 [04:25<10:46,  2.01it/s] 29%|██▉       | 536/1836 [04:25<10:36,  2.04it/s] 29%|██▉       | 537/1836 [04:26<10:50,  2.00it/s] 29%|██▉       | 538/1836 [04:26<10:41,  2.02it/s] 29%|██▉       | 539/1836 [04:27<10:25,  2.07it/s] 29%|██▉       | 540/1836 [04:27<10:38,  2.03it/s] 29%|██▉       | 541/1836 [04:28<10:53,  1.98it/s] 30%|██▉       | 542/1836 [04:28<11:01,  1.96it/s] 30%|██▉       | 543/1836 [04:29<10:55,  1.97it/s] 30%|██▉       | 544/1836 [04:29<10:50,  1.99it/s] 30%|██▉       | 545/1836 [04:30<10:40,  2.01it/s] 30%|██▉       | 546/1836 [04:30<10:19,  2.08it/s] 30%|██▉       | 547/1836 [04:30<10:06,  2.13it/s] 30%|██▉       | 548/1836 [04:31<10:25,  2.06it/s] 30%|██▉       | 549/1836 [04:32<10:49,  1.98it/s] 30%|██▉       | 550/1836 [04:32<11:07,  1.93it/s] 30%|███       | 551/1836 [04:33<11:08,  1.92it/s] 30%|███       | 552/1836 [04:33<10:41,  2.00it/s] 30%|███       | 553/1836 [04:34<11:02,  1.94it/s] 30%|███       | 554/1836 [04:34<10:34,  2.02it/s] 30%|███       | 555/1836 [04:35<10:54,  1.96it/s] 30%|███       | 556/1836 [04:35<10:58,  1.94it/s] 30%|███       | 557/1836 [04:36<10:30,  2.03it/s] 30%|███       | 558/1836 [04:36<10:12,  2.09it/s] 30%|███       | 559/1836 [04:37<10:09,  2.10it/s] 31%|███       | 560/1836 [04:37<10:36,  2.00it/s] 31%|███       | 561/1836 [04:38<10:28,  2.03it/s] 31%|███       | 562/1836 [04:38<10:39,  1.99it/s] 31%|███       | 563/1836 [04:39<11:31,  1.84it/s] 31%|███       | 564/1836 [04:39<10:46,  1.97it/s] 31%|███       | 565/1836 [04:40<10:41,  1.98it/s] 31%|███       | 566/1836 [04:40<10:45,  1.97it/s] 31%|███       | 567/1836 [04:41<10:21,  2.04it/s] 31%|███       | 568/1836 [04:41<11:31,  1.83it/s] 31%|███       | 569/1836 [04:42<11:14,  1.88it/s] 31%|███       | 570/1836 [04:42<10:30,  2.01it/s] 31%|███       | 571/1836 [04:43<10:48,  1.95it/s] 31%|███       | 572/1836 [04:43<10:23,  2.03it/s] 31%|███       | 573/1836 [04:44<10:31,  2.00it/s] 31%|███▏      | 574/1836 [04:44<10:38,  1.98it/s] 31%|███▏      | 575/1836 [04:45<10:17,  2.04it/s] 31%|███▏      | 576/1836 [04:45<10:44,  1.95it/s] 31%|███▏      | 577/1836 [04:46<10:53,  1.93it/s] 31%|███▏      | 578/1836 [04:46<10:44,  1.95it/s] 32%|███▏      | 579/1836 [04:47<10:46,  1.94it/s] 32%|███▏      | 580/1836 [04:47<10:47,  1.94it/s] 32%|███▏      | 581/1836 [04:48<11:00,  1.90it/s] 32%|███▏      | 582/1836 [04:48<10:50,  1.93it/s] 32%|███▏      | 583/1836 [04:49<11:06,  1.88it/s] 32%|███▏      | 584/1836 [04:49<10:33,  1.97it/s] 32%|███▏      | 585/1836 [04:50<10:51,  1.92it/s] 32%|███▏      | 586/1836 [04:50<10:33,  1.97it/s] 32%|███▏      | 587/1836 [04:51<10:00,  2.08it/s] 32%|███▏      | 588/1836 [04:51<10:04,  2.07it/s] 32%|███▏      | 589/1836 [04:52<10:32,  1.97it/s] 32%|███▏      | 590/1836 [04:52<10:19,  2.01it/s] 32%|███▏      | 591/1836 [04:53<10:11,  2.04it/s] 32%|███▏      | 592/1836 [04:53<09:44,  2.13it/s] 32%|███▏      | 593/1836 [04:54<09:53,  2.09it/s] 32%|███▏      | 594/1836 [04:54<09:51,  2.10it/s] 32%|███▏      | 595/1836 [04:55<10:12,  2.03it/s] 32%|███▏      | 596/1836 [04:55<10:13,  2.02it/s] 33%|███▎      | 597/1836 [04:56<10:12,  2.02it/s] 33%|███▎      | 598/1836 [04:56<10:22,  1.99it/s] 33%|███▎      | 599/1836 [04:57<10:13,  2.02it/s] 33%|███▎      | 600/1836 [04:57<10:13,  2.01it/s] 33%|███▎      | 601/1836 [04:58<10:05,  2.04it/s] 33%|███▎      | 602/1836 [04:58<09:49,  2.09it/s] 33%|███▎      | 603/1836 [04:59<10:03,  2.04it/s] 33%|███▎      | 604/1836 [04:59<09:47,  2.10it/s] 33%|███▎      | 605/1836 [05:00<09:52,  2.08it/s] 33%|███▎      | 606/1836 [05:00<09:36,  2.13it/s] 33%|███▎      | 607/1836 [05:01<09:46,  2.10it/s] 33%|███▎      | 608/1836 [05:01<10:03,  2.03it/s] 33%|███▎      | 609/1836 [05:02<10:25,  1.96it/s] 33%|███▎      | 610/1836 [05:02<10:20,  1.98it/s] 33%|███▎      | 611/1836 [05:03<10:29,  1.94it/s] 33%|███▎      | 612/1836 [05:03<09:17,  2.19it/s]03/13/2024 20:35:39 - INFO - __main__ - epoch 0: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}
 33%|███▎      | 613/1836 [05:11<58:12,  2.86s/it] 33%|███▎      | 614/1836 [05:12<43:24,  2.13s/it] 33%|███▎      | 615/1836 [05:12<33:23,  1.64s/it] 34%|███▎      | 616/1836 [05:13<26:15,  1.29s/it] 34%|███▎      | 617/1836 [05:13<21:45,  1.07s/it] 34%|███▎      | 618/1836 [05:14<17:57,  1.13it/s] 34%|███▎      | 619/1836 [05:14<15:34,  1.30it/s] 34%|███▍      | 620/1836 [05:15<14:06,  1.44it/s] 34%|███▍      | 621/1836 [05:15<13:03,  1.55it/s] 34%|███▍      | 622/1836 [05:16<11:51,  1.71it/s] 34%|███▍      | 623/1836 [05:16<11:00,  1.84it/s] 34%|███▍      | 624/1836 [05:17<10:34,  1.91it/s] 34%|███▍      | 625/1836 [05:17<10:05,  2.00it/s] 34%|███▍      | 626/1836 [05:18<10:16,  1.96it/s] 34%|███▍      | 627/1836 [05:18<10:04,  2.00it/s] 34%|███▍      | 628/1836 [05:19<10:15,  1.96it/s] 34%|███▍      | 629/1836 [05:19<10:22,  1.94it/s] 34%|███▍      | 630/1836 [05:20<10:15,  1.96it/s] 34%|███▍      | 631/1836 [05:20<10:20,  1.94it/s] 34%|███▍      | 632/1836 [05:21<10:21,  1.94it/s] 34%|███▍      | 633/1836 [05:21<10:07,  1.98it/s] 35%|███▍      | 634/1836 [05:22<09:47,  2.05it/s] 35%|███▍      | 635/1836 [05:22<09:42,  2.06it/s] 35%|███▍      | 636/1836 [05:23<09:57,  2.01it/s] 35%|███▍      | 637/1836 [05:23<10:07,  1.97it/s] 35%|███▍      | 638/1836 [05:24<09:47,  2.04it/s] 35%|███▍      | 639/1836 [05:24<09:30,  2.10it/s] 35%|███▍      | 640/1836 [05:25<09:47,  2.04it/s] 35%|███▍      | 641/1836 [05:25<09:58,  2.00it/s] 35%|███▍      | 642/1836 [05:26<09:50,  2.02it/s] 35%|███▌      | 643/1836 [05:26<09:50,  2.02it/s] 35%|███▌      | 644/1836 [05:27<09:49,  2.02it/s] 35%|███▌      | 645/1836 [05:27<10:06,  1.96it/s] 35%|███▌      | 646/1836 [05:28<10:13,  1.94it/s] 35%|███▌      | 647/1836 [05:28<10:25,  1.90it/s] 35%|███▌      | 648/1836 [05:29<10:22,  1.91it/s] 35%|███▌      | 649/1836 [05:29<10:04,  1.96it/s] 35%|███▌      | 650/1836 [05:30<10:08,  1.95it/s] 35%|███▌      | 651/1836 [05:30<10:13,  1.93it/s] 36%|███▌      | 652/1836 [05:31<10:05,  1.95it/s] 36%|███▌      | 653/1836 [05:31<10:09,  1.94it/s] 36%|███▌      | 654/1836 [05:32<10:14,  1.92it/s] 36%|███▌      | 655/1836 [05:32<10:18,  1.91it/s] 36%|███▌      | 656/1836 [05:33<10:09,  1.94it/s] 36%|███▌      | 657/1836 [05:33<09:52,  1.99it/s] 36%|███▌      | 658/1836 [05:34<09:51,  1.99it/s] 36%|███▌      | 659/1836 [05:34<09:57,  1.97it/s] 36%|███▌      | 660/1836 [05:35<09:53,  1.98it/s] 36%|███▌      | 661/1836 [05:35<09:50,  1.99it/s] 36%|███▌      | 662/1836 [05:36<09:56,  1.97it/s] 36%|███▌      | 663/1836 [05:36<09:44,  2.01it/s] 36%|███▌      | 664/1836 [05:37<09:37,  2.03it/s] 36%|███▌      | 665/1836 [05:37<09:45,  2.00it/s] 36%|███▋      | 666/1836 [05:38<09:45,  2.00it/s] 36%|███▋      | 667/1836 [05:38<09:44,  2.00it/s] 36%|███▋      | 668/1836 [05:39<09:53,  1.97it/s] 36%|███▋      | 669/1836 [05:39<09:43,  2.00it/s] 36%|███▋      | 670/1836 [05:40<09:22,  2.07it/s] 37%|███▋      | 671/1836 [05:40<09:34,  2.03it/s] 37%|███▋      | 672/1836 [05:41<09:44,  1.99it/s] 37%|███▋      | 673/1836 [05:41<09:58,  1.94it/s] 37%|███▋      | 674/1836 [05:42<09:44,  1.99it/s] 37%|███▋      | 675/1836 [05:42<09:42,  1.99it/s] 37%|███▋      | 676/1836 [05:43<09:48,  1.97it/s] 37%|███▋      | 677/1836 [05:43<09:38,  2.00it/s] 37%|███▋      | 678/1836 [05:44<09:53,  1.95it/s] 37%|███▋      | 679/1836 [05:44<09:39,  2.00it/s] 37%|███▋      | 680/1836 [05:45<09:56,  1.94it/s] 37%|███▋      | 681/1836 [05:46<10:22,  1.86it/s] 37%|███▋      | 682/1836 [05:46<09:53,  1.95it/s] 37%|███▋      | 683/1836 [05:47<09:38,  1.99it/s] 37%|███▋      | 684/1836 [05:47<09:16,  2.07it/s] 37%|███▋      | 685/1836 [05:47<09:21,  2.05it/s] 37%|███▋      | 686/1836 [05:48<09:36,  2.00it/s] 37%|███▋      | 687/1836 [05:48<09:25,  2.03it/s] 37%|███▋      | 688/1836 [05:49<09:37,  1.99it/s] 38%|███▊      | 689/1836 [05:50<09:47,  1.95it/s] 38%|███▊      | 690/1836 [05:50<09:51,  1.94it/s] 38%|███▊      | 691/1836 [05:51<09:52,  1.93it/s] 38%|███▊      | 692/1836 [05:51<09:37,  1.98it/s] 38%|███▊      | 693/1836 [05:52<09:51,  1.93it/s] 38%|███▊      | 694/1836 [05:52<09:54,  1.92it/s] 38%|███▊      | 695/1836 [05:53<09:53,  1.92it/s] 38%|███▊      | 696/1836 [05:53<09:44,  1.95it/s] 38%|███▊      | 697/1836 [05:54<09:30,  2.00it/s] 38%|███▊      | 698/1836 [05:54<09:21,  2.03it/s] 38%|███▊      | 699/1836 [05:55<09:40,  1.96it/s] 38%|███▊      | 700/1836 [05:55<09:53,  1.91it/s] 38%|███▊      | 701/1836 [05:56<09:36,  1.97it/s] 38%|███▊      | 702/1836 [05:56<09:49,  1.92it/s] 38%|███▊      | 703/1836 [05:57<10:02,  1.88it/s] 38%|███▊      | 704/1836 [05:57<09:43,  1.94it/s] 38%|███▊      | 705/1836 [05:58<09:46,  1.93it/s] 38%|███▊      | 706/1836 [05:58<09:47,  1.92it/s] 39%|███▊      | 707/1836 [05:59<09:33,  1.97it/s] 39%|███▊      | 708/1836 [05:59<09:45,  1.93it/s] 39%|███▊      | 709/1836 [06:00<09:44,  1.93it/s] 39%|███▊      | 710/1836 [06:00<09:45,  1.92it/s] 39%|███▊      | 711/1836 [06:01<09:21,  2.00it/s] 39%|███▉      | 712/1836 [06:01<09:27,  1.98it/s] 39%|███▉      | 713/1836 [06:02<09:31,  1.96it/s] 39%|███▉      | 714/1836 [06:02<09:26,  1.98it/s] 39%|███▉      | 715/1836 [06:03<09:35,  1.95it/s] 39%|███▉      | 716/1836 [06:03<09:38,  1.94it/s] 39%|███▉      | 717/1836 [06:04<09:43,  1.92it/s] 39%|███▉      | 718/1836 [06:04<09:47,  1.90it/s] 39%|███▉      | 719/1836 [06:05<09:45,  1.91it/s] 39%|███▉      | 720/1836 [06:05<09:18,  2.00it/s] 39%|███▉      | 721/1836 [06:06<09:38,  1.93it/s] 39%|███▉      | 722/1836 [06:06<09:15,  2.01it/s] 39%|███▉      | 723/1836 [06:07<09:14,  2.01it/s] 39%|███▉      | 724/1836 [06:08<09:33,  1.94it/s] 39%|███▉      | 725/1836 [06:08<09:35,  1.93it/s] 40%|███▉      | 726/1836 [06:08<09:12,  2.01it/s] 40%|███▉      | 727/1836 [06:09<08:45,  2.11it/s] 40%|███▉      | 728/1836 [06:09<09:04,  2.04it/s] 40%|███▉      | 729/1836 [06:10<09:13,  2.00it/s] 40%|███▉      | 730/1836 [06:11<09:29,  1.94it/s] 40%|███▉      | 731/1836 [06:11<08:58,  2.05it/s] 40%|███▉      | 732/1836 [06:11<08:45,  2.10it/s] 40%|███▉      | 733/1836 [06:12<08:25,  2.18it/s] 40%|███▉      | 734/1836 [06:12<08:22,  2.19it/s] 40%|████      | 735/1836 [06:13<08:18,  2.21it/s] 40%|████      | 736/1836 [06:13<08:40,  2.12it/s] 40%|████      | 737/1836 [06:14<09:04,  2.02it/s] 40%|████      | 738/1836 [06:14<08:46,  2.08it/s] 40%|████      | 739/1836 [06:15<08:58,  2.04it/s] 40%|████      | 740/1836 [06:15<08:52,  2.06it/s] 40%|████      | 741/1836 [06:16<09:03,  2.02it/s] 40%|████      | 742/1836 [06:16<08:38,  2.11it/s] 40%|████      | 743/1836 [06:17<08:36,  2.12it/s] 41%|████      | 744/1836 [06:17<09:01,  2.02it/s] 41%|████      | 745/1836 [06:18<09:08,  1.99it/s] 41%|████      | 746/1836 [06:18<08:52,  2.05it/s] 41%|████      | 747/1836 [06:19<08:29,  2.14it/s] 41%|████      | 748/1836 [06:19<08:36,  2.11it/s] 41%|████      | 749/1836 [06:20<08:52,  2.04it/s] 41%|████      | 750/1836 [06:20<08:39,  2.09it/s] 41%|████      | 751/1836 [06:21<09:01,  2.00it/s] 41%|████      | 752/1836 [06:21<08:54,  2.03it/s] 41%|████      | 753/1836 [06:22<08:46,  2.06it/s] 41%|████      | 754/1836 [06:22<09:06,  1.98it/s] 41%|████      | 755/1836 [06:23<08:46,  2.05it/s] 41%|████      | 756/1836 [06:23<08:49,  2.04it/s] 41%|████      | 757/1836 [06:23<08:32,  2.11it/s] 41%|████▏     | 758/1836 [06:24<08:48,  2.04it/s] 41%|████▏     | 759/1836 [06:24<08:33,  2.10it/s] 41%|████▏     | 760/1836 [06:25<08:55,  2.01it/s] 41%|████▏     | 761/1836 [06:25<08:49,  2.03it/s] 42%|████▏     | 762/1836 [06:26<08:49,  2.03it/s] 42%|████▏     | 763/1836 [06:26<08:59,  1.99it/s] 42%|████▏     | 764/1836 [06:27<09:08,  1.96it/s] 42%|████▏     | 765/1836 [06:27<08:57,  1.99it/s] 42%|████▏     | 766/1836 [06:28<08:48,  2.02it/s] 42%|████▏     | 767/1836 [06:28<08:33,  2.08it/s] 42%|████▏     | 768/1836 [06:29<08:29,  2.10it/s] 42%|████▏     | 769/1836 [06:29<08:21,  2.13it/s] 42%|████▏     | 770/1836 [06:30<08:29,  2.09it/s] 42%|████▏     | 771/1836 [06:30<08:42,  2.04it/s] 42%|████▏     | 772/1836 [06:31<08:44,  2.03it/s] 42%|████▏     | 773/1836 [06:31<08:39,  2.05it/s] 42%|████▏     | 774/1836 [06:32<08:57,  1.98it/s] 42%|████▏     | 775/1836 [06:32<08:39,  2.04it/s] 42%|████▏     | 776/1836 [06:33<09:25,  1.87it/s] 42%|████▏     | 777/1836 [06:33<09:14,  1.91it/s] 42%|████▏     | 778/1836 [06:34<08:59,  1.96it/s] 42%|████▏     | 779/1836 [06:34<08:39,  2.03it/s] 42%|████▏     | 780/1836 [06:35<08:40,  2.03it/s] 43%|████▎     | 781/1836 [06:35<08:52,  1.98it/s] 43%|████▎     | 782/1836 [06:36<08:43,  2.01it/s] 43%|████▎     | 783/1836 [06:36<08:42,  2.01it/s] 43%|████▎     | 784/1836 [06:37<08:52,  1.98it/s] 43%|████▎     | 785/1836 [06:37<08:48,  1.99it/s] 43%|████▎     | 786/1836 [06:38<09:29,  1.84it/s] 43%|████▎     | 787/1836 [06:39<09:47,  1.79it/s] 43%|████▎     | 788/1836 [06:39<09:12,  1.90it/s] 43%|████▎     | 789/1836 [06:40<08:44,  2.00it/s] 43%|████▎     | 790/1836 [06:40<08:48,  1.98it/s] 43%|████▎     | 791/1836 [06:41<08:37,  2.02it/s] 43%|████▎     | 792/1836 [06:41<08:21,  2.08it/s] 43%|████▎     | 793/1836 [06:41<08:19,  2.09it/s] 43%|████▎     | 794/1836 [06:42<08:10,  2.12it/s] 43%|████▎     | 795/1836 [06:42<08:01,  2.16it/s] 43%|████▎     | 796/1836 [06:43<08:22,  2.07it/s] 43%|████▎     | 797/1836 [06:43<08:25,  2.06it/s] 43%|████▎     | 798/1836 [06:44<08:12,  2.11it/s] 44%|████▎     | 799/1836 [06:44<08:18,  2.08it/s] 44%|████▎     | 800/1836 [06:45<08:06,  2.13it/s] 44%|████▎     | 801/1836 [06:45<08:22,  2.06it/s] 44%|████▎     | 802/1836 [06:46<08:20,  2.07it/s] 44%|████▎     | 803/1836 [06:46<08:53,  1.94it/s] 44%|████▍     | 804/1836 [06:47<08:42,  1.97it/s] 44%|████▍     | 805/1836 [06:47<08:40,  1.98it/s] 44%|████▍     | 806/1836 [06:48<08:31,  2.01it/s] 44%|████▍     | 807/1836 [06:48<08:17,  2.07it/s] 44%|████▍     | 808/1836 [06:49<08:22,  2.05it/s] 44%|████▍     | 809/1836 [06:49<08:31,  2.01it/s] 44%|████▍     | 810/1836 [06:50<08:07,  2.10it/s] 44%|████▍     | 811/1836 [06:50<07:48,  2.19it/s] 44%|████▍     | 812/1836 [06:51<08:10,  2.09it/s] 44%|████▍     | 813/1836 [06:51<08:30,  2.00it/s] 44%|████▍     | 814/1836 [06:52<08:36,  1.98it/s] 44%|████▍     | 815/1836 [06:52<08:25,  2.02it/s] 44%|████▍     | 816/1836 [06:53<09:21,  1.82it/s] 44%|████▍     | 817/1836 [06:53<08:59,  1.89it/s] 45%|████▍     | 818/1836 [06:54<08:24,  2.02it/s] 45%|████▍     | 819/1836 [06:54<08:31,  1.99it/s] 45%|████▍     | 820/1836 [06:55<08:21,  2.02it/s] 45%|████▍     | 821/1836 [06:55<08:15,  2.05it/s] 45%|████▍     | 822/1836 [06:56<08:18,  2.03it/s] 45%|████▍     | 823/1836 [06:56<08:13,  2.05it/s] 45%|████▍     | 824/1836 [06:57<08:16,  2.04it/s] 45%|████▍     | 825/1836 [06:57<08:29,  1.98it/s] 45%|████▍     | 826/1836 [06:58<08:10,  2.06it/s] 45%|████▌     | 827/1836 [06:58<08:20,  2.01it/s] 45%|████▌     | 828/1836 [06:59<08:30,  1.98it/s] 45%|████▌     | 829/1836 [06:59<08:41,  1.93it/s] 45%|████▌     | 830/1836 [07:00<08:28,  1.98it/s] 45%|████▌     | 831/1836 [07:00<08:25,  1.99it/s] 45%|████▌     | 832/1836 [07:01<08:30,  1.97it/s] 45%|████▌     | 833/1836 [07:01<08:25,  1.98it/s] 45%|████▌     | 834/1836 [07:02<08:32,  1.95it/s] 45%|████▌     | 835/1836 [07:02<08:13,  2.03it/s] 46%|████▌     | 836/1836 [07:03<08:24,  1.98it/s] 46%|████▌     | 837/1836 [07:03<08:40,  1.92it/s] 46%|████▌     | 838/1836 [07:04<08:41,  1.91it/s] 46%|████▌     | 839/1836 [07:04<08:27,  1.96it/s] 46%|████▌     | 840/1836 [07:05<08:06,  2.05it/s] 46%|████▌     | 841/1836 [07:05<07:50,  2.11it/s] 46%|████▌     | 842/1836 [07:06<07:41,  2.15it/s] 46%|████▌     | 843/1836 [07:06<07:42,  2.15it/s] 46%|████▌     | 844/1836 [07:07<07:44,  2.14it/s] 46%|████▌     | 845/1836 [07:07<07:47,  2.12it/s] 46%|████▌     | 846/1836 [07:08<07:53,  2.09it/s] 46%|████▌     | 847/1836 [07:08<08:13,  2.00it/s] 46%|████▌     | 848/1836 [07:09<08:19,  1.98it/s] 46%|████▌     | 849/1836 [07:09<08:23,  1.96it/s] 46%|████▋     | 850/1836 [07:10<08:04,  2.04it/s] 46%|████▋     | 851/1836 [07:10<07:50,  2.09it/s] 46%|████▋     | 852/1836 [07:11<08:03,  2.03it/s] 46%|████▋     | 853/1836 [07:11<07:43,  2.12it/s] 47%|████▋     | 854/1836 [07:12<07:58,  2.05it/s] 47%|████▋     | 855/1836 [07:12<07:55,  2.06it/s] 47%|████▋     | 856/1836 [07:12<07:50,  2.08it/s] 47%|████▋     | 857/1836 [07:13<07:53,  2.07it/s] 47%|████▋     | 858/1836 [07:13<07:43,  2.11it/s] 47%|████▋     | 859/1836 [07:14<07:48,  2.08it/s] 47%|████▋     | 860/1836 [07:14<08:00,  2.03it/s] 47%|████▋     | 861/1836 [07:15<07:48,  2.08it/s] 47%|████▋     | 862/1836 [07:15<08:01,  2.02it/s] 47%|████▋     | 863/1836 [07:16<08:08,  1.99it/s] 47%|████▋     | 864/1836 [07:16<08:14,  1.96it/s] 47%|████▋     | 865/1836 [07:17<08:25,  1.92it/s] 47%|████▋     | 866/1836 [07:18<08:29,  1.90it/s] 47%|████▋     | 867/1836 [07:18<08:13,  1.96it/s] 47%|████▋     | 868/1836 [07:19<08:26,  1.91it/s] 47%|████▋     | 869/1836 [07:19<08:02,  2.00it/s] 47%|████▋     | 870/1836 [07:19<07:38,  2.11it/s] 47%|████▋     | 871/1836 [07:20<07:37,  2.11it/s] 47%|████▋     | 872/1836 [07:20<07:35,  2.11it/s] 48%|████▊     | 873/1836 [07:21<07:36,  2.11it/s] 48%|████▊     | 874/1836 [07:21<07:57,  2.01it/s] 48%|████▊     | 875/1836 [07:22<07:45,  2.07it/s] 48%|████▊     | 876/1836 [07:22<08:06,  1.97it/s] 48%|████▊     | 877/1836 [07:23<07:56,  2.01it/s] 48%|████▊     | 878/1836 [07:23<08:04,  1.98it/s] 48%|████▊     | 879/1836 [07:24<08:10,  1.95it/s] 48%|████▊     | 880/1836 [07:24<07:58,  2.00it/s] 48%|████▊     | 881/1836 [07:25<07:44,  2.06it/s] 48%|████▊     | 882/1836 [07:25<07:24,  2.15it/s] 48%|████▊     | 883/1836 [07:26<07:39,  2.07it/s] 48%|████▊     | 884/1836 [07:26<07:43,  2.06it/s] 48%|████▊     | 885/1836 [07:27<07:38,  2.07it/s] 48%|████▊     | 886/1836 [07:27<07:48,  2.03it/s] 48%|████▊     | 887/1836 [07:28<07:56,  1.99it/s] 48%|████▊     | 888/1836 [07:28<07:41,  2.06it/s] 48%|████▊     | 889/1836 [07:29<07:17,  2.16it/s] 48%|████▊     | 890/1836 [07:29<07:11,  2.19it/s] 49%|████▊     | 891/1836 [07:30<07:21,  2.14it/s] 49%|████▊     | 892/1836 [07:30<07:23,  2.13it/s] 49%|████▊     | 893/1836 [07:31<07:30,  2.09it/s] 49%|████▊     | 894/1836 [07:31<07:41,  2.04it/s] 49%|████▊     | 895/1836 [07:32<07:27,  2.10it/s] 49%|████▉     | 896/1836 [07:32<07:50,  2.00it/s] 49%|████▉     | 897/1836 [07:33<08:29,  1.84it/s] 49%|████▉     | 898/1836 [07:33<07:55,  1.97it/s] 49%|████▉     | 899/1836 [07:34<07:51,  1.99it/s] 49%|████▉     | 900/1836 [07:34<07:58,  1.96it/s] 49%|████▉     | 901/1836 [07:35<07:54,  1.97it/s] 49%|████▉     | 902/1836 [07:35<07:51,  1.98it/s] 49%|████▉     | 903/1836 [07:36<07:36,  2.05it/s] 49%|████▉     | 904/1836 [07:36<07:43,  2.01it/s] 49%|████▉     | 905/1836 [07:37<07:49,  1.98it/s] 49%|████▉     | 906/1836 [07:37<07:41,  2.01it/s] 49%|████▉     | 907/1836 [07:38<07:35,  2.04it/s] 49%|████▉     | 908/1836 [07:38<07:53,  1.96it/s] 50%|████▉     | 909/1836 [07:39<08:00,  1.93it/s] 50%|████▉     | 910/1836 [07:39<07:39,  2.01it/s] 50%|████▉     | 911/1836 [07:40<07:22,  2.09it/s] 50%|████▉     | 912/1836 [07:40<07:35,  2.03it/s] 50%|████▉     | 913/1836 [07:41<07:45,  1.98it/s] 50%|████▉     | 914/1836 [07:41<07:52,  1.95it/s] 50%|████▉     | 915/1836 [07:42<07:47,  1.97it/s] 50%|████▉     | 916/1836 [07:42<07:53,  1.94it/s] 50%|████▉     | 917/1836 [07:43<07:47,  1.96it/s] 50%|█████     | 918/1836 [07:43<08:00,  1.91it/s] 50%|█████     | 919/1836 [07:44<08:06,  1.88it/s] 50%|█████     | 920/1836 [07:44<07:52,  1.94it/s] 50%|█████     | 921/1836 [07:45<07:54,  1.93it/s] 50%|█████     | 922/1836 [07:45<07:43,  1.97it/s] 50%|█████     | 923/1836 [07:46<07:48,  1.95it/s] 50%|█████     | 924/1836 [07:46<07:38,  1.99it/s] 50%|█████     | 925/1836 [07:47<07:30,  2.02it/s] 50%|█████     | 926/1836 [07:47<07:29,  2.02it/s] 50%|█████     | 927/1836 [07:48<08:00,  1.89it/s] 51%|█████     | 928/1836 [07:48<07:46,  1.95it/s] 51%|█████     | 929/1836 [07:49<07:40,  1.97it/s] 51%|█████     | 930/1836 [07:49<07:23,  2.04it/s] 51%|█████     | 931/1836 [07:50<07:19,  2.06it/s] 51%|█████     | 932/1836 [07:50<07:22,  2.04it/s] 51%|█████     | 933/1836 [07:51<07:17,  2.06it/s] 51%|█████     | 934/1836 [07:51<07:07,  2.11it/s] 51%|█████     | 935/1836 [07:52<07:08,  2.10it/s] 51%|█████     | 936/1836 [07:52<07:13,  2.07it/s] 51%|█████     | 937/1836 [07:53<07:02,  2.13it/s] 51%|█████     | 938/1836 [07:53<07:03,  2.12it/s] 51%|█████     | 939/1836 [07:54<07:17,  2.05it/s] 51%|█████     | 940/1836 [07:54<07:15,  2.06it/s] 51%|█████▏    | 941/1836 [07:55<07:11,  2.08it/s] 51%|█████▏    | 942/1836 [07:55<07:14,  2.06it/s] 51%|█████▏    | 943/1836 [07:56<07:23,  2.01it/s] 51%|█████▏    | 944/1836 [07:56<07:11,  2.07it/s] 51%|█████▏    | 945/1836 [07:57<07:13,  2.06it/s] 52%|█████▏    | 946/1836 [07:57<07:22,  2.01it/s] 52%|█████▏    | 947/1836 [07:58<07:30,  1.97it/s] 52%|█████▏    | 948/1836 [07:58<07:27,  1.99it/s] 52%|█████▏    | 949/1836 [07:59<07:37,  1.94it/s] 52%|█████▏    | 950/1836 [07:59<07:38,  1.93it/s] 52%|█████▏    | 951/1836 [08:00<07:26,  1.98it/s] 52%|█████▏    | 952/1836 [08:00<07:23,  1.99it/s] 52%|█████▏    | 953/1836 [08:01<07:36,  1.94it/s] 52%|█████▏    | 954/1836 [08:01<07:24,  1.98it/s] 52%|█████▏    | 955/1836 [08:02<07:08,  2.06it/s] 52%|█████▏    | 956/1836 [08:02<06:58,  2.10it/s] 52%|█████▏    | 957/1836 [08:03<07:03,  2.08it/s] 52%|█████▏    | 958/1836 [08:03<07:24,  1.98it/s] 52%|█████▏    | 959/1836 [08:04<07:28,  1.96it/s] 52%|█████▏    | 960/1836 [08:04<07:29,  1.95it/s] 52%|█████▏    | 961/1836 [08:05<07:42,  1.89it/s] 52%|█████▏    | 962/1836 [08:05<07:33,  1.93it/s] 52%|█████▏    | 963/1836 [08:06<07:36,  1.91it/s] 53%|█████▎    | 964/1836 [08:06<07:46,  1.87it/s] 53%|█████▎    | 965/1836 [08:07<07:31,  1.93it/s] 53%|█████▎    | 966/1836 [08:07<07:11,  2.02it/s] 53%|█████▎    | 967/1836 [08:08<07:10,  2.02it/s] 53%|█████▎    | 968/1836 [08:08<06:57,  2.08it/s] 53%|█████▎    | 969/1836 [08:09<07:08,  2.02it/s] 53%|█████▎    | 970/1836 [08:09<06:57,  2.07it/s] 53%|█████▎    | 971/1836 [08:10<07:08,  2.02it/s] 53%|█████▎    | 972/1836 [08:10<07:25,  1.94it/s] 53%|█████▎    | 973/1836 [08:11<07:25,  1.94it/s] 53%|█████▎    | 974/1836 [08:11<07:45,  1.85it/s] 53%|█████▎    | 975/1836 [08:12<07:48,  1.84it/s] 53%|█████▎    | 976/1836 [08:12<07:31,  1.90it/s] 53%|█████▎    | 977/1836 [08:13<07:12,  1.99it/s] 53%|█████▎    | 978/1836 [08:13<06:57,  2.05it/s] 53%|█████▎    | 979/1836 [08:14<07:06,  2.01it/s] 53%|█████▎    | 980/1836 [08:14<07:14,  1.97it/s] 53%|█████▎    | 981/1836 [08:15<07:20,  1.94it/s] 53%|█████▎    | 982/1836 [08:15<07:23,  1.93it/s] 54%|█████▎    | 983/1836 [08:16<06:58,  2.04it/s] 54%|█████▎    | 984/1836 [08:16<06:59,  2.03it/s] 54%|█████▎    | 985/1836 [08:17<06:48,  2.09it/s] 54%|█████▎    | 986/1836 [08:17<06:39,  2.13it/s] 54%|█████▍    | 987/1836 [08:18<06:45,  2.09it/s] 54%|█████▍    | 988/1836 [08:18<06:35,  2.14it/s] 54%|█████▍    | 989/1836 [08:19<06:30,  2.17it/s] 54%|█████▍    | 990/1836 [08:19<06:44,  2.09it/s] 54%|█████▍    | 991/1836 [08:20<06:48,  2.07it/s] 54%|█████▍    | 992/1836 [08:20<06:36,  2.13it/s] 54%|█████▍    | 993/1836 [08:21<06:51,  2.05it/s] 54%|█████▍    | 994/1836 [08:21<06:49,  2.06it/s] 54%|█████▍    | 995/1836 [08:22<06:57,  2.01it/s] 54%|█████▍    | 996/1836 [08:22<06:36,  2.12it/s] 54%|█████▍    | 997/1836 [08:22<06:40,  2.09it/s] 54%|█████▍    | 998/1836 [08:23<06:50,  2.04it/s] 54%|█████▍    | 999/1836 [08:23<06:47,  2.05it/s] 54%|█████▍    | 1000/1836 [08:24<06:50,  2.04it/s] 55%|█████▍    | 1001/1836 [08:25<07:00,  1.99it/s] 55%|█████▍    | 1002/1836 [08:25<07:23,  1.88it/s] 55%|█████▍    | 1003/1836 [08:26<07:15,  1.91it/s] 55%|█████▍    | 1004/1836 [08:26<07:21,  1.88it/s] 55%|█████▍    | 1005/1836 [08:27<07:22,  1.88it/s] 55%|█████▍    | 1006/1836 [08:27<07:09,  1.93it/s] 55%|█████▍    | 1007/1836 [08:28<07:18,  1.89it/s] 55%|█████▍    | 1008/1836 [08:28<07:25,  1.86it/s] 55%|█████▍    | 1009/1836 [08:29<07:15,  1.90it/s] 55%|█████▌    | 1010/1836 [08:29<07:13,  1.91it/s] 55%|█████▌    | 1011/1836 [08:30<07:18,  1.88it/s] 55%|█████▌    | 1012/1836 [08:30<07:18,  1.88it/s] 55%|█████▌    | 1013/1836 [08:31<07:16,  1.89it/s] 55%|█████▌    | 1014/1836 [08:31<07:07,  1.92it/s] 55%|█████▌    | 1015/1836 [08:32<06:56,  1.97it/s] 55%|█████▌    | 1016/1836 [08:32<07:02,  1.94it/s] 55%|█████▌    | 1017/1836 [08:33<06:44,  2.02it/s] 55%|█████▌    | 1018/1836 [08:33<06:53,  1.98it/s] 56%|█████▌    | 1019/1836 [08:34<06:39,  2.05it/s] 56%|█████▌    | 1020/1836 [08:34<06:45,  2.01it/s] 56%|█████▌    | 1021/1836 [08:35<06:45,  2.01it/s] 56%|█████▌    | 1022/1836 [08:35<06:23,  2.12it/s] 56%|█████▌    | 1023/1836 [08:36<06:09,  2.20it/s] 56%|█████▌    | 1024/1836 [08:36<06:23,  2.12it/s] 56%|█████▌    | 1025/1836 [08:37<06:29,  2.08it/s] 56%|█████▌    | 1026/1836 [08:37<06:26,  2.09it/s] 56%|█████▌    | 1027/1836 [08:38<06:18,  2.14it/s] 56%|█████▌    | 1028/1836 [08:38<06:25,  2.09it/s] 56%|█████▌    | 1029/1836 [08:39<06:18,  2.13it/s] 56%|█████▌    | 1030/1836 [08:39<06:33,  2.05it/s] 56%|█████▌    | 1031/1836 [08:40<06:23,  2.10it/s] 56%|█████▌    | 1032/1836 [08:40<06:35,  2.03it/s] 56%|█████▋    | 1033/1836 [08:41<06:31,  2.05it/s] 56%|█████▋    | 1034/1836 [08:41<06:27,  2.07it/s] 56%|█████▋    | 1035/1836 [08:42<06:30,  2.05it/s] 56%|█████▋    | 1036/1836 [08:42<06:25,  2.07it/s] 56%|█████▋    | 1037/1836 [08:42<06:15,  2.13it/s] 57%|█████▋    | 1038/1836 [08:43<06:29,  2.05it/s] 57%|█████▋    | 1039/1836 [08:44<06:37,  2.00it/s] 57%|█████▋    | 1040/1836 [08:44<06:44,  1.97it/s] 57%|█████▋    | 1041/1836 [08:45<06:46,  1.95it/s] 57%|█████▋    | 1042/1836 [08:45<06:38,  1.99it/s] 57%|█████▋    | 1043/1836 [08:45<06:24,  2.06it/s] 57%|█████▋    | 1044/1836 [08:46<06:14,  2.12it/s] 57%|█████▋    | 1045/1836 [08:46<06:08,  2.15it/s] 57%|█████▋    | 1046/1836 [08:47<06:15,  2.10it/s] 57%|█████▋    | 1047/1836 [08:47<06:08,  2.14it/s] 57%|█████▋    | 1048/1836 [08:48<06:14,  2.10it/s] 57%|█████▋    | 1049/1836 [08:48<06:13,  2.11it/s] 57%|█████▋    | 1050/1836 [08:49<06:00,  2.18it/s] 57%|█████▋    | 1051/1836 [08:49<06:08,  2.13it/s] 57%|█████▋    | 1052/1836 [08:50<06:20,  2.06it/s] 57%|█████▋    | 1053/1836 [08:50<06:22,  2.05it/s] 57%|█████▋    | 1054/1836 [08:51<06:24,  2.03it/s] 57%|█████▋    | 1055/1836 [08:51<06:07,  2.13it/s] 58%|█████▊    | 1056/1836 [08:52<06:01,  2.16it/s] 58%|█████▊    | 1057/1836 [08:52<05:56,  2.18it/s] 58%|█████▊    | 1058/1836 [08:53<06:04,  2.13it/s] 58%|█████▊    | 1059/1836 [08:53<06:17,  2.06it/s] 58%|█████▊    | 1060/1836 [08:53<06:02,  2.14it/s] 58%|█████▊    | 1061/1836 [08:54<06:20,  2.04it/s] 58%|█████▊    | 1062/1836 [08:54<06:10,  2.09it/s] 58%|█████▊    | 1063/1836 [08:55<06:03,  2.12it/s] 58%|█████▊    | 1064/1836 [08:55<06:14,  2.06it/s] 58%|█████▊    | 1065/1836 [08:56<06:05,  2.11it/s] 58%|█████▊    | 1066/1836 [08:56<06:10,  2.08it/s] 58%|█████▊    | 1067/1836 [08:57<06:12,  2.06it/s] 58%|█████▊    | 1068/1836 [08:57<06:23,  2.00it/s] 58%|█████▊    | 1069/1836 [08:58<06:10,  2.07it/s] 58%|█████▊    | 1070/1836 [08:58<06:12,  2.06it/s] 58%|█████▊    | 1071/1836 [08:59<06:14,  2.04it/s] 58%|█████▊    | 1072/1836 [08:59<06:20,  2.01it/s] 58%|█████▊    | 1073/1836 [09:00<06:25,  1.98it/s] 58%|█████▊    | 1074/1836 [09:00<06:30,  1.95it/s] 59%|█████▊    | 1075/1836 [09:01<06:16,  2.02it/s] 59%|█████▊    | 1076/1836 [09:01<06:06,  2.08it/s] 59%|█████▊    | 1077/1836 [09:02<06:09,  2.05it/s] 59%|█████▊    | 1078/1836 [09:02<06:05,  2.07it/s] 59%|█████▉    | 1079/1836 [09:03<05:58,  2.11it/s] 59%|█████▉    | 1080/1836 [09:03<05:52,  2.14it/s] 59%|█████▉    | 1081/1836 [09:04<05:54,  2.13it/s] 59%|█████▉    | 1082/1836 [09:04<05:50,  2.15it/s] 59%|█████▉    | 1083/1836 [09:05<05:52,  2.14it/s] 59%|█████▉    | 1084/1836 [09:05<05:45,  2.17it/s] 59%|█████▉    | 1085/1836 [09:06<05:53,  2.13it/s] 59%|█████▉    | 1086/1836 [09:06<06:03,  2.06it/s] 59%|█████▉    | 1087/1836 [09:07<06:01,  2.07it/s] 59%|█████▉    | 1088/1836 [09:07<05:47,  2.15it/s] 59%|█████▉    | 1089/1836 [09:08<06:04,  2.05it/s] 59%|█████▉    | 1090/1836 [09:08<06:13,  2.00it/s] 59%|█████▉    | 1091/1836 [09:09<06:24,  1.94it/s] 59%|█████▉    | 1092/1836 [09:09<06:02,  2.05it/s] 60%|█████▉    | 1093/1836 [09:10<06:17,  1.97it/s] 60%|█████▉    | 1094/1836 [09:10<06:15,  1.98it/s] 60%|█████▉    | 1095/1836 [09:11<06:27,  1.91it/s] 60%|█████▉    | 1096/1836 [09:11<06:29,  1.90it/s] 60%|█████▉    | 1097/1836 [09:12<06:06,  2.02it/s] 60%|█████▉    | 1098/1836 [09:12<06:10,  1.99it/s] 60%|█████▉    | 1099/1836 [09:13<06:17,  1.95it/s] 60%|█████▉    | 1100/1836 [09:13<06:08,  2.00it/s] 60%|█████▉    | 1101/1836 [09:14<06:15,  1.96it/s] 60%|██████    | 1102/1836 [09:14<06:11,  1.98it/s] 60%|██████    | 1103/1836 [09:15<05:58,  2.04it/s] 60%|██████    | 1104/1836 [09:15<05:48,  2.10it/s] 60%|██████    | 1105/1836 [09:16<05:52,  2.08it/s] 60%|██████    | 1106/1836 [09:16<06:06,  1.99it/s] 60%|██████    | 1107/1836 [09:17<06:01,  2.02it/s] 60%|██████    | 1108/1836 [09:17<06:00,  2.02it/s] 60%|██████    | 1109/1836 [09:18<06:06,  1.99it/s] 60%|██████    | 1110/1836 [09:18<06:17,  1.92it/s] 61%|██████    | 1111/1836 [09:19<06:11,  1.95it/s] 61%|██████    | 1112/1836 [09:19<06:07,  1.97it/s] 61%|██████    | 1113/1836 [09:20<05:59,  2.01it/s] 61%|██████    | 1114/1836 [09:20<05:53,  2.04it/s] 61%|██████    | 1115/1836 [09:21<05:50,  2.06it/s] 61%|██████    | 1116/1836 [09:21<06:02,  1.98it/s] 61%|██████    | 1117/1836 [09:22<06:21,  1.88it/s] 61%|██████    | 1118/1836 [09:22<06:34,  1.82it/s] 61%|██████    | 1119/1836 [09:23<06:14,  1.92it/s] 61%|██████    | 1120/1836 [09:23<06:07,  1.95it/s] 61%|██████    | 1121/1836 [09:24<05:59,  1.99it/s] 61%|██████    | 1122/1836 [09:24<05:52,  2.02it/s] 61%|██████    | 1123/1836 [09:25<05:43,  2.08it/s] 61%|██████    | 1124/1836 [09:25<05:59,  1.98it/s] 61%|██████▏   | 1125/1836 [09:26<05:52,  2.02it/s] 61%|██████▏   | 1126/1836 [09:26<05:52,  2.01it/s] 61%|██████▏   | 1127/1836 [09:27<05:42,  2.07it/s] 61%|██████▏   | 1128/1836 [09:27<05:40,  2.08it/s] 61%|██████▏   | 1129/1836 [09:28<05:48,  2.03it/s] 62%|██████▏   | 1130/1836 [09:28<05:45,  2.04it/s] 62%|██████▏   | 1131/1836 [09:29<05:54,  1.99it/s] 62%|██████▏   | 1132/1836 [09:29<05:59,  1.96it/s] 62%|██████▏   | 1133/1836 [09:30<05:51,  2.00it/s] 62%|██████▏   | 1134/1836 [09:30<05:34,  2.10it/s] 62%|██████▏   | 1135/1836 [09:31<05:44,  2.04it/s] 62%|██████▏   | 1136/1836 [09:31<06:15,  1.87it/s] 62%|██████▏   | 1137/1836 [09:32<06:07,  1.90it/s] 62%|██████▏   | 1138/1836 [09:32<05:56,  1.96it/s] 62%|██████▏   | 1139/1836 [09:33<06:05,  1.91it/s] 62%|██████▏   | 1140/1836 [09:33<05:43,  2.02it/s] 62%|██████▏   | 1141/1836 [09:34<05:33,  2.08it/s] 62%|██████▏   | 1142/1836 [09:34<05:26,  2.13it/s] 62%|██████▏   | 1143/1836 [09:35<05:37,  2.05it/s] 62%|██████▏   | 1144/1836 [09:35<05:44,  2.01it/s] 62%|██████▏   | 1145/1836 [09:36<05:33,  2.07it/s] 62%|██████▏   | 1146/1836 [09:36<05:35,  2.06it/s] 62%|██████▏   | 1147/1836 [09:37<05:44,  2.00it/s] 63%|██████▎   | 1148/1836 [09:37<05:48,  1.97it/s] 63%|██████▎   | 1149/1836 [09:38<05:52,  1.95it/s] 63%|██████▎   | 1150/1836 [09:38<05:56,  1.92it/s] 63%|██████▎   | 1151/1836 [09:39<05:46,  1.98it/s] 63%|██████▎   | 1152/1836 [09:39<05:50,  1.95it/s] 63%|██████▎   | 1153/1836 [09:40<05:41,  2.00it/s] 63%|██████▎   | 1154/1836 [09:40<05:53,  1.93it/s] 63%|██████▎   | 1155/1836 [09:41<05:48,  1.95it/s] 63%|██████▎   | 1156/1836 [09:41<05:49,  1.94it/s] 63%|██████▎   | 1157/1836 [09:42<05:41,  1.99it/s] 63%|██████▎   | 1158/1836 [09:42<05:39,  2.00it/s] 63%|██████▎   | 1159/1836 [09:43<05:37,  2.00it/s] 63%|██████▎   | 1160/1836 [09:43<05:42,  1.97it/s] 63%|██████▎   | 1161/1836 [09:44<05:45,  1.95it/s] 63%|██████▎   | 1162/1836 [09:44<05:46,  1.95it/s] 63%|██████▎   | 1163/1836 [09:45<05:49,  1.93it/s] 63%|██████▎   | 1164/1836 [09:45<05:40,  1.97it/s] 63%|██████▎   | 1165/1836 [09:46<05:49,  1.92it/s] 64%|██████▎   | 1166/1836 [09:46<05:40,  1.97it/s] 64%|██████▎   | 1167/1836 [09:47<05:51,  1.91it/s] 64%|██████▎   | 1168/1836 [09:47<05:36,  1.98it/s] 64%|██████▎   | 1169/1836 [09:48<05:47,  1.92it/s] 64%|██████▎   | 1170/1836 [09:48<05:32,  2.00it/s] 64%|██████▍   | 1171/1836 [09:49<05:38,  1.96it/s] 64%|██████▍   | 1172/1836 [09:49<05:27,  2.03it/s] 64%|██████▍   | 1173/1836 [09:50<05:33,  1.99it/s] 64%|██████▍   | 1174/1836 [09:50<05:42,  1.93it/s] 64%|██████▍   | 1175/1836 [09:51<05:28,  2.01it/s] 64%|██████▍   | 1176/1836 [09:51<05:33,  1.98it/s] 64%|██████▍   | 1177/1836 [09:52<05:23,  2.04it/s] 64%|██████▍   | 1178/1836 [09:52<05:24,  2.03it/s] 64%|██████▍   | 1179/1836 [09:53<05:29,  2.00it/s] 64%|██████▍   | 1180/1836 [09:53<05:33,  1.97it/s] 64%|██████▍   | 1181/1836 [09:54<05:26,  2.01it/s] 64%|██████▍   | 1182/1836 [09:54<05:25,  2.01it/s] 64%|██████▍   | 1183/1836 [09:55<05:32,  1.97it/s] 64%|██████▍   | 1184/1836 [09:55<05:35,  1.94it/s] 65%|██████▍   | 1185/1836 [09:56<05:31,  1.97it/s] 65%|██████▍   | 1186/1836 [09:56<05:18,  2.04it/s] 65%|██████▍   | 1187/1836 [09:57<05:15,  2.06it/s] 65%|██████▍   | 1188/1836 [09:57<05:38,  1.91it/s] 65%|██████▍   | 1189/1836 [09:58<05:28,  1.97it/s] 65%|██████▍   | 1190/1836 [09:58<05:21,  2.01it/s] 65%|██████▍   | 1191/1836 [09:59<05:25,  1.98it/s] 65%|██████▍   | 1192/1836 [09:59<05:18,  2.02it/s] 65%|██████▍   | 1193/1836 [10:00<05:10,  2.07it/s] 65%|██████▌   | 1194/1836 [10:00<05:12,  2.06it/s] 65%|██████▌   | 1195/1836 [10:01<05:18,  2.01it/s] 65%|██████▌   | 1196/1836 [10:01<05:18,  2.01it/s] 65%|██████▌   | 1197/1836 [10:02<05:07,  2.08it/s] 65%|██████▌   | 1198/1836 [10:02<05:10,  2.06it/s] 65%|██████▌   | 1199/1836 [10:03<05:38,  1.88it/s] 65%|██████▌   | 1200/1836 [10:03<05:28,  1.94it/s] 65%|██████▌   | 1201/1836 [10:04<05:14,  2.02it/s] 65%|██████▌   | 1202/1836 [10:04<05:09,  2.05it/s] 66%|██████▌   | 1203/1836 [10:05<05:06,  2.06it/s] 66%|██████▌   | 1204/1836 [10:05<04:58,  2.12it/s] 66%|██████▌   | 1205/1836 [10:06<05:07,  2.05it/s] 66%|██████▌   | 1206/1836 [10:06<04:59,  2.11it/s] 66%|██████▌   | 1207/1836 [10:07<05:12,  2.01it/s] 66%|██████▌   | 1208/1836 [10:07<05:03,  2.07it/s] 66%|██████▌   | 1209/1836 [10:08<05:15,  1.99it/s] 66%|██████▌   | 1210/1836 [10:08<05:10,  2.01it/s] 66%|██████▌   | 1211/1836 [10:09<05:17,  1.97it/s] 66%|██████▌   | 1212/1836 [10:09<05:20,  1.95it/s] 66%|██████▌   | 1213/1836 [10:10<05:13,  1.99it/s] 66%|██████▌   | 1214/1836 [10:10<05:18,  1.95it/s] 66%|██████▌   | 1215/1836 [10:11<05:25,  1.91it/s] 66%|██████▌   | 1216/1836 [10:11<05:10,  1.99it/s] 66%|██████▋   | 1217/1836 [10:12<05:14,  1.97it/s] 66%|██████▋   | 1218/1836 [10:12<05:18,  1.94it/s] 66%|██████▋   | 1219/1836 [10:13<05:20,  1.93it/s] 66%|██████▋   | 1220/1836 [10:13<05:11,  1.98it/s] 67%|██████▋   | 1221/1836 [10:14<05:09,  1.99it/s] 67%|██████▋   | 1222/1836 [10:14<05:12,  1.97it/s] 67%|██████▋   | 1223/1836 [10:15<05:15,  1.94it/s] 67%|██████▋   | 1224/1836 [10:15<04:47,  2.13it/s]03/13/2024 20:40:51 - INFO - __main__ - epoch 1: {'accuracy': 0.6176470588235294, 'f1': 0.7132352941176471}
 67%|██████▋   | 1225/1836 [10:24<29:08,  2.86s/it] 67%|██████▋   | 1226/1836 [10:24<21:56,  2.16s/it] 67%|██████▋   | 1227/1836 [10:25<16:51,  1.66s/it] 67%|██████▋   | 1228/1836 [10:25<13:13,  1.31s/it] 67%|██████▋   | 1229/1836 [10:26<10:35,  1.05s/it] 67%|██████▋   | 1230/1836 [10:26<08:45,  1.15it/s] 67%|██████▋   | 1231/1836 [10:27<07:27,  1.35it/s] 67%|██████▋   | 1232/1836 [10:27<06:47,  1.48it/s] 67%|██████▋   | 1233/1836 [10:28<06:14,  1.61it/s] 67%|██████▋   | 1234/1836 [10:28<05:42,  1.76it/s] 67%|██████▋   | 1235/1836 [10:29<05:34,  1.80it/s] 67%|██████▋   | 1236/1836 [10:29<05:13,  1.91it/s] 67%|██████▋   | 1237/1836 [10:30<05:14,  1.90it/s] 67%|██████▋   | 1238/1836 [10:30<05:09,  1.93it/s] 67%|██████▋   | 1239/1836 [10:31<05:24,  1.84it/s] 68%|██████▊   | 1240/1836 [10:31<05:20,  1.86it/s] 68%|██████▊   | 1241/1836 [10:32<05:16,  1.88it/s] 68%|██████▊   | 1242/1836 [10:32<05:09,  1.92it/s] 68%|██████▊   | 1243/1836 [10:33<05:01,  1.97it/s] 68%|██████▊   | 1244/1836 [10:33<05:15,  1.87it/s] 68%|██████▊   | 1245/1836 [10:34<05:13,  1.88it/s] 68%|██████▊   | 1246/1836 [10:34<05:02,  1.95it/s] 68%|██████▊   | 1247/1836 [10:35<04:49,  2.03it/s] 68%|██████▊   | 1248/1836 [10:35<04:49,  2.03it/s] 68%|██████▊   | 1249/1836 [10:36<04:49,  2.03it/s] 68%|██████▊   | 1250/1836 [10:36<04:49,  2.03it/s] 68%|██████▊   | 1251/1836 [10:37<04:55,  1.98it/s] 68%|██████▊   | 1252/1836 [10:37<04:44,  2.06it/s] 68%|██████▊   | 1253/1836 [10:38<04:50,  2.01it/s] 68%|██████▊   | 1254/1836 [10:38<04:41,  2.07it/s] 68%|██████▊   | 1255/1836 [10:39<04:49,  2.01it/s] 68%|██████▊   | 1256/1836 [10:39<04:41,  2.06it/s] 68%|██████▊   | 1257/1836 [10:40<04:42,  2.05it/s] 69%|██████▊   | 1258/1836 [10:40<04:43,  2.04it/s] 69%|██████▊   | 1259/1836 [10:41<04:43,  2.03it/s] 69%|██████▊   | 1260/1836 [10:41<04:54,  1.95it/s] 69%|██████▊   | 1261/1836 [10:42<04:52,  1.96it/s] 69%|██████▊   | 1262/1836 [10:42<04:42,  2.03it/s] 69%|██████▉   | 1263/1836 [10:43<04:38,  2.06it/s] 69%|██████▉   | 1264/1836 [10:43<04:30,  2.12it/s] 69%|██████▉   | 1265/1836 [10:44<04:33,  2.09it/s] 69%|██████▉   | 1266/1836 [10:44<04:28,  2.12it/s] 69%|██████▉   | 1267/1836 [10:44<04:35,  2.06it/s] 69%|██████▉   | 1268/1836 [10:45<04:28,  2.12it/s] 69%|██████▉   | 1269/1836 [10:45<04:28,  2.11it/s] 69%|██████▉   | 1270/1836 [10:46<04:24,  2.14it/s] 69%|██████▉   | 1271/1836 [10:46<04:39,  2.03it/s] 69%|██████▉   | 1272/1836 [10:47<04:30,  2.09it/s] 69%|██████▉   | 1273/1836 [10:47<04:37,  2.03it/s] 69%|██████▉   | 1274/1836 [10:48<04:24,  2.12it/s] 69%|██████▉   | 1275/1836 [10:48<04:33,  2.05it/s] 69%|██████▉   | 1276/1836 [10:49<04:45,  1.96it/s] 70%|██████▉   | 1277/1836 [10:49<04:36,  2.03it/s] 70%|██████▉   | 1278/1836 [10:50<04:31,  2.05it/s] 70%|██████▉   | 1279/1836 [10:50<04:41,  1.98it/s] 70%|██████▉   | 1280/1836 [10:51<04:39,  1.99it/s] 70%|██████▉   | 1281/1836 [10:51<04:30,  2.05it/s] 70%|██████▉   | 1282/1836 [10:52<04:36,  2.00it/s] 70%|██████▉   | 1283/1836 [10:52<04:36,  2.00it/s] 70%|██████▉   | 1284/1836 [10:53<04:46,  1.93it/s] 70%|██████▉   | 1285/1836 [10:53<04:48,  1.91it/s] 70%|███████   | 1286/1836 [10:54<04:51,  1.89it/s] 70%|███████   | 1287/1836 [10:55<04:50,  1.89it/s] 70%|███████   | 1288/1836 [10:55<04:48,  1.90it/s] 70%|███████   | 1289/1836 [10:56<04:49,  1.89it/s] 70%|███████   | 1290/1836 [10:56<04:44,  1.92it/s] 70%|███████   | 1291/1836 [10:57<04:39,  1.95it/s] 70%|███████   | 1292/1836 [10:57<04:23,  2.06it/s] 70%|███████   | 1293/1836 [10:57<04:21,  2.08it/s] 70%|███████   | 1294/1836 [10:58<04:28,  2.02it/s] 71%|███████   | 1295/1836 [10:58<04:32,  1.99it/s] 71%|███████   | 1296/1836 [10:59<04:30,  2.00it/s] 71%|███████   | 1297/1836 [10:59<04:29,  2.00it/s] 71%|███████   | 1298/1836 [11:00<04:31,  1.98it/s] 71%|███████   | 1299/1836 [11:00<04:21,  2.06it/s] 71%|███████   | 1300/1836 [11:01<04:18,  2.07it/s] 71%|███████   | 1301/1836 [11:01<04:27,  2.00it/s] 71%|███████   | 1302/1836 [11:02<04:30,  1.97it/s] 71%|███████   | 1303/1836 [11:02<04:16,  2.08it/s] 71%|███████   | 1304/1836 [11:03<04:28,  1.98it/s] 71%|███████   | 1305/1836 [11:04<04:32,  1.95it/s] 71%|███████   | 1306/1836 [11:04<04:21,  2.02it/s] 71%|███████   | 1307/1836 [11:04<04:21,  2.02it/s] 71%|███████   | 1308/1836 [11:05<04:13,  2.08it/s] 71%|███████▏  | 1309/1836 [11:05<04:19,  2.03it/s] 71%|███████▏  | 1310/1836 [11:06<04:24,  1.99it/s] 71%|███████▏  | 1311/1836 [11:06<04:22,  2.00it/s] 71%|███████▏  | 1312/1836 [11:07<04:17,  2.04it/s] 72%|███████▏  | 1313/1836 [11:07<04:17,  2.03it/s] 72%|███████▏  | 1314/1836 [11:08<04:17,  2.03it/s] 72%|███████▏  | 1315/1836 [11:08<04:26,  1.95it/s] 72%|███████▏  | 1316/1836 [11:09<04:23,  1.97it/s] 72%|███████▏  | 1317/1836 [11:09<04:18,  2.01it/s] 72%|███████▏  | 1318/1836 [11:10<04:39,  1.86it/s] 72%|███████▏  | 1319/1836 [11:11<04:37,  1.87it/s] 72%|███████▏  | 1320/1836 [11:11<04:26,  1.94it/s] 72%|███████▏  | 1321/1836 [11:12<04:31,  1.90it/s] 72%|███████▏  | 1322/1836 [11:12<04:29,  1.90it/s] 72%|███████▏  | 1323/1836 [11:13<04:28,  1.91it/s] 72%|███████▏  | 1324/1836 [11:13<04:20,  1.97it/s] 72%|███████▏  | 1325/1836 [11:14<04:17,  1.98it/s] 72%|███████▏  | 1326/1836 [11:14<04:23,  1.93it/s] 72%|███████▏  | 1327/1836 [11:15<04:17,  1.98it/s] 72%|███████▏  | 1328/1836 [11:15<04:14,  1.99it/s] 72%|███████▏  | 1329/1836 [11:16<04:10,  2.02it/s] 72%|███████▏  | 1330/1836 [11:16<04:10,  2.02it/s] 72%|███████▏  | 1331/1836 [11:17<04:10,  2.01it/s] 73%|███████▎  | 1332/1836 [11:17<04:06,  2.04it/s] 73%|███████▎  | 1333/1836 [11:18<03:59,  2.10it/s] 73%|███████▎  | 1334/1836 [11:18<04:02,  2.07it/s] 73%|███████▎  | 1335/1836 [11:19<04:08,  2.01it/s] 73%|███████▎  | 1336/1836 [11:19<03:57,  2.11it/s] 73%|███████▎  | 1337/1836 [11:19<03:56,  2.11it/s] 73%|███████▎  | 1338/1836 [11:20<03:58,  2.09it/s] 73%|███████▎  | 1339/1836 [11:20<04:04,  2.03it/s] 73%|███████▎  | 1340/1836 [11:21<04:04,  2.03it/s] 73%|███████▎  | 1341/1836 [11:21<04:07,  2.00it/s] 73%|███████▎  | 1342/1836 [11:22<04:02,  2.04it/s] 73%|███████▎  | 1343/1836 [11:22<04:05,  2.01it/s] 73%|███████▎  | 1344/1836 [11:23<04:14,  1.93it/s] 73%|███████▎  | 1345/1836 [11:23<04:03,  2.01it/s] 73%|███████▎  | 1346/1836 [11:24<04:07,  1.98it/s] 73%|███████▎  | 1347/1836 [11:24<04:02,  2.02it/s] 73%|███████▎  | 1348/1836 [11:25<04:06,  1.98it/s] 73%|███████▎  | 1349/1836 [11:26<04:08,  1.96it/s] 74%|███████▎  | 1350/1836 [11:26<04:11,  1.93it/s] 74%|███████▎  | 1351/1836 [11:27<04:08,  1.95it/s] 74%|███████▎  | 1352/1836 [11:27<04:09,  1.94it/s] 74%|███████▎  | 1353/1836 [11:28<04:09,  1.93it/s] 74%|███████▎  | 1354/1836 [11:28<04:02,  1.98it/s] 74%|███████▍  | 1355/1836 [11:29<03:57,  2.02it/s] 74%|███████▍  | 1356/1836 [11:29<04:00,  1.99it/s] 74%|███████▍  | 1357/1836 [11:30<03:57,  2.02it/s] 74%|███████▍  | 1358/1836 [11:30<04:01,  1.98it/s] 74%|███████▍  | 1359/1836 [11:31<04:04,  1.95it/s] 74%|███████▍  | 1360/1836 [11:31<03:59,  1.99it/s] 74%|███████▍  | 1361/1836 [11:32<03:49,  2.07it/s] 74%|███████▍  | 1362/1836 [11:32<03:53,  2.03it/s] 74%|███████▍  | 1363/1836 [11:32<03:46,  2.08it/s] 74%|███████▍  | 1364/1836 [11:33<03:44,  2.10it/s] 74%|███████▍  | 1365/1836 [11:33<03:38,  2.15it/s] 74%|███████▍  | 1366/1836 [11:34<03:46,  2.07it/s] 74%|███████▍  | 1367/1836 [11:34<03:45,  2.08it/s] 75%|███████▍  | 1368/1836 [11:35<04:02,  1.93it/s] 75%|███████▍  | 1369/1836 [11:35<03:57,  1.97it/s] 75%|███████▍  | 1370/1836 [11:36<03:59,  1.95it/s] 75%|███████▍  | 1371/1836 [11:37<04:00,  1.93it/s] 75%|███████▍  | 1372/1836 [11:37<03:57,  1.96it/s] 75%|███████▍  | 1373/1836 [11:38<03:57,  1.95it/s] 75%|███████▍  | 1374/1836 [11:38<03:59,  1.93it/s] 75%|███████▍  | 1375/1836 [11:39<03:55,  1.95it/s] 75%|███████▍  | 1376/1836 [11:39<03:42,  2.06it/s] 75%|███████▌  | 1377/1836 [11:40<03:46,  2.02it/s] 75%|███████▌  | 1378/1836 [11:40<03:50,  1.98it/s] 75%|███████▌  | 1379/1836 [11:41<03:52,  1.97it/s] 75%|███████▌  | 1380/1836 [11:41<03:52,  1.96it/s] 75%|███████▌  | 1381/1836 [11:42<03:55,  1.93it/s] 75%|███████▌  | 1382/1836 [11:42<04:01,  1.88it/s] 75%|███████▌  | 1383/1836 [11:43<04:04,  1.85it/s] 75%|███████▌  | 1384/1836 [11:43<03:52,  1.95it/s] 75%|███████▌  | 1385/1836 [11:44<03:53,  1.94it/s] 75%|███████▌  | 1386/1836 [11:44<03:53,  1.93it/s] 76%|███████▌  | 1387/1836 [11:45<03:46,  1.98it/s] 76%|███████▌  | 1388/1836 [11:45<03:52,  1.93it/s] 76%|███████▌  | 1389/1836 [11:46<03:52,  1.92it/s] 76%|███████▌  | 1390/1836 [11:46<03:51,  1.93it/s] 76%|███████▌  | 1391/1836 [11:47<03:40,  2.02it/s] 76%|███████▌  | 1392/1836 [11:47<03:43,  1.99it/s] 76%|███████▌  | 1393/1836 [11:48<03:35,  2.06it/s] 76%|███████▌  | 1394/1836 [11:48<03:36,  2.04it/s] 76%|███████▌  | 1395/1836 [11:49<03:26,  2.14it/s] 76%|███████▌  | 1396/1836 [11:49<03:29,  2.10it/s] 76%|███████▌  | 1397/1836 [11:50<03:28,  2.10it/s] 76%|███████▌  | 1398/1836 [11:50<03:28,  2.10it/s] 76%|███████▌  | 1399/1836 [11:51<03:34,  2.04it/s] 76%|███████▋  | 1400/1836 [11:51<03:32,  2.05it/s] 76%|███████▋  | 1401/1836 [11:52<03:26,  2.11it/s] 76%|███████▋  | 1402/1836 [11:52<03:32,  2.04it/s] 76%|███████▋  | 1403/1836 [11:53<03:37,  2.00it/s] 76%|███████▋  | 1404/1836 [11:53<03:32,  2.03it/s] 77%|███████▋  | 1405/1836 [11:54<03:30,  2.05it/s] 77%|███████▋  | 1406/1836 [11:54<03:31,  2.04it/s] 77%|███████▋  | 1407/1836 [11:55<03:35,  1.99it/s] 77%|███████▋  | 1408/1836 [11:55<03:34,  2.00it/s] 77%|███████▋  | 1409/1836 [11:56<03:29,  2.04it/s] 77%|███████▋  | 1410/1836 [11:56<03:32,  2.00it/s] 77%|███████▋  | 1411/1836 [11:56<03:28,  2.04it/s] 77%|███████▋  | 1412/1836 [11:57<03:32,  2.00it/s] 77%|███████▋  | 1413/1836 [11:58<03:31,  2.00it/s] 77%|███████▋  | 1414/1836 [11:58<03:23,  2.07it/s] 77%|███████▋  | 1415/1836 [11:59<03:32,  1.98it/s] 77%|███████▋  | 1416/1836 [11:59<03:28,  2.01it/s] 77%|███████▋  | 1417/1836 [11:59<03:18,  2.11it/s] 77%|███████▋  | 1418/1836 [12:00<03:24,  2.05it/s] 77%|███████▋  | 1419/1836 [12:00<03:18,  2.11it/s] 77%|███████▋  | 1420/1836 [12:01<03:32,  1.96it/s] 77%|███████▋  | 1421/1836 [12:01<03:33,  1.94it/s] 77%|███████▋  | 1422/1836 [12:02<03:28,  1.99it/s] 78%|███████▊  | 1423/1836 [12:02<03:20,  2.06it/s] 78%|███████▊  | 1424/1836 [12:03<03:20,  2.05it/s] 78%|███████▊  | 1425/1836 [12:03<03:25,  2.00it/s] 78%|███████▊  | 1426/1836 [12:04<03:27,  1.98it/s] 78%|███████▊  | 1427/1836 [12:04<03:23,  2.01it/s] 78%|███████▊  | 1428/1836 [12:05<03:22,  2.02it/s] 78%|███████▊  | 1429/1836 [12:05<03:21,  2.02it/s] 78%|███████▊  | 1430/1836 [12:06<03:18,  2.05it/s] 78%|███████▊  | 1431/1836 [12:06<03:11,  2.11it/s] 78%|███████▊  | 1432/1836 [12:07<03:20,  2.01it/s] 78%|███████▊  | 1433/1836 [12:07<03:17,  2.04it/s] 78%|███████▊  | 1434/1836 [12:08<03:12,  2.09it/s] 78%|███████▊  | 1435/1836 [12:08<03:08,  2.13it/s] 78%|███████▊  | 1436/1836 [12:09<03:05,  2.16it/s] 78%|███████▊  | 1437/1836 [12:09<02:59,  2.22it/s] 78%|███████▊  | 1438/1836 [12:10<03:07,  2.12it/s] 78%|███████▊  | 1439/1836 [12:10<03:08,  2.11it/s] 78%|███████▊  | 1440/1836 [12:11<03:13,  2.05it/s] 78%|███████▊  | 1441/1836 [12:11<03:16,  2.01it/s] 79%|███████▊  | 1442/1836 [12:12<03:19,  1.98it/s] 79%|███████▊  | 1443/1836 [12:12<03:21,  1.95it/s] 79%|███████▊  | 1444/1836 [12:13<03:13,  2.02it/s] 79%|███████▊  | 1445/1836 [12:13<03:08,  2.08it/s] 79%|███████▉  | 1446/1836 [12:14<03:16,  1.99it/s] 79%|███████▉  | 1447/1836 [12:14<03:12,  2.02it/s] 79%|███████▉  | 1448/1836 [12:15<03:09,  2.05it/s] 79%|███████▉  | 1449/1836 [12:15<03:15,  1.98it/s] 79%|███████▉  | 1450/1836 [12:16<03:08,  2.05it/s] 79%|███████▉  | 1451/1836 [12:16<03:13,  1.99it/s] 79%|███████▉  | 1452/1836 [12:17<03:28,  1.85it/s] 79%|███████▉  | 1453/1836 [12:17<03:29,  1.83it/s] 79%|███████▉  | 1454/1836 [12:18<03:27,  1.84it/s] 79%|███████▉  | 1455/1836 [12:18<03:15,  1.95it/s] 79%|███████▉  | 1456/1836 [12:19<03:18,  1.91it/s] 79%|███████▉  | 1457/1836 [12:19<03:13,  1.96it/s] 79%|███████▉  | 1458/1836 [12:20<03:08,  2.00it/s] 79%|███████▉  | 1459/1836 [12:20<03:05,  2.03it/s] 80%|███████▉  | 1460/1836 [12:21<03:05,  2.02it/s] 80%|███████▉  | 1461/1836 [12:21<03:09,  1.98it/s] 80%|███████▉  | 1462/1836 [12:22<03:06,  2.01it/s] 80%|███████▉  | 1463/1836 [12:22<03:08,  1.98it/s] 80%|███████▉  | 1464/1836 [12:23<03:06,  1.99it/s] 80%|███████▉  | 1465/1836 [12:23<03:03,  2.02it/s] 80%|███████▉  | 1466/1836 [12:24<03:05,  1.99it/s] 80%|███████▉  | 1467/1836 [12:25<03:24,  1.81it/s] 80%|███████▉  | 1468/1836 [12:25<03:17,  1.86it/s] 80%|████████  | 1469/1836 [12:25<03:10,  1.92it/s] 80%|████████  | 1470/1836 [12:26<03:02,  2.01it/s] 80%|████████  | 1471/1836 [12:26<02:58,  2.04it/s] 80%|████████  | 1472/1836 [12:27<03:01,  2.00it/s] 80%|████████  | 1473/1836 [12:27<03:07,  1.94it/s] 80%|████████  | 1474/1836 [12:28<03:04,  1.96it/s] 80%|████████  | 1475/1836 [12:28<02:57,  2.04it/s] 80%|████████  | 1476/1836 [12:29<03:00,  2.00it/s] 80%|████████  | 1477/1836 [12:29<03:02,  1.97it/s] 81%|████████  | 1478/1836 [12:30<02:58,  2.01it/s] 81%|████████  | 1479/1836 [12:30<02:56,  2.02it/s] 81%|████████  | 1480/1836 [12:31<02:51,  2.07it/s] 81%|████████  | 1481/1836 [12:31<02:50,  2.08it/s] 81%|████████  | 1482/1836 [12:32<02:46,  2.13it/s] 81%|████████  | 1483/1836 [12:32<02:42,  2.17it/s] 81%|████████  | 1484/1836 [12:33<02:45,  2.13it/s] 81%|████████  | 1485/1836 [12:33<02:44,  2.13it/s] 81%|████████  | 1486/1836 [12:34<02:37,  2.22it/s] 81%|████████  | 1487/1836 [12:34<02:41,  2.16it/s] 81%|████████  | 1488/1836 [12:35<02:44,  2.11it/s] 81%|████████  | 1489/1836 [12:35<02:46,  2.09it/s] 81%|████████  | 1490/1836 [12:36<02:47,  2.06it/s] 81%|████████  | 1491/1836 [12:36<02:48,  2.05it/s] 81%|████████▏ | 1492/1836 [12:37<02:54,  1.98it/s] 81%|████████▏ | 1493/1836 [12:37<02:55,  1.95it/s] 81%|████████▏ | 1494/1836 [12:38<02:56,  1.94it/s] 81%|████████▏ | 1495/1836 [12:38<02:46,  2.05it/s] 81%|████████▏ | 1496/1836 [12:39<02:44,  2.07it/s] 82%|████████▏ | 1497/1836 [12:39<02:40,  2.11it/s] 82%|████████▏ | 1498/1836 [12:40<02:42,  2.08it/s] 82%|████████▏ | 1499/1836 [12:40<02:48,  2.00it/s] 82%|████████▏ | 1500/1836 [12:41<02:43,  2.06it/s] 82%|████████▏ | 1501/1836 [12:41<02:39,  2.10it/s] 82%|████████▏ | 1502/1836 [12:41<02:38,  2.11it/s] 82%|████████▏ | 1503/1836 [12:42<02:37,  2.11it/s] 82%|████████▏ | 1504/1836 [12:42<02:44,  2.02it/s] 82%|████████▏ | 1505/1836 [12:43<02:50,  1.94it/s] 82%|████████▏ | 1506/1836 [12:44<02:53,  1.91it/s] 82%|████████▏ | 1507/1836 [12:44<02:53,  1.90it/s] 82%|████████▏ | 1508/1836 [12:45<02:45,  1.99it/s] 82%|████████▏ | 1509/1836 [12:45<02:41,  2.02it/s] 82%|████████▏ | 1510/1836 [12:45<02:33,  2.12it/s] 82%|████████▏ | 1511/1836 [12:46<02:30,  2.16it/s] 82%|████████▏ | 1512/1836 [12:46<02:36,  2.07it/s] 82%|████████▏ | 1513/1836 [12:47<02:39,  2.03it/s] 82%|████████▏ | 1514/1836 [12:47<02:39,  2.02it/s] 83%|████████▎ | 1515/1836 [12:48<02:40,  2.00it/s] 83%|████████▎ | 1516/1836 [12:48<02:34,  2.08it/s] 83%|████████▎ | 1517/1836 [12:49<02:29,  2.13it/s] 83%|████████▎ | 1518/1836 [12:49<02:29,  2.13it/s] 83%|████████▎ | 1519/1836 [12:50<02:29,  2.13it/s] 83%|████████▎ | 1520/1836 [12:50<02:30,  2.09it/s] 83%|████████▎ | 1521/1836 [12:51<02:37,  2.01it/s] 83%|████████▎ | 1522/1836 [12:51<02:31,  2.07it/s] 83%|████████▎ | 1523/1836 [12:52<02:32,  2.05it/s] 83%|████████▎ | 1524/1836 [12:52<02:37,  1.98it/s] 83%|████████▎ | 1525/1836 [12:53<02:31,  2.05it/s] 83%|████████▎ | 1526/1836 [12:53<02:36,  1.98it/s] 83%|████████▎ | 1527/1836 [12:54<02:35,  1.99it/s] 83%|████████▎ | 1528/1836 [12:54<02:38,  1.94it/s] 83%|████████▎ | 1529/1836 [12:55<02:32,  2.02it/s] 83%|████████▎ | 1530/1836 [12:55<02:31,  2.02it/s] 83%|████████▎ | 1531/1836 [12:56<02:39,  1.91it/s] 83%|████████▎ | 1532/1836 [12:56<02:46,  1.83it/s] 83%|████████▎ | 1533/1836 [12:57<02:43,  1.85it/s] 84%|████████▎ | 1534/1836 [12:58<02:44,  1.84it/s] 84%|████████▎ | 1535/1836 [12:58<02:34,  1.94it/s] 84%|████████▎ | 1536/1836 [12:58<02:28,  2.03it/s] 84%|████████▎ | 1537/1836 [12:59<02:23,  2.09it/s] 84%|████████▍ | 1538/1836 [12:59<02:24,  2.07it/s] 84%|████████▍ | 1539/1836 [13:00<02:20,  2.11it/s] 84%|████████▍ | 1540/1836 [13:00<02:24,  2.04it/s] 84%|████████▍ | 1541/1836 [13:01<02:27,  2.01it/s] 84%|████████▍ | 1542/1836 [13:01<02:19,  2.10it/s] 84%|████████▍ | 1543/1836 [13:02<02:23,  2.04it/s] 84%|████████▍ | 1544/1836 [13:02<02:25,  2.00it/s] 84%|████████▍ | 1545/1836 [13:03<02:28,  1.97it/s] 84%|████████▍ | 1546/1836 [13:03<02:28,  1.95it/s] 84%|████████▍ | 1547/1836 [13:04<02:24,  2.00it/s] 84%|████████▍ | 1548/1836 [13:04<02:26,  1.97it/s] 84%|████████▍ | 1549/1836 [13:05<02:22,  2.01it/s] 84%|████████▍ | 1550/1836 [13:05<02:15,  2.11it/s] 84%|████████▍ | 1551/1836 [13:06<02:12,  2.15it/s] 85%|████████▍ | 1552/1836 [13:06<02:08,  2.22it/s] 85%|████████▍ | 1553/1836 [13:07<02:13,  2.13it/s] 85%|████████▍ | 1554/1836 [13:07<02:10,  2.16it/s] 85%|████████▍ | 1555/1836 [13:08<02:12,  2.12it/s] 85%|████████▍ | 1556/1836 [13:08<02:09,  2.16it/s] 85%|████████▍ | 1557/1836 [13:09<02:10,  2.15it/s] 85%|████████▍ | 1558/1836 [13:09<02:05,  2.22it/s] 85%|████████▍ | 1559/1836 [13:09<02:08,  2.16it/s] 85%|████████▍ | 1560/1836 [13:10<02:04,  2.22it/s] 85%|████████▌ | 1561/1836 [13:10<02:09,  2.13it/s] 85%|████████▌ | 1562/1836 [13:11<02:10,  2.10it/s] 85%|████████▌ | 1563/1836 [13:11<02:16,  2.01it/s] 85%|████████▌ | 1564/1836 [13:12<02:19,  1.95it/s] 85%|████████▌ | 1565/1836 [13:12<02:13,  2.02it/s] 85%|████████▌ | 1566/1836 [13:13<02:13,  2.02it/s] 85%|████████▌ | 1567/1836 [13:13<02:15,  1.99it/s] 85%|████████▌ | 1568/1836 [13:14<02:13,  2.00it/s] 85%|████████▌ | 1569/1836 [13:14<02:11,  2.03it/s] 86%|████████▌ | 1570/1836 [13:15<02:09,  2.06it/s] 86%|████████▌ | 1571/1836 [13:15<02:13,  1.98it/s] 86%|████████▌ | 1572/1836 [13:16<02:08,  2.05it/s] 86%|████████▌ | 1573/1836 [13:16<02:11,  2.00it/s] 86%|████████▌ | 1574/1836 [13:17<02:06,  2.07it/s] 86%|████████▌ | 1575/1836 [13:17<02:11,  1.99it/s] 86%|████████▌ | 1576/1836 [13:18<02:06,  2.05it/s] 86%|████████▌ | 1577/1836 [13:18<02:09,  2.00it/s] 86%|████████▌ | 1578/1836 [13:19<02:10,  1.98it/s] 86%|████████▌ | 1579/1836 [13:19<02:07,  2.01it/s] 86%|████████▌ | 1580/1836 [13:20<02:03,  2.07it/s] 86%|████████▌ | 1581/1836 [13:20<02:02,  2.09it/s] 86%|████████▌ | 1582/1836 [13:21<02:01,  2.09it/s] 86%|████████▌ | 1583/1836 [13:21<02:03,  2.04it/s] 86%|████████▋ | 1584/1836 [13:22<01:57,  2.14it/s] 86%|████████▋ | 1585/1836 [13:22<02:01,  2.07it/s] 86%|████████▋ | 1586/1836 [13:23<02:06,  1.98it/s] 86%|████████▋ | 1587/1836 [13:23<02:07,  1.95it/s] 86%|████████▋ | 1588/1836 [13:24<02:04,  2.00it/s] 87%|████████▋ | 1589/1836 [13:24<02:06,  1.96it/s] 87%|████████▋ | 1590/1836 [13:25<02:06,  1.95it/s] 87%|████████▋ | 1591/1836 [13:25<02:01,  2.02it/s] 87%|████████▋ | 1592/1836 [13:26<01:57,  2.09it/s] 87%|████████▋ | 1593/1836 [13:26<01:56,  2.09it/s] 87%|████████▋ | 1594/1836 [13:27<01:59,  2.02it/s] 87%|████████▋ | 1595/1836 [13:27<01:59,  2.02it/s] 87%|████████▋ | 1596/1836 [13:28<01:55,  2.08it/s] 87%|████████▋ | 1597/1836 [13:28<01:54,  2.09it/s] 87%|████████▋ | 1598/1836 [13:29<01:53,  2.10it/s] 87%|████████▋ | 1599/1836 [13:29<01:51,  2.13it/s] 87%|████████▋ | 1600/1836 [13:30<01:53,  2.07it/s] 87%|████████▋ | 1601/1836 [13:30<01:52,  2.08it/s] 87%|████████▋ | 1602/1836 [13:31<01:51,  2.09it/s] 87%|████████▋ | 1603/1836 [13:31<01:51,  2.10it/s] 87%|████████▋ | 1604/1836 [13:31<01:51,  2.08it/s] 87%|████████▋ | 1605/1836 [13:32<01:54,  2.02it/s] 87%|████████▋ | 1606/1836 [13:32<01:52,  2.04it/s] 88%|████████▊ | 1607/1836 [13:33<01:52,  2.03it/s] 88%|████████▊ | 1608/1836 [13:33<01:49,  2.09it/s] 88%|████████▊ | 1609/1836 [13:34<01:48,  2.10it/s] 88%|████████▊ | 1610/1836 [13:34<01:49,  2.07it/s] 88%|████████▊ | 1611/1836 [13:35<01:44,  2.16it/s] 88%|████████▊ | 1612/1836 [13:35<01:47,  2.09it/s] 88%|████████▊ | 1613/1836 [13:36<01:46,  2.09it/s] 88%|████████▊ | 1614/1836 [13:36<01:44,  2.13it/s] 88%|████████▊ | 1615/1836 [13:37<01:45,  2.10it/s] 88%|████████▊ | 1616/1836 [13:37<01:40,  2.18it/s] 88%|████████▊ | 1617/1836 [13:38<01:44,  2.10it/s] 88%|████████▊ | 1618/1836 [13:38<01:45,  2.08it/s] 88%|████████▊ | 1619/1836 [13:39<01:42,  2.13it/s] 88%|████████▊ | 1620/1836 [13:39<01:49,  1.98it/s] 88%|████████▊ | 1621/1836 [13:40<01:49,  1.96it/s] 88%|████████▊ | 1622/1836 [13:40<01:52,  1.90it/s] 88%|████████▊ | 1623/1836 [13:41<01:52,  1.89it/s] 88%|████████▊ | 1624/1836 [13:41<01:48,  1.95it/s] 89%|████████▊ | 1625/1836 [13:42<01:49,  1.93it/s] 89%|████████▊ | 1626/1836 [13:42<01:49,  1.92it/s] 89%|████████▊ | 1627/1836 [13:43<01:47,  1.94it/s] 89%|████████▊ | 1628/1836 [13:43<01:46,  1.94it/s] 89%|████████▊ | 1629/1836 [13:44<01:42,  2.03it/s] 89%|████████▉ | 1630/1836 [13:44<01:37,  2.12it/s] 89%|████████▉ | 1631/1836 [13:45<01:39,  2.05it/s] 89%|████████▉ | 1632/1836 [13:45<01:35,  2.14it/s] 89%|████████▉ | 1633/1836 [13:46<01:37,  2.08it/s] 89%|████████▉ | 1634/1836 [13:46<01:34,  2.13it/s] 89%|████████▉ | 1635/1836 [13:47<01:37,  2.07it/s] 89%|████████▉ | 1636/1836 [13:47<01:36,  2.08it/s] 89%|████████▉ | 1637/1836 [13:48<01:38,  2.01it/s] 89%|████████▉ | 1638/1836 [13:48<01:38,  2.01it/s] 89%|████████▉ | 1639/1836 [13:49<01:37,  2.01it/s] 89%|████████▉ | 1640/1836 [13:49<01:39,  1.98it/s] 89%|████████▉ | 1641/1836 [13:50<01:43,  1.89it/s] 89%|████████▉ | 1642/1836 [13:50<01:42,  1.90it/s] 89%|████████▉ | 1643/1836 [13:51<01:43,  1.87it/s] 90%|████████▉ | 1644/1836 [13:51<01:41,  1.89it/s] 90%|████████▉ | 1645/1836 [13:52<01:38,  1.95it/s] 90%|████████▉ | 1646/1836 [13:52<01:34,  2.02it/s] 90%|████████▉ | 1647/1836 [13:53<01:36,  1.95it/s] 90%|████████▉ | 1648/1836 [13:53<01:34,  1.99it/s] 90%|████████▉ | 1649/1836 [13:54<01:33,  2.00it/s] 90%|████████▉ | 1650/1836 [13:54<01:34,  1.96it/s] 90%|████████▉ | 1651/1836 [13:55<01:29,  2.07it/s] 90%|████████▉ | 1652/1836 [13:55<01:26,  2.12it/s] 90%|█████████ | 1653/1836 [13:56<01:29,  2.04it/s] 90%|█████████ | 1654/1836 [13:56<01:28,  2.06it/s] 90%|█████████ | 1655/1836 [13:57<01:25,  2.11it/s] 90%|█████████ | 1656/1836 [13:57<01:29,  2.02it/s] 90%|█████████ | 1657/1836 [13:58<01:28,  2.02it/s] 90%|█████████ | 1658/1836 [13:58<01:29,  2.00it/s] 90%|█████████ | 1659/1836 [13:59<01:27,  2.03it/s] 90%|█████████ | 1660/1836 [13:59<01:26,  2.02it/s] 90%|█████████ | 1661/1836 [14:00<01:25,  2.04it/s] 91%|█████████ | 1662/1836 [14:00<01:27,  1.99it/s] 91%|█████████ | 1663/1836 [14:01<01:28,  1.96it/s] 91%|█████████ | 1664/1836 [14:01<01:26,  2.00it/s] 91%|█████████ | 1665/1836 [14:02<01:26,  1.97it/s] 91%|█████████ | 1666/1836 [14:02<01:27,  1.95it/s] 91%|█████████ | 1667/1836 [14:03<01:25,  1.97it/s] 91%|█████████ | 1668/1836 [14:03<01:25,  1.96it/s] 91%|█████████ | 1669/1836 [14:04<01:25,  1.95it/s] 91%|█████████ | 1670/1836 [14:04<01:24,  1.96it/s] 91%|█████████ | 1671/1836 [14:05<01:22,  2.00it/s] 91%|█████████ | 1672/1836 [14:05<01:24,  1.94it/s] 91%|█████████ | 1673/1836 [14:06<01:25,  1.90it/s] 91%|█████████ | 1674/1836 [14:06<01:26,  1.88it/s] 91%|█████████ | 1675/1836 [14:07<01:23,  1.94it/s] 91%|█████████▏| 1676/1836 [14:07<01:20,  1.99it/s] 91%|█████████▏| 1677/1836 [14:08<01:16,  2.09it/s] 91%|█████████▏| 1678/1836 [14:08<01:18,  2.01it/s] 91%|█████████▏| 1679/1836 [14:09<01:17,  2.01it/s] 92%|█████████▏| 1680/1836 [14:09<01:15,  2.08it/s] 92%|█████████▏| 1681/1836 [14:10<01:12,  2.13it/s] 92%|█████████▏| 1682/1836 [14:10<01:11,  2.16it/s] 92%|█████████▏| 1683/1836 [14:11<01:09,  2.19it/s] 92%|█████████▏| 1684/1836 [14:11<01:13,  2.07it/s] 92%|█████████▏| 1685/1836 [14:12<01:11,  2.11it/s] 92%|█████████▏| 1686/1836 [14:12<01:11,  2.09it/s] 92%|█████████▏| 1687/1836 [14:13<01:10,  2.13it/s] 92%|█████████▏| 1688/1836 [14:13<01:09,  2.12it/s] 92%|█████████▏| 1689/1836 [14:14<01:11,  2.05it/s] 92%|█████████▏| 1690/1836 [14:14<01:13,  2.00it/s] 92%|█████████▏| 1691/1836 [14:15<01:14,  1.94it/s] 92%|█████████▏| 1692/1836 [14:15<01:12,  1.99it/s] 92%|█████████▏| 1693/1836 [14:16<01:10,  2.02it/s] 92%|█████████▏| 1694/1836 [14:16<01:08,  2.09it/s] 92%|█████████▏| 1695/1836 [14:17<01:10,  2.00it/s] 92%|█████████▏| 1696/1836 [14:17<01:11,  1.97it/s] 92%|█████████▏| 1697/1836 [14:18<01:12,  1.91it/s] 92%|█████████▏| 1698/1836 [14:18<01:11,  1.94it/s] 93%|█████████▎| 1699/1836 [14:19<01:10,  1.94it/s] 93%|█████████▎| 1700/1836 [14:19<01:10,  1.92it/s] 93%|█████████▎| 1701/1836 [14:20<01:07,  2.01it/s] 93%|█████████▎| 1702/1836 [14:20<01:06,  2.02it/s] 93%|█████████▎| 1703/1836 [14:21<01:04,  2.05it/s] 93%|█████████▎| 1704/1836 [14:21<01:04,  2.06it/s] 93%|█████████▎| 1705/1836 [14:22<01:03,  2.08it/s] 93%|█████████▎| 1706/1836 [14:22<01:04,  2.02it/s] 93%|█████████▎| 1707/1836 [14:23<01:03,  2.04it/s] 93%|█████████▎| 1708/1836 [14:23<01:00,  2.10it/s] 93%|█████████▎| 1709/1836 [14:23<01:00,  2.11it/s] 93%|█████████▎| 1710/1836 [14:24<01:00,  2.09it/s] 93%|█████████▎| 1711/1836 [14:24<00:59,  2.09it/s] 93%|█████████▎| 1712/1836 [14:25<01:00,  2.04it/s] 93%|█████████▎| 1713/1836 [14:25<00:58,  2.09it/s] 93%|█████████▎| 1714/1836 [14:26<00:57,  2.14it/s] 93%|█████████▎| 1715/1836 [14:26<00:55,  2.17it/s] 93%|█████████▎| 1716/1836 [14:27<00:55,  2.15it/s] 94%|█████████▎| 1717/1836 [14:27<00:55,  2.13it/s] 94%|█████████▎| 1718/1836 [14:28<00:56,  2.10it/s] 94%|█████████▎| 1719/1836 [14:28<00:57,  2.03it/s] 94%|█████████▎| 1720/1836 [14:29<00:56,  2.05it/s] 94%|█████████▎| 1721/1836 [14:29<00:55,  2.07it/s] 94%|█████████▍| 1722/1836 [14:30<01:00,  1.89it/s] 94%|█████████▍| 1723/1836 [14:30<00:58,  1.92it/s] 94%|█████████▍| 1724/1836 [14:31<00:58,  1.92it/s] 94%|█████████▍| 1725/1836 [14:31<00:54,  2.04it/s] 94%|█████████▍| 1726/1836 [14:32<00:54,  2.04it/s] 94%|█████████▍| 1727/1836 [14:32<00:55,  1.97it/s] 94%|█████████▍| 1728/1836 [14:33<00:55,  1.94it/s] 94%|█████████▍| 1729/1836 [14:33<00:53,  1.99it/s] 94%|█████████▍| 1730/1836 [14:34<00:53,  1.99it/s] 94%|█████████▍| 1731/1836 [14:34<00:52,  2.00it/s] 94%|█████████▍| 1732/1836 [14:35<00:51,  2.01it/s] 94%|█████████▍| 1733/1836 [14:35<00:52,  1.98it/s] 94%|█████████▍| 1734/1836 [14:36<00:52,  1.96it/s] 94%|█████████▍| 1735/1836 [14:36<00:52,  1.94it/s] 95%|█████████▍| 1736/1836 [14:37<00:50,  1.99it/s] 95%|█████████▍| 1737/1836 [14:37<00:49,  2.00it/s] 95%|█████████▍| 1738/1836 [14:38<00:49,  1.98it/s] 95%|█████████▍| 1739/1836 [14:38<00:49,  1.95it/s] 95%|█████████▍| 1740/1836 [14:39<00:49,  1.92it/s] 95%|█████████▍| 1741/1836 [14:39<00:49,  1.91it/s] 95%|█████████▍| 1742/1836 [14:40<00:49,  1.91it/s] 95%|█████████▍| 1743/1836 [14:40<00:46,  1.99it/s] 95%|█████████▍| 1744/1836 [14:41<00:45,  2.03it/s] 95%|█████████▌| 1745/1836 [14:41<00:46,  1.95it/s] 95%|█████████▌| 1746/1836 [14:42<00:46,  1.94it/s] 95%|█████████▌| 1747/1836 [14:43<00:45,  1.96it/s] 95%|█████████▌| 1748/1836 [14:43<00:43,  2.03it/s] 95%|█████████▌| 1749/1836 [14:43<00:42,  2.06it/s] 95%|█████████▌| 1750/1836 [14:44<00:42,  2.00it/s] 95%|█████████▌| 1751/1836 [14:44<00:41,  2.07it/s] 95%|█████████▌| 1752/1836 [14:45<00:41,  2.02it/s] 95%|█████████▌| 1753/1836 [14:45<00:42,  1.96it/s] 96%|█████████▌| 1754/1836 [14:46<00:39,  2.07it/s] 96%|█████████▌| 1755/1836 [14:46<00:38,  2.08it/s] 96%|█████████▌| 1756/1836 [14:47<00:37,  2.13it/s] 96%|█████████▌| 1757/1836 [14:47<00:41,  1.92it/s] 96%|█████████▌| 1758/1836 [14:48<00:41,  1.89it/s] 96%|█████████▌| 1759/1836 [14:49<00:40,  1.90it/s] 96%|█████████▌| 1760/1836 [14:49<00:40,  1.90it/s] 96%|█████████▌| 1761/1836 [14:50<00:40,  1.87it/s] 96%|█████████▌| 1762/1836 [14:50<00:38,  1.93it/s] 96%|█████████▌| 1763/1836 [14:51<00:37,  1.93it/s] 96%|█████████▌| 1764/1836 [14:51<00:36,  1.98it/s] 96%|█████████▌| 1765/1836 [14:52<00:34,  2.09it/s] 96%|█████████▌| 1766/1836 [14:52<00:35,  1.99it/s] 96%|█████████▌| 1767/1836 [14:53<00:35,  1.93it/s] 96%|█████████▋| 1768/1836 [14:53<00:36,  1.88it/s] 96%|█████████▋| 1769/1836 [14:54<00:33,  1.97it/s] 96%|█████████▋| 1770/1836 [14:54<00:32,  2.01it/s] 96%|█████████▋| 1771/1836 [14:55<00:32,  1.98it/s] 97%|█████████▋| 1772/1836 [14:55<00:32,  1.99it/s] 97%|█████████▋| 1773/1836 [14:56<00:31,  2.00it/s] 97%|█████████▋| 1774/1836 [14:56<00:31,  1.97it/s] 97%|█████████▋| 1775/1836 [14:57<00:31,  1.96it/s] 97%|█████████▋| 1776/1836 [14:57<00:29,  2.03it/s] 97%|█████████▋| 1777/1836 [14:58<00:30,  1.97it/s] 97%|█████████▋| 1778/1836 [14:58<00:28,  2.00it/s] 97%|█████████▋| 1779/1836 [14:59<00:28,  1.98it/s] 97%|█████████▋| 1780/1836 [14:59<00:27,  2.06it/s] 97%|█████████▋| 1781/1836 [15:00<00:26,  2.11it/s] 97%|█████████▋| 1782/1836 [15:00<00:25,  2.12it/s] 97%|█████████▋| 1783/1836 [15:01<00:25,  2.06it/s] 97%|█████████▋| 1784/1836 [15:01<00:26,  2.00it/s] 97%|█████████▋| 1785/1836 [15:02<00:24,  2.06it/s] 97%|█████████▋| 1786/1836 [15:02<00:24,  2.05it/s] 97%|█████████▋| 1787/1836 [15:03<00:24,  2.01it/s] 97%|█████████▋| 1788/1836 [15:03<00:23,  2.08it/s] 97%|█████████▋| 1789/1836 [15:03<00:22,  2.08it/s] 97%|█████████▋| 1790/1836 [15:04<00:22,  2.09it/s] 98%|█████████▊| 1791/1836 [15:04<00:22,  2.03it/s] 98%|█████████▊| 1792/1836 [15:05<00:23,  1.87it/s] 98%|█████████▊| 1793/1836 [15:06<00:23,  1.84it/s] 98%|█████████▊| 1794/1836 [15:06<00:22,  1.86it/s] 98%|█████████▊| 1795/1836 [15:07<00:21,  1.93it/s] 98%|█████████▊| 1796/1836 [15:07<00:19,  2.01it/s] 98%|█████████▊| 1797/1836 [15:08<00:19,  2.01it/s] 98%|█████████▊| 1798/1836 [15:08<00:18,  2.04it/s] 98%|█████████▊| 1799/1836 [15:09<00:17,  2.07it/s] 98%|█████████▊| 1800/1836 [15:09<00:17,  2.02it/s] 98%|█████████▊| 1801/1836 [15:09<00:16,  2.08it/s] 98%|█████████▊| 1802/1836 [15:10<00:16,  2.12it/s] 98%|█████████▊| 1803/1836 [15:10<00:16,  2.04it/s] 98%|█████████▊| 1804/1836 [15:11<00:15,  2.04it/s] 98%|█████████▊| 1805/1836 [15:11<00:15,  2.06it/s] 98%|█████████▊| 1806/1836 [15:12<00:13,  2.16it/s] 98%|█████████▊| 1807/1836 [15:12<00:13,  2.12it/s] 98%|█████████▊| 1808/1836 [15:13<00:13,  2.15it/s] 99%|█████████▊| 1809/1836 [15:13<00:12,  2.08it/s] 99%|█████████▊| 1810/1836 [15:14<00:12,  2.03it/s] 99%|█████████▊| 1811/1836 [15:14<00:12,  1.95it/s] 99%|█████████▊| 1812/1836 [15:15<00:12,  1.94it/s] 99%|█████████▊| 1813/1836 [15:15<00:11,  1.99it/s] 99%|█████████▉| 1814/1836 [15:16<00:11,  2.00it/s] 99%|█████████▉| 1815/1836 [15:16<00:10,  1.97it/s] 99%|█████████▉| 1816/1836 [15:17<00:10,  1.88it/s] 99%|█████████▉| 1817/1836 [15:18<00:10,  1.86it/s] 99%|█████████▉| 1818/1836 [15:18<00:09,  1.96it/s] 99%|█████████▉| 1819/1836 [15:19<00:08,  1.95it/s] 99%|█████████▉| 1820/1836 [15:19<00:08,  1.93it/s] 99%|█████████▉| 1821/1836 [15:20<00:07,  1.92it/s] 99%|█████████▉| 1822/1836 [15:20<00:07,  1.97it/s] 99%|█████████▉| 1823/1836 [15:21<00:06,  1.94it/s] 99%|█████████▉| 1824/1836 [15:21<00:06,  1.93it/s] 99%|█████████▉| 1825/1836 [15:22<00:05,  1.90it/s] 99%|█████████▉| 1826/1836 [15:22<00:05,  1.93it/s]100%|█████████▉| 1827/1836 [15:23<00:04,  1.92it/s]100%|█████████▉| 1828/1836 [15:23<00:04,  1.97it/s]100%|█████████▉| 1829/1836 [15:24<00:03,  2.04it/s]100%|█████████▉| 1830/1836 [15:24<00:03,  1.99it/s]100%|█████████▉| 1831/1836 [15:25<00:02,  1.99it/s]100%|█████████▉| 1832/1836 [15:25<00:01,  2.10it/s]100%|█████████▉| 1833/1836 [15:25<00:01,  2.18it/s]100%|█████████▉| 1834/1836 [15:26<00:00,  2.08it/s]100%|█████████▉| 1835/1836 [15:26<00:00,  2.08it/s]100%|██████████| 1836/1836 [15:27<00:00,  2.37it/s]03/13/2024 20:46:03 - INFO - __main__ - epoch 2: {'accuracy': 0.6691176470588235, 'f1': 0.7700170357751278}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-03-13 20:46:04,295] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/model.safetensors.index.json.
tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tokenizer_config.json
Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/special_tokens_map.json
100%|██████████| 1836/1836 [16:15<00:00,  1.88it/s]
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 20:51:54 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6953.57 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 7074.42 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 7183.67 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 7113.87 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6666.37 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7332.80 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7012.06 examples/s]
03/13/2024 20:52:02 - INFO - __main__ - Sample 361 of the training set: {'input_ids': [1, 6171, 310, 278, 390, 886, 8881, 5310, 11886, 322, 18708, 1352, 5260, 845, 18331, 472, 278, 29871, 29955, 29946, 386, 8081, 950, 10355, 9220, 869, 1, 6171, 310, 278, 390, 886, 8881, 5310, 11886, 322, 1472, 2230, 18708, 1352, 5260, 845, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 20:52:02 - INFO - __main__ - Sample 1875 of the training set: {'input_ids': [1, 5853, 1135, 29871, 29953, 29900, 10151, 310, 278, 5001, 525, 29879, 29871, 29896, 29892, 29900, 29900, 29900, 22873, 892, 9445, 297, 278, 5337, 869, 1, 5853, 1135, 29871, 29953, 29900, 639, 1644, 310, 967, 29871, 29896, 29892, 29900, 29900, 29900, 22873, 472, 278, 931, 892, 9445, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 20:52:02 - INFO - __main__ - Sample 2358 of the training set: {'input_ids': [1, 11821, 7679, 15983, 438, 29920, 1537, 6657, 17418, 471, 297, 938, 6270, 2562, 9826, 408, 540, 7572, 670, 1346, 27357, 26622, 24205, 515, 263, 18890, 4768, 446, 11423, 869, 1, 11821, 7679, 15983, 438, 29920, 1537, 6657, 17418, 1033, 367, 8126, 297, 938, 6270, 2562, 363, 1346, 3196, 3841, 26622, 1494, 670, 18890, 4768, 446, 11423, 1919, 670, 11619, 1497, 15243, 523, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 20:52:03 - INFO - __main__ - ***** Running training *****
03/13/2024 20:52:03 - INFO - __main__ -   Num examples = 3668
03/13/2024 20:52:03 - INFO - __main__ -   Num Epochs = 3
03/13/2024 20:52:03 - INFO - __main__ -   Instantaneous batch size per device = 8
03/13/2024 20:52:03 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
03/13/2024 20:52:03 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 20:52:03 - INFO - __main__ -   Total optimization steps = 1377
  0%|          | 0/1377 [00:00<?, ?it/s]  0%|          | 1/1377 [00:01<28:50,  1.26s/it]  0%|          | 2/1377 [00:01<17:55,  1.28it/s]  0%|          | 3/1377 [00:02<15:12,  1.50it/s]  0%|          | 4/1377 [00:02<14:55,  1.53it/s]  0%|          | 5/1377 [00:03<14:24,  1.59it/s]  0%|          | 6/1377 [00:03<13:38,  1.68it/s]  1%|          | 7/1377 [00:04<13:21,  1.71it/s]  1%|          | 8/1377 [00:05<12:58,  1.76it/s]  1%|          | 9/1377 [00:05<13:29,  1.69it/s]  1%|          | 10/1377 [00:06<13:29,  1.69it/s]  1%|          | 11/1377 [00:06<13:35,  1.67it/s]  1%|          | 12/1377 [00:07<13:35,  1.67it/s]  1%|          | 13/1377 [00:08<13:32,  1.68it/s]  1%|          | 14/1377 [00:08<13:01,  1.74it/s]  1%|          | 15/1377 [00:09<13:12,  1.72it/s]  1%|          | 16/1377 [00:09<12:58,  1.75it/s]  1%|          | 17/1377 [00:10<14:40,  1.54it/s]  1%|▏         | 18/1377 [00:11<14:54,  1.52it/s]  1%|▏         | 19/1377 [00:11<14:58,  1.51it/s]  1%|▏         | 20/1377 [00:12<14:32,  1.56it/s]  2%|▏         | 21/1377 [00:13<15:10,  1.49it/s]  2%|▏         | 22/1377 [00:13<14:16,  1.58it/s]  2%|▏         | 23/1377 [00:14<13:59,  1.61it/s]  2%|▏         | 24/1377 [00:15<13:50,  1.63it/s]  2%|▏         | 25/1377 [00:15<13:59,  1.61it/s]  2%|▏         | 26/1377 [00:16<13:53,  1.62it/s]  2%|▏         | 27/1377 [00:16<13:20,  1.69it/s]  2%|▏         | 28/1377 [00:17<12:52,  1.75it/s]  2%|▏         | 29/1377 [00:17<12:58,  1.73it/s]  2%|▏         | 30/1377 [00:18<13:24,  1.67it/s]  2%|▏         | 31/1377 [00:19<13:11,  1.70it/s]  2%|▏         | 32/1377 [00:19<13:01,  1.72it/s]  2%|▏         | 33/1377 [00:20<13:35,  1.65it/s]  2%|▏         | 34/1377 [00:21<14:03,  1.59it/s]  3%|▎         | 35/1377 [00:21<13:49,  1.62it/s]  3%|▎         | 36/1377 [00:22<13:46,  1.62it/s]  3%|▎         | 37/1377 [00:22<13:55,  1.60it/s]  3%|▎         | 38/1377 [00:23<13:30,  1.65it/s]  3%|▎         | 39/1377 [00:24<13:12,  1.69it/s]  3%|▎         | 40/1377 [00:24<12:58,  1.72it/s]  3%|▎         | 41/1377 [00:25<13:21,  1.67it/s]  3%|▎         | 42/1377 [00:25<13:05,  1.70it/s]  3%|▎         | 43/1377 [00:26<13:38,  1.63it/s]  3%|▎         | 44/1377 [00:27<13:38,  1.63it/s]  3%|▎         | 45/1377 [00:27<13:28,  1.65it/s]  3%|▎         | 46/1377 [00:28<13:22,  1.66it/s]  3%|▎         | 47/1377 [00:28<13:18,  1.67it/s]  3%|▎         | 48/1377 [00:29<12:51,  1.72it/s]  4%|▎         | 49/1377 [00:29<13:02,  1.70it/s]  4%|▎         | 50/1377 [00:30<13:05,  1.69it/s]  4%|▎         | 51/1377 [00:31<13:24,  1.65it/s]  4%|▍         | 52/1377 [00:31<13:25,  1.65it/s]  4%|▍         | 53/1377 [00:32<13:38,  1.62it/s]  4%|▍         | 54/1377 [00:33<13:36,  1.62it/s]  4%|▍         | 55/1377 [00:33<13:14,  1.66it/s]  4%|▍         | 56/1377 [00:34<12:53,  1.71it/s]  4%|▍         | 57/1377 [00:34<13:28,  1.63it/s]  4%|▍         | 58/1377 [00:35<13:22,  1.64it/s]  4%|▍         | 59/1377 [00:36<13:34,  1.62it/s]  4%|▍         | 60/1377 [00:36<13:25,  1.64it/s]  4%|▍         | 61/1377 [00:37<14:14,  1.54it/s]  5%|▍         | 62/1377 [00:38<13:53,  1.58it/s]  5%|▍         | 63/1377 [00:38<13:55,  1.57it/s]  5%|▍         | 64/1377 [00:39<14:24,  1.52it/s]  5%|▍         | 65/1377 [00:39<13:49,  1.58it/s]  5%|▍         | 66/1377 [00:40<13:38,  1.60it/s]  5%|▍         | 67/1377 [00:41<13:31,  1.62it/s]  5%|▍         | 68/1377 [00:41<13:20,  1.63it/s]  5%|▌         | 69/1377 [00:42<13:15,  1.64it/s]  5%|▌         | 70/1377 [00:43<13:16,  1.64it/s]  5%|▌         | 71/1377 [00:43<12:59,  1.68it/s]  5%|▌         | 72/1377 [00:44<12:41,  1.71it/s]  5%|▌         | 73/1377 [00:44<13:04,  1.66it/s]  5%|▌         | 74/1377 [00:45<13:19,  1.63it/s]  5%|▌         | 75/1377 [00:45<12:58,  1.67it/s]  6%|▌         | 76/1377 [00:46<12:40,  1.71it/s]  6%|▌         | 77/1377 [00:47<12:25,  1.74it/s]  6%|▌         | 78/1377 [00:47<12:14,  1.77it/s]  6%|▌         | 79/1377 [00:48<12:44,  1.70it/s]  6%|▌         | 80/1377 [00:48<12:24,  1.74it/s]  6%|▌         | 81/1377 [00:49<12:06,  1.78it/s]  6%|▌         | 82/1377 [00:50<13:01,  1.66it/s]  6%|▌         | 83/1377 [00:50<13:42,  1.57it/s]  6%|▌         | 84/1377 [00:51<13:11,  1.63it/s]  6%|▌         | 85/1377 [00:51<13:03,  1.65it/s]  6%|▌         | 86/1377 [00:52<13:17,  1.62it/s]  6%|▋         | 87/1377 [00:53<12:57,  1.66it/s]  6%|▋         | 88/1377 [00:53<13:23,  1.60it/s]  6%|▋         | 89/1377 [00:54<13:17,  1.61it/s]  7%|▋         | 90/1377 [00:54<12:54,  1.66it/s]  7%|▋         | 91/1377 [00:55<12:56,  1.66it/s]  7%|▋         | 92/1377 [00:56<13:35,  1.58it/s]  7%|▋         | 93/1377 [00:56<13:52,  1.54it/s]  7%|▋         | 94/1377 [00:57<14:12,  1.51it/s]  7%|▋         | 95/1377 [00:58<13:28,  1.59it/s]  7%|▋         | 96/1377 [00:58<12:56,  1.65it/s]  7%|▋         | 97/1377 [00:59<12:50,  1.66it/s]  7%|▋         | 98/1377 [00:59<12:28,  1.71it/s]  7%|▋         | 99/1377 [01:00<12:50,  1.66it/s]  7%|▋         | 100/1377 [01:01<13:04,  1.63it/s]  7%|▋         | 101/1377 [01:01<13:27,  1.58it/s]  7%|▋         | 102/1377 [01:02<13:13,  1.61it/s]  7%|▋         | 103/1377 [01:03<13:08,  1.62it/s]  8%|▊         | 104/1377 [01:03<12:41,  1.67it/s]  8%|▊         | 105/1377 [01:04<12:43,  1.67it/s]  8%|▊         | 106/1377 [01:04<13:00,  1.63it/s]  8%|▊         | 107/1377 [01:05<12:53,  1.64it/s]  8%|▊         | 108/1377 [01:06<13:20,  1.59it/s]  8%|▊         | 109/1377 [01:06<13:25,  1.57it/s]  8%|▊         | 110/1377 [01:07<12:52,  1.64it/s]  8%|▊         | 111/1377 [01:07<13:05,  1.61it/s]  8%|▊         | 112/1377 [01:08<13:14,  1.59it/s]  8%|▊         | 113/1377 [01:09<13:02,  1.62it/s]  8%|▊         | 114/1377 [01:09<13:23,  1.57it/s]  8%|▊         | 115/1377 [01:10<12:51,  1.64it/s]  8%|▊         | 116/1377 [01:11<13:02,  1.61it/s]  8%|▊         | 117/1377 [01:11<13:23,  1.57it/s]  9%|▊         | 118/1377 [01:12<12:42,  1.65it/s]  9%|▊         | 119/1377 [01:12<12:53,  1.63it/s]  9%|▊         | 120/1377 [01:13<12:45,  1.64it/s]  9%|▉         | 121/1377 [01:14<12:44,  1.64it/s]  9%|▉         | 122/1377 [01:14<12:44,  1.64it/s]  9%|▉         | 123/1377 [01:15<12:46,  1.64it/s]  9%|▉         | 124/1377 [01:15<12:45,  1.64it/s]  9%|▉         | 125/1377 [01:16<12:44,  1.64it/s]  9%|▉         | 126/1377 [01:17<12:25,  1.68it/s]  9%|▉         | 127/1377 [01:17<12:41,  1.64it/s]  9%|▉         | 128/1377 [01:18<12:20,  1.69it/s]  9%|▉         | 129/1377 [01:18<12:37,  1.65it/s]  9%|▉         | 130/1377 [01:19<12:09,  1.71it/s] 10%|▉         | 131/1377 [01:20<12:16,  1.69it/s] 10%|▉         | 132/1377 [01:20<13:22,  1.55it/s] 10%|▉         | 133/1377 [01:21<13:07,  1.58it/s] 10%|▉         | 134/1377 [01:22<13:11,  1.57it/s] 10%|▉         | 135/1377 [01:22<13:26,  1.54it/s] 10%|▉         | 136/1377 [01:23<13:11,  1.57it/s] 10%|▉         | 137/1377 [01:24<13:26,  1.54it/s] 10%|█         | 138/1377 [01:24<12:52,  1.60it/s] 10%|█         | 139/1377 [01:25<12:42,  1.62it/s] 10%|█         | 140/1377 [01:25<12:23,  1.66it/s] 10%|█         | 141/1377 [01:26<12:10,  1.69it/s] 10%|█         | 142/1377 [01:27<12:52,  1.60it/s] 10%|█         | 143/1377 [01:27<13:10,  1.56it/s] 10%|█         | 144/1377 [01:28<12:58,  1.58it/s] 11%|█         | 145/1377 [01:28<12:46,  1.61it/s] 11%|█         | 146/1377 [01:29<12:41,  1.62it/s] 11%|█         | 147/1377 [01:30<12:20,  1.66it/s] 11%|█         | 148/1377 [01:30<12:34,  1.63it/s] 11%|█         | 149/1377 [01:31<12:46,  1.60it/s] 11%|█         | 150/1377 [01:32<12:36,  1.62it/s] 11%|█         | 151/1377 [01:32<12:30,  1.63it/s] 11%|█         | 152/1377 [01:33<12:39,  1.61it/s] 11%|█         | 153/1377 [01:33<12:58,  1.57it/s] 11%|█         | 154/1377 [01:34<13:01,  1.57it/s] 11%|█▏        | 155/1377 [01:35<13:16,  1.54it/s] 11%|█▏        | 156/1377 [01:35<12:39,  1.61it/s] 11%|█▏        | 157/1377 [01:36<13:07,  1.55it/s] 11%|█▏        | 158/1377 [01:37<13:07,  1.55it/s] 12%|█▏        | 159/1377 [01:37<12:54,  1.57it/s] 12%|█▏        | 160/1377 [01:38<12:45,  1.59it/s] 12%|█▏        | 161/1377 [01:39<13:04,  1.55it/s] 12%|█▏        | 162/1377 [01:39<12:46,  1.58it/s] 12%|█▏        | 163/1377 [01:40<12:52,  1.57it/s] 12%|█▏        | 164/1377 [01:40<12:54,  1.57it/s] 12%|█▏        | 165/1377 [01:41<12:27,  1.62it/s] 12%|█▏        | 166/1377 [01:42<12:23,  1.63it/s] 12%|█▏        | 167/1377 [01:42<11:54,  1.69it/s] 12%|█▏        | 168/1377 [01:43<12:14,  1.65it/s] 12%|█▏        | 169/1377 [01:43<12:16,  1.64it/s] 12%|█▏        | 170/1377 [01:44<12:28,  1.61it/s] 12%|█▏        | 171/1377 [01:45<13:22,  1.50it/s] 12%|█▏        | 172/1377 [01:46<13:15,  1.51it/s] 13%|█▎        | 173/1377 [01:46<12:39,  1.58it/s] 13%|█▎        | 174/1377 [01:47<12:32,  1.60it/s] 13%|█▎        | 175/1377 [01:47<12:29,  1.60it/s] 13%|█▎        | 176/1377 [01:48<12:34,  1.59it/s] 13%|█▎        | 177/1377 [01:49<12:27,  1.61it/s] 13%|█▎        | 178/1377 [01:49<12:34,  1.59it/s] 13%|█▎        | 179/1377 [01:50<12:39,  1.58it/s] 13%|█▎        | 180/1377 [01:50<12:27,  1.60it/s] 13%|█▎        | 181/1377 [01:51<12:16,  1.62it/s] 13%|█▎        | 182/1377 [01:52<11:54,  1.67it/s] 13%|█▎        | 183/1377 [01:52<11:53,  1.67it/s] 13%|█▎        | 184/1377 [01:53<12:11,  1.63it/s] 13%|█▎        | 185/1377 [01:54<12:22,  1.61it/s] 14%|█▎        | 186/1377 [01:54<12:00,  1.65it/s] 14%|█▎        | 187/1377 [01:55<12:02,  1.65it/s] 14%|█▎        | 188/1377 [01:55<12:28,  1.59it/s] 14%|█▎        | 189/1377 [01:56<12:45,  1.55it/s] 14%|█▍        | 190/1377 [01:57<12:46,  1.55it/s] 14%|█▍        | 191/1377 [01:57<12:47,  1.54it/s] 14%|█▍        | 192/1377 [01:58<12:31,  1.58it/s] 14%|█▍        | 193/1377 [01:58<11:42,  1.69it/s] 14%|█▍        | 194/1377 [01:59<11:46,  1.68it/s] 14%|█▍        | 195/1377 [02:00<11:30,  1.71it/s] 14%|█▍        | 196/1377 [02:00<11:49,  1.66it/s] 14%|█▍        | 197/1377 [02:01<11:27,  1.72it/s] 14%|█▍        | 198/1377 [02:01<11:37,  1.69it/s] 14%|█▍        | 199/1377 [02:02<11:28,  1.71it/s] 15%|█▍        | 200/1377 [02:03<12:09,  1.61it/s] 15%|█▍        | 201/1377 [02:03<12:20,  1.59it/s] 15%|█▍        | 202/1377 [02:04<11:56,  1.64it/s] 15%|█▍        | 203/1377 [02:04<11:39,  1.68it/s] 15%|█▍        | 204/1377 [02:05<11:43,  1.67it/s] 15%|█▍        | 205/1377 [02:06<11:47,  1.66it/s] 15%|█▍        | 206/1377 [02:06<11:48,  1.65it/s] 15%|█▌        | 207/1377 [02:07<11:50,  1.65it/s] 15%|█▌        | 208/1377 [02:08<12:12,  1.60it/s] 15%|█▌        | 209/1377 [02:08<12:42,  1.53it/s] 15%|█▌        | 210/1377 [02:09<12:23,  1.57it/s] 15%|█▌        | 211/1377 [02:10<12:38,  1.54it/s] 15%|█▌        | 212/1377 [02:10<12:36,  1.54it/s] 15%|█▌        | 213/1377 [02:11<12:34,  1.54it/s] 16%|█▌        | 214/1377 [02:11<11:56,  1.62it/s] 16%|█▌        | 215/1377 [02:12<11:35,  1.67it/s] 16%|█▌        | 216/1377 [02:13<11:22,  1.70it/s] 16%|█▌        | 217/1377 [02:13<11:09,  1.73it/s] 16%|█▌        | 218/1377 [02:14<11:31,  1.68it/s] 16%|█▌        | 219/1377 [02:14<11:48,  1.63it/s] 16%|█▌        | 220/1377 [02:15<11:31,  1.67it/s] 16%|█▌        | 221/1377 [02:16<11:47,  1.63it/s] 16%|█▌        | 222/1377 [02:16<11:25,  1.68it/s] 16%|█▌        | 223/1377 [02:17<11:54,  1.61it/s] 16%|█▋        | 224/1377 [02:17<12:14,  1.57it/s] 16%|█▋        | 225/1377 [02:18<11:45,  1.63it/s] 16%|█▋        | 226/1377 [02:19<11:40,  1.64it/s] 16%|█▋        | 227/1377 [02:19<11:23,  1.68it/s] 17%|█▋        | 228/1377 [02:20<11:41,  1.64it/s] 17%|█▋        | 229/1377 [02:20<11:36,  1.65it/s] 17%|█▋        | 230/1377 [02:21<11:22,  1.68it/s] 17%|█▋        | 231/1377 [02:22<11:11,  1.71it/s] 17%|█▋        | 232/1377 [02:22<11:18,  1.69it/s] 17%|█▋        | 233/1377 [02:23<11:48,  1.61it/s] 17%|█▋        | 234/1377 [02:23<11:18,  1.69it/s] 17%|█▋        | 235/1377 [02:24<11:17,  1.69it/s] 17%|█▋        | 236/1377 [02:25<11:33,  1.64it/s] 17%|█▋        | 237/1377 [02:25<11:57,  1.59it/s] 17%|█▋        | 238/1377 [02:26<12:00,  1.58it/s] 17%|█▋        | 239/1377 [02:27<11:48,  1.61it/s] 17%|█▋        | 240/1377 [02:27<11:28,  1.65it/s] 18%|█▊        | 241/1377 [02:28<11:14,  1.68it/s] 18%|█▊        | 242/1377 [02:28<11:16,  1.68it/s] 18%|█▊        | 243/1377 [02:29<11:34,  1.63it/s] 18%|█▊        | 244/1377 [02:30<11:44,  1.61it/s] 18%|█▊        | 245/1377 [02:30<11:16,  1.67it/s] 18%|█▊        | 246/1377 [02:31<11:41,  1.61it/s] 18%|█▊        | 247/1377 [02:31<11:48,  1.59it/s] 18%|█▊        | 248/1377 [02:32<11:27,  1.64it/s] 18%|█▊        | 249/1377 [02:33<11:36,  1.62it/s] 18%|█▊        | 250/1377 [02:33<11:36,  1.62it/s] 18%|█▊        | 251/1377 [02:34<11:35,  1.62it/s] 18%|█▊        | 252/1377 [02:34<11:34,  1.62it/s] 18%|█▊        | 253/1377 [02:35<11:17,  1.66it/s] 18%|█▊        | 254/1377 [02:36<11:42,  1.60it/s] 19%|█▊        | 255/1377 [02:36<11:22,  1.64it/s] 19%|█▊        | 256/1377 [02:37<11:35,  1.61it/s] 19%|█▊        | 257/1377 [02:38<11:55,  1.57it/s] 19%|█▊        | 258/1377 [02:38<11:57,  1.56it/s] 19%|█▉        | 259/1377 [02:39<12:09,  1.53it/s] 19%|█▉        | 260/1377 [02:40<12:06,  1.54it/s] 19%|█▉        | 261/1377 [02:40<12:32,  1.48it/s] 19%|█▉        | 262/1377 [02:41<11:43,  1.58it/s] 19%|█▉        | 263/1377 [02:41<11:36,  1.60it/s] 19%|█▉        | 264/1377 [02:42<11:25,  1.62it/s] 19%|█▉        | 265/1377 [02:43<11:24,  1.62it/s] 19%|█▉        | 266/1377 [02:43<10:45,  1.72it/s] 19%|█▉        | 267/1377 [02:44<11:05,  1.67it/s] 19%|█▉        | 268/1377 [02:45<11:32,  1.60it/s] 20%|█▉        | 269/1377 [02:45<11:51,  1.56it/s] 20%|█▉        | 270/1377 [02:46<12:16,  1.50it/s] 20%|█▉        | 271/1377 [02:47<12:09,  1.52it/s] 20%|█▉        | 272/1377 [02:47<11:39,  1.58it/s] 20%|█▉        | 273/1377 [02:48<11:44,  1.57it/s] 20%|█▉        | 274/1377 [02:48<11:46,  1.56it/s] 20%|█▉        | 275/1377 [02:49<11:45,  1.56it/s] 20%|██        | 276/1377 [02:50<11:31,  1.59it/s] 20%|██        | 277/1377 [02:50<11:05,  1.65it/s] 20%|██        | 278/1377 [02:51<10:46,  1.70it/s] 20%|██        | 279/1377 [02:51<11:02,  1.66it/s] 20%|██        | 280/1377 [02:52<11:17,  1.62it/s] 20%|██        | 281/1377 [02:53<11:35,  1.57it/s] 20%|██        | 282/1377 [02:53<11:13,  1.63it/s] 21%|██        | 283/1377 [02:54<10:47,  1.69it/s] 21%|██        | 284/1377 [02:54<11:03,  1.65it/s] 21%|██        | 285/1377 [02:55<10:26,  1.74it/s] 21%|██        | 286/1377 [02:56<10:36,  1.71it/s] 21%|██        | 287/1377 [02:56<10:57,  1.66it/s] 21%|██        | 288/1377 [02:57<10:41,  1.70it/s] 21%|██        | 289/1377 [02:57<11:09,  1.63it/s] 21%|██        | 290/1377 [02:58<10:53,  1.66it/s] 21%|██        | 291/1377 [02:59<11:07,  1.63it/s] 21%|██        | 292/1377 [02:59<10:50,  1.67it/s] 21%|██▏       | 293/1377 [03:00<10:37,  1.70it/s] 21%|██▏       | 294/1377 [03:00<10:56,  1.65it/s] 21%|██▏       | 295/1377 [03:01<10:44,  1.68it/s] 21%|██▏       | 296/1377 [03:02<10:50,  1.66it/s] 22%|██▏       | 297/1377 [03:02<10:54,  1.65it/s] 22%|██▏       | 298/1377 [03:03<10:40,  1.68it/s] 22%|██▏       | 299/1377 [03:03<10:27,  1.72it/s] 22%|██▏       | 300/1377 [03:04<10:31,  1.71it/s] 22%|██▏       | 301/1377 [03:05<10:16,  1.75it/s] 22%|██▏       | 302/1377 [03:05<10:28,  1.71it/s] 22%|██▏       | 303/1377 [03:06<10:32,  1.70it/s] 22%|██▏       | 304/1377 [03:06<10:15,  1.74it/s] 22%|██▏       | 305/1377 [03:07<10:46,  1.66it/s] 22%|██▏       | 306/1377 [03:08<11:00,  1.62it/s] 22%|██▏       | 307/1377 [03:08<11:19,  1.57it/s] 22%|██▏       | 308/1377 [03:09<10:58,  1.62it/s] 22%|██▏       | 309/1377 [03:09<10:37,  1.67it/s] 23%|██▎       | 310/1377 [03:10<10:27,  1.70it/s] 23%|██▎       | 311/1377 [03:11<10:18,  1.72it/s] 23%|██▎       | 312/1377 [03:11<10:27,  1.70it/s] 23%|██▎       | 313/1377 [03:12<10:44,  1.65it/s] 23%|██▎       | 314/1377 [03:12<10:58,  1.61it/s] 23%|██▎       | 315/1377 [03:13<11:07,  1.59it/s] 23%|██▎       | 316/1377 [03:14<11:11,  1.58it/s] 23%|██▎       | 317/1377 [03:14<11:15,  1.57it/s] 23%|██▎       | 318/1377 [03:15<11:16,  1.57it/s] 23%|██▎       | 319/1377 [03:16<10:44,  1.64it/s] 23%|██▎       | 320/1377 [03:16<10:53,  1.62it/s] 23%|██▎       | 321/1377 [03:17<11:12,  1.57it/s] 23%|██▎       | 322/1377 [03:18<11:16,  1.56it/s] 23%|██▎       | 323/1377 [03:18<11:19,  1.55it/s] 24%|██▎       | 324/1377 [03:19<11:08,  1.57it/s] 24%|██▎       | 325/1377 [03:19<11:10,  1.57it/s] 24%|██▎       | 326/1377 [03:20<11:23,  1.54it/s] 24%|██▎       | 327/1377 [03:21<11:22,  1.54it/s] 24%|██▍       | 328/1377 [03:21<11:19,  1.54it/s] 24%|██▍       | 329/1377 [03:22<10:52,  1.61it/s] 24%|██▍       | 330/1377 [03:23<11:08,  1.57it/s] 24%|██▍       | 331/1377 [03:23<10:46,  1.62it/s] 24%|██▍       | 332/1377 [03:24<10:42,  1.63it/s] 24%|██▍       | 333/1377 [03:24<10:35,  1.64it/s] 24%|██▍       | 334/1377 [03:25<10:45,  1.62it/s] 24%|██▍       | 335/1377 [03:26<10:53,  1.59it/s] 24%|██▍       | 336/1377 [03:26<10:58,  1.58it/s] 24%|██▍       | 337/1377 [03:27<10:48,  1.60it/s] 25%|██▍       | 338/1377 [03:28<10:56,  1.58it/s] 25%|██▍       | 339/1377 [03:28<10:50,  1.60it/s] 25%|██▍       | 340/1377 [03:29<10:30,  1.65it/s] 25%|██▍       | 341/1377 [03:29<10:39,  1.62it/s] 25%|██▍       | 342/1377 [03:30<10:48,  1.60it/s] 25%|██▍       | 343/1377 [03:31<10:29,  1.64it/s] 25%|██▍       | 344/1377 [03:31<10:42,  1.61it/s] 25%|██▌       | 345/1377 [03:32<10:17,  1.67it/s] 25%|██▌       | 346/1377 [03:32<09:57,  1.72it/s] 25%|██▌       | 347/1377 [03:33<10:07,  1.70it/s] 25%|██▌       | 348/1377 [03:34<10:23,  1.65it/s] 25%|██▌       | 349/1377 [03:34<10:24,  1.65it/s] 25%|██▌       | 350/1377 [03:35<10:06,  1.69it/s] 25%|██▌       | 351/1377 [03:35<09:49,  1.74it/s] 26%|██▌       | 352/1377 [03:36<09:57,  1.72it/s] 26%|██▌       | 353/1377 [03:37<10:05,  1.69it/s] 26%|██▌       | 354/1377 [03:37<09:57,  1.71it/s] 26%|██▌       | 355/1377 [03:38<10:24,  1.64it/s] 26%|██▌       | 356/1377 [03:38<10:55,  1.56it/s] 26%|██▌       | 357/1377 [03:39<10:48,  1.57it/s] 26%|██▌       | 358/1377 [03:40<11:00,  1.54it/s] 26%|██▌       | 359/1377 [03:40<10:50,  1.57it/s] 26%|██▌       | 360/1377 [03:41<10:40,  1.59it/s] 26%|██▌       | 361/1377 [03:42<10:20,  1.64it/s] 26%|██▋       | 362/1377 [03:42<10:48,  1.57it/s] 26%|██▋       | 363/1377 [03:43<10:23,  1.63it/s] 26%|██▋       | 364/1377 [03:43<10:16,  1.64it/s] 27%|██▋       | 365/1377 [03:44<10:27,  1.61it/s] 27%|██▋       | 366/1377 [03:45<10:10,  1.66it/s] 27%|██▋       | 367/1377 [03:45<10:21,  1.63it/s] 27%|██▋       | 368/1377 [03:46<10:02,  1.67it/s] 27%|██▋       | 369/1377 [03:46<10:03,  1.67it/s] 27%|██▋       | 370/1377 [03:47<10:17,  1.63it/s] 27%|██▋       | 371/1377 [03:48<10:16,  1.63it/s] 27%|██▋       | 372/1377 [03:48<10:17,  1.63it/s] 27%|██▋       | 373/1377 [03:49<09:52,  1.69it/s] 27%|██▋       | 374/1377 [03:49<09:53,  1.69it/s] 27%|██▋       | 375/1377 [03:50<10:28,  1.59it/s] 27%|██▋       | 376/1377 [03:51<10:07,  1.65it/s] 27%|██▋       | 377/1377 [03:51<09:51,  1.69it/s] 27%|██▋       | 378/1377 [03:52<10:15,  1.62it/s] 28%|██▊       | 379/1377 [03:53<10:14,  1.62it/s] 28%|██▊       | 380/1377 [03:53<10:22,  1.60it/s] 28%|██▊       | 381/1377 [03:54<09:57,  1.67it/s] 28%|██▊       | 382/1377 [03:54<10:11,  1.63it/s] 28%|██▊       | 383/1377 [03:55<10:21,  1.60it/s] 28%|██▊       | 384/1377 [03:56<10:03,  1.64it/s] 28%|██▊       | 385/1377 [03:56<09:42,  1.70it/s] 28%|██▊       | 386/1377 [03:57<09:58,  1.66it/s] 28%|██▊       | 387/1377 [03:57<10:09,  1.62it/s] 28%|██▊       | 388/1377 [03:58<09:44,  1.69it/s] 28%|██▊       | 389/1377 [03:59<09:30,  1.73it/s] 28%|██▊       | 390/1377 [03:59<09:50,  1.67it/s] 28%|██▊       | 391/1377 [04:00<09:40,  1.70it/s] 28%|██▊       | 392/1377 [04:00<09:46,  1.68it/s] 29%|██▊       | 393/1377 [04:01<09:28,  1.73it/s] 29%|██▊       | 394/1377 [04:01<09:21,  1.75it/s] 29%|██▊       | 395/1377 [04:02<09:16,  1.77it/s] 29%|██▉       | 396/1377 [04:03<09:29,  1.72it/s] 29%|██▉       | 397/1377 [04:03<09:33,  1.71it/s] 29%|██▉       | 398/1377 [04:04<09:40,  1.69it/s] 29%|██▉       | 399/1377 [04:05<10:15,  1.59it/s] 29%|██▉       | 400/1377 [04:05<10:21,  1.57it/s] 29%|██▉       | 401/1377 [04:06<09:56,  1.64it/s] 29%|██▉       | 402/1377 [04:06<09:50,  1.65it/s] 29%|██▉       | 403/1377 [04:07<09:51,  1.65it/s] 29%|██▉       | 404/1377 [04:08<09:40,  1.68it/s] 29%|██▉       | 405/1377 [04:08<09:44,  1.66it/s] 29%|██▉       | 406/1377 [04:09<09:58,  1.62it/s] 30%|██▉       | 407/1377 [04:09<09:44,  1.66it/s] 30%|██▉       | 408/1377 [04:10<09:32,  1.69it/s] 30%|██▉       | 409/1377 [04:10<09:20,  1.73it/s] 30%|██▉       | 410/1377 [04:11<09:47,  1.65it/s] 30%|██▉       | 411/1377 [04:12<09:45,  1.65it/s] 30%|██▉       | 412/1377 [04:12<09:23,  1.71it/s] 30%|██▉       | 413/1377 [04:13<09:17,  1.73it/s] 30%|███       | 414/1377 [04:13<09:21,  1.71it/s] 30%|███       | 415/1377 [04:14<09:30,  1.69it/s] 30%|███       | 416/1377 [04:15<09:12,  1.74it/s] 30%|███       | 417/1377 [04:15<09:07,  1.75it/s] 30%|███       | 418/1377 [04:16<09:36,  1.66it/s] 30%|███       | 419/1377 [04:16<09:59,  1.60it/s] 31%|███       | 420/1377 [04:17<09:34,  1.67it/s] 31%|███       | 421/1377 [04:18<10:02,  1.59it/s] 31%|███       | 422/1377 [04:18<09:42,  1.64it/s] 31%|███       | 423/1377 [04:19<09:50,  1.61it/s] 31%|███       | 424/1377 [04:20<09:49,  1.62it/s] 31%|███       | 425/1377 [04:20<10:14,  1.55it/s] 31%|███       | 426/1377 [04:21<09:43,  1.63it/s] 31%|███       | 427/1377 [04:21<09:17,  1.70it/s] 31%|███       | 428/1377 [04:22<09:40,  1.63it/s] 31%|███       | 429/1377 [04:23<09:39,  1.64it/s] 31%|███       | 430/1377 [04:23<09:49,  1.61it/s] 31%|███▏      | 431/1377 [04:24<09:43,  1.62it/s] 31%|███▏      | 432/1377 [04:24<09:26,  1.67it/s] 31%|███▏      | 433/1377 [04:25<09:11,  1.71it/s] 32%|███▏      | 434/1377 [04:26<09:27,  1.66it/s] 32%|███▏      | 435/1377 [04:26<09:13,  1.70it/s] 32%|███▏      | 436/1377 [04:27<09:19,  1.68it/s] 32%|███▏      | 437/1377 [04:27<09:32,  1.64it/s] 32%|███▏      | 438/1377 [04:28<09:17,  1.68it/s] 32%|███▏      | 439/1377 [04:29<09:54,  1.58it/s] 32%|███▏      | 440/1377 [04:29<09:44,  1.60it/s] 32%|███▏      | 441/1377 [04:30<10:17,  1.52it/s] 32%|███▏      | 442/1377 [04:31<10:04,  1.55it/s] 32%|███▏      | 443/1377 [04:31<09:33,  1.63it/s] 32%|███▏      | 444/1377 [04:32<09:19,  1.67it/s] 32%|███▏      | 445/1377 [04:32<09:10,  1.69it/s] 32%|███▏      | 446/1377 [04:33<09:26,  1.64it/s] 32%|███▏      | 447/1377 [04:34<09:47,  1.58it/s] 33%|███▎      | 448/1377 [04:34<09:29,  1.63it/s] 33%|███▎      | 449/1377 [04:35<08:56,  1.73it/s] 33%|███▎      | 450/1377 [04:35<08:48,  1.76it/s] 33%|███▎      | 451/1377 [04:36<09:07,  1.69it/s] 33%|███▎      | 452/1377 [04:37<09:09,  1.68it/s] 33%|███▎      | 453/1377 [04:37<09:32,  1.61it/s] 33%|███▎      | 454/1377 [04:38<09:56,  1.55it/s] 33%|███▎      | 455/1377 [04:39<10:04,  1.52it/s] 33%|███▎      | 456/1377 [04:39<09:53,  1.55it/s] 33%|███▎      | 457/1377 [04:40<09:44,  1.57it/s] 33%|███▎      | 458/1377 [04:40<09:35,  1.60it/s] 33%|███▎      | 459/1377 [04:41<08:25,  1.82it/s]03/13/2024 20:56:53 - INFO - __main__ - epoch 0: {'accuracy': 0.7279411764705882, 'f1': 0.8310502283105022}
 33%|███▎      | 460/1377 [04:49<44:58,  2.94s/it] 33%|███▎      | 461/1377 [04:50<34:23,  2.25s/it] 34%|███▎      | 462/1377 [04:51<26:36,  1.74s/it] 34%|███▎      | 463/1377 [04:51<21:25,  1.41s/it] 34%|███▎      | 464/1377 [04:52<17:33,  1.15s/it] 34%|███▍      | 465/1377 [04:52<15:20,  1.01s/it] 34%|███▍      | 466/1377 [04:53<13:31,  1.12it/s] 34%|███▍      | 467/1377 [04:54<12:16,  1.24it/s] 34%|███▍      | 468/1377 [04:54<11:17,  1.34it/s] 34%|███▍      | 469/1377 [04:55<10:40,  1.42it/s] 34%|███▍      | 470/1377 [04:55<10:23,  1.45it/s] 34%|███▍      | 471/1377 [04:56<10:04,  1.50it/s] 34%|███▍      | 472/1377 [04:57<09:46,  1.54it/s] 34%|███▍      | 473/1377 [04:57<09:35,  1.57it/s] 34%|███▍      | 474/1377 [04:58<09:28,  1.59it/s] 34%|███▍      | 475/1377 [04:58<09:04,  1.66it/s] 35%|███▍      | 476/1377 [04:59<09:15,  1.62it/s] 35%|███▍      | 477/1377 [05:00<08:53,  1.69it/s] 35%|███▍      | 478/1377 [05:00<08:57,  1.67it/s] 35%|███▍      | 479/1377 [05:01<08:44,  1.71it/s] 35%|███▍      | 480/1377 [05:01<08:59,  1.66it/s] 35%|███▍      | 481/1377 [05:02<08:58,  1.66it/s] 35%|███▌      | 482/1377 [05:03<09:00,  1.66it/s] 35%|███▌      | 483/1377 [05:03<08:42,  1.71it/s] 35%|███▌      | 484/1377 [05:04<08:57,  1.66it/s] 35%|███▌      | 485/1377 [05:04<08:57,  1.66it/s] 35%|███▌      | 486/1377 [05:05<09:00,  1.65it/s] 35%|███▌      | 487/1377 [05:06<08:46,  1.69it/s] 35%|███▌      | 488/1377 [05:06<08:59,  1.65it/s] 36%|███▌      | 489/1377 [05:07<08:58,  1.65it/s] 36%|███▌      | 490/1377 [05:08<09:16,  1.59it/s] 36%|███▌      | 491/1377 [05:08<09:31,  1.55it/s] 36%|███▌      | 492/1377 [05:09<09:03,  1.63it/s] 36%|███▌      | 493/1377 [05:09<08:49,  1.67it/s] 36%|███▌      | 494/1377 [05:10<09:17,  1.58it/s] 36%|███▌      | 495/1377 [05:11<09:01,  1.63it/s] 36%|███▌      | 496/1377 [05:11<09:19,  1.58it/s] 36%|███▌      | 497/1377 [05:12<08:42,  1.68it/s] 36%|███▌      | 498/1377 [05:12<08:33,  1.71it/s] 36%|███▌      | 499/1377 [05:13<08:27,  1.73it/s] 36%|███▋      | 500/1377 [05:14<08:35,  1.70it/s] 36%|███▋      | 501/1377 [05:14<08:41,  1.68it/s] 36%|███▋      | 502/1377 [05:15<08:45,  1.66it/s] 37%|███▋      | 503/1377 [05:15<08:57,  1.62it/s] 37%|███▋      | 504/1377 [05:16<08:58,  1.62it/s] 37%|███▋      | 505/1377 [05:17<08:56,  1.62it/s] 37%|███▋      | 506/1377 [05:17<08:35,  1.69it/s] 37%|███▋      | 507/1377 [05:18<08:25,  1.72it/s] 37%|███▋      | 508/1377 [05:18<08:41,  1.67it/s] 37%|███▋      | 509/1377 [05:19<09:00,  1.60it/s] 37%|███▋      | 510/1377 [05:20<09:08,  1.58it/s] 37%|███▋      | 511/1377 [05:20<08:48,  1.64it/s] 37%|███▋      | 512/1377 [05:21<08:46,  1.64it/s] 37%|███▋      | 513/1377 [05:21<08:45,  1.64it/s] 37%|███▋      | 514/1377 [05:22<09:02,  1.59it/s] 37%|███▋      | 515/1377 [05:23<08:54,  1.61it/s] 37%|███▋      | 516/1377 [05:23<08:47,  1.63it/s] 38%|███▊      | 517/1377 [05:24<08:46,  1.63it/s] 38%|███▊      | 518/1377 [05:25<09:03,  1.58it/s] 38%|███▊      | 519/1377 [05:25<09:14,  1.55it/s] 38%|███▊      | 520/1377 [05:26<08:53,  1.61it/s] 38%|███▊      | 521/1377 [05:26<08:38,  1.65it/s] 38%|███▊      | 522/1377 [05:27<08:38,  1.65it/s] 38%|███▊      | 523/1377 [05:28<08:24,  1.69it/s] 38%|███▊      | 524/1377 [05:28<08:54,  1.59it/s] 38%|███▊      | 525/1377 [05:29<08:37,  1.65it/s] 38%|███▊      | 526/1377 [05:30<08:46,  1.62it/s] 38%|███▊      | 527/1377 [05:30<09:00,  1.57it/s] 38%|███▊      | 528/1377 [05:31<08:54,  1.59it/s] 38%|███▊      | 529/1377 [05:31<08:58,  1.57it/s] 38%|███▊      | 530/1377 [05:32<08:48,  1.60it/s] 39%|███▊      | 531/1377 [05:33<08:52,  1.59it/s] 39%|███▊      | 532/1377 [05:33<08:35,  1.64it/s] 39%|███▊      | 533/1377 [05:34<08:50,  1.59it/s] 39%|███▉      | 534/1377 [05:35<08:30,  1.65it/s] 39%|███▉      | 535/1377 [05:35<08:12,  1.71it/s] 39%|███▉      | 536/1377 [05:36<08:18,  1.69it/s] 39%|███▉      | 537/1377 [05:36<08:11,  1.71it/s] 39%|███▉      | 538/1377 [05:37<08:06,  1.73it/s] 39%|███▉      | 539/1377 [05:37<08:22,  1.67it/s] 39%|███▉      | 540/1377 [05:38<08:32,  1.63it/s] 39%|███▉      | 541/1377 [05:39<08:40,  1.61it/s] 39%|███▉      | 542/1377 [05:39<08:26,  1.65it/s] 39%|███▉      | 543/1377 [05:40<08:51,  1.57it/s] 40%|███▉      | 544/1377 [05:41<08:26,  1.65it/s] 40%|███▉      | 545/1377 [05:41<08:21,  1.66it/s] 40%|███▉      | 546/1377 [05:42<08:08,  1.70it/s] 40%|███▉      | 547/1377 [05:42<07:57,  1.74it/s] 40%|███▉      | 548/1377 [05:43<08:13,  1.68it/s] 40%|███▉      | 549/1377 [05:43<08:03,  1.71it/s] 40%|███▉      | 550/1377 [05:44<07:51,  1.76it/s] 40%|████      | 551/1377 [05:45<07:56,  1.73it/s] 40%|████      | 552/1377 [05:45<08:21,  1.65it/s] 40%|████      | 553/1377 [05:46<08:36,  1.59it/s] 40%|████      | 554/1377 [05:47<08:48,  1.56it/s] 40%|████      | 555/1377 [05:47<08:49,  1.55it/s] 40%|████      | 556/1377 [05:48<08:41,  1.57it/s] 40%|████      | 557/1377 [05:49<08:51,  1.54it/s] 41%|████      | 558/1377 [05:49<08:28,  1.61it/s] 41%|████      | 559/1377 [05:50<08:10,  1.67it/s] 41%|████      | 560/1377 [05:50<08:02,  1.69it/s] 41%|████      | 561/1377 [05:51<08:15,  1.65it/s] 41%|████      | 562/1377 [05:51<08:02,  1.69it/s] 41%|████      | 563/1377 [05:52<08:03,  1.68it/s] 41%|████      | 564/1377 [05:53<08:03,  1.68it/s] 41%|████      | 565/1377 [05:53<08:15,  1.64it/s] 41%|████      | 566/1377 [05:54<08:24,  1.61it/s] 41%|████      | 567/1377 [05:54<08:01,  1.68it/s] 41%|████      | 568/1377 [05:55<07:50,  1.72it/s] 41%|████▏     | 569/1377 [05:56<07:45,  1.73it/s] 41%|████▏     | 570/1377 [05:56<08:00,  1.68it/s] 41%|████▏     | 571/1377 [05:57<07:52,  1.71it/s] 42%|████▏     | 572/1377 [05:57<07:46,  1.73it/s] 42%|████▏     | 573/1377 [05:58<07:32,  1.78it/s] 42%|████▏     | 574/1377 [05:59<08:05,  1.65it/s] 42%|████▏     | 575/1377 [05:59<07:56,  1.68it/s] 42%|████▏     | 576/1377 [06:00<07:59,  1.67it/s] 42%|████▏     | 577/1377 [06:00<08:25,  1.58it/s] 42%|████▏     | 578/1377 [06:01<08:17,  1.61it/s] 42%|████▏     | 579/1377 [06:02<08:23,  1.58it/s] 42%|████▏     | 580/1377 [06:02<08:05,  1.64it/s] 42%|████▏     | 581/1377 [06:03<07:47,  1.70it/s] 42%|████▏     | 582/1377 [06:03<07:53,  1.68it/s] 42%|████▏     | 583/1377 [06:04<07:45,  1.71it/s] 42%|████▏     | 584/1377 [06:05<07:40,  1.72it/s] 42%|████▏     | 585/1377 [06:05<07:35,  1.74it/s] 43%|████▎     | 586/1377 [06:06<07:42,  1.71it/s] 43%|████▎     | 587/1377 [06:06<07:45,  1.70it/s] 43%|████▎     | 588/1377 [06:07<07:56,  1.65it/s] 43%|████▎     | 589/1377 [06:08<08:14,  1.59it/s] 43%|████▎     | 590/1377 [06:08<07:57,  1.65it/s] 43%|████▎     | 591/1377 [06:09<08:06,  1.62it/s] 43%|████▎     | 592/1377 [06:09<08:20,  1.57it/s] 43%|████▎     | 593/1377 [06:10<08:03,  1.62it/s] 43%|████▎     | 594/1377 [06:11<08:01,  1.63it/s] 43%|████▎     | 595/1377 [06:11<07:59,  1.63it/s] 43%|████▎     | 596/1377 [06:12<07:48,  1.67it/s] 43%|████▎     | 597/1377 [06:13<07:58,  1.63it/s] 43%|████▎     | 598/1377 [06:13<07:57,  1.63it/s] 44%|████▎     | 599/1377 [06:14<08:03,  1.61it/s] 44%|████▎     | 600/1377 [06:14<08:00,  1.62it/s] 44%|████▎     | 601/1377 [06:15<08:14,  1.57it/s] 44%|████▎     | 602/1377 [06:16<07:55,  1.63it/s] 44%|████▍     | 603/1377 [06:16<07:36,  1.69it/s] 44%|████▍     | 604/1377 [06:17<08:02,  1.60it/s] 44%|████▍     | 605/1377 [06:17<07:55,  1.62it/s] 44%|████▍     | 606/1377 [06:18<07:53,  1.63it/s] 44%|████▍     | 607/1377 [06:19<07:41,  1.67it/s] 44%|████▍     | 608/1377 [06:19<07:51,  1.63it/s] 44%|████▍     | 609/1377 [06:20<07:34,  1.69it/s] 44%|████▍     | 610/1377 [06:20<07:38,  1.67it/s] 44%|████▍     | 611/1377 [06:21<07:48,  1.63it/s] 44%|████▍     | 612/1377 [06:22<07:37,  1.67it/s] 45%|████▍     | 613/1377 [06:22<07:36,  1.67it/s] 45%|████▍     | 614/1377 [06:23<07:26,  1.71it/s] 45%|████▍     | 615/1377 [06:23<07:17,  1.74it/s] 45%|████▍     | 616/1377 [06:24<07:40,  1.65it/s] 45%|████▍     | 617/1377 [06:25<07:49,  1.62it/s] 45%|████▍     | 618/1377 [06:25<07:56,  1.59it/s] 45%|████▍     | 619/1377 [06:26<07:53,  1.60it/s] 45%|████▌     | 620/1377 [06:27<07:50,  1.61it/s] 45%|████▌     | 621/1377 [06:27<07:45,  1.62it/s] 45%|████▌     | 622/1377 [06:28<07:51,  1.60it/s] 45%|████▌     | 623/1377 [06:28<07:57,  1.58it/s] 45%|████▌     | 624/1377 [06:29<08:28,  1.48it/s] 45%|████▌     | 625/1377 [06:30<08:15,  1.52it/s] 45%|████▌     | 626/1377 [06:30<07:54,  1.58it/s] 46%|████▌     | 627/1377 [06:31<07:45,  1.61it/s] 46%|████▌     | 628/1377 [06:32<07:25,  1.68it/s] 46%|████▌     | 629/1377 [06:32<07:27,  1.67it/s] 46%|████▌     | 630/1377 [06:33<07:37,  1.63it/s] 46%|████▌     | 631/1377 [06:33<07:43,  1.61it/s] 46%|████▌     | 632/1377 [06:34<07:36,  1.63it/s] 46%|████▌     | 633/1377 [06:35<07:49,  1.58it/s] 46%|████▌     | 634/1377 [06:35<07:31,  1.64it/s] 46%|████▌     | 635/1377 [06:36<07:38,  1.62it/s] 46%|████▌     | 636/1377 [06:37<07:57,  1.55it/s] 46%|████▋     | 637/1377 [06:37<08:04,  1.53it/s] 46%|████▋     | 638/1377 [06:38<07:54,  1.56it/s] 46%|████▋     | 639/1377 [06:38<07:46,  1.58it/s] 46%|████▋     | 640/1377 [06:39<07:55,  1.55it/s] 47%|████▋     | 641/1377 [06:40<07:55,  1.55it/s] 47%|████▋     | 642/1377 [06:40<07:28,  1.64it/s] 47%|████▋     | 643/1377 [06:41<07:24,  1.65it/s] 47%|████▋     | 644/1377 [06:42<07:33,  1.62it/s] 47%|████▋     | 645/1377 [06:42<07:27,  1.64it/s] 47%|████▋     | 646/1377 [06:43<07:13,  1.68it/s] 47%|████▋     | 647/1377 [06:44<07:51,  1.55it/s] 47%|████▋     | 648/1377 [06:44<07:52,  1.54it/s] 47%|████▋     | 649/1377 [06:45<07:59,  1.52it/s] 47%|████▋     | 650/1377 [06:45<07:55,  1.53it/s] 47%|████▋     | 651/1377 [06:46<07:52,  1.54it/s] 47%|████▋     | 652/1377 [06:47<07:50,  1.54it/s] 47%|████▋     | 653/1377 [06:47<07:56,  1.52it/s] 47%|████▋     | 654/1377 [06:48<07:52,  1.53it/s] 48%|████▊     | 655/1377 [06:49<07:39,  1.57it/s] 48%|████▊     | 656/1377 [06:49<07:15,  1.65it/s] 48%|████▊     | 657/1377 [06:50<07:23,  1.62it/s] 48%|████▊     | 658/1377 [06:50<07:22,  1.62it/s] 48%|████▊     | 659/1377 [06:51<07:09,  1.67it/s] 48%|████▊     | 660/1377 [06:52<06:57,  1.72it/s] 48%|████▊     | 661/1377 [06:52<07:00,  1.70it/s] 48%|████▊     | 662/1377 [06:53<07:05,  1.68it/s] 48%|████▊     | 663/1377 [06:53<07:09,  1.66it/s] 48%|████▊     | 664/1377 [06:54<07:19,  1.62it/s] 48%|████▊     | 665/1377 [06:55<07:08,  1.66it/s] 48%|████▊     | 666/1377 [06:55<07:16,  1.63it/s] 48%|████▊     | 667/1377 [06:56<07:16,  1.63it/s] 49%|████▊     | 668/1377 [06:57<07:21,  1.60it/s] 49%|████▊     | 669/1377 [06:57<07:19,  1.61it/s] 49%|████▊     | 670/1377 [06:58<07:05,  1.66it/s] 49%|████▊     | 671/1377 [06:58<07:20,  1.60it/s] 49%|████▉     | 672/1377 [06:59<07:25,  1.58it/s] 49%|████▉     | 673/1377 [07:00<07:29,  1.57it/s] 49%|████▉     | 674/1377 [07:00<06:54,  1.70it/s] 49%|████▉     | 675/1377 [07:01<06:41,  1.75it/s] 49%|████▉     | 676/1377 [07:01<06:45,  1.73it/s] 49%|████▉     | 677/1377 [07:02<07:06,  1.64it/s] 49%|████▉     | 678/1377 [07:03<06:51,  1.70it/s] 49%|████▉     | 679/1377 [07:03<06:39,  1.75it/s] 49%|████▉     | 680/1377 [07:04<07:32,  1.54it/s] 49%|████▉     | 681/1377 [07:05<07:33,  1.54it/s] 50%|████▉     | 682/1377 [07:05<07:25,  1.56it/s] 50%|████▉     | 683/1377 [07:06<07:03,  1.64it/s] 50%|████▉     | 684/1377 [07:06<07:09,  1.61it/s] 50%|████▉     | 685/1377 [07:07<06:58,  1.66it/s] 50%|████▉     | 686/1377 [07:07<06:47,  1.70it/s] 50%|████▉     | 687/1377 [07:08<06:41,  1.72it/s] 50%|████▉     | 688/1377 [07:09<06:53,  1.66it/s] 50%|█████     | 689/1377 [07:09<07:03,  1.62it/s] 50%|█████     | 690/1377 [07:10<06:59,  1.64it/s] 50%|█████     | 691/1377 [07:11<06:56,  1.65it/s] 50%|█████     | 692/1377 [07:11<06:45,  1.69it/s] 50%|█████     | 693/1377 [07:12<06:36,  1.72it/s] 50%|█████     | 694/1377 [07:12<06:50,  1.66it/s] 50%|█████     | 695/1377 [07:13<06:42,  1.69it/s] 51%|█████     | 696/1377 [07:13<06:43,  1.69it/s] 51%|█████     | 697/1377 [07:14<06:46,  1.67it/s] 51%|█████     | 698/1377 [07:15<06:54,  1.64it/s] 51%|█████     | 699/1377 [07:15<06:50,  1.65it/s] 51%|█████     | 700/1377 [07:16<06:42,  1.68it/s] 51%|█████     | 701/1377 [07:16<06:50,  1.65it/s] 51%|█████     | 702/1377 [07:17<06:57,  1.62it/s] 51%|█████     | 703/1377 [07:18<06:45,  1.66it/s] 51%|█████     | 704/1377 [07:18<06:44,  1.66it/s] 51%|█████     | 705/1377 [07:19<06:59,  1.60it/s] 51%|█████▏    | 706/1377 [07:20<06:45,  1.65it/s] 51%|█████▏    | 707/1377 [07:20<06:28,  1.72it/s] 51%|█████▏    | 708/1377 [07:21<06:40,  1.67it/s] 51%|█████▏    | 709/1377 [07:21<06:48,  1.63it/s] 52%|█████▏    | 710/1377 [07:22<06:55,  1.61it/s] 52%|█████▏    | 711/1377 [07:23<06:53,  1.61it/s] 52%|█████▏    | 712/1377 [07:23<06:39,  1.67it/s] 52%|█████▏    | 713/1377 [07:24<06:38,  1.67it/s] 52%|█████▏    | 714/1377 [07:24<06:31,  1.69it/s] 52%|█████▏    | 715/1377 [07:25<06:41,  1.65it/s] 52%|█████▏    | 716/1377 [07:26<06:42,  1.64it/s] 52%|█████▏    | 717/1377 [07:26<06:40,  1.65it/s] 52%|█████▏    | 718/1377 [07:27<06:40,  1.65it/s] 52%|█████▏    | 719/1377 [07:27<06:47,  1.61it/s] 52%|█████▏    | 720/1377 [07:28<06:33,  1.67it/s] 52%|█████▏    | 721/1377 [07:29<06:19,  1.73it/s] 52%|█████▏    | 722/1377 [07:29<06:14,  1.75it/s] 53%|█████▎    | 723/1377 [07:30<06:12,  1.76it/s] 53%|█████▎    | 724/1377 [07:30<06:08,  1.77it/s] 53%|█████▎    | 725/1377 [07:31<06:13,  1.74it/s] 53%|█████▎    | 726/1377 [07:31<06:39,  1.63it/s] 53%|█████▎    | 727/1377 [07:32<06:24,  1.69it/s] 53%|█████▎    | 728/1377 [07:33<06:33,  1.65it/s] 53%|█████▎    | 729/1377 [07:33<06:46,  1.59it/s] 53%|█████▎    | 730/1377 [07:34<06:49,  1.58it/s] 53%|█████▎    | 731/1377 [07:35<06:58,  1.55it/s] 53%|█████▎    | 732/1377 [07:35<06:58,  1.54it/s] 53%|█████▎    | 733/1377 [07:36<07:09,  1.50it/s] 53%|█████▎    | 734/1377 [07:37<07:06,  1.51it/s] 53%|█████▎    | 735/1377 [07:37<07:01,  1.52it/s] 53%|█████▎    | 736/1377 [07:38<06:49,  1.56it/s] 54%|█████▎    | 737/1377 [07:39<06:43,  1.59it/s] 54%|█████▎    | 738/1377 [07:39<06:36,  1.61it/s] 54%|█████▎    | 739/1377 [07:40<06:23,  1.66it/s] 54%|█████▎    | 740/1377 [07:40<06:37,  1.60it/s] 54%|█████▍    | 741/1377 [07:41<06:22,  1.66it/s] 54%|█████▍    | 742/1377 [07:42<06:21,  1.66it/s] 54%|█████▍    | 743/1377 [07:42<06:11,  1.71it/s] 54%|█████▍    | 744/1377 [07:43<06:21,  1.66it/s] 54%|█████▍    | 745/1377 [07:43<06:21,  1.66it/s] 54%|█████▍    | 746/1377 [07:44<06:27,  1.63it/s] 54%|█████▍    | 747/1377 [07:45<06:24,  1.64it/s] 54%|█████▍    | 748/1377 [07:45<06:13,  1.69it/s] 54%|█████▍    | 749/1377 [07:46<06:22,  1.64it/s] 54%|█████▍    | 750/1377 [07:46<06:28,  1.61it/s] 55%|█████▍    | 751/1377 [07:47<06:32,  1.59it/s] 55%|█████▍    | 752/1377 [07:48<06:29,  1.61it/s] 55%|█████▍    | 753/1377 [07:48<06:31,  1.59it/s] 55%|█████▍    | 754/1377 [07:49<06:27,  1.61it/s] 55%|█████▍    | 755/1377 [07:50<06:37,  1.57it/s] 55%|█████▍    | 756/1377 [07:50<06:32,  1.58it/s] 55%|█████▍    | 757/1377 [07:51<06:28,  1.60it/s] 55%|█████▌    | 758/1377 [07:51<06:22,  1.62it/s] 55%|█████▌    | 759/1377 [07:52<06:26,  1.60it/s] 55%|█████▌    | 760/1377 [07:53<06:15,  1.64it/s] 55%|█████▌    | 761/1377 [07:53<06:06,  1.68it/s] 55%|█████▌    | 762/1377 [07:54<06:14,  1.64it/s] 55%|█████▌    | 763/1377 [07:54<06:19,  1.62it/s] 55%|█████▌    | 764/1377 [07:55<06:30,  1.57it/s] 56%|█████▌    | 765/1377 [07:56<06:25,  1.59it/s] 56%|█████▌    | 766/1377 [07:56<06:37,  1.54it/s] 56%|█████▌    | 767/1377 [07:57<06:42,  1.52it/s] 56%|█████▌    | 768/1377 [07:58<06:33,  1.55it/s] 56%|█████▌    | 769/1377 [07:58<06:32,  1.55it/s] 56%|█████▌    | 770/1377 [07:59<06:24,  1.58it/s] 56%|█████▌    | 771/1377 [08:00<06:05,  1.66it/s] 56%|█████▌    | 772/1377 [08:00<06:12,  1.62it/s] 56%|█████▌    | 773/1377 [08:01<05:58,  1.68it/s] 56%|█████▌    | 774/1377 [08:01<06:20,  1.58it/s] 56%|█████▋    | 775/1377 [08:02<06:15,  1.60it/s] 56%|█████▋    | 776/1377 [08:03<06:10,  1.62it/s] 56%|█████▋    | 777/1377 [08:03<06:09,  1.62it/s] 56%|█████▋    | 778/1377 [08:04<06:25,  1.55it/s] 57%|█████▋    | 779/1377 [08:05<06:05,  1.64it/s] 57%|█████▋    | 780/1377 [08:05<06:04,  1.64it/s] 57%|█████▋    | 781/1377 [08:06<06:03,  1.64it/s] 57%|█████▋    | 782/1377 [08:06<05:48,  1.71it/s] 57%|█████▋    | 783/1377 [08:07<05:42,  1.74it/s] 57%|█████▋    | 784/1377 [08:07<05:36,  1.76it/s] 57%|█████▋    | 785/1377 [08:08<05:34,  1.77it/s] 57%|█████▋    | 786/1377 [08:08<05:33,  1.77it/s] 57%|█████▋    | 787/1377 [08:09<05:47,  1.70it/s] 57%|█████▋    | 788/1377 [08:10<05:51,  1.68it/s] 57%|█████▋    | 789/1377 [08:10<05:51,  1.67it/s] 57%|█████▋    | 790/1377 [08:11<05:52,  1.66it/s] 57%|█████▋    | 791/1377 [08:12<05:46,  1.69it/s] 58%|█████▊    | 792/1377 [08:12<05:49,  1.67it/s] 58%|█████▊    | 793/1377 [08:13<05:49,  1.67it/s] 58%|█████▊    | 794/1377 [08:13<05:51,  1.66it/s] 58%|█████▊    | 795/1377 [08:14<05:50,  1.66it/s] 58%|█████▊    | 796/1377 [08:15<05:48,  1.67it/s] 58%|█████▊    | 797/1377 [08:15<05:49,  1.66it/s] 58%|█████▊    | 798/1377 [08:16<05:57,  1.62it/s] 58%|█████▊    | 799/1377 [08:16<06:01,  1.60it/s] 58%|█████▊    | 800/1377 [08:17<05:49,  1.65it/s] 58%|█████▊    | 801/1377 [08:18<05:39,  1.70it/s] 58%|█████▊    | 802/1377 [08:18<05:32,  1.73it/s] 58%|█████▊    | 803/1377 [08:19<05:27,  1.76it/s] 58%|█████▊    | 804/1377 [08:19<05:45,  1.66it/s] 58%|█████▊    | 805/1377 [08:20<05:58,  1.60it/s] 59%|█████▊    | 806/1377 [08:21<06:12,  1.53it/s] 59%|█████▊    | 807/1377 [08:21<05:55,  1.60it/s] 59%|█████▊    | 808/1377 [08:22<05:44,  1.65it/s] 59%|█████▉    | 809/1377 [08:23<06:01,  1.57it/s] 59%|█████▉    | 810/1377 [08:23<06:01,  1.57it/s] 59%|█████▉    | 811/1377 [08:24<06:02,  1.56it/s] 59%|█████▉    | 812/1377 [08:24<05:57,  1.58it/s] 59%|█████▉    | 813/1377 [08:25<05:59,  1.57it/s] 59%|█████▉    | 814/1377 [08:26<06:01,  1.56it/s] 59%|█████▉    | 815/1377 [08:26<05:54,  1.59it/s] 59%|█████▉    | 816/1377 [08:27<06:01,  1.55it/s] 59%|█████▉    | 817/1377 [08:28<06:00,  1.55it/s] 59%|█████▉    | 818/1377 [08:28<06:00,  1.55it/s] 59%|█████▉    | 819/1377 [08:29<05:59,  1.55it/s] 60%|█████▉    | 820/1377 [08:30<05:58,  1.55it/s] 60%|█████▉    | 821/1377 [08:30<05:50,  1.59it/s] 60%|█████▉    | 822/1377 [08:31<05:58,  1.55it/s] 60%|█████▉    | 823/1377 [08:32<06:03,  1.52it/s] 60%|█████▉    | 824/1377 [08:32<06:07,  1.50it/s] 60%|█████▉    | 825/1377 [08:33<05:56,  1.55it/s] 60%|█████▉    | 826/1377 [08:33<05:50,  1.57it/s] 60%|██████    | 827/1377 [08:34<05:51,  1.56it/s] 60%|██████    | 828/1377 [08:35<05:51,  1.56it/s] 60%|██████    | 829/1377 [08:35<05:38,  1.62it/s] 60%|██████    | 830/1377 [08:36<05:47,  1.57it/s] 60%|██████    | 831/1377 [08:37<05:44,  1.59it/s] 60%|██████    | 832/1377 [08:37<05:45,  1.58it/s] 60%|██████    | 833/1377 [08:38<05:52,  1.54it/s] 61%|██████    | 834/1377 [08:39<05:51,  1.54it/s] 61%|██████    | 835/1377 [08:39<05:37,  1.61it/s] 61%|██████    | 836/1377 [08:40<05:35,  1.61it/s] 61%|██████    | 837/1377 [08:40<05:24,  1.66it/s] 61%|██████    | 838/1377 [08:41<05:25,  1.65it/s] 61%|██████    | 839/1377 [08:42<05:19,  1.69it/s] 61%|██████    | 840/1377 [08:42<05:31,  1.62it/s] 61%|██████    | 841/1377 [08:43<05:36,  1.59it/s] 61%|██████    | 842/1377 [08:44<05:43,  1.56it/s] 61%|██████    | 843/1377 [08:44<05:36,  1.59it/s] 61%|██████▏   | 844/1377 [08:45<05:23,  1.65it/s] 61%|██████▏   | 845/1377 [08:45<05:17,  1.68it/s] 61%|██████▏   | 846/1377 [08:46<05:16,  1.68it/s] 62%|██████▏   | 847/1377 [08:46<05:08,  1.72it/s] 62%|██████▏   | 848/1377 [08:47<05:11,  1.70it/s] 62%|██████▏   | 849/1377 [08:48<05:35,  1.57it/s] 62%|██████▏   | 850/1377 [08:48<05:23,  1.63it/s] 62%|██████▏   | 851/1377 [08:49<05:42,  1.54it/s] 62%|██████▏   | 852/1377 [08:50<05:41,  1.54it/s] 62%|██████▏   | 853/1377 [08:50<05:40,  1.54it/s] 62%|██████▏   | 854/1377 [08:51<05:39,  1.54it/s] 62%|██████▏   | 855/1377 [08:52<05:25,  1.60it/s] 62%|██████▏   | 856/1377 [08:52<05:43,  1.52it/s] 62%|██████▏   | 857/1377 [08:53<05:40,  1.53it/s] 62%|██████▏   | 858/1377 [08:54<05:31,  1.57it/s] 62%|██████▏   | 859/1377 [08:54<05:18,  1.63it/s] 62%|██████▏   | 860/1377 [08:55<05:09,  1.67it/s] 63%|██████▎   | 861/1377 [08:55<05:15,  1.63it/s] 63%|██████▎   | 862/1377 [08:56<05:26,  1.58it/s] 63%|██████▎   | 863/1377 [08:57<05:27,  1.57it/s] 63%|██████▎   | 864/1377 [08:57<05:28,  1.56it/s] 63%|██████▎   | 865/1377 [08:58<05:24,  1.58it/s] 63%|██████▎   | 866/1377 [08:59<05:30,  1.55it/s] 63%|██████▎   | 867/1377 [08:59<05:25,  1.57it/s] 63%|██████▎   | 868/1377 [09:00<05:26,  1.56it/s] 63%|██████▎   | 869/1377 [09:00<05:26,  1.55it/s] 63%|██████▎   | 870/1377 [09:01<05:21,  1.58it/s] 63%|██████▎   | 871/1377 [09:02<05:09,  1.64it/s] 63%|██████▎   | 872/1377 [09:02<05:01,  1.67it/s] 63%|██████▎   | 873/1377 [09:03<05:12,  1.61it/s] 63%|██████▎   | 874/1377 [09:04<05:16,  1.59it/s] 64%|██████▎   | 875/1377 [09:04<05:14,  1.60it/s] 64%|██████▎   | 876/1377 [09:05<05:09,  1.62it/s] 64%|██████▎   | 877/1377 [09:05<05:01,  1.66it/s] 64%|██████▍   | 878/1377 [09:06<05:02,  1.65it/s] 64%|██████▍   | 879/1377 [09:07<05:07,  1.62it/s] 64%|██████▍   | 880/1377 [09:07<05:06,  1.62it/s] 64%|██████▍   | 881/1377 [09:08<05:03,  1.63it/s] 64%|██████▍   | 882/1377 [09:08<05:07,  1.61it/s] 64%|██████▍   | 883/1377 [09:09<05:14,  1.57it/s] 64%|██████▍   | 884/1377 [09:10<04:53,  1.68it/s] 64%|██████▍   | 885/1377 [09:10<05:08,  1.60it/s] 64%|██████▍   | 886/1377 [09:11<05:06,  1.60it/s] 64%|██████▍   | 887/1377 [09:11<04:53,  1.67it/s] 64%|██████▍   | 888/1377 [09:12<04:48,  1.70it/s] 65%|██████▍   | 889/1377 [09:13<04:38,  1.75it/s] 65%|██████▍   | 890/1377 [09:13<04:53,  1.66it/s] 65%|██████▍   | 891/1377 [09:14<04:59,  1.62it/s] 65%|██████▍   | 892/1377 [09:15<05:08,  1.57it/s] 65%|██████▍   | 893/1377 [09:15<05:03,  1.60it/s] 65%|██████▍   | 894/1377 [09:16<04:58,  1.62it/s] 65%|██████▍   | 895/1377 [09:16<05:01,  1.60it/s] 65%|██████▌   | 896/1377 [09:17<05:03,  1.58it/s] 65%|██████▌   | 897/1377 [09:18<05:00,  1.60it/s] 65%|██████▌   | 898/1377 [09:18<04:48,  1.66it/s] 65%|██████▌   | 899/1377 [09:19<04:46,  1.67it/s] 65%|██████▌   | 900/1377 [09:19<04:47,  1.66it/s] 65%|██████▌   | 901/1377 [09:20<04:57,  1.60it/s] 66%|██████▌   | 902/1377 [09:21<05:12,  1.52it/s] 66%|██████▌   | 903/1377 [09:21<05:03,  1.56it/s] 66%|██████▌   | 904/1377 [09:22<05:04,  1.56it/s] 66%|██████▌   | 905/1377 [09:23<05:13,  1.51it/s] 66%|██████▌   | 906/1377 [09:24<05:18,  1.48it/s] 66%|██████▌   | 907/1377 [09:24<05:02,  1.56it/s] 66%|██████▌   | 908/1377 [09:25<04:55,  1.59it/s] 66%|██████▌   | 909/1377 [09:25<04:56,  1.58it/s] 66%|██████▌   | 910/1377 [09:26<05:05,  1.53it/s] 66%|██████▌   | 911/1377 [09:27<04:57,  1.57it/s] 66%|██████▌   | 912/1377 [09:27<04:57,  1.56it/s] 66%|██████▋   | 913/1377 [09:28<04:57,  1.56it/s] 66%|██████▋   | 914/1377 [09:28<04:44,  1.63it/s] 66%|██████▋   | 915/1377 [09:29<04:41,  1.64it/s] 67%|██████▋   | 916/1377 [09:30<04:38,  1.65it/s] 67%|██████▋   | 917/1377 [09:30<04:47,  1.60it/s] 67%|██████▋   | 918/1377 [09:31<04:11,  1.82it/s]03/13/2024 21:01:43 - INFO - __main__ - epoch 1: {'accuracy': 0.8112745098039216, 'f1': 0.8697123519458544}
 67%|██████▋   | 919/1377 [09:39<22:20,  2.93s/it] 67%|██████▋   | 920/1377 [09:40<16:59,  2.23s/it] 67%|██████▋   | 921/1377 [09:40<13:13,  1.74s/it] 67%|██████▋   | 922/1377 [09:41<10:42,  1.41s/it] 67%|██████▋   | 923/1377 [09:42<08:51,  1.17s/it] 67%|██████▋   | 924/1377 [09:42<07:26,  1.01it/s] 67%|██████▋   | 925/1377 [09:43<06:34,  1.14it/s] 67%|██████▋   | 926/1377 [09:44<06:20,  1.18it/s] 67%|██████▋   | 927/1377 [09:44<05:39,  1.33it/s] 67%|██████▋   | 928/1377 [09:45<05:28,  1.37it/s] 67%|██████▋   | 929/1377 [09:45<05:05,  1.47it/s] 68%|██████▊   | 930/1377 [09:46<04:55,  1.51it/s] 68%|██████▊   | 931/1377 [09:47<04:52,  1.52it/s] 68%|██████▊   | 932/1377 [09:47<04:46,  1.55it/s] 68%|██████▊   | 933/1377 [09:48<04:50,  1.53it/s] 68%|██████▊   | 934/1377 [09:49<04:49,  1.53it/s] 68%|██████▊   | 935/1377 [09:49<04:48,  1.53it/s] 68%|██████▊   | 936/1377 [09:50<04:51,  1.51it/s] 68%|██████▊   | 937/1377 [09:50<04:38,  1.58it/s] 68%|██████▊   | 938/1377 [09:51<04:32,  1.61it/s] 68%|██████▊   | 939/1377 [09:52<04:31,  1.61it/s] 68%|██████▊   | 940/1377 [09:52<04:23,  1.66it/s] 68%|██████▊   | 941/1377 [09:53<04:23,  1.65it/s] 68%|██████▊   | 942/1377 [09:53<04:24,  1.64it/s] 68%|██████▊   | 943/1377 [09:54<04:24,  1.64it/s] 69%|██████▊   | 944/1377 [09:55<04:28,  1.61it/s] 69%|██████▊   | 945/1377 [09:55<04:31,  1.59it/s] 69%|██████▊   | 946/1377 [09:56<04:34,  1.57it/s] 69%|██████▉   | 947/1377 [09:57<04:43,  1.52it/s] 69%|██████▉   | 948/1377 [09:57<04:41,  1.53it/s] 69%|██████▉   | 949/1377 [09:58<04:35,  1.56it/s] 69%|██████▉   | 950/1377 [09:59<04:28,  1.59it/s] 69%|██████▉   | 951/1377 [09:59<04:20,  1.64it/s] 69%|██████▉   | 952/1377 [10:00<04:19,  1.64it/s] 69%|██████▉   | 953/1377 [10:00<04:19,  1.64it/s] 69%|██████▉   | 954/1377 [10:01<04:23,  1.61it/s] 69%|██████▉   | 955/1377 [10:02<04:21,  1.62it/s] 69%|██████▉   | 956/1377 [10:02<04:10,  1.68it/s] 69%|██████▉   | 957/1377 [10:03<04:09,  1.68it/s] 70%|██████▉   | 958/1377 [10:03<04:11,  1.67it/s] 70%|██████▉   | 959/1377 [10:04<04:10,  1.67it/s] 70%|██████▉   | 960/1377 [10:05<04:14,  1.64it/s] 70%|██████▉   | 961/1377 [10:05<04:05,  1.70it/s] 70%|██████▉   | 962/1377 [10:06<04:19,  1.60it/s] 70%|██████▉   | 963/1377 [10:07<04:22,  1.58it/s] 70%|███████   | 964/1377 [10:07<04:18,  1.59it/s] 70%|███████   | 965/1377 [10:08<04:08,  1.65it/s] 70%|███████   | 966/1377 [10:08<04:02,  1.70it/s] 70%|███████   | 967/1377 [10:09<03:58,  1.72it/s] 70%|███████   | 968/1377 [10:09<04:02,  1.69it/s] 70%|███████   | 969/1377 [10:10<04:02,  1.68it/s] 70%|███████   | 970/1377 [10:11<04:03,  1.67it/s] 71%|███████   | 971/1377 [10:11<03:57,  1.71it/s] 71%|███████   | 972/1377 [10:12<03:53,  1.74it/s] 71%|███████   | 973/1377 [10:12<03:47,  1.77it/s] 71%|███████   | 974/1377 [10:13<04:04,  1.65it/s] 71%|███████   | 975/1377 [10:14<03:54,  1.71it/s] 71%|███████   | 976/1377 [10:14<03:57,  1.69it/s] 71%|███████   | 977/1377 [10:15<03:59,  1.67it/s] 71%|███████   | 978/1377 [10:15<04:04,  1.63it/s] 71%|███████   | 979/1377 [10:16<04:03,  1.63it/s] 71%|███████   | 980/1377 [10:17<03:57,  1.67it/s] 71%|███████   | 981/1377 [10:17<03:57,  1.67it/s] 71%|███████▏  | 982/1377 [10:18<03:57,  1.66it/s] 71%|███████▏  | 983/1377 [10:18<03:51,  1.70it/s] 71%|███████▏  | 984/1377 [10:19<03:46,  1.73it/s] 72%|███████▏  | 985/1377 [10:19<03:48,  1.72it/s] 72%|███████▏  | 986/1377 [10:20<03:54,  1.67it/s] 72%|███████▏  | 987/1377 [10:21<03:50,  1.69it/s] 72%|███████▏  | 988/1377 [10:21<03:41,  1.75it/s] 72%|███████▏  | 989/1377 [10:22<03:54,  1.66it/s] 72%|███████▏  | 990/1377 [10:23<03:59,  1.62it/s] 72%|███████▏  | 991/1377 [10:23<04:01,  1.60it/s] 72%|███████▏  | 992/1377 [10:24<04:06,  1.56it/s] 72%|███████▏  | 993/1377 [10:24<04:01,  1.59it/s] 72%|███████▏  | 994/1377 [10:25<03:57,  1.61it/s] 72%|███████▏  | 995/1377 [10:26<04:03,  1.57it/s] 72%|███████▏  | 996/1377 [10:26<03:58,  1.60it/s] 72%|███████▏  | 997/1377 [10:27<04:00,  1.58it/s] 72%|███████▏  | 998/1377 [10:28<03:56,  1.60it/s] 73%|███████▎  | 999/1377 [10:28<03:47,  1.66it/s] 73%|███████▎  | 1000/1377 [10:29<03:51,  1.63it/s] 73%|███████▎  | 1001/1377 [10:29<03:42,  1.69it/s] 73%|███████▎  | 1002/1377 [10:30<03:42,  1.68it/s] 73%|███████▎  | 1003/1377 [10:30<03:37,  1.72it/s] 73%|███████▎  | 1004/1377 [10:31<03:39,  1.70it/s] 73%|███████▎  | 1005/1377 [10:32<03:32,  1.75it/s] 73%|███████▎  | 1006/1377 [10:32<03:43,  1.66it/s] 73%|███████▎  | 1007/1377 [10:33<03:39,  1.69it/s] 73%|███████▎  | 1008/1377 [10:33<03:33,  1.73it/s] 73%|███████▎  | 1009/1377 [10:34<03:35,  1.71it/s] 73%|███████▎  | 1010/1377 [10:35<03:31,  1.74it/s] 73%|███████▎  | 1011/1377 [10:35<03:28,  1.76it/s] 73%|███████▎  | 1012/1377 [10:36<03:32,  1.72it/s] 74%|███████▎  | 1013/1377 [10:36<03:49,  1.58it/s] 74%|███████▎  | 1014/1377 [10:37<03:53,  1.55it/s] 74%|███████▎  | 1015/1377 [10:38<03:42,  1.63it/s] 74%|███████▍  | 1016/1377 [10:38<03:41,  1.63it/s] 74%|███████▍  | 1017/1377 [10:39<03:44,  1.60it/s] 74%|███████▍  | 1018/1377 [10:40<03:46,  1.59it/s] 74%|███████▍  | 1019/1377 [10:40<03:44,  1.60it/s] 74%|███████▍  | 1020/1377 [10:41<03:45,  1.58it/s] 74%|███████▍  | 1021/1377 [10:41<03:46,  1.57it/s] 74%|███████▍  | 1022/1377 [10:42<03:47,  1.56it/s] 74%|███████▍  | 1023/1377 [10:43<03:44,  1.58it/s] 74%|███████▍  | 1024/1377 [10:43<03:45,  1.56it/s] 74%|███████▍  | 1025/1377 [10:44<03:37,  1.62it/s] 75%|███████▍  | 1026/1377 [10:45<03:36,  1.62it/s] 75%|███████▍  | 1027/1377 [10:45<03:29,  1.67it/s] 75%|███████▍  | 1028/1377 [10:46<03:30,  1.65it/s] 75%|███████▍  | 1029/1377 [10:46<03:24,  1.70it/s] 75%|███████▍  | 1030/1377 [10:47<03:18,  1.75it/s] 75%|███████▍  | 1031/1377 [10:47<03:22,  1.71it/s] 75%|███████▍  | 1032/1377 [10:48<03:31,  1.63it/s] 75%|███████▌  | 1033/1377 [10:49<03:30,  1.64it/s] 75%|███████▌  | 1034/1377 [10:49<03:29,  1.63it/s] 75%|███████▌  | 1035/1377 [10:50<03:25,  1.67it/s] 75%|███████▌  | 1036/1377 [10:51<03:35,  1.59it/s] 75%|███████▌  | 1037/1377 [10:51<03:39,  1.55it/s] 75%|███████▌  | 1038/1377 [10:52<03:39,  1.55it/s] 75%|███████▌  | 1039/1377 [10:53<03:35,  1.57it/s] 76%|███████▌  | 1040/1377 [10:53<03:31,  1.59it/s] 76%|███████▌  | 1041/1377 [10:54<03:33,  1.58it/s] 76%|███████▌  | 1042/1377 [10:54<03:25,  1.63it/s] 76%|███████▌  | 1043/1377 [10:55<03:25,  1.63it/s] 76%|███████▌  | 1044/1377 [10:56<03:27,  1.60it/s] 76%|███████▌  | 1045/1377 [10:56<03:26,  1.61it/s] 76%|███████▌  | 1046/1377 [10:57<03:24,  1.62it/s] 76%|███████▌  | 1047/1377 [10:57<03:18,  1.66it/s] 76%|███████▌  | 1048/1377 [10:58<03:32,  1.55it/s] 76%|███████▌  | 1049/1377 [10:59<03:27,  1.58it/s] 76%|███████▋  | 1050/1377 [10:59<03:24,  1.60it/s] 76%|███████▋  | 1051/1377 [11:00<03:15,  1.67it/s] 76%|███████▋  | 1052/1377 [11:01<03:16,  1.65it/s] 76%|███████▋  | 1053/1377 [11:01<03:29,  1.54it/s] 77%|███████▋  | 1054/1377 [11:02<03:18,  1.63it/s] 77%|███████▋  | 1055/1377 [11:02<03:10,  1.69it/s] 77%|███████▋  | 1056/1377 [11:03<03:04,  1.74it/s] 77%|███████▋  | 1057/1377 [11:04<03:05,  1.72it/s] 77%|███████▋  | 1058/1377 [11:04<03:01,  1.76it/s] 77%|███████▋  | 1059/1377 [11:05<03:07,  1.69it/s] 77%|███████▋  | 1060/1377 [11:05<03:04,  1.72it/s] 77%|███████▋  | 1061/1377 [11:06<03:02,  1.73it/s] 77%|███████▋  | 1062/1377 [11:06<03:04,  1.70it/s] 77%|███████▋  | 1063/1377 [11:07<03:07,  1.68it/s] 77%|███████▋  | 1064/1377 [11:08<03:10,  1.64it/s] 77%|███████▋  | 1065/1377 [11:08<03:13,  1.61it/s] 77%|███████▋  | 1066/1377 [11:09<03:15,  1.59it/s] 77%|███████▋  | 1067/1377 [11:10<03:13,  1.60it/s] 78%|███████▊  | 1068/1377 [11:10<03:12,  1.61it/s] 78%|███████▊  | 1069/1377 [11:11<02:58,  1.73it/s] 78%|███████▊  | 1070/1377 [11:11<02:56,  1.74it/s] 78%|███████▊  | 1071/1377 [11:12<02:53,  1.76it/s] 78%|███████▊  | 1072/1377 [11:12<03:00,  1.69it/s] 78%|███████▊  | 1073/1377 [11:13<02:56,  1.73it/s] 78%|███████▊  | 1074/1377 [11:14<03:07,  1.62it/s] 78%|███████▊  | 1075/1377 [11:14<03:01,  1.66it/s] 78%|███████▊  | 1076/1377 [11:15<03:01,  1.66it/s] 78%|███████▊  | 1077/1377 [11:16<03:04,  1.62it/s] 78%|███████▊  | 1078/1377 [11:16<02:59,  1.66it/s] 78%|███████▊  | 1079/1377 [11:17<03:03,  1.63it/s] 78%|███████▊  | 1080/1377 [11:17<03:05,  1.60it/s] 79%|███████▊  | 1081/1377 [11:18<03:06,  1.58it/s] 79%|███████▊  | 1082/1377 [11:19<02:58,  1.66it/s] 79%|███████▊  | 1083/1377 [11:19<03:01,  1.62it/s] 79%|███████▊  | 1084/1377 [11:20<02:53,  1.69it/s] 79%|███████▉  | 1085/1377 [11:20<02:57,  1.65it/s] 79%|███████▉  | 1086/1377 [11:21<03:00,  1.61it/s] 79%|███████▉  | 1087/1377 [11:22<02:59,  1.62it/s] 79%|███████▉  | 1088/1377 [11:22<03:00,  1.60it/s] 79%|███████▉  | 1089/1377 [11:23<03:02,  1.58it/s] 79%|███████▉  | 1090/1377 [11:24<03:00,  1.59it/s] 79%|███████▉  | 1091/1377 [11:24<03:04,  1.55it/s] 79%|███████▉  | 1092/1377 [11:25<03:03,  1.55it/s] 79%|███████▉  | 1093/1377 [11:25<02:54,  1.63it/s] 79%|███████▉  | 1094/1377 [11:26<02:55,  1.61it/s] 80%|███████▉  | 1095/1377 [11:27<02:53,  1.62it/s] 80%|███████▉  | 1096/1377 [11:27<03:01,  1.55it/s] 80%|███████▉  | 1097/1377 [11:28<02:58,  1.57it/s] 80%|███████▉  | 1098/1377 [11:29<02:55,  1.59it/s] 80%|███████▉  | 1099/1377 [11:29<02:53,  1.60it/s] 80%|███████▉  | 1100/1377 [11:30<02:55,  1.58it/s] 80%|███████▉  | 1101/1377 [11:30<02:48,  1.64it/s] 80%|████████  | 1102/1377 [11:31<02:53,  1.59it/s] 80%|████████  | 1103/1377 [11:32<02:46,  1.64it/s] 80%|████████  | 1104/1377 [11:32<02:42,  1.68it/s] 80%|████████  | 1105/1377 [11:33<02:43,  1.67it/s] 80%|████████  | 1106/1377 [11:34<02:43,  1.65it/s] 80%|████████  | 1107/1377 [11:34<02:46,  1.62it/s] 80%|████████  | 1108/1377 [11:35<02:45,  1.62it/s] 81%|████████  | 1109/1377 [11:35<02:41,  1.66it/s] 81%|████████  | 1110/1377 [11:36<02:44,  1.62it/s] 81%|████████  | 1111/1377 [11:37<02:42,  1.64it/s] 81%|████████  | 1112/1377 [11:37<02:44,  1.61it/s] 81%|████████  | 1113/1377 [11:38<02:43,  1.61it/s] 81%|████████  | 1114/1377 [11:38<02:38,  1.66it/s] 81%|████████  | 1115/1377 [11:39<02:33,  1.71it/s] 81%|████████  | 1116/1377 [11:40<02:40,  1.63it/s] 81%|████████  | 1117/1377 [11:40<02:42,  1.60it/s] 81%|████████  | 1118/1377 [11:41<02:39,  1.62it/s] 81%|████████▏ | 1119/1377 [11:42<02:46,  1.55it/s] 81%|████████▏ | 1120/1377 [11:42<02:43,  1.57it/s] 81%|████████▏ | 1121/1377 [11:43<02:43,  1.56it/s] 81%|████████▏ | 1122/1377 [11:43<02:43,  1.56it/s] 82%|████████▏ | 1123/1377 [11:44<02:43,  1.56it/s] 82%|████████▏ | 1124/1377 [11:45<02:34,  1.64it/s] 82%|████████▏ | 1125/1377 [11:45<02:40,  1.57it/s] 82%|████████▏ | 1126/1377 [11:46<02:29,  1.68it/s] 82%|████████▏ | 1127/1377 [11:46<02:30,  1.67it/s] 82%|████████▏ | 1128/1377 [11:47<02:29,  1.67it/s] 82%|████████▏ | 1129/1377 [11:48<02:23,  1.73it/s] 82%|████████▏ | 1130/1377 [11:48<02:31,  1.63it/s] 82%|████████▏ | 1131/1377 [11:49<02:27,  1.67it/s] 82%|████████▏ | 1132/1377 [11:49<02:26,  1.67it/s] 82%|████████▏ | 1133/1377 [11:50<02:21,  1.72it/s] 82%|████████▏ | 1134/1377 [11:51<02:25,  1.67it/s] 82%|████████▏ | 1135/1377 [11:51<02:28,  1.63it/s] 82%|████████▏ | 1136/1377 [11:52<02:23,  1.68it/s] 83%|████████▎ | 1137/1377 [11:53<02:26,  1.63it/s] 83%|████████▎ | 1138/1377 [11:53<02:23,  1.67it/s] 83%|████████▎ | 1139/1377 [11:54<02:32,  1.57it/s] 83%|████████▎ | 1140/1377 [11:54<02:32,  1.55it/s] 83%|████████▎ | 1141/1377 [11:55<02:32,  1.55it/s] 83%|████████▎ | 1142/1377 [11:56<02:29,  1.57it/s] 83%|████████▎ | 1143/1377 [11:56<02:24,  1.62it/s] 83%|████████▎ | 1144/1377 [11:57<02:27,  1.57it/s] 83%|████████▎ | 1145/1377 [11:58<02:32,  1.52it/s] 83%|████████▎ | 1146/1377 [11:58<02:25,  1.59it/s] 83%|████████▎ | 1147/1377 [11:59<02:28,  1.55it/s] 83%|████████▎ | 1148/1377 [11:59<02:20,  1.63it/s] 83%|████████▎ | 1149/1377 [12:00<02:18,  1.64it/s] 84%|████████▎ | 1150/1377 [12:01<02:15,  1.68it/s] 84%|████████▎ | 1151/1377 [12:01<02:09,  1.74it/s] 84%|████████▎ | 1152/1377 [12:02<02:10,  1.72it/s] 84%|████████▎ | 1153/1377 [12:02<02:08,  1.75it/s] 84%|████████▍ | 1154/1377 [12:03<02:09,  1.72it/s] 84%|████████▍ | 1155/1377 [12:04<02:13,  1.66it/s] 84%|████████▍ | 1156/1377 [12:04<02:10,  1.69it/s] 84%|████████▍ | 1157/1377 [12:05<02:11,  1.67it/s] 84%|████████▍ | 1158/1377 [12:05<02:11,  1.66it/s] 84%|████████▍ | 1159/1377 [12:06<02:11,  1.65it/s] 84%|████████▍ | 1160/1377 [12:07<02:08,  1.68it/s] 84%|████████▍ | 1161/1377 [12:07<02:09,  1.67it/s] 84%|████████▍ | 1162/1377 [12:08<02:08,  1.67it/s] 84%|████████▍ | 1163/1377 [12:08<02:04,  1.72it/s] 85%|████████▍ | 1164/1377 [12:09<02:11,  1.62it/s] 85%|████████▍ | 1165/1377 [12:10<02:08,  1.66it/s] 85%|████████▍ | 1166/1377 [12:10<02:09,  1.63it/s] 85%|████████▍ | 1167/1377 [12:11<02:08,  1.64it/s] 85%|████████▍ | 1168/1377 [12:11<02:09,  1.61it/s] 85%|████████▍ | 1169/1377 [12:12<02:11,  1.58it/s] 85%|████████▍ | 1170/1377 [12:13<02:08,  1.61it/s] 85%|████████▌ | 1171/1377 [12:13<02:02,  1.68it/s] 85%|████████▌ | 1172/1377 [12:14<02:02,  1.67it/s] 85%|████████▌ | 1173/1377 [12:14<02:05,  1.63it/s] 85%|████████▌ | 1174/1377 [12:15<02:08,  1.58it/s] 85%|████████▌ | 1175/1377 [12:16<02:08,  1.57it/s] 85%|████████▌ | 1176/1377 [12:16<02:08,  1.56it/s] 85%|████████▌ | 1177/1377 [12:17<02:10,  1.53it/s] 86%|████████▌ | 1178/1377 [12:18<02:13,  1.50it/s] 86%|████████▌ | 1179/1377 [12:18<02:08,  1.54it/s] 86%|████████▌ | 1180/1377 [12:19<02:02,  1.61it/s] 86%|████████▌ | 1181/1377 [12:20<02:00,  1.62it/s] 86%|████████▌ | 1182/1377 [12:20<01:59,  1.63it/s] 86%|████████▌ | 1183/1377 [12:21<02:03,  1.57it/s] 86%|████████▌ | 1184/1377 [12:22<02:07,  1.52it/s] 86%|████████▌ | 1185/1377 [12:22<02:03,  1.55it/s] 86%|████████▌ | 1186/1377 [12:23<02:03,  1.55it/s] 86%|████████▌ | 1187/1377 [12:24<02:02,  1.55it/s] 86%|████████▋ | 1188/1377 [12:24<01:57,  1.61it/s] 86%|████████▋ | 1189/1377 [12:25<01:52,  1.67it/s] 86%|████████▋ | 1190/1377 [12:25<01:51,  1.67it/s] 86%|████████▋ | 1191/1377 [12:26<01:51,  1.66it/s] 87%|████████▋ | 1192/1377 [12:26<01:51,  1.65it/s] 87%|████████▋ | 1193/1377 [12:27<01:55,  1.59it/s] 87%|████████▋ | 1194/1377 [12:28<01:54,  1.60it/s] 87%|████████▋ | 1195/1377 [12:28<01:55,  1.58it/s] 87%|████████▋ | 1196/1377 [12:29<01:50,  1.64it/s] 87%|████████▋ | 1197/1377 [12:29<01:45,  1.70it/s] 87%|████████▋ | 1198/1377 [12:30<01:45,  1.70it/s] 87%|████████▋ | 1199/1377 [12:31<01:49,  1.62it/s] 87%|████████▋ | 1200/1377 [12:31<01:48,  1.64it/s] 87%|████████▋ | 1201/1377 [12:32<01:51,  1.58it/s] 87%|████████▋ | 1202/1377 [12:33<01:47,  1.63it/s] 87%|████████▋ | 1203/1377 [12:33<01:48,  1.61it/s] 87%|████████▋ | 1204/1377 [12:34<01:46,  1.62it/s] 88%|████████▊ | 1205/1377 [12:34<01:45,  1.63it/s] 88%|████████▊ | 1206/1377 [12:35<01:48,  1.58it/s] 88%|████████▊ | 1207/1377 [12:36<01:46,  1.60it/s] 88%|████████▊ | 1208/1377 [12:36<01:48,  1.56it/s] 88%|████████▊ | 1209/1377 [12:37<01:49,  1.53it/s] 88%|████████▊ | 1210/1377 [12:38<01:44,  1.60it/s] 88%|████████▊ | 1211/1377 [12:38<01:40,  1.65it/s] 88%|████████▊ | 1212/1377 [12:39<01:43,  1.60it/s] 88%|████████▊ | 1213/1377 [12:40<01:42,  1.60it/s] 88%|████████▊ | 1214/1377 [12:40<01:38,  1.66it/s] 88%|████████▊ | 1215/1377 [12:41<01:37,  1.65it/s] 88%|████████▊ | 1216/1377 [12:41<01:39,  1.62it/s] 88%|████████▊ | 1217/1377 [12:42<01:35,  1.67it/s] 88%|████████▊ | 1218/1377 [12:42<01:33,  1.70it/s] 89%|████████▊ | 1219/1377 [12:43<01:30,  1.74it/s] 89%|████████▊ | 1220/1377 [12:44<01:33,  1.68it/s] 89%|████████▊ | 1221/1377 [12:44<01:33,  1.67it/s] 89%|████████▊ | 1222/1377 [12:45<01:31,  1.70it/s] 89%|████████▉ | 1223/1377 [12:45<01:29,  1.72it/s] 89%|████████▉ | 1224/1377 [12:46<01:31,  1.67it/s] 89%|████████▉ | 1225/1377 [12:47<01:33,  1.63it/s] 89%|████████▉ | 1226/1377 [12:47<01:29,  1.70it/s] 89%|████████▉ | 1227/1377 [12:48<01:30,  1.66it/s] 89%|████████▉ | 1228/1377 [12:48<01:29,  1.66it/s] 89%|████████▉ | 1229/1377 [12:49<01:32,  1.60it/s] 89%|████████▉ | 1230/1377 [12:50<01:28,  1.66it/s] 89%|████████▉ | 1231/1377 [12:50<01:26,  1.69it/s] 89%|████████▉ | 1232/1377 [12:51<01:26,  1.69it/s] 90%|████████▉ | 1233/1377 [12:51<01:26,  1.67it/s] 90%|████████▉ | 1234/1377 [12:52<01:23,  1.71it/s] 90%|████████▉ | 1235/1377 [12:53<01:27,  1.63it/s] 90%|████████▉ | 1236/1377 [12:53<01:28,  1.60it/s] 90%|████████▉ | 1237/1377 [12:54<01:24,  1.67it/s] 90%|████████▉ | 1238/1377 [12:54<01:21,  1.71it/s] 90%|████████▉ | 1239/1377 [12:55<01:26,  1.60it/s] 90%|█████████ | 1240/1377 [12:56<01:28,  1.55it/s] 90%|█████████ | 1241/1377 [12:56<01:24,  1.61it/s] 90%|█████████ | 1242/1377 [12:57<01:23,  1.62it/s] 90%|█████████ | 1243/1377 [12:58<01:22,  1.62it/s] 90%|█████████ | 1244/1377 [12:58<01:18,  1.69it/s] 90%|█████████ | 1245/1377 [12:59<01:18,  1.68it/s] 90%|█████████ | 1246/1377 [12:59<01:15,  1.73it/s] 91%|█████████ | 1247/1377 [13:00<01:17,  1.67it/s] 91%|█████████ | 1248/1377 [13:01<01:21,  1.58it/s] 91%|█████████ | 1249/1377 [13:01<01:18,  1.63it/s] 91%|█████████ | 1250/1377 [13:02<01:19,  1.61it/s] 91%|█████████ | 1251/1377 [13:03<01:20,  1.56it/s] 91%|█████████ | 1252/1377 [13:03<01:17,  1.61it/s] 91%|█████████ | 1253/1377 [13:04<01:17,  1.59it/s] 91%|█████████ | 1254/1377 [13:04<01:18,  1.57it/s] 91%|█████████ | 1255/1377 [13:05<01:18,  1.56it/s] 91%|█████████ | 1256/1377 [13:06<01:13,  1.64it/s] 91%|█████████▏| 1257/1377 [13:06<01:12,  1.65it/s] 91%|█████████▏| 1258/1377 [13:07<01:11,  1.66it/s] 91%|█████████▏| 1259/1377 [13:08<01:14,  1.57it/s] 92%|█████████▏| 1260/1377 [13:08<01:13,  1.59it/s] 92%|█████████▏| 1261/1377 [13:09<01:13,  1.58it/s] 92%|█████████▏| 1262/1377 [13:09<01:12,  1.59it/s] 92%|█████████▏| 1263/1377 [13:10<01:09,  1.63it/s] 92%|█████████▏| 1264/1377 [13:11<01:07,  1.67it/s] 92%|█████████▏| 1265/1377 [13:11<01:09,  1.60it/s] 92%|█████████▏| 1266/1377 [13:12<01:07,  1.65it/s] 92%|█████████▏| 1267/1377 [13:12<01:07,  1.62it/s] 92%|█████████▏| 1268/1377 [13:13<01:06,  1.63it/s] 92%|█████████▏| 1269/1377 [13:14<01:04,  1.67it/s] 92%|█████████▏| 1270/1377 [13:14<01:04,  1.67it/s] 92%|█████████▏| 1271/1377 [13:15<01:02,  1.70it/s] 92%|█████████▏| 1272/1377 [13:15<00:59,  1.75it/s] 92%|█████████▏| 1273/1377 [13:16<01:01,  1.69it/s] 93%|█████████▎| 1274/1377 [13:17<01:02,  1.64it/s] 93%|█████████▎| 1275/1377 [13:17<01:04,  1.58it/s] 93%|█████████▎| 1276/1377 [13:18<01:05,  1.54it/s] 93%|█████████▎| 1277/1377 [13:19<01:04,  1.54it/s] 93%|█████████▎| 1278/1377 [13:19<01:02,  1.57it/s] 93%|█████████▎| 1279/1377 [13:20<01:02,  1.56it/s] 93%|█████████▎| 1280/1377 [13:20<01:02,  1.56it/s] 93%|█████████▎| 1281/1377 [13:21<00:59,  1.61it/s] 93%|█████████▎| 1282/1377 [13:22<00:57,  1.66it/s] 93%|█████████▎| 1283/1377 [13:22<00:57,  1.62it/s] 93%|█████████▎| 1284/1377 [13:23<00:57,  1.62it/s] 93%|█████████▎| 1285/1377 [13:24<00:58,  1.57it/s] 93%|█████████▎| 1286/1377 [13:24<00:58,  1.56it/s] 93%|█████████▎| 1287/1377 [13:25<00:53,  1.69it/s] 94%|█████████▎| 1288/1377 [13:25<00:50,  1.75it/s] 94%|█████████▎| 1289/1377 [13:26<00:52,  1.68it/s] 94%|█████████▎| 1290/1377 [13:26<00:51,  1.68it/s] 94%|█████████▍| 1291/1377 [13:27<00:52,  1.64it/s] 94%|█████████▍| 1292/1377 [13:28<00:52,  1.63it/s] 94%|█████████▍| 1293/1377 [13:28<00:50,  1.67it/s] 94%|█████████▍| 1294/1377 [13:29<00:50,  1.66it/s] 94%|█████████▍| 1295/1377 [13:30<00:49,  1.65it/s] 94%|█████████▍| 1296/1377 [13:30<00:49,  1.64it/s] 94%|█████████▍| 1297/1377 [13:31<00:49,  1.61it/s] 94%|█████████▍| 1298/1377 [13:31<00:49,  1.59it/s] 94%|█████████▍| 1299/1377 [13:32<00:49,  1.57it/s] 94%|█████████▍| 1300/1377 [13:33<00:49,  1.56it/s] 94%|█████████▍| 1301/1377 [13:33<00:47,  1.61it/s] 95%|█████████▍| 1302/1377 [13:34<00:47,  1.57it/s] 95%|█████████▍| 1303/1377 [13:35<00:45,  1.62it/s] 95%|█████████▍| 1304/1377 [13:35<00:43,  1.66it/s] 95%|█████████▍| 1305/1377 [13:36<00:44,  1.62it/s] 95%|█████████▍| 1306/1377 [13:36<00:42,  1.67it/s] 95%|█████████▍| 1307/1377 [13:37<00:40,  1.73it/s] 95%|█████████▍| 1308/1377 [13:38<00:41,  1.64it/s] 95%|█████████▌| 1309/1377 [13:38<00:39,  1.70it/s] 95%|█████████▌| 1310/1377 [13:39<00:40,  1.65it/s] 95%|█████████▌| 1311/1377 [13:39<00:40,  1.62it/s] 95%|█████████▌| 1312/1377 [13:40<00:40,  1.62it/s] 95%|█████████▌| 1313/1377 [13:41<00:40,  1.59it/s] 95%|█████████▌| 1314/1377 [13:41<00:38,  1.64it/s] 95%|█████████▌| 1315/1377 [13:42<00:36,  1.69it/s] 96%|█████████▌| 1316/1377 [13:42<00:37,  1.65it/s] 96%|█████████▌| 1317/1377 [13:43<00:36,  1.64it/s] 96%|█████████▌| 1318/1377 [13:44<00:34,  1.70it/s] 96%|█████████▌| 1319/1377 [13:44<00:33,  1.74it/s] 96%|█████████▌| 1320/1377 [13:45<00:31,  1.78it/s] 96%|█████████▌| 1321/1377 [13:45<00:34,  1.64it/s] 96%|█████████▌| 1322/1377 [13:46<00:34,  1.59it/s] 96%|█████████▌| 1323/1377 [13:47<00:34,  1.57it/s] 96%|█████████▌| 1324/1377 [13:47<00:32,  1.64it/s] 96%|█████████▌| 1325/1377 [13:48<00:30,  1.71it/s] 96%|█████████▋| 1326/1377 [13:48<00:29,  1.72it/s] 96%|█████████▋| 1327/1377 [13:49<00:30,  1.61it/s] 96%|█████████▋| 1328/1377 [13:50<00:29,  1.66it/s] 97%|█████████▋| 1329/1377 [13:50<00:29,  1.63it/s] 97%|█████████▋| 1330/1377 [13:51<00:29,  1.58it/s] 97%|█████████▋| 1331/1377 [13:52<00:28,  1.59it/s] 97%|█████████▋| 1332/1377 [13:52<00:28,  1.55it/s] 97%|█████████▋| 1333/1377 [13:53<00:28,  1.55it/s]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 557, in main
    optimizer.step()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 187, in step
    adamw(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 339, in adamw
    func(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/optim/adamw.py", line 608, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 31.31 MiB is free. Including non-PyTorch memory, this process has 31.71 GiB memory in use. Of the allocated memory 29.33 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 97%|█████████▋| 1333/1377 [13:54<00:27,  1.60it/s]
[2024-03-13 21:06:01,773] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 77888) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:06:01
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 77888)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:08:30 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6790.64 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 6858.32 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 6867.54 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 6763.33 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6282.38 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7117.05 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7136.42 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7014.59 examples/s]
03/13/2024 21:08:37 - INFO - __main__ - Sample 2110 of the training set: {'input_ids': [1, 376, 8512, 278, 758, 1116, 2187, 363, 24205, 3933, 297, 2058, 1919, 278, 27289, 363, 7029, 9667, 363, 10261, 1962, 338, 591, 5790, 1135, 9251, 3806, 869, 376, 1, 940, 7572, 1919, 376, 5998, 278, 758, 1116, 2187, 363, 24205, 3933, 297, 2058, 1919, 376, 27289, 29879, 363, 4908, 29586, 892, 376, 591, 5790, 1135, 9251, 3806, 869, 376], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:08:37 - INFO - __main__ - Sample 3392 of the training set: {'input_ids': [1, 376, 306, 525, 29885, 766, 29887, 16656, 1919, 4824, 1711, 17319, 297, 278, 380, 290, 496, 1919, 376, 1497, 1771, 296, 1570, 11059, 1919, 6673, 310, 278, 8027, 1049, 5127, 4121, 1467, 1527, 525, 29879, 350, 1600, 1555, 296, 7993, 869, 1, 376, 450, 3353, 2655, 338, 263, 6427, 14596, 1919, 376, 285, 21571, 1771, 296, 1570, 11059, 1919, 6673, 310, 278, 8027, 1049, 5127, 4121, 1467, 1527, 525, 29879, 350, 1600, 1555, 296, 7993, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:08:37 - INFO - __main__ - Sample 59 of the training set: {'input_ids': [1, 1551, 5468, 29871, 29941, 1919, 323, 4727, 338, 3806, 304, 367, 2665, 9223, 304, 2834, 297, 8475, 1728, 610, 1772, 869, 1, 323, 4727, 17240, 2834, 297, 8475, 1728, 610, 1772, 472, 670, 5468, 29871, 29941, 29900, 2665, 16750, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:08:39 - INFO - __main__ - ***** Running training *****
03/13/2024 21:08:39 - INFO - __main__ -   Num examples = 3668
03/13/2024 21:08:39 - INFO - __main__ -   Num Epochs = 3
03/13/2024 21:08:39 - INFO - __main__ -   Instantaneous batch size per device = 7
03/13/2024 21:08:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 7
03/13/2024 21:08:39 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 21:08:39 - INFO - __main__ -   Total optimization steps = 1572
  0%|          | 0/1572 [00:00<?, ?it/s]  0%|          | 1/1572 [00:01<35:06,  1.34s/it]  0%|          | 2/1572 [00:01<20:54,  1.25it/s]  0%|          | 3/1572 [00:02<17:54,  1.46it/s]  0%|          | 4/1572 [00:02<16:29,  1.58it/s]  0%|          | 5/1572 [00:03<15:31,  1.68it/s]  0%|          | 6/1572 [00:03<14:36,  1.79it/s]  0%|          | 7/1572 [00:04<14:28,  1.80it/s]  1%|          | 8/1572 [00:04<14:16,  1.83it/s]  1%|          | 9/1572 [00:05<13:50,  1.88it/s]  1%|          | 10/1572 [00:05<13:55,  1.87it/s]  1%|          | 11/1572 [00:06<13:28,  1.93it/s]  1%|          | 12/1572 [00:07<13:30,  1.92it/s]  1%|          | 13/1572 [00:07<13:48,  1.88it/s]  1%|          | 14/1572 [00:08<13:54,  1.87it/s]  1%|          | 15/1572 [00:08<14:30,  1.79it/s]  1%|          | 16/1572 [00:09<14:27,  1.79it/s]  1%|          | 17/1572 [00:09<14:47,  1.75it/s]  1%|          | 18/1572 [00:10<14:38,  1.77it/s]  1%|          | 19/1572 [00:10<14:22,  1.80it/s]  1%|▏         | 20/1572 [00:11<14:09,  1.83it/s]  1%|▏         | 21/1572 [00:12<14:00,  1.84it/s]  1%|▏         | 22/1572 [00:12<14:02,  1.84it/s]  1%|▏         | 23/1572 [00:13<13:41,  1.88it/s]  2%|▏         | 24/1572 [00:13<13:53,  1.86it/s]  2%|▏         | 25/1572 [00:14<14:38,  1.76it/s]  2%|▏         | 26/1572 [00:14<14:18,  1.80it/s]  2%|▏         | 27/1572 [00:15<14:02,  1.83it/s]  2%|▏         | 28/1572 [00:15<14:06,  1.82it/s]  2%|▏         | 29/1572 [00:16<14:25,  1.78it/s]  2%|▏         | 30/1572 [00:16<13:56,  1.84it/s]  2%|▏         | 31/1572 [00:17<14:19,  1.79it/s]  2%|▏         | 32/1572 [00:18<14:57,  1.72it/s]  2%|▏         | 33/1572 [00:18<14:34,  1.76it/s]  2%|▏         | 34/1572 [00:19<14:01,  1.83it/s]  2%|▏         | 35/1572 [00:19<14:05,  1.82it/s]  2%|▏         | 36/1572 [00:20<14:03,  1.82it/s]  2%|▏         | 37/1572 [00:20<13:38,  1.88it/s]  2%|▏         | 38/1572 [00:21<13:49,  1.85it/s]  2%|▏         | 39/1572 [00:21<13:57,  1.83it/s]  3%|▎         | 40/1572 [00:22<13:49,  1.85it/s]  3%|▎         | 41/1572 [00:22<13:45,  1.85it/s]  3%|▎         | 42/1572 [00:23<13:42,  1.86it/s]  3%|▎         | 43/1572 [00:24<13:36,  1.87it/s]  3%|▎         | 44/1572 [00:24<13:41,  1.86it/s]  3%|▎         | 45/1572 [00:25<14:25,  1.76it/s]  3%|▎         | 46/1572 [00:25<14:09,  1.80it/s]  3%|▎         | 47/1572 [00:26<13:55,  1.83it/s]  3%|▎         | 48/1572 [00:26<14:37,  1.74it/s]  3%|▎         | 49/1572 [00:27<14:32,  1.75it/s]  3%|▎         | 50/1572 [00:28<14:24,  1.76it/s]  3%|▎         | 51/1572 [00:28<14:19,  1.77it/s]  3%|▎         | 52/1572 [00:29<14:30,  1.75it/s]  3%|▎         | 53/1572 [00:29<14:08,  1.79it/s]  3%|▎         | 54/1572 [00:30<14:21,  1.76it/s]  3%|▎         | 55/1572 [00:30<14:32,  1.74it/s]  4%|▎         | 56/1572 [00:31<13:56,  1.81it/s]  4%|▎         | 57/1572 [00:32<14:22,  1.76it/s]  4%|▎         | 58/1572 [00:32<14:07,  1.79it/s]  4%|▍         | 59/1572 [00:33<14:22,  1.75it/s]  4%|▍         | 60/1572 [00:33<14:17,  1.76it/s]  4%|▍         | 61/1572 [00:34<14:36,  1.72it/s]  4%|▍         | 62/1572 [00:34<13:49,  1.82it/s]  4%|▍         | 63/1572 [00:35<13:48,  1.82it/s]  4%|▍         | 64/1572 [00:35<13:36,  1.85it/s]  4%|▍         | 65/1572 [00:36<13:16,  1.89it/s]  4%|▍         | 66/1572 [00:36<13:31,  1.86it/s]  4%|▍         | 67/1572 [00:37<13:56,  1.80it/s]  4%|▍         | 68/1572 [00:38<13:43,  1.83it/s]  4%|▍         | 69/1572 [00:38<13:32,  1.85it/s]  4%|▍         | 70/1572 [00:39<14:57,  1.67it/s]  5%|▍         | 71/1572 [00:39<15:18,  1.63it/s]  5%|▍         | 72/1572 [00:40<14:50,  1.69it/s]  5%|▍         | 73/1572 [00:41<14:23,  1.74it/s]  5%|▍         | 74/1572 [00:41<15:05,  1.65it/s]  5%|▍         | 75/1572 [00:42<14:42,  1.70it/s]  5%|▍         | 76/1572 [00:42<14:44,  1.69it/s]  5%|▍         | 77/1572 [00:43<15:07,  1.65it/s]  5%|▍         | 78/1572 [00:44<14:34,  1.71it/s]  5%|▌         | 79/1572 [00:44<13:53,  1.79it/s]  5%|▌         | 80/1572 [00:44<13:02,  1.91it/s]  5%|▌         | 81/1572 [00:45<13:04,  1.90it/s]  5%|▌         | 82/1572 [00:46<13:33,  1.83it/s]  5%|▌         | 83/1572 [00:46<13:37,  1.82it/s]  5%|▌         | 84/1572 [00:47<13:27,  1.84it/s]  5%|▌         | 85/1572 [00:47<13:23,  1.85it/s]  5%|▌         | 86/1572 [00:48<13:28,  1.84it/s]  6%|▌         | 87/1572 [00:48<13:52,  1.78it/s]  6%|▌         | 88/1572 [00:49<14:08,  1.75it/s]  6%|▌         | 89/1572 [00:49<13:36,  1.82it/s]  6%|▌         | 90/1572 [00:50<13:39,  1.81it/s]  6%|▌         | 91/1572 [00:51<13:32,  1.82it/s]  6%|▌         | 92/1572 [00:51<13:35,  1.81it/s]  6%|▌         | 93/1572 [00:52<13:23,  1.84it/s]  6%|▌         | 94/1572 [00:52<14:05,  1.75it/s]  6%|▌         | 95/1572 [00:53<13:33,  1.81it/s]  6%|▌         | 96/1572 [00:53<13:22,  1.84it/s]  6%|▌         | 97/1572 [00:54<13:45,  1.79it/s]  6%|▌         | 98/1572 [00:54<13:08,  1.87it/s]  6%|▋         | 99/1572 [00:55<13:03,  1.88it/s]  6%|▋         | 100/1572 [00:55<12:50,  1.91it/s]  6%|▋         | 101/1572 [00:56<13:04,  1.87it/s]  6%|▋         | 102/1572 [00:57<13:37,  1.80it/s]  7%|▋         | 103/1572 [00:57<13:28,  1.82it/s]  7%|▋         | 104/1572 [00:58<13:48,  1.77it/s]  7%|▋         | 105/1572 [00:58<14:03,  1.74it/s]  7%|▋         | 106/1572 [00:59<13:52,  1.76it/s]  7%|▋         | 107/1572 [00:59<13:32,  1.80it/s]  7%|▋         | 108/1572 [01:00<14:11,  1.72it/s]  7%|▋         | 109/1572 [01:01<13:52,  1.76it/s]  7%|▋         | 110/1572 [01:01<14:25,  1.69it/s]  7%|▋         | 111/1572 [01:02<14:26,  1.69it/s]  7%|▋         | 112/1572 [01:02<14:27,  1.68it/s]  7%|▋         | 113/1572 [01:03<14:08,  1.72it/s]  7%|▋         | 114/1572 [01:04<13:46,  1.76it/s]  7%|▋         | 115/1572 [01:04<13:45,  1.76it/s]  7%|▋         | 116/1572 [01:05<14:04,  1.72it/s]  7%|▋         | 117/1572 [01:05<14:18,  1.70it/s]  8%|▊         | 118/1572 [01:06<14:07,  1.72it/s]  8%|▊         | 119/1572 [01:06<14:10,  1.71it/s]  8%|▊         | 120/1572 [01:07<14:22,  1.68it/s]  8%|▊         | 121/1572 [01:08<14:42,  1.64it/s]  8%|▊         | 122/1572 [01:08<14:35,  1.66it/s]  8%|▊         | 123/1572 [01:09<14:10,  1.70it/s]  8%|▊         | 124/1572 [01:09<13:45,  1.75it/s]  8%|▊         | 125/1572 [01:10<13:37,  1.77it/s]  8%|▊         | 126/1572 [01:11<13:36,  1.77it/s]  8%|▊         | 127/1572 [01:11<13:28,  1.79it/s]  8%|▊         | 128/1572 [01:12<13:01,  1.85it/s]  8%|▊         | 129/1572 [01:12<13:22,  1.80it/s]  8%|▊         | 130/1572 [01:13<13:26,  1.79it/s]  8%|▊         | 131/1572 [01:13<13:26,  1.79it/s]  8%|▊         | 132/1572 [01:14<14:26,  1.66it/s]  8%|▊         | 133/1572 [01:15<14:24,  1.66it/s]  9%|▊         | 134/1572 [01:15<14:02,  1.71it/s]  9%|▊         | 135/1572 [01:16<13:38,  1.76it/s]  9%|▊         | 136/1572 [01:16<12:58,  1.84it/s]  9%|▊         | 137/1572 [01:17<12:54,  1.85it/s]  9%|▉         | 138/1572 [01:17<12:37,  1.89it/s]  9%|▉         | 139/1572 [01:18<12:37,  1.89it/s]  9%|▉         | 140/1572 [01:18<12:37,  1.89it/s]  9%|▉         | 141/1572 [01:19<12:35,  1.89it/s]  9%|▉         | 142/1572 [01:19<12:00,  1.99it/s]  9%|▉         | 143/1572 [01:20<12:12,  1.95it/s]  9%|▉         | 144/1572 [01:20<12:31,  1.90it/s]  9%|▉         | 145/1572 [01:21<12:45,  1.86it/s]  9%|▉         | 146/1572 [01:21<12:30,  1.90it/s]  9%|▉         | 147/1572 [01:22<12:30,  1.90it/s]  9%|▉         | 148/1572 [01:22<12:30,  1.90it/s]  9%|▉         | 149/1572 [01:23<12:33,  1.89it/s] 10%|▉         | 150/1572 [01:23<12:09,  1.95it/s] 10%|▉         | 151/1572 [01:24<11:52,  2.00it/s] 10%|▉         | 152/1572 [01:24<11:59,  1.97it/s] 10%|▉         | 153/1572 [01:25<12:08,  1.95it/s] 10%|▉         | 154/1572 [01:25<12:23,  1.91it/s] 10%|▉         | 155/1572 [01:26<13:13,  1.79it/s] 10%|▉         | 156/1572 [01:27<13:34,  1.74it/s] 10%|▉         | 157/1572 [01:27<13:27,  1.75it/s] 10%|█         | 158/1572 [01:28<13:23,  1.76it/s] 10%|█         | 159/1572 [01:28<13:15,  1.78it/s] 10%|█         | 160/1572 [01:29<13:15,  1.77it/s] 10%|█         | 161/1572 [01:30<13:02,  1.80it/s] 10%|█         | 162/1572 [01:30<12:50,  1.83it/s] 10%|█         | 163/1572 [01:31<12:38,  1.86it/s] 10%|█         | 164/1572 [01:31<12:34,  1.87it/s] 10%|█         | 165/1572 [01:32<12:26,  1.89it/s] 11%|█         | 166/1572 [01:32<12:38,  1.85it/s] 11%|█         | 167/1572 [01:33<12:20,  1.90it/s] 11%|█         | 168/1572 [01:33<12:23,  1.89it/s] 11%|█         | 169/1572 [01:34<12:23,  1.89it/s] 11%|█         | 170/1572 [01:34<12:02,  1.94it/s] 11%|█         | 171/1572 [01:35<12:05,  1.93it/s] 11%|█         | 172/1572 [01:35<12:16,  1.90it/s] 11%|█         | 173/1572 [01:36<12:16,  1.90it/s] 11%|█         | 174/1572 [01:36<13:04,  1.78it/s] 11%|█         | 175/1572 [01:37<12:54,  1.80it/s] 11%|█         | 176/1572 [01:38<12:55,  1.80it/s] 11%|█▏        | 177/1572 [01:38<12:46,  1.82it/s] 11%|█▏        | 178/1572 [01:39<12:49,  1.81it/s] 11%|█▏        | 179/1572 [01:39<12:53,  1.80it/s] 11%|█▏        | 180/1572 [01:40<12:50,  1.81it/s] 12%|█▏        | 181/1572 [01:40<13:08,  1.76it/s] 12%|█▏        | 182/1572 [01:41<13:02,  1.78it/s] 12%|█▏        | 183/1572 [01:42<13:33,  1.71it/s] 12%|█▏        | 184/1572 [01:42<13:12,  1.75it/s] 12%|█▏        | 185/1572 [01:43<12:56,  1.79it/s] 12%|█▏        | 186/1572 [01:43<13:15,  1.74it/s] 12%|█▏        | 187/1572 [01:44<12:45,  1.81it/s] 12%|█▏        | 188/1572 [01:44<12:46,  1.80it/s] 12%|█▏        | 189/1572 [01:45<12:35,  1.83it/s] 12%|█▏        | 190/1572 [01:45<12:56,  1.78it/s] 12%|█▏        | 191/1572 [01:46<12:58,  1.77it/s] 12%|█▏        | 192/1572 [01:47<12:47,  1.80it/s] 12%|█▏        | 193/1572 [01:47<13:22,  1.72it/s] 12%|█▏        | 194/1572 [01:48<12:59,  1.77it/s] 12%|█▏        | 195/1572 [01:48<12:46,  1.80it/s] 12%|█▏        | 196/1572 [01:49<13:01,  1.76it/s] 13%|█▎        | 197/1572 [01:49<13:14,  1.73it/s] 13%|█▎        | 198/1572 [01:50<12:51,  1.78it/s] 13%|█▎        | 199/1572 [01:50<12:39,  1.81it/s] 13%|█▎        | 200/1572 [01:51<11:54,  1.92it/s] 13%|█▎        | 201/1572 [01:51<12:09,  1.88it/s] 13%|█▎        | 202/1572 [01:52<12:22,  1.84it/s] 13%|█▎        | 203/1572 [01:53<12:06,  1.88it/s] 13%|█▎        | 204/1572 [01:53<11:52,  1.92it/s] 13%|█▎        | 205/1572 [01:54<11:43,  1.94it/s] 13%|█▎        | 206/1572 [01:54<12:34,  1.81it/s] 13%|█▎        | 207/1572 [01:55<12:35,  1.81it/s] 13%|█▎        | 208/1572 [01:55<12:33,  1.81it/s] 13%|█▎        | 209/1572 [01:56<12:38,  1.80it/s] 13%|█▎        | 210/1572 [01:56<12:55,  1.76it/s] 13%|█▎        | 211/1572 [01:57<13:06,  1.73it/s] 13%|█▎        | 212/1572 [01:58<12:48,  1.77it/s] 14%|█▎        | 213/1572 [01:58<12:34,  1.80it/s] 14%|█▎        | 214/1572 [01:59<12:25,  1.82it/s] 14%|█▎        | 215/1572 [01:59<12:18,  1.84it/s] 14%|█▎        | 216/1572 [02:00<13:11,  1.71it/s] 14%|█▍        | 217/1572 [02:00<13:21,  1.69it/s] 14%|█▍        | 218/1572 [02:01<13:24,  1.68it/s] 14%|█▍        | 219/1572 [02:02<13:06,  1.72it/s] 14%|█▍        | 220/1572 [02:02<12:54,  1.75it/s] 14%|█▍        | 221/1572 [02:03<13:04,  1.72it/s] 14%|█▍        | 222/1572 [02:03<12:54,  1.74it/s] 14%|█▍        | 223/1572 [02:04<12:16,  1.83it/s] 14%|█▍        | 224/1572 [02:04<12:21,  1.82it/s] 14%|█▍        | 225/1572 [02:05<12:10,  1.84it/s] 14%|█▍        | 226/1572 [02:05<12:16,  1.83it/s] 14%|█▍        | 227/1572 [02:06<11:33,  1.94it/s] 15%|█▍        | 228/1572 [02:06<11:46,  1.90it/s] 15%|█▍        | 229/1572 [02:07<11:45,  1.90it/s] 15%|█▍        | 230/1572 [02:08<11:55,  1.87it/s] 15%|█▍        | 231/1572 [02:08<11:43,  1.91it/s] 15%|█▍        | 232/1572 [02:09<11:44,  1.90it/s] 15%|█▍        | 233/1572 [02:09<11:45,  1.90it/s] 15%|█▍        | 234/1572 [02:10<12:32,  1.78it/s] 15%|█▍        | 235/1572 [02:10<11:43,  1.90it/s] 15%|█▌        | 236/1572 [02:11<11:51,  1.88it/s] 15%|█▌        | 237/1572 [02:11<11:52,  1.87it/s] 15%|█▌        | 238/1572 [02:12<11:29,  1.94it/s] 15%|█▌        | 239/1572 [02:12<11:35,  1.92it/s] 15%|█▌        | 240/1572 [02:13<12:03,  1.84it/s] 15%|█▌        | 241/1572 [02:14<12:43,  1.74it/s] 15%|█▌        | 242/1572 [02:14<12:07,  1.83it/s] 15%|█▌        | 243/1572 [02:15<12:25,  1.78it/s] 16%|█▌        | 244/1572 [02:15<12:08,  1.82it/s] 16%|█▌        | 245/1572 [02:16<12:12,  1.81it/s] 16%|█▌        | 246/1572 [02:16<12:33,  1.76it/s] 16%|█▌        | 247/1572 [02:17<13:03,  1.69it/s] 16%|█▌        | 248/1572 [02:17<12:39,  1.74it/s] 16%|█▌        | 249/1572 [02:18<13:05,  1.69it/s] 16%|█▌        | 250/1572 [02:19<12:48,  1.72it/s] 16%|█▌        | 251/1572 [02:19<13:10,  1.67it/s] 16%|█▌        | 252/1572 [02:20<12:55,  1.70it/s] 16%|█▌        | 253/1572 [02:20<12:31,  1.76it/s] 16%|█▌        | 254/1572 [02:21<12:02,  1.82it/s] 16%|█▌        | 255/1572 [02:22<12:35,  1.74it/s] 16%|█▋        | 256/1572 [02:22<12:07,  1.81it/s] 16%|█▋        | 257/1572 [02:23<12:55,  1.70it/s] 16%|█▋        | 258/1572 [02:23<12:35,  1.74it/s] 16%|█▋        | 259/1572 [02:24<12:05,  1.81it/s] 17%|█▋        | 260/1572 [02:24<11:56,  1.83it/s] 17%|█▋        | 261/1572 [02:25<11:50,  1.85it/s] 17%|█▋        | 262/1572 [02:25<12:11,  1.79it/s] 17%|█▋        | 263/1572 [02:26<12:26,  1.75it/s] 17%|█▋        | 264/1572 [02:27<12:14,  1.78it/s] 17%|█▋        | 265/1572 [02:27<12:26,  1.75it/s] 17%|█▋        | 266/1572 [02:28<12:35,  1.73it/s] 17%|█▋        | 267/1572 [02:28<13:02,  1.67it/s] 17%|█▋        | 268/1572 [02:29<12:36,  1.72it/s] 17%|█▋        | 269/1572 [02:29<12:16,  1.77it/s] 17%|█▋        | 270/1572 [02:30<12:29,  1.74it/s] 17%|█▋        | 271/1572 [02:31<11:52,  1.83it/s] 17%|█▋        | 272/1572 [02:31<11:45,  1.84it/s] 17%|█▋        | 273/1572 [02:32<11:28,  1.89it/s] 17%|█▋        | 274/1572 [02:32<11:36,  1.86it/s] 17%|█▋        | 275/1572 [02:33<12:03,  1.79it/s] 18%|█▊        | 276/1572 [02:33<11:54,  1.81it/s] 18%|█▊        | 277/1572 [02:34<11:42,  1.84it/s] 18%|█▊        | 278/1572 [02:34<11:25,  1.89it/s] 18%|█▊        | 279/1572 [02:35<11:24,  1.89it/s] 18%|█▊        | 280/1572 [02:35<11:35,  1.86it/s] 18%|█▊        | 281/1572 [02:36<11:30,  1.87it/s] 18%|█▊        | 282/1572 [02:36<11:35,  1.85it/s] 18%|█▊        | 283/1572 [02:37<11:44,  1.83it/s] 18%|█▊        | 284/1572 [02:38<11:39,  1.84it/s] 18%|█▊        | 285/1572 [02:38<11:36,  1.85it/s] 18%|█▊        | 286/1572 [02:39<12:03,  1.78it/s] 18%|█▊        | 287/1572 [02:39<11:50,  1.81it/s] 18%|█▊        | 288/1572 [02:40<11:48,  1.81it/s] 18%|█▊        | 289/1572 [02:40<11:47,  1.81it/s] 18%|█▊        | 290/1572 [02:41<12:10,  1.76it/s] 19%|█▊        | 291/1572 [02:42<12:21,  1.73it/s] 19%|█▊        | 292/1572 [02:42<11:42,  1.82it/s] 19%|█▊        | 293/1572 [02:43<11:58,  1.78it/s] 19%|█▊        | 294/1572 [02:43<11:46,  1.81it/s] 19%|█▉        | 295/1572 [02:44<12:01,  1.77it/s] 19%|█▉        | 296/1572 [02:44<12:10,  1.75it/s] 19%|█▉        | 297/1572 [02:45<11:54,  1.78it/s] 19%|█▉        | 298/1572 [02:45<12:13,  1.74it/s] 19%|█▉        | 299/1572 [02:46<12:09,  1.74it/s] 19%|█▉        | 300/1572 [02:47<11:59,  1.77it/s] 19%|█▉        | 301/1572 [02:47<11:56,  1.77it/s] 19%|█▉        | 302/1572 [02:48<11:25,  1.85it/s] 19%|█▉        | 303/1572 [02:48<11:34,  1.83it/s] 19%|█▉        | 304/1572 [02:49<11:30,  1.84it/s] 19%|█▉        | 305/1572 [02:49<12:18,  1.71it/s] 19%|█▉        | 306/1572 [02:50<12:02,  1.75it/s] 20%|█▉        | 307/1572 [02:51<12:12,  1.73it/s] 20%|█▉        | 308/1572 [02:51<13:02,  1.62it/s] 20%|█▉        | 309/1572 [02:52<12:18,  1.71it/s] 20%|█▉        | 310/1572 [02:52<12:05,  1.74it/s] 20%|█▉        | 311/1572 [02:53<12:00,  1.75it/s] 20%|█▉        | 312/1572 [02:54<12:38,  1.66it/s] 20%|█▉        | 313/1572 [02:54<11:59,  1.75it/s] 20%|█▉        | 314/1572 [02:55<11:48,  1.78it/s] 20%|██        | 315/1572 [02:55<11:36,  1.80it/s] 20%|██        | 316/1572 [02:56<11:22,  1.84it/s] 20%|██        | 317/1572 [02:56<11:17,  1.85it/s] 20%|██        | 318/1572 [02:57<11:25,  1.83it/s] 20%|██        | 319/1572 [02:57<11:51,  1.76it/s] 20%|██        | 320/1572 [02:58<11:36,  1.80it/s] 20%|██        | 321/1572 [02:58<11:49,  1.76it/s] 20%|██        | 322/1572 [02:59<12:38,  1.65it/s] 21%|██        | 323/1572 [03:00<13:15,  1.57it/s] 21%|██        | 324/1572 [03:00<12:04,  1.72it/s] 21%|██        | 325/1572 [03:01<11:56,  1.74it/s] 21%|██        | 326/1572 [03:01<11:48,  1.76it/s] 21%|██        | 327/1572 [03:02<11:46,  1.76it/s] 21%|██        | 328/1572 [03:03<11:43,  1.77it/s] 21%|██        | 329/1572 [03:03<11:40,  1.77it/s] 21%|██        | 330/1572 [03:04<11:56,  1.73it/s] 21%|██        | 331/1572 [03:04<11:36,  1.78it/s] 21%|██        | 332/1572 [03:05<11:24,  1.81it/s] 21%|██        | 333/1572 [03:05<11:16,  1.83it/s] 21%|██        | 334/1572 [03:06<10:58,  1.88it/s] 21%|██▏       | 335/1572 [03:06<10:54,  1.89it/s] 21%|██▏       | 336/1572 [03:07<11:05,  1.86it/s] 21%|██▏       | 337/1572 [03:07<11:01,  1.87it/s] 22%|██▏       | 338/1572 [03:08<11:07,  1.85it/s] 22%|██▏       | 339/1572 [03:09<11:13,  1.83it/s] 22%|██▏       | 340/1572 [03:09<11:08,  1.84it/s] 22%|██▏       | 341/1572 [03:10<11:05,  1.85it/s] 22%|██▏       | 342/1572 [03:10<11:08,  1.84it/s] 22%|██▏       | 343/1572 [03:11<11:12,  1.83it/s] 22%|██▏       | 344/1572 [03:11<11:19,  1.81it/s] 22%|██▏       | 345/1572 [03:12<11:38,  1.76it/s] 22%|██▏       | 346/1572 [03:12<11:48,  1.73it/s] 22%|██▏       | 347/1572 [03:13<11:30,  1.77it/s] 22%|██▏       | 348/1572 [03:14<11:05,  1.84it/s] 22%|██▏       | 349/1572 [03:14<11:22,  1.79it/s] 22%|██▏       | 350/1572 [03:15<11:26,  1.78it/s] 22%|██▏       | 351/1572 [03:15<12:08,  1.68it/s] 22%|██▏       | 352/1572 [03:16<11:46,  1.73it/s] 22%|██▏       | 353/1572 [03:16<11:57,  1.70it/s] 23%|██▎       | 354/1572 [03:17<11:25,  1.78it/s] 23%|██▎       | 355/1572 [03:18<11:21,  1.78it/s] 23%|██▎       | 356/1572 [03:18<11:50,  1.71it/s] 23%|██▎       | 357/1572 [03:19<12:11,  1.66it/s] 23%|██▎       | 358/1572 [03:19<12:11,  1.66it/s] 23%|██▎       | 359/1572 [03:20<12:14,  1.65it/s] 23%|██▎       | 360/1572 [03:21<12:30,  1.61it/s] 23%|██▎       | 361/1572 [03:21<11:57,  1.69it/s] 23%|██▎       | 362/1572 [03:22<12:16,  1.64it/s] 23%|██▎       | 363/1572 [03:22<12:10,  1.65it/s] 23%|██▎       | 364/1572 [03:23<11:42,  1.72it/s] 23%|██▎       | 365/1572 [03:24<11:52,  1.69it/s] 23%|██▎       | 366/1572 [03:24<12:11,  1.65it/s] 23%|██▎       | 367/1572 [03:25<11:46,  1.70it/s] 23%|██▎       | 368/1572 [03:25<11:28,  1.75it/s] 23%|██▎       | 369/1572 [03:26<11:01,  1.82it/s] 24%|██▎       | 370/1572 [03:26<11:01,  1.82it/s] 24%|██▎       | 371/1572 [03:27<11:00,  1.82it/s] 24%|██▎       | 372/1572 [03:27<10:56,  1.83it/s] 24%|██▎       | 373/1572 [03:28<11:00,  1.82it/s] 24%|██▍       | 374/1572 [03:29<10:50,  1.84it/s] 24%|██▍       | 375/1572 [03:29<11:09,  1.79it/s] 24%|██▍       | 376/1572 [03:30<10:57,  1.82it/s] 24%|██▍       | 377/1572 [03:30<10:39,  1.87it/s] 24%|██▍       | 378/1572 [03:31<10:48,  1.84it/s] 24%|██▍       | 379/1572 [03:31<10:44,  1.85it/s] 24%|██▍       | 380/1572 [03:32<10:38,  1.87it/s] 24%|██▍       | 381/1572 [03:32<10:44,  1.85it/s] 24%|██▍       | 382/1572 [03:33<10:21,  1.92it/s] 24%|██▍       | 383/1572 [03:33<10:31,  1.88it/s] 24%|██▍       | 384/1572 [03:34<10:54,  1.82it/s] 24%|██▍       | 385/1572 [03:35<10:45,  1.84it/s] 25%|██▍       | 386/1572 [03:35<10:47,  1.83it/s] 25%|██▍       | 387/1572 [03:36<11:03,  1.79it/s] 25%|██▍       | 388/1572 [03:36<11:03,  1.78it/s] 25%|██▍       | 389/1572 [03:37<10:49,  1.82it/s] 25%|██▍       | 390/1572 [03:37<10:50,  1.82it/s] 25%|██▍       | 391/1572 [03:38<10:42,  1.84it/s] 25%|██▍       | 392/1572 [03:38<10:45,  1.83it/s] 25%|██▌       | 393/1572 [03:39<10:40,  1.84it/s] 25%|██▌       | 394/1572 [03:39<10:36,  1.85it/s] 25%|██▌       | 395/1572 [03:40<10:54,  1.80it/s] 25%|██▌       | 396/1572 [03:41<10:28,  1.87it/s] 25%|██▌       | 397/1572 [03:41<10:14,  1.91it/s] 25%|██▌       | 398/1572 [03:42<10:17,  1.90it/s] 25%|██▌       | 399/1572 [03:42<10:06,  1.93it/s] 25%|██▌       | 400/1572 [03:43<10:22,  1.88it/s] 26%|██▌       | 401/1572 [03:43<10:11,  1.91it/s] 26%|██▌       | 402/1572 [03:44<10:20,  1.89it/s] 26%|██▌       | 403/1572 [03:44<10:16,  1.90it/s] 26%|██▌       | 404/1572 [03:45<10:18,  1.89it/s] 26%|██▌       | 405/1572 [03:45<10:01,  1.94it/s] 26%|██▌       | 406/1572 [03:46<10:13,  1.90it/s] 26%|██▌       | 407/1572 [03:46<10:52,  1.79it/s] 26%|██▌       | 408/1572 [03:47<10:49,  1.79it/s] 26%|██▌       | 409/1572 [03:47<10:45,  1.80it/s] 26%|██▌       | 410/1572 [03:48<10:36,  1.82it/s] 26%|██▌       | 411/1572 [03:49<10:57,  1.77it/s] 26%|██▌       | 412/1572 [03:49<10:54,  1.77it/s] 26%|██▋       | 413/1572 [03:50<11:19,  1.71it/s] 26%|██▋       | 414/1572 [03:50<11:40,  1.65it/s] 26%|██▋       | 415/1572 [03:51<11:15,  1.71it/s] 26%|██▋       | 416/1572 [03:52<11:04,  1.74it/s] 27%|██▋       | 417/1572 [03:52<11:00,  1.75it/s] 27%|██▋       | 418/1572 [03:53<10:46,  1.79it/s] 27%|██▋       | 419/1572 [03:53<10:46,  1.78it/s] 27%|██▋       | 420/1572 [03:54<10:48,  1.78it/s] 27%|██▋       | 421/1572 [03:54<10:44,  1.79it/s] 27%|██▋       | 422/1572 [03:55<10:44,  1.78it/s] 27%|██▋       | 423/1572 [03:55<10:32,  1.82it/s] 27%|██▋       | 424/1572 [03:56<10:35,  1.81it/s] 27%|██▋       | 425/1572 [03:57<10:28,  1.82it/s] 27%|██▋       | 426/1572 [03:57<10:34,  1.81it/s] 27%|██▋       | 427/1572 [03:58<11:04,  1.72it/s] 27%|██▋       | 428/1572 [03:58<11:11,  1.70it/s] 27%|██▋       | 429/1572 [03:59<10:36,  1.80it/s] 27%|██▋       | 430/1572 [03:59<09:55,  1.92it/s] 27%|██▋       | 431/1572 [04:00<09:46,  1.94it/s] 27%|██▋       | 432/1572 [04:00<10:00,  1.90it/s] 28%|██▊       | 433/1572 [04:01<10:03,  1.89it/s] 28%|██▊       | 434/1572 [04:01<10:26,  1.82it/s] 28%|██▊       | 435/1572 [04:02<10:57,  1.73it/s] 28%|██▊       | 436/1572 [04:03<11:17,  1.68it/s] 28%|██▊       | 437/1572 [04:03<11:06,  1.70it/s] 28%|██▊       | 438/1572 [04:04<11:14,  1.68it/s] 28%|██▊       | 439/1572 [04:04<10:53,  1.73it/s] 28%|██▊       | 440/1572 [04:05<10:58,  1.72it/s] 28%|██▊       | 441/1572 [04:06<10:44,  1.76it/s] 28%|██▊       | 442/1572 [04:06<10:37,  1.77it/s] 28%|██▊       | 443/1572 [04:07<10:37,  1.77it/s] 28%|██▊       | 444/1572 [04:07<10:36,  1.77it/s] 28%|██▊       | 445/1572 [04:08<10:23,  1.81it/s] 28%|██▊       | 446/1572 [04:08<10:42,  1.75it/s] 28%|██▊       | 447/1572 [04:09<11:04,  1.69it/s] 28%|██▊       | 448/1572 [04:10<10:41,  1.75it/s] 29%|██▊       | 449/1572 [04:10<10:32,  1.77it/s] 29%|██▊       | 450/1572 [04:11<10:21,  1.80it/s] 29%|██▊       | 451/1572 [04:11<10:41,  1.75it/s] 29%|██▉       | 452/1572 [04:12<10:28,  1.78it/s] 29%|██▉       | 453/1572 [04:12<10:14,  1.82it/s] 29%|██▉       | 454/1572 [04:13<09:51,  1.89it/s] 29%|██▉       | 455/1572 [04:13<09:22,  1.98it/s] 29%|██▉       | 456/1572 [04:14<09:37,  1.93it/s] 29%|██▉       | 457/1572 [04:14<09:40,  1.92it/s] 29%|██▉       | 458/1572 [04:15<09:26,  1.97it/s] 29%|██▉       | 459/1572 [04:15<09:21,  1.98it/s] 29%|██▉       | 460/1572 [04:16<09:12,  2.01it/s] 29%|██▉       | 461/1572 [04:16<09:25,  1.97it/s] 29%|██▉       | 462/1572 [04:17<10:09,  1.82it/s] 29%|██▉       | 463/1572 [04:17<09:53,  1.87it/s] 30%|██▉       | 464/1572 [04:18<09:58,  1.85it/s] 30%|██▉       | 465/1572 [04:19<09:45,  1.89it/s] 30%|██▉       | 466/1572 [04:19<09:52,  1.87it/s] 30%|██▉       | 467/1572 [04:20<09:51,  1.87it/s] 30%|██▉       | 468/1572 [04:20<10:01,  1.83it/s] 30%|██▉       | 469/1572 [04:21<10:03,  1.83it/s] 30%|██▉       | 470/1572 [04:21<09:59,  1.84it/s] 30%|██▉       | 471/1572 [04:22<10:00,  1.83it/s] 30%|███       | 472/1572 [04:22<10:02,  1.82it/s] 30%|███       | 473/1572 [04:23<09:48,  1.87it/s] 30%|███       | 474/1572 [04:23<09:46,  1.87it/s] 30%|███       | 475/1572 [04:24<09:50,  1.86it/s] 30%|███       | 476/1572 [04:24<09:57,  1.83it/s] 30%|███       | 477/1572 [04:25<09:50,  1.85it/s] 30%|███       | 478/1572 [04:26<09:59,  1.83it/s] 30%|███       | 479/1572 [04:26<10:05,  1.80it/s] 31%|███       | 480/1572 [04:27<10:24,  1.75it/s] 31%|███       | 481/1572 [04:27<10:13,  1.78it/s] 31%|███       | 482/1572 [04:28<09:53,  1.84it/s] 31%|███       | 483/1572 [04:28<09:50,  1.84it/s] 31%|███       | 484/1572 [04:29<10:06,  1.79it/s] 31%|███       | 485/1572 [04:29<10:03,  1.80it/s] 31%|███       | 486/1572 [04:30<10:05,  1.79it/s] 31%|███       | 487/1572 [04:31<10:22,  1.74it/s] 31%|███       | 488/1572 [04:31<10:16,  1.76it/s] 31%|███       | 489/1572 [04:32<10:30,  1.72it/s] 31%|███       | 490/1572 [04:32<10:24,  1.73it/s] 31%|███       | 491/1572 [04:33<10:16,  1.75it/s] 31%|███▏      | 492/1572 [04:34<10:14,  1.76it/s] 31%|███▏      | 493/1572 [04:34<09:58,  1.80it/s] 31%|███▏      | 494/1572 [04:35<10:11,  1.76it/s] 31%|███▏      | 495/1572 [04:35<09:57,  1.80it/s] 32%|███▏      | 496/1572 [04:36<09:49,  1.83it/s] 32%|███▏      | 497/1572 [04:36<09:26,  1.90it/s] 32%|███▏      | 498/1572 [04:37<09:17,  1.93it/s] 32%|███▏      | 499/1572 [04:37<09:30,  1.88it/s] 32%|███▏      | 500/1572 [04:38<09:31,  1.87it/s] 32%|███▏      | 501/1572 [04:38<09:52,  1.81it/s] 32%|███▏      | 502/1572 [04:39<09:45,  1.83it/s] 32%|███▏      | 503/1572 [04:40<10:06,  1.76it/s] 32%|███▏      | 504/1572 [04:40<10:01,  1.77it/s] 32%|███▏      | 505/1572 [04:41<10:01,  1.77it/s] 32%|███▏      | 506/1572 [04:41<09:52,  1.80it/s] 32%|███▏      | 507/1572 [04:42<09:28,  1.87it/s] 32%|███▏      | 508/1572 [04:42<09:33,  1.85it/s] 32%|███▏      | 509/1572 [04:43<09:52,  1.79it/s] 32%|███▏      | 510/1572 [04:43<10:04,  1.76it/s] 33%|███▎      | 511/1572 [04:44<09:52,  1.79it/s] 33%|███▎      | 512/1572 [04:45<10:02,  1.76it/s] 33%|███▎      | 513/1572 [04:45<10:27,  1.69it/s] 33%|███▎      | 514/1572 [04:46<10:17,  1.71it/s] 33%|███▎      | 515/1572 [04:46<09:57,  1.77it/s] 33%|███▎      | 516/1572 [04:47<09:36,  1.83it/s] 33%|███▎      | 517/1572 [04:47<09:32,  1.84it/s] 33%|███▎      | 518/1572 [04:48<09:26,  1.86it/s] 33%|███▎      | 519/1572 [04:48<09:34,  1.83it/s] 33%|███▎      | 520/1572 [04:49<09:36,  1.83it/s] 33%|███▎      | 521/1572 [04:49<09:04,  1.93it/s] 33%|███▎      | 522/1572 [04:50<08:57,  1.95it/s] 33%|███▎      | 523/1572 [04:50<09:03,  1.93it/s] 33%|███▎      | 524/1572 [04:51<09:26,  1.85it/s]03/13/2024 21:13:38 - INFO - __main__ - epoch 0: {'accuracy': 0.6813725490196079, 'f1': 0.758364312267658}
 33%|███▎      | 525/1572 [05:00<51:13,  2.94s/it] 33%|███▎      | 526/1572 [05:00<38:43,  2.22s/it] 34%|███▎      | 527/1572 [05:01<30:16,  1.74s/it] 34%|███▎      | 528/1572 [05:01<24:17,  1.40s/it] 34%|███▎      | 529/1572 [05:02<19:47,  1.14s/it] 34%|███▎      | 530/1572 [05:02<16:47,  1.03it/s] 34%|███▍      | 531/1572 [05:03<14:41,  1.18it/s] 34%|███▍      | 532/1572 [05:04<12:59,  1.33it/s] 34%|███▍      | 533/1572 [05:04<12:14,  1.41it/s] 34%|███▍      | 534/1572 [05:05<11:04,  1.56it/s] 34%|███▍      | 535/1572 [05:05<10:49,  1.60it/s] 34%|███▍      | 536/1572 [05:06<10:26,  1.65it/s] 34%|███▍      | 537/1572 [05:06<10:05,  1.71it/s] 34%|███▍      | 538/1572 [05:07<09:54,  1.74it/s] 34%|███▍      | 539/1572 [05:07<09:29,  1.81it/s] 34%|███▍      | 540/1572 [05:08<09:45,  1.76it/s] 34%|███▍      | 541/1572 [05:08<09:26,  1.82it/s] 34%|███▍      | 542/1572 [05:09<08:52,  1.93it/s] 35%|███▍      | 543/1572 [05:09<08:46,  1.95it/s] 35%|███▍      | 544/1572 [05:10<08:42,  1.97it/s] 35%|███▍      | 545/1572 [05:10<08:45,  1.96it/s] 35%|███▍      | 546/1572 [05:11<08:55,  1.92it/s] 35%|███▍      | 547/1572 [05:12<09:18,  1.84it/s] 35%|███▍      | 548/1572 [05:12<09:34,  1.78it/s] 35%|███▍      | 549/1572 [05:13<09:16,  1.84it/s] 35%|███▍      | 550/1572 [05:13<09:21,  1.82it/s] 35%|███▌      | 551/1572 [05:14<09:16,  1.83it/s] 35%|███▌      | 552/1572 [05:14<09:20,  1.82it/s] 35%|███▌      | 553/1572 [05:15<09:12,  1.84it/s] 35%|███▌      | 554/1572 [05:15<09:17,  1.83it/s] 35%|███▌      | 555/1572 [05:16<09:21,  1.81it/s] 35%|███▌      | 556/1572 [05:17<09:39,  1.75it/s] 35%|███▌      | 557/1572 [05:17<09:18,  1.82it/s] 35%|███▌      | 558/1572 [05:18<09:44,  1.73it/s] 36%|███▌      | 559/1572 [05:18<09:31,  1.77it/s] 36%|███▌      | 560/1572 [05:19<09:28,  1.78it/s] 36%|███▌      | 561/1572 [05:19<09:24,  1.79it/s] 36%|███▌      | 562/1572 [05:20<09:39,  1.74it/s] 36%|███▌      | 563/1572 [05:21<09:34,  1.75it/s] 36%|███▌      | 564/1572 [05:21<09:27,  1.77it/s] 36%|███▌      | 565/1572 [05:22<09:22,  1.79it/s] 36%|███▌      | 566/1572 [05:22<09:34,  1.75it/s] 36%|███▌      | 567/1572 [05:23<09:22,  1.79it/s] 36%|███▌      | 568/1572 [05:23<09:11,  1.82it/s] 36%|███▌      | 569/1572 [05:24<09:05,  1.84it/s] 36%|███▋      | 570/1572 [05:24<09:20,  1.79it/s] 36%|███▋      | 571/1572 [05:25<09:45,  1.71it/s] 36%|███▋      | 572/1572 [05:26<09:36,  1.73it/s] 36%|███▋      | 573/1572 [05:26<09:29,  1.75it/s] 37%|███▋      | 574/1572 [05:27<09:14,  1.80it/s] 37%|███▋      | 575/1572 [05:27<09:39,  1.72it/s] 37%|███▋      | 576/1572 [05:28<09:58,  1.66it/s] 37%|███▋      | 577/1572 [05:28<09:27,  1.75it/s] 37%|███▋      | 578/1572 [05:29<09:14,  1.79it/s] 37%|███▋      | 579/1572 [05:30<09:38,  1.72it/s] 37%|███▋      | 580/1572 [05:30<09:24,  1.76it/s] 37%|███▋      | 581/1572 [05:31<09:31,  1.73it/s] 37%|███▋      | 582/1572 [05:31<09:24,  1.76it/s] 37%|███▋      | 583/1572 [05:32<09:22,  1.76it/s] 37%|███▋      | 584/1572 [05:33<09:43,  1.69it/s] 37%|███▋      | 585/1572 [05:33<09:27,  1.74it/s] 37%|███▋      | 586/1572 [05:34<09:03,  1.81it/s] 37%|███▋      | 587/1572 [05:34<08:41,  1.89it/s] 37%|███▋      | 588/1572 [05:35<09:14,  1.77it/s] 37%|███▋      | 589/1572 [05:35<09:24,  1.74it/s] 38%|███▊      | 590/1572 [05:36<09:31,  1.72it/s] 38%|███▊      | 591/1572 [05:36<09:23,  1.74it/s] 38%|███▊      | 592/1572 [05:37<09:33,  1.71it/s] 38%|███▊      | 593/1572 [05:38<09:26,  1.73it/s] 38%|███▊      | 594/1572 [05:38<09:19,  1.75it/s] 38%|███▊      | 595/1572 [05:39<09:17,  1.75it/s] 38%|███▊      | 596/1572 [05:39<08:56,  1.82it/s] 38%|███▊      | 597/1572 [05:40<08:49,  1.84it/s] 38%|███▊      | 598/1572 [05:40<09:03,  1.79it/s] 38%|███▊      | 599/1572 [05:41<09:27,  1.72it/s] 38%|███▊      | 600/1572 [05:42<09:19,  1.74it/s] 38%|███▊      | 601/1572 [05:42<09:13,  1.75it/s] 38%|███▊      | 602/1572 [05:43<09:11,  1.76it/s] 38%|███▊      | 603/1572 [05:43<09:05,  1.78it/s] 38%|███▊      | 604/1572 [05:44<08:46,  1.84it/s] 38%|███▊      | 605/1572 [05:44<08:51,  1.82it/s] 39%|███▊      | 606/1572 [05:45<08:52,  1.81it/s] 39%|███▊      | 607/1572 [05:45<09:04,  1.77it/s] 39%|███▊      | 608/1572 [05:46<09:13,  1.74it/s] 39%|███▊      | 609/1572 [05:47<09:23,  1.71it/s] 39%|███▉      | 610/1572 [05:47<09:25,  1.70it/s] 39%|███▉      | 611/1572 [05:48<09:27,  1.69it/s] 39%|███▉      | 612/1572 [05:48<09:11,  1.74it/s] 39%|███▉      | 613/1572 [05:49<08:57,  1.78it/s] 39%|███▉      | 614/1572 [05:50<09:20,  1.71it/s] 39%|███▉      | 615/1572 [05:50<08:42,  1.83it/s] 39%|███▉      | 616/1572 [05:51<08:45,  1.82it/s] 39%|███▉      | 617/1572 [05:51<08:59,  1.77it/s] 39%|███▉      | 618/1572 [05:52<09:07,  1.74it/s] 39%|███▉      | 619/1572 [05:52<09:44,  1.63it/s] 39%|███▉      | 620/1572 [05:53<09:26,  1.68it/s] 40%|███▉      | 621/1572 [05:54<08:59,  1.76it/s] 40%|███▉      | 622/1572 [05:54<08:56,  1.77it/s] 40%|███▉      | 623/1572 [05:55<08:37,  1.83it/s] 40%|███▉      | 624/1572 [05:55<08:32,  1.85it/s] 40%|███▉      | 625/1572 [05:56<08:46,  1.80it/s] 40%|███▉      | 626/1572 [05:56<08:45,  1.80it/s] 40%|███▉      | 627/1572 [05:57<08:56,  1.76it/s] 40%|███▉      | 628/1572 [05:57<08:47,  1.79it/s] 40%|████      | 629/1572 [05:58<08:36,  1.82it/s] 40%|████      | 630/1572 [05:58<08:32,  1.84it/s] 40%|████      | 631/1572 [05:59<08:26,  1.86it/s] 40%|████      | 632/1572 [05:59<07:57,  1.97it/s] 40%|████      | 633/1572 [06:00<08:12,  1.91it/s] 40%|████      | 634/1572 [06:00<08:05,  1.93it/s] 40%|████      | 635/1572 [06:01<08:16,  1.89it/s] 40%|████      | 636/1572 [06:02<08:21,  1.87it/s] 41%|████      | 637/1572 [06:02<08:41,  1.79it/s] 41%|████      | 638/1572 [06:03<08:23,  1.85it/s] 41%|████      | 639/1572 [06:03<08:18,  1.87it/s] 41%|████      | 640/1572 [06:04<08:14,  1.88it/s] 41%|████      | 641/1572 [06:04<08:30,  1.82it/s] 41%|████      | 642/1572 [06:05<08:27,  1.83it/s] 41%|████      | 643/1572 [06:05<08:13,  1.88it/s] 41%|████      | 644/1572 [06:06<09:01,  1.71it/s] 41%|████      | 645/1572 [06:07<08:49,  1.75it/s] 41%|████      | 646/1572 [06:07<08:56,  1.73it/s] 41%|████      | 647/1572 [06:08<08:42,  1.77it/s] 41%|████      | 648/1572 [06:08<08:37,  1.79it/s] 41%|████▏     | 649/1572 [06:09<08:20,  1.84it/s] 41%|████▏     | 650/1572 [06:09<08:45,  1.75it/s] 41%|████▏     | 651/1572 [06:10<08:33,  1.79it/s] 41%|████▏     | 652/1572 [06:10<08:25,  1.82it/s] 42%|████▏     | 653/1572 [06:11<08:41,  1.76it/s] 42%|████▏     | 654/1572 [06:12<08:32,  1.79it/s] 42%|████▏     | 655/1572 [06:12<08:09,  1.87it/s] 42%|████▏     | 656/1572 [06:13<08:16,  1.84it/s] 42%|████▏     | 657/1572 [06:13<08:34,  1.78it/s] 42%|████▏     | 658/1572 [06:14<08:11,  1.86it/s] 42%|████▏     | 659/1572 [06:14<08:15,  1.84it/s] 42%|████▏     | 660/1572 [06:15<08:19,  1.82it/s] 42%|████▏     | 661/1572 [06:16<08:43,  1.74it/s] 42%|████▏     | 662/1572 [06:16<08:17,  1.83it/s] 42%|████▏     | 663/1572 [06:17<08:14,  1.84it/s] 42%|████▏     | 664/1572 [06:17<08:17,  1.82it/s] 42%|████▏     | 665/1572 [06:18<08:17,  1.82it/s] 42%|████▏     | 666/1572 [06:18<07:42,  1.96it/s] 42%|████▏     | 667/1572 [06:19<07:47,  1.94it/s] 42%|████▏     | 668/1572 [06:19<07:58,  1.89it/s] 43%|████▎     | 669/1572 [06:20<07:57,  1.89it/s] 43%|████▎     | 670/1572 [06:20<08:04,  1.86it/s] 43%|████▎     | 671/1572 [06:21<08:19,  1.80it/s] 43%|████▎     | 672/1572 [06:21<08:03,  1.86it/s] 43%|████▎     | 673/1572 [06:22<08:18,  1.80it/s] 43%|████▎     | 674/1572 [06:23<08:41,  1.72it/s] 43%|████▎     | 675/1572 [06:23<08:19,  1.80it/s] 43%|████▎     | 676/1572 [06:24<08:11,  1.82it/s] 43%|████▎     | 677/1572 [06:24<08:07,  1.83it/s] 43%|████▎     | 678/1572 [06:25<08:03,  1.85it/s] 43%|████▎     | 679/1572 [06:25<07:59,  1.86it/s] 43%|████▎     | 680/1572 [06:26<08:05,  1.84it/s] 43%|████▎     | 681/1572 [06:26<08:06,  1.83it/s] 43%|████▎     | 682/1572 [06:27<08:09,  1.82it/s] 43%|████▎     | 683/1572 [06:27<08:12,  1.81it/s] 44%|████▎     | 684/1572 [06:28<08:13,  1.80it/s] 44%|████▎     | 685/1572 [06:29<08:22,  1.76it/s] 44%|████▎     | 686/1572 [06:29<08:09,  1.81it/s] 44%|████▎     | 687/1572 [06:30<08:20,  1.77it/s] 44%|████▍     | 688/1572 [06:30<08:13,  1.79it/s] 44%|████▍     | 689/1572 [06:31<08:12,  1.79it/s] 44%|████▍     | 690/1572 [06:31<08:34,  1.71it/s] 44%|████▍     | 691/1572 [06:32<08:23,  1.75it/s] 44%|████▍     | 692/1572 [06:33<08:29,  1.73it/s] 44%|████▍     | 693/1572 [06:33<08:23,  1.75it/s] 44%|████▍     | 694/1572 [06:34<08:11,  1.79it/s] 44%|████▍     | 695/1572 [06:34<08:01,  1.82it/s] 44%|████▍     | 696/1572 [06:35<08:25,  1.73it/s] 44%|████▍     | 697/1572 [06:35<08:18,  1.75it/s] 44%|████▍     | 698/1572 [06:36<08:24,  1.73it/s] 44%|████▍     | 699/1572 [06:37<08:17,  1.75it/s] 45%|████▍     | 700/1572 [06:37<08:35,  1.69it/s] 45%|████▍     | 701/1572 [06:38<08:25,  1.72it/s] 45%|████▍     | 702/1572 [06:38<07:58,  1.82it/s] 45%|████▍     | 703/1572 [06:39<07:58,  1.82it/s] 45%|████▍     | 704/1572 [06:39<08:00,  1.81it/s] 45%|████▍     | 705/1572 [06:40<07:46,  1.86it/s] 45%|████▍     | 706/1572 [06:40<07:48,  1.85it/s] 45%|████▍     | 707/1572 [06:41<08:14,  1.75it/s] 45%|████▌     | 708/1572 [06:42<08:13,  1.75it/s] 45%|████▌     | 709/1572 [06:42<07:41,  1.87it/s] 45%|████▌     | 710/1572 [06:43<08:07,  1.77it/s] 45%|████▌     | 711/1572 [06:43<07:57,  1.80it/s] 45%|████▌     | 712/1572 [06:44<08:11,  1.75it/s] 45%|████▌     | 713/1572 [06:44<07:39,  1.87it/s] 45%|████▌     | 714/1572 [06:45<08:14,  1.73it/s] 45%|████▌     | 715/1572 [06:46<08:24,  1.70it/s] 46%|████▌     | 716/1572 [06:46<08:30,  1.68it/s] 46%|████▌     | 717/1572 [06:47<08:14,  1.73it/s] 46%|████▌     | 718/1572 [06:47<08:08,  1.75it/s] 46%|████▌     | 719/1572 [06:48<08:01,  1.77it/s] 46%|████▌     | 720/1572 [06:48<08:08,  1.75it/s] 46%|████▌     | 721/1572 [06:49<08:39,  1.64it/s] 46%|████▌     | 722/1572 [06:50<08:17,  1.71it/s] 46%|████▌     | 723/1572 [06:50<08:11,  1.73it/s] 46%|████▌     | 724/1572 [06:51<08:05,  1.75it/s] 46%|████▌     | 725/1572 [06:51<07:46,  1.82it/s] 46%|████▌     | 726/1572 [06:52<07:40,  1.84it/s] 46%|████▌     | 727/1572 [06:52<08:04,  1.74it/s] 46%|████▋     | 728/1572 [06:53<07:52,  1.79it/s] 46%|████▋     | 729/1572 [06:53<07:43,  1.82it/s] 46%|████▋     | 730/1572 [06:54<07:42,  1.82it/s] 47%|████▋     | 731/1572 [06:54<07:24,  1.89it/s] 47%|████▋     | 732/1572 [06:55<07:27,  1.88it/s] 47%|████▋     | 733/1572 [06:56<07:24,  1.89it/s] 47%|████▋     | 734/1572 [06:56<07:21,  1.90it/s] 47%|████▋     | 735/1572 [06:57<07:30,  1.86it/s] 47%|████▋     | 736/1572 [06:57<07:35,  1.83it/s] 47%|████▋     | 737/1572 [06:58<07:31,  1.85it/s] 47%|████▋     | 738/1572 [06:58<07:32,  1.84it/s] 47%|████▋     | 739/1572 [06:59<07:28,  1.86it/s] 47%|████▋     | 740/1572 [06:59<07:52,  1.76it/s] 47%|████▋     | 741/1572 [07:00<07:35,  1.82it/s] 47%|████▋     | 742/1572 [07:00<07:29,  1.85it/s] 47%|████▋     | 743/1572 [07:01<07:42,  1.79it/s] 47%|████▋     | 744/1572 [07:02<07:32,  1.83it/s] 47%|████▋     | 745/1572 [07:02<07:25,  1.86it/s] 47%|████▋     | 746/1572 [07:03<07:41,  1.79it/s] 48%|████▊     | 747/1572 [07:03<07:42,  1.78it/s] 48%|████▊     | 748/1572 [07:04<07:27,  1.84it/s] 48%|████▊     | 749/1572 [07:04<07:16,  1.88it/s] 48%|████▊     | 750/1572 [07:05<07:14,  1.89it/s] 48%|████▊     | 751/1572 [07:05<07:13,  1.89it/s] 48%|████▊     | 752/1572 [07:06<07:04,  1.93it/s] 48%|████▊     | 753/1572 [07:06<07:21,  1.85it/s] 48%|████▊     | 754/1572 [07:07<06:57,  1.96it/s] 48%|████▊     | 755/1572 [07:07<07:08,  1.91it/s] 48%|████▊     | 756/1572 [07:08<07:07,  1.91it/s] 48%|████▊     | 757/1572 [07:08<06:47,  2.00it/s] 48%|████▊     | 758/1572 [07:09<06:55,  1.96it/s] 48%|████▊     | 759/1572 [07:09<06:58,  1.94it/s] 48%|████▊     | 760/1572 [07:10<07:02,  1.92it/s] 48%|████▊     | 761/1572 [07:11<07:08,  1.89it/s] 48%|████▊     | 762/1572 [07:11<07:27,  1.81it/s] 49%|████▊     | 763/1572 [07:12<07:29,  1.80it/s] 49%|████▊     | 764/1572 [07:12<07:37,  1.77it/s] 49%|████▊     | 765/1572 [07:13<07:57,  1.69it/s] 49%|████▊     | 766/1572 [07:13<07:35,  1.77it/s] 49%|████▉     | 767/1572 [07:14<07:18,  1.83it/s] 49%|████▉     | 768/1572 [07:14<07:19,  1.83it/s] 49%|████▉     | 769/1572 [07:15<07:23,  1.81it/s] 49%|████▉     | 770/1572 [07:16<07:17,  1.83it/s] 49%|████▉     | 771/1572 [07:16<07:17,  1.83it/s] 49%|████▉     | 772/1572 [07:17<07:21,  1.81it/s] 49%|████▉     | 773/1572 [07:17<07:14,  1.84it/s] 49%|████▉     | 774/1572 [07:18<06:57,  1.91it/s] 49%|████▉     | 775/1572 [07:18<07:06,  1.87it/s] 49%|████▉     | 776/1572 [07:19<07:05,  1.87it/s] 49%|████▉     | 777/1572 [07:19<07:09,  1.85it/s] 49%|████▉     | 778/1572 [07:20<07:22,  1.79it/s] 50%|████▉     | 779/1572 [07:21<07:32,  1.75it/s] 50%|████▉     | 780/1572 [07:21<07:09,  1.84it/s] 50%|████▉     | 781/1572 [07:22<06:58,  1.89it/s] 50%|████▉     | 782/1572 [07:22<06:50,  1.92it/s] 50%|████▉     | 783/1572 [07:23<07:00,  1.88it/s] 50%|████▉     | 784/1572 [07:23<06:57,  1.89it/s] 50%|████▉     | 785/1572 [07:24<07:22,  1.78it/s] 50%|█████     | 786/1572 [07:24<07:21,  1.78it/s] 50%|█████     | 787/1572 [07:25<07:32,  1.73it/s] 50%|█████     | 788/1572 [07:25<07:25,  1.76it/s] 50%|█████     | 789/1572 [07:26<07:20,  1.78it/s] 50%|█████     | 790/1572 [07:27<07:04,  1.84it/s] 50%|█████     | 791/1572 [07:27<07:16,  1.79it/s] 50%|█████     | 792/1572 [07:28<06:57,  1.87it/s] 50%|█████     | 793/1572 [07:28<06:53,  1.89it/s] 51%|█████     | 794/1572 [07:29<07:06,  1.82it/s] 51%|█████     | 795/1572 [07:29<07:02,  1.84it/s] 51%|█████     | 796/1572 [07:30<06:58,  1.85it/s] 51%|█████     | 797/1572 [07:30<07:03,  1.83it/s] 51%|█████     | 798/1572 [07:31<07:06,  1.82it/s] 51%|█████     | 799/1572 [07:31<07:08,  1.81it/s] 51%|█████     | 800/1572 [07:32<07:28,  1.72it/s] 51%|█████     | 801/1572 [07:33<07:42,  1.67it/s] 51%|█████     | 802/1572 [07:33<07:27,  1.72it/s] 51%|█████     | 803/1572 [07:34<07:16,  1.76it/s] 51%|█████     | 804/1572 [07:34<07:06,  1.80it/s] 51%|█████     | 805/1572 [07:35<07:00,  1.82it/s] 51%|█████▏    | 806/1572 [07:36<07:27,  1.71it/s] 51%|█████▏    | 807/1572 [07:36<07:23,  1.72it/s] 51%|█████▏    | 808/1572 [07:37<07:12,  1.77it/s] 51%|█████▏    | 809/1572 [07:37<07:01,  1.81it/s] 52%|█████▏    | 810/1572 [07:38<07:11,  1.77it/s] 52%|█████▏    | 811/1572 [07:38<06:43,  1.89it/s] 52%|█████▏    | 812/1572 [07:39<06:46,  1.87it/s] 52%|█████▏    | 813/1572 [07:39<07:09,  1.77it/s] 52%|█████▏    | 814/1572 [07:40<07:19,  1.72it/s] 52%|█████▏    | 815/1572 [07:41<07:06,  1.78it/s] 52%|█████▏    | 816/1572 [07:41<06:59,  1.80it/s] 52%|█████▏    | 817/1572 [07:42<06:53,  1.82it/s] 52%|█████▏    | 818/1572 [07:42<06:56,  1.81it/s] 52%|█████▏    | 819/1572 [07:43<06:58,  1.80it/s] 52%|█████▏    | 820/1572 [07:43<06:59,  1.79it/s] 52%|█████▏    | 821/1572 [07:44<06:54,  1.81it/s] 52%|█████▏    | 822/1572 [07:44<06:52,  1.82it/s] 52%|█████▏    | 823/1572 [07:45<06:48,  1.83it/s] 52%|█████▏    | 824/1572 [07:46<07:10,  1.74it/s] 52%|█████▏    | 825/1572 [07:46<07:06,  1.75it/s] 53%|█████▎    | 826/1572 [07:47<06:56,  1.79it/s] 53%|█████▎    | 827/1572 [07:47<06:53,  1.80it/s] 53%|█████▎    | 828/1572 [07:48<07:01,  1.77it/s] 53%|█████▎    | 829/1572 [07:48<07:00,  1.77it/s] 53%|█████▎    | 830/1572 [07:49<06:32,  1.89it/s] 53%|█████▎    | 831/1572 [07:49<06:46,  1.82it/s] 53%|█████▎    | 832/1572 [07:50<06:34,  1.87it/s] 53%|█████▎    | 833/1572 [07:50<06:46,  1.82it/s] 53%|█████▎    | 834/1572 [07:51<06:46,  1.82it/s] 53%|█████▎    | 835/1572 [07:52<06:48,  1.81it/s] 53%|█████▎    | 836/1572 [07:52<06:45,  1.81it/s] 53%|█████▎    | 837/1572 [07:53<06:41,  1.83it/s] 53%|█████▎    | 838/1572 [07:53<06:41,  1.83it/s] 53%|█████▎    | 839/1572 [07:54<06:25,  1.90it/s] 53%|█████▎    | 840/1572 [07:54<06:26,  1.89it/s] 53%|█████▎    | 841/1572 [07:55<06:56,  1.75it/s] 54%|█████▎    | 842/1572 [07:55<06:55,  1.76it/s] 54%|█████▎    | 843/1572 [07:56<06:45,  1.80it/s] 54%|█████▎    | 844/1572 [07:57<07:09,  1.69it/s] 54%|█████▍    | 845/1572 [07:57<06:55,  1.75it/s] 54%|█████▍    | 846/1572 [07:58<06:47,  1.78it/s] 54%|█████▍    | 847/1572 [07:58<06:56,  1.74it/s] 54%|█████▍    | 848/1572 [07:59<06:47,  1.77it/s] 54%|█████▍    | 849/1572 [07:59<06:44,  1.79it/s] 54%|█████▍    | 850/1572 [08:00<06:42,  1.80it/s] 54%|█████▍    | 851/1572 [08:00<06:25,  1.87it/s] 54%|█████▍    | 852/1572 [08:01<06:24,  1.87it/s] 54%|█████▍    | 853/1572 [08:01<06:12,  1.93it/s] 54%|█████▍    | 854/1572 [08:02<06:18,  1.89it/s] 54%|█████▍    | 855/1572 [08:03<06:26,  1.86it/s] 54%|█████▍    | 856/1572 [08:03<06:41,  1.78it/s] 55%|█████▍    | 857/1572 [08:04<06:38,  1.79it/s] 55%|█████▍    | 858/1572 [08:04<06:25,  1.85it/s] 55%|█████▍    | 859/1572 [08:05<06:16,  1.89it/s] 55%|█████▍    | 860/1572 [08:05<06:29,  1.83it/s] 55%|█████▍    | 861/1572 [08:06<06:32,  1.81it/s] 55%|█████▍    | 862/1572 [08:06<06:26,  1.84it/s] 55%|█████▍    | 863/1572 [08:07<06:15,  1.89it/s] 55%|█████▍    | 864/1572 [08:07<06:15,  1.89it/s] 55%|█████▌    | 865/1572 [08:08<06:32,  1.80it/s] 55%|█████▌    | 866/1572 [08:09<06:27,  1.82it/s] 55%|█████▌    | 867/1572 [08:09<06:38,  1.77it/s] 55%|█████▌    | 868/1572 [08:10<06:45,  1.74it/s] 55%|█████▌    | 869/1572 [08:10<06:41,  1.75it/s] 55%|█████▌    | 870/1572 [08:11<06:36,  1.77it/s] 55%|█████▌    | 871/1572 [08:11<06:29,  1.80it/s] 55%|█████▌    | 872/1572 [08:12<06:27,  1.81it/s] 56%|█████▌    | 873/1572 [08:13<06:35,  1.77it/s] 56%|█████▌    | 874/1572 [08:13<06:22,  1.83it/s] 56%|█████▌    | 875/1572 [08:14<06:16,  1.85it/s] 56%|█████▌    | 876/1572 [08:14<06:18,  1.84it/s] 56%|█████▌    | 877/1572 [08:15<06:22,  1.82it/s] 56%|█████▌    | 878/1572 [08:15<06:16,  1.84it/s] 56%|█████▌    | 879/1572 [08:16<06:28,  1.78it/s] 56%|█████▌    | 880/1572 [08:16<06:22,  1.81it/s] 56%|█████▌    | 881/1572 [08:17<06:14,  1.84it/s] 56%|█████▌    | 882/1572 [08:17<06:15,  1.84it/s] 56%|█████▌    | 883/1572 [08:18<06:24,  1.79it/s] 56%|█████▌    | 884/1572 [08:19<06:24,  1.79it/s] 56%|█████▋    | 885/1572 [08:19<06:16,  1.83it/s] 56%|█████▋    | 886/1572 [08:20<06:05,  1.88it/s] 56%|█████▋    | 887/1572 [08:20<06:18,  1.81it/s] 56%|█████▋    | 888/1572 [08:21<06:12,  1.83it/s] 57%|█████▋    | 889/1572 [08:21<05:52,  1.94it/s] 57%|█████▋    | 890/1572 [08:22<06:28,  1.76it/s] 57%|█████▋    | 891/1572 [08:22<06:38,  1.71it/s] 57%|█████▋    | 892/1572 [08:23<06:43,  1.68it/s] 57%|█████▋    | 893/1572 [08:24<06:34,  1.72it/s] 57%|█████▋    | 894/1572 [08:24<06:28,  1.74it/s] 57%|█████▋    | 895/1572 [08:25<06:12,  1.82it/s] 57%|█████▋    | 896/1572 [08:25<06:35,  1.71it/s] 57%|█████▋    | 897/1572 [08:26<06:18,  1.78it/s] 57%|█████▋    | 898/1572 [08:26<06:11,  1.81it/s] 57%|█████▋    | 899/1572 [08:27<06:06,  1.84it/s] 57%|█████▋    | 900/1572 [08:27<05:53,  1.90it/s] 57%|█████▋    | 901/1572 [08:28<06:06,  1.83it/s] 57%|█████▋    | 902/1572 [08:29<05:53,  1.90it/s] 57%|█████▋    | 903/1572 [08:29<05:57,  1.87it/s] 58%|█████▊    | 904/1572 [08:30<05:56,  1.88it/s] 58%|█████▊    | 905/1572 [08:30<05:55,  1.88it/s] 58%|█████▊    | 906/1572 [08:31<05:58,  1.86it/s] 58%|█████▊    | 907/1572 [08:31<05:56,  1.87it/s] 58%|█████▊    | 908/1572 [08:32<05:44,  1.93it/s] 58%|█████▊    | 909/1572 [08:32<05:47,  1.91it/s] 58%|█████▊    | 910/1572 [08:33<06:03,  1.82it/s] 58%|█████▊    | 911/1572 [08:33<06:06,  1.80it/s] 58%|█████▊    | 912/1572 [08:34<05:50,  1.88it/s] 58%|█████▊    | 913/1572 [08:34<05:43,  1.92it/s] 58%|█████▊    | 914/1572 [08:35<05:51,  1.87it/s] 58%|█████▊    | 915/1572 [08:35<05:54,  1.85it/s] 58%|█████▊    | 916/1572 [08:36<05:53,  1.86it/s] 58%|█████▊    | 917/1572 [08:37<05:56,  1.84it/s] 58%|█████▊    | 918/1572 [08:37<06:06,  1.79it/s] 58%|█████▊    | 919/1572 [08:38<06:06,  1.78it/s] 59%|█████▊    | 920/1572 [08:38<06:06,  1.78it/s] 59%|█████▊    | 921/1572 [08:39<06:04,  1.79it/s] 59%|█████▊    | 922/1572 [08:39<05:52,  1.84it/s] 59%|█████▊    | 923/1572 [08:40<05:54,  1.83it/s] 59%|█████▉    | 924/1572 [08:41<06:04,  1.78it/s] 59%|█████▉    | 925/1572 [08:41<06:13,  1.73it/s] 59%|█████▉    | 926/1572 [08:42<06:09,  1.75it/s] 59%|█████▉    | 927/1572 [08:42<06:01,  1.78it/s] 59%|█████▉    | 928/1572 [08:43<06:16,  1.71it/s] 59%|█████▉    | 929/1572 [08:43<06:11,  1.73it/s] 59%|█████▉    | 930/1572 [08:44<05:52,  1.82it/s] 59%|█████▉    | 931/1572 [08:45<06:03,  1.76it/s] 59%|█████▉    | 932/1572 [08:45<06:01,  1.77it/s] 59%|█████▉    | 933/1572 [08:46<05:45,  1.85it/s] 59%|█████▉    | 934/1572 [08:46<05:48,  1.83it/s] 59%|█████▉    | 935/1572 [08:47<05:51,  1.81it/s] 60%|█████▉    | 936/1572 [08:47<05:47,  1.83it/s] 60%|█████▉    | 937/1572 [08:48<05:37,  1.88it/s] 60%|█████▉    | 938/1572 [08:48<05:48,  1.82it/s] 60%|█████▉    | 939/1572 [08:49<05:38,  1.87it/s] 60%|█████▉    | 940/1572 [08:49<05:49,  1.81it/s] 60%|█████▉    | 941/1572 [08:50<05:49,  1.81it/s] 60%|█████▉    | 942/1572 [08:51<06:23,  1.64it/s] 60%|█████▉    | 943/1572 [08:51<06:21,  1.65it/s] 60%|██████    | 944/1572 [08:52<06:11,  1.69it/s] 60%|██████    | 945/1572 [08:52<06:00,  1.74it/s] 60%|██████    | 946/1572 [08:53<06:03,  1.72it/s] 60%|██████    | 947/1572 [08:54<05:58,  1.74it/s] 60%|██████    | 948/1572 [08:54<05:54,  1.76it/s] 60%|██████    | 949/1572 [08:55<05:52,  1.77it/s] 60%|██████    | 950/1572 [08:55<05:49,  1.78it/s] 60%|██████    | 951/1572 [08:56<05:55,  1.75it/s] 61%|██████    | 952/1572 [08:56<05:50,  1.77it/s] 61%|██████    | 953/1572 [08:57<05:42,  1.81it/s] 61%|██████    | 954/1572 [08:58<05:57,  1.73it/s] 61%|██████    | 955/1572 [08:58<05:46,  1.78it/s] 61%|██████    | 956/1572 [08:59<05:45,  1.78it/s] 61%|██████    | 957/1572 [08:59<05:39,  1.81it/s] 61%|██████    | 958/1572 [09:00<05:28,  1.87it/s] 61%|██████    | 959/1572 [09:00<05:16,  1.94it/s] 61%|██████    | 960/1572 [09:01<05:17,  1.93it/s] 61%|██████    | 961/1572 [09:01<05:32,  1.84it/s] 61%|██████    | 962/1572 [09:02<05:48,  1.75it/s] 61%|██████▏   | 963/1572 [09:02<05:41,  1.78it/s] 61%|██████▏   | 964/1572 [09:03<05:40,  1.79it/s] 61%|██████▏   | 965/1572 [09:03<05:24,  1.87it/s] 61%|██████▏   | 966/1572 [09:04<05:26,  1.85it/s] 62%|██████▏   | 967/1572 [09:05<05:24,  1.86it/s] 62%|██████▏   | 968/1572 [09:05<05:34,  1.81it/s] 62%|██████▏   | 969/1572 [09:06<05:34,  1.80it/s] 62%|██████▏   | 970/1572 [09:06<05:41,  1.76it/s] 62%|██████▏   | 971/1572 [09:07<05:35,  1.79it/s] 62%|██████▏   | 972/1572 [09:07<05:49,  1.71it/s] 62%|██████▏   | 973/1572 [09:08<05:39,  1.76it/s] 62%|██████▏   | 974/1572 [09:08<05:23,  1.85it/s] 62%|██████▏   | 975/1572 [09:09<05:24,  1.84it/s] 62%|██████▏   | 976/1572 [09:10<05:24,  1.84it/s] 62%|██████▏   | 977/1572 [09:10<05:21,  1.85it/s] 62%|██████▏   | 978/1572 [09:11<05:19,  1.86it/s] 62%|██████▏   | 979/1572 [09:11<05:23,  1.83it/s] 62%|██████▏   | 980/1572 [09:12<05:19,  1.85it/s] 62%|██████▏   | 981/1572 [09:12<05:23,  1.83it/s] 62%|██████▏   | 982/1572 [09:13<05:31,  1.78it/s] 63%|██████▎   | 983/1572 [09:13<05:26,  1.80it/s] 63%|██████▎   | 984/1572 [09:14<05:15,  1.86it/s] 63%|██████▎   | 985/1572 [09:14<05:17,  1.85it/s] 63%|██████▎   | 986/1572 [09:15<05:14,  1.86it/s] 63%|██████▎   | 987/1572 [09:16<05:37,  1.73it/s] 63%|██████▎   | 988/1572 [09:16<05:30,  1.77it/s] 63%|██████▎   | 989/1572 [09:17<05:26,  1.79it/s] 63%|██████▎   | 990/1572 [09:17<05:25,  1.79it/s] 63%|██████▎   | 991/1572 [09:18<05:20,  1.81it/s] 63%|██████▎   | 992/1572 [09:18<05:19,  1.82it/s] 63%|██████▎   | 993/1572 [09:19<05:15,  1.83it/s] 63%|██████▎   | 994/1572 [09:19<05:15,  1.83it/s] 63%|██████▎   | 995/1572 [09:20<05:18,  1.81it/s] 63%|██████▎   | 996/1572 [09:21<05:18,  1.81it/s] 63%|██████▎   | 997/1572 [09:21<05:14,  1.83it/s] 63%|██████▎   | 998/1572 [09:22<05:24,  1.77it/s] 64%|██████▎   | 999/1572 [09:22<05:17,  1.80it/s] 64%|██████▎   | 1000/1572 [09:23<05:07,  1.86it/s] 64%|██████▎   | 1001/1572 [09:23<05:09,  1.85it/s] 64%|██████▎   | 1002/1572 [09:24<05:18,  1.79it/s] 64%|██████▍   | 1003/1572 [09:24<05:19,  1.78it/s] 64%|██████▍   | 1004/1572 [09:25<05:04,  1.87it/s] 64%|██████▍   | 1005/1572 [09:25<05:01,  1.88it/s] 64%|██████▍   | 1006/1572 [09:26<05:05,  1.85it/s] 64%|██████▍   | 1007/1572 [09:27<05:02,  1.86it/s] 64%|██████▍   | 1008/1572 [09:27<05:02,  1.86it/s] 64%|██████▍   | 1009/1572 [09:28<05:20,  1.76it/s] 64%|██████▍   | 1010/1572 [09:28<05:26,  1.72it/s] 64%|██████▍   | 1011/1572 [09:29<05:28,  1.71it/s] 64%|██████▍   | 1012/1572 [09:29<05:19,  1.75it/s] 64%|██████▍   | 1013/1572 [09:30<05:15,  1.77it/s] 65%|██████▍   | 1014/1572 [09:31<05:11,  1.79it/s] 65%|██████▍   | 1015/1572 [09:31<05:07,  1.81it/s] 65%|██████▍   | 1016/1572 [09:32<05:20,  1.73it/s] 65%|██████▍   | 1017/1572 [09:32<05:22,  1.72it/s] 65%|██████▍   | 1018/1572 [09:33<05:14,  1.76it/s] 65%|██████▍   | 1019/1572 [09:33<05:12,  1.77it/s] 65%|██████▍   | 1020/1572 [09:34<05:24,  1.70it/s] 65%|██████▍   | 1021/1572 [09:35<05:19,  1.72it/s] 65%|██████▌   | 1022/1572 [09:35<05:12,  1.76it/s] 65%|██████▌   | 1023/1572 [09:36<05:10,  1.77it/s] 65%|██████▌   | 1024/1572 [09:36<05:09,  1.77it/s] 65%|██████▌   | 1025/1572 [09:37<05:08,  1.77it/s] 65%|██████▌   | 1026/1572 [09:37<05:01,  1.81it/s] 65%|██████▌   | 1027/1572 [09:38<04:51,  1.87it/s] 65%|██████▌   | 1028/1572 [09:38<04:54,  1.85it/s] 65%|██████▌   | 1029/1572 [09:39<04:47,  1.89it/s] 66%|██████▌   | 1030/1572 [09:40<05:04,  1.78it/s] 66%|██████▌   | 1031/1572 [09:40<05:04,  1.78it/s] 66%|██████▌   | 1032/1572 [09:41<05:16,  1.71it/s] 66%|██████▌   | 1033/1572 [09:41<05:12,  1.72it/s] 66%|██████▌   | 1034/1572 [09:42<05:03,  1.77it/s] 66%|██████▌   | 1035/1572 [09:42<05:00,  1.78it/s] 66%|██████▌   | 1036/1572 [09:43<04:59,  1.79it/s] 66%|██████▌   | 1037/1572 [09:43<04:58,  1.79it/s] 66%|██████▌   | 1038/1572 [09:44<04:51,  1.83it/s] 66%|██████▌   | 1039/1572 [09:45<05:05,  1.74it/s] 66%|██████▌   | 1040/1572 [09:45<05:02,  1.76it/s] 66%|██████▌   | 1041/1572 [09:46<04:56,  1.79it/s] 66%|██████▋   | 1042/1572 [09:46<04:47,  1.85it/s] 66%|██████▋   | 1043/1572 [09:47<04:45,  1.85it/s] 66%|██████▋   | 1044/1572 [09:47<04:41,  1.87it/s] 66%|██████▋   | 1045/1572 [09:48<04:51,  1.81it/s] 67%|██████▋   | 1046/1572 [09:48<04:42,  1.86it/s] 67%|██████▋   | 1047/1572 [09:49<04:50,  1.81it/s] 67%|██████▋   | 1048/1572 [09:50<04:55,  1.77it/s]03/13/2024 21:18:37 - INFO - __main__ - epoch 1: {'accuracy': 0.7426470588235294, 'f1': 0.8367029548989114}
 67%|██████▋   | 1049/1572 [09:58<25:36,  2.94s/it] 67%|██████▋   | 1050/1572 [09:59<19:25,  2.23s/it] 67%|██████▋   | 1051/1572 [09:59<14:56,  1.72s/it] 67%|██████▋   | 1052/1572 [10:00<11:53,  1.37s/it] 67%|██████▋   | 1053/1572 [10:00<10:02,  1.16s/it] 67%|██████▋   | 1054/1572 [10:01<08:10,  1.06it/s] 67%|██████▋   | 1055/1572 [10:01<07:14,  1.19it/s] 67%|██████▋   | 1056/1572 [10:02<06:12,  1.38it/s] 67%|██████▋   | 1057/1572 [10:02<05:46,  1.49it/s] 67%|██████▋   | 1058/1572 [10:03<05:24,  1.58it/s] 67%|██████▋   | 1059/1572 [10:04<05:24,  1.58it/s] 67%|██████▋   | 1060/1572 [10:04<05:12,  1.64it/s] 67%|██████▋   | 1061/1572 [10:05<05:04,  1.68it/s] 68%|██████▊   | 1062/1572 [10:05<04:57,  1.72it/s] 68%|██████▊   | 1063/1572 [10:06<04:43,  1.79it/s] 68%|██████▊   | 1064/1572 [10:06<04:39,  1.82it/s] 68%|██████▊   | 1065/1572 [10:07<04:36,  1.83it/s] 68%|██████▊   | 1066/1572 [10:07<04:28,  1.88it/s] 68%|██████▊   | 1067/1572 [10:08<04:23,  1.92it/s] 68%|██████▊   | 1068/1572 [10:08<04:22,  1.92it/s] 68%|██████▊   | 1069/1572 [10:09<04:24,  1.90it/s] 68%|██████▊   | 1070/1572 [10:10<04:35,  1.82it/s] 68%|██████▊   | 1071/1572 [10:10<04:31,  1.84it/s] 68%|██████▊   | 1072/1572 [10:11<04:29,  1.86it/s] 68%|██████▊   | 1073/1572 [10:11<04:30,  1.84it/s] 68%|██████▊   | 1074/1572 [10:12<04:39,  1.78it/s] 68%|██████▊   | 1075/1572 [10:12<04:35,  1.81it/s] 68%|██████▊   | 1076/1572 [10:13<04:33,  1.81it/s] 69%|██████▊   | 1077/1572 [10:13<04:39,  1.77it/s] 69%|██████▊   | 1078/1572 [10:14<04:32,  1.81it/s] 69%|██████▊   | 1079/1572 [10:14<04:33,  1.81it/s] 69%|██████▊   | 1080/1572 [10:15<04:40,  1.75it/s] 69%|██████▉   | 1081/1572 [10:16<04:39,  1.76it/s] 69%|██████▉   | 1082/1572 [10:16<04:37,  1.77it/s] 69%|██████▉   | 1083/1572 [10:17<04:31,  1.80it/s] 69%|██████▉   | 1084/1572 [10:17<04:21,  1.86it/s] 69%|██████▉   | 1085/1572 [10:18<04:20,  1.87it/s] 69%|██████▉   | 1086/1572 [10:18<04:21,  1.86it/s] 69%|██████▉   | 1087/1572 [10:19<04:22,  1.85it/s] 69%|██████▉   | 1088/1572 [10:19<04:24,  1.83it/s] 69%|██████▉   | 1089/1572 [10:20<04:20,  1.85it/s] 69%|██████▉   | 1090/1572 [10:20<04:21,  1.84it/s] 69%|██████▉   | 1091/1572 [10:21<04:35,  1.75it/s] 69%|██████▉   | 1092/1572 [10:22<04:38,  1.72it/s] 70%|██████▉   | 1093/1572 [10:22<04:35,  1.74it/s] 70%|██████▉   | 1094/1572 [10:23<04:32,  1.75it/s] 70%|██████▉   | 1095/1572 [10:23<04:28,  1.77it/s] 70%|██████▉   | 1096/1572 [10:24<04:28,  1.77it/s] 70%|██████▉   | 1097/1572 [10:25<04:25,  1.79it/s] 70%|██████▉   | 1098/1572 [10:25<04:30,  1.75it/s] 70%|██████▉   | 1099/1572 [10:26<04:39,  1.69it/s] 70%|██████▉   | 1100/1572 [10:26<04:35,  1.71it/s] 70%|███████   | 1101/1572 [10:27<04:28,  1.76it/s] 70%|███████   | 1102/1572 [10:27<04:31,  1.73it/s] 70%|███████   | 1103/1572 [10:28<04:33,  1.71it/s] 70%|███████   | 1104/1572 [10:29<04:28,  1.74it/s] 70%|███████   | 1105/1572 [10:29<04:22,  1.78it/s] 70%|███████   | 1106/1572 [10:30<04:20,  1.79it/s] 70%|███████   | 1107/1572 [10:30<04:18,  1.80it/s] 70%|███████   | 1108/1572 [10:31<04:14,  1.83it/s] 71%|███████   | 1109/1572 [10:31<03:59,  1.93it/s] 71%|███████   | 1110/1572 [10:32<03:56,  1.95it/s] 71%|███████   | 1111/1572 [10:32<04:03,  1.90it/s] 71%|███████   | 1112/1572 [10:33<04:03,  1.89it/s] 71%|███████   | 1113/1572 [10:33<04:02,  1.89it/s] 71%|███████   | 1114/1572 [10:34<04:11,  1.82it/s] 71%|███████   | 1115/1572 [10:34<04:04,  1.87it/s] 71%|███████   | 1116/1572 [10:35<04:03,  1.88it/s] 71%|███████   | 1117/1572 [10:36<04:12,  1.80it/s] 71%|███████   | 1118/1572 [10:36<04:11,  1.81it/s] 71%|███████   | 1119/1572 [10:37<04:11,  1.80it/s] 71%|███████   | 1120/1572 [10:37<04:06,  1.83it/s] 71%|███████▏  | 1121/1572 [10:38<04:03,  1.85it/s] 71%|███████▏  | 1122/1572 [10:38<04:20,  1.73it/s] 71%|███████▏  | 1123/1572 [10:39<04:29,  1.67it/s] 72%|███████▏  | 1124/1572 [10:40<04:19,  1.72it/s] 72%|███████▏  | 1125/1572 [10:40<04:11,  1.78it/s] 72%|███████▏  | 1126/1572 [10:41<03:59,  1.86it/s] 72%|███████▏  | 1127/1572 [10:41<03:57,  1.88it/s] 72%|███████▏  | 1128/1572 [10:42<03:56,  1.88it/s] 72%|███████▏  | 1129/1572 [10:42<04:05,  1.80it/s] 72%|███████▏  | 1130/1572 [10:43<04:05,  1.80it/s] 72%|███████▏  | 1131/1572 [10:43<04:04,  1.81it/s] 72%|███████▏  | 1132/1572 [10:44<04:00,  1.83it/s] 72%|███████▏  | 1133/1572 [10:44<04:01,  1.82it/s] 72%|███████▏  | 1134/1572 [10:45<04:09,  1.76it/s] 72%|███████▏  | 1135/1572 [10:46<04:04,  1.79it/s] 72%|███████▏  | 1136/1572 [10:46<04:02,  1.80it/s] 72%|███████▏  | 1137/1572 [10:47<03:51,  1.88it/s] 72%|███████▏  | 1138/1572 [10:47<03:54,  1.85it/s] 72%|███████▏  | 1139/1572 [10:48<03:51,  1.87it/s] 73%|███████▎  | 1140/1572 [10:48<03:59,  1.81it/s] 73%|███████▎  | 1141/1572 [10:49<03:56,  1.82it/s] 73%|███████▎  | 1142/1572 [10:49<03:53,  1.84it/s] 73%|███████▎  | 1143/1572 [10:50<03:50,  1.86it/s] 73%|███████▎  | 1144/1572 [10:50<03:58,  1.79it/s] 73%|███████▎  | 1145/1572 [10:51<03:50,  1.85it/s] 73%|███████▎  | 1146/1572 [10:52<03:58,  1.79it/s] 73%|███████▎  | 1147/1572 [10:52<03:55,  1.81it/s] 73%|███████▎  | 1148/1572 [10:53<04:09,  1.70it/s] 73%|███████▎  | 1149/1572 [10:53<04:01,  1.76it/s] 73%|███████▎  | 1150/1572 [10:54<03:58,  1.77it/s] 73%|███████▎  | 1151/1572 [10:54<03:55,  1.79it/s] 73%|███████▎  | 1152/1572 [10:55<03:53,  1.80it/s] 73%|███████▎  | 1153/1572 [10:56<03:53,  1.80it/s] 73%|███████▎  | 1154/1572 [10:56<03:50,  1.82it/s] 73%|███████▎  | 1155/1572 [10:57<03:45,  1.85it/s] 74%|███████▎  | 1156/1572 [10:57<03:44,  1.85it/s] 74%|███████▎  | 1157/1572 [10:58<03:56,  1.75it/s] 74%|███████▎  | 1158/1572 [10:58<03:48,  1.82it/s] 74%|███████▎  | 1159/1572 [10:59<03:38,  1.89it/s] 74%|███████▍  | 1160/1572 [10:59<03:46,  1.82it/s] 74%|███████▍  | 1161/1572 [11:00<03:53,  1.76it/s] 74%|███████▍  | 1162/1572 [11:01<04:01,  1.70it/s] 74%|███████▍  | 1163/1572 [11:01<03:58,  1.72it/s] 74%|███████▍  | 1164/1572 [11:02<03:59,  1.70it/s] 74%|███████▍  | 1165/1572 [11:02<03:46,  1.80it/s] 74%|███████▍  | 1166/1572 [11:03<03:42,  1.82it/s] 74%|███████▍  | 1167/1572 [11:03<03:42,  1.82it/s] 74%|███████▍  | 1168/1572 [11:04<03:39,  1.84it/s] 74%|███████▍  | 1169/1572 [11:04<03:45,  1.79it/s] 74%|███████▍  | 1170/1572 [11:05<03:43,  1.80it/s] 74%|███████▍  | 1171/1572 [11:06<03:47,  1.76it/s] 75%|███████▍  | 1172/1572 [11:06<03:52,  1.72it/s] 75%|███████▍  | 1173/1572 [11:07<03:45,  1.77it/s] 75%|███████▍  | 1174/1572 [11:07<03:49,  1.73it/s] 75%|███████▍  | 1175/1572 [11:08<03:43,  1.78it/s] 75%|███████▍  | 1176/1572 [11:08<03:46,  1.75it/s] 75%|███████▍  | 1177/1572 [11:09<03:44,  1.76it/s] 75%|███████▍  | 1178/1572 [11:10<03:52,  1.69it/s] 75%|███████▌  | 1179/1572 [11:10<03:58,  1.64it/s] 75%|███████▌  | 1180/1572 [11:11<03:52,  1.68it/s] 75%|███████▌  | 1181/1572 [11:12<03:57,  1.64it/s] 75%|███████▌  | 1182/1572 [11:12<03:51,  1.68it/s] 75%|███████▌  | 1183/1572 [11:13<03:56,  1.65it/s] 75%|███████▌  | 1184/1572 [11:13<03:56,  1.64it/s] 75%|███████▌  | 1185/1572 [11:14<03:56,  1.64it/s] 75%|███████▌  | 1186/1572 [11:14<03:42,  1.73it/s] 76%|███████▌  | 1187/1572 [11:15<03:40,  1.75it/s] 76%|███████▌  | 1188/1572 [11:16<03:42,  1.72it/s] 76%|███████▌  | 1189/1572 [11:16<03:39,  1.75it/s] 76%|███████▌  | 1190/1572 [11:17<03:30,  1.81it/s] 76%|███████▌  | 1191/1572 [11:17<03:23,  1.87it/s] 76%|███████▌  | 1192/1572 [11:18<03:25,  1.84it/s] 76%|███████▌  | 1193/1572 [11:18<03:20,  1.89it/s] 76%|███████▌  | 1194/1572 [11:19<03:29,  1.81it/s] 76%|███████▌  | 1195/1572 [11:19<03:22,  1.86it/s] 76%|███████▌  | 1196/1572 [11:20<03:17,  1.90it/s] 76%|███████▌  | 1197/1572 [11:20<03:18,  1.89it/s] 76%|███████▌  | 1198/1572 [11:21<03:25,  1.82it/s] 76%|███████▋  | 1199/1572 [11:22<03:23,  1.83it/s] 76%|███████▋  | 1200/1572 [11:22<03:15,  1.90it/s] 76%|███████▋  | 1201/1572 [11:23<03:21,  1.84it/s] 76%|███████▋  | 1202/1572 [11:23<03:27,  1.78it/s] 77%|███████▋  | 1203/1572 [11:24<03:42,  1.66it/s] 77%|███████▋  | 1204/1572 [11:24<03:34,  1.71it/s] 77%|███████▋  | 1205/1572 [11:25<03:29,  1.76it/s] 77%|███████▋  | 1206/1572 [11:25<03:26,  1.77it/s] 77%|███████▋  | 1207/1572 [11:26<03:22,  1.80it/s] 77%|███████▋  | 1208/1572 [11:27<03:22,  1.80it/s] 77%|███████▋  | 1209/1572 [11:27<03:21,  1.80it/s] 77%|███████▋  | 1210/1572 [11:28<03:25,  1.77it/s] 77%|███████▋  | 1211/1572 [11:28<03:17,  1.82it/s] 77%|███████▋  | 1212/1572 [11:29<03:27,  1.73it/s] 77%|███████▋  | 1213/1572 [11:29<03:25,  1.74it/s] 77%|███████▋  | 1214/1572 [11:30<03:23,  1.76it/s] 77%|███████▋  | 1215/1572 [11:31<03:27,  1.72it/s] 77%|███████▋  | 1216/1572 [11:31<03:33,  1.67it/s] 77%|███████▋  | 1217/1572 [11:32<03:27,  1.71it/s] 77%|███████▋  | 1218/1572 [11:32<03:24,  1.73it/s] 78%|███████▊  | 1219/1572 [11:33<03:26,  1.71it/s] 78%|███████▊  | 1220/1572 [11:34<03:28,  1.69it/s] 78%|███████▊  | 1221/1572 [11:34<03:22,  1.74it/s] 78%|███████▊  | 1222/1572 [11:35<03:17,  1.77it/s] 78%|███████▊  | 1223/1572 [11:35<03:20,  1.74it/s] 78%|███████▊  | 1224/1572 [11:36<03:18,  1.75it/s] 78%|███████▊  | 1225/1572 [11:36<03:10,  1.82it/s] 78%|███████▊  | 1226/1572 [11:37<03:10,  1.81it/s] 78%|███████▊  | 1227/1572 [11:38<03:26,  1.67it/s] 78%|███████▊  | 1228/1572 [11:38<03:20,  1.72it/s] 78%|███████▊  | 1229/1572 [11:39<03:22,  1.69it/s] 78%|███████▊  | 1230/1572 [11:39<03:19,  1.71it/s] 78%|███████▊  | 1231/1572 [11:40<03:10,  1.79it/s] 78%|███████▊  | 1232/1572 [11:40<03:07,  1.81it/s] 78%|███████▊  | 1233/1572 [11:41<03:11,  1.77it/s] 78%|███████▊  | 1234/1572 [11:41<03:07,  1.80it/s] 79%|███████▊  | 1235/1572 [11:42<03:16,  1.72it/s] 79%|███████▊  | 1236/1572 [11:43<03:18,  1.69it/s] 79%|███████▊  | 1237/1572 [11:43<03:12,  1.74it/s] 79%|███████▉  | 1238/1572 [11:44<03:07,  1.78it/s] 79%|███████▉  | 1239/1572 [11:44<03:04,  1.80it/s] 79%|███████▉  | 1240/1572 [11:45<03:08,  1.76it/s] 79%|███████▉  | 1241/1572 [11:45<03:04,  1.80it/s] 79%|███████▉  | 1242/1572 [11:46<03:01,  1.82it/s] 79%|███████▉  | 1243/1572 [11:47<02:58,  1.84it/s] 79%|███████▉  | 1244/1572 [11:47<02:59,  1.82it/s] 79%|███████▉  | 1245/1572 [11:48<02:57,  1.84it/s] 79%|███████▉  | 1246/1572 [11:48<02:56,  1.85it/s] 79%|███████▉  | 1247/1572 [11:49<02:53,  1.87it/s] 79%|███████▉  | 1248/1572 [11:49<02:53,  1.87it/s] 79%|███████▉  | 1249/1572 [11:50<02:53,  1.86it/s] 80%|███████▉  | 1250/1572 [11:50<02:52,  1.86it/s] 80%|███████▉  | 1251/1572 [11:51<02:54,  1.84it/s] 80%|███████▉  | 1252/1572 [11:51<02:58,  1.79it/s] 80%|███████▉  | 1253/1572 [11:52<02:58,  1.79it/s] 80%|███████▉  | 1254/1572 [11:53<03:01,  1.75it/s] 80%|███████▉  | 1255/1572 [11:53<03:03,  1.72it/s] 80%|███████▉  | 1256/1572 [11:54<03:12,  1.64it/s] 80%|███████▉  | 1257/1572 [11:54<03:07,  1.68it/s] 80%|████████  | 1258/1572 [11:55<02:57,  1.77it/s] 80%|████████  | 1259/1572 [11:56<02:56,  1.77it/s] 80%|████████  | 1260/1572 [11:56<02:53,  1.80it/s] 80%|████████  | 1261/1572 [11:57<02:51,  1.82it/s] 80%|████████  | 1262/1572 [11:57<02:54,  1.77it/s] 80%|████████  | 1263/1572 [11:58<02:57,  1.74it/s] 80%|████████  | 1264/1572 [11:58<02:53,  1.78it/s] 80%|████████  | 1265/1572 [11:59<02:52,  1.78it/s] 81%|████████  | 1266/1572 [12:00<03:07,  1.63it/s] 81%|████████  | 1267/1572 [12:00<03:02,  1.67it/s] 81%|████████  | 1268/1572 [12:01<02:55,  1.73it/s] 81%|████████  | 1269/1572 [12:01<02:53,  1.75it/s] 81%|████████  | 1270/1572 [12:02<02:55,  1.72it/s] 81%|████████  | 1271/1572 [12:02<02:52,  1.75it/s] 81%|████████  | 1272/1572 [12:03<02:58,  1.68it/s] 81%|████████  | 1273/1572 [12:04<02:51,  1.74it/s] 81%|████████  | 1274/1572 [12:04<02:52,  1.72it/s] 81%|████████  | 1275/1572 [12:05<02:49,  1.75it/s] 81%|████████  | 1276/1572 [12:05<02:45,  1.79it/s] 81%|████████  | 1277/1572 [12:06<02:45,  1.78it/s] 81%|████████▏ | 1278/1572 [12:06<02:45,  1.78it/s] 81%|████████▏ | 1279/1572 [12:07<02:47,  1.75it/s] 81%|████████▏ | 1280/1572 [12:08<02:46,  1.75it/s] 81%|████████▏ | 1281/1572 [12:08<02:48,  1.73it/s] 82%|████████▏ | 1282/1572 [12:09<02:49,  1.71it/s] 82%|████████▏ | 1283/1572 [12:09<02:41,  1.79it/s] 82%|████████▏ | 1284/1572 [12:10<02:47,  1.72it/s] 82%|████████▏ | 1285/1572 [12:10<02:43,  1.75it/s] 82%|████████▏ | 1286/1572 [12:11<02:42,  1.76it/s] 82%|████████▏ | 1287/1572 [12:12<02:41,  1.76it/s] 82%|████████▏ | 1288/1572 [12:12<02:39,  1.78it/s] 82%|████████▏ | 1289/1572 [12:13<02:31,  1.86it/s] 82%|████████▏ | 1290/1572 [12:13<02:37,  1.79it/s] 82%|████████▏ | 1291/1572 [12:14<02:41,  1.75it/s] 82%|████████▏ | 1292/1572 [12:14<02:36,  1.79it/s] 82%|████████▏ | 1293/1572 [12:15<02:30,  1.85it/s] 82%|████████▏ | 1294/1572 [12:15<02:38,  1.76it/s] 82%|████████▏ | 1295/1572 [12:16<02:27,  1.87it/s] 82%|████████▏ | 1296/1572 [12:16<02:27,  1.87it/s] 83%|████████▎ | 1297/1572 [12:17<02:21,  1.94it/s] 83%|████████▎ | 1298/1572 [12:17<02:24,  1.90it/s] 83%|████████▎ | 1299/1572 [12:18<02:23,  1.90it/s] 83%|████████▎ | 1300/1572 [12:19<02:23,  1.89it/s] 83%|████████▎ | 1301/1572 [12:19<02:19,  1.95it/s] 83%|████████▎ | 1302/1572 [12:20<02:19,  1.94it/s] 83%|████████▎ | 1303/1572 [12:20<02:17,  1.96it/s] 83%|████████▎ | 1304/1572 [12:21<02:14,  1.99it/s] 83%|████████▎ | 1305/1572 [12:21<02:18,  1.92it/s] 83%|████████▎ | 1306/1572 [12:22<02:21,  1.89it/s] 83%|████████▎ | 1307/1572 [12:22<02:26,  1.81it/s] 83%|████████▎ | 1308/1572 [12:23<02:33,  1.72it/s] 83%|████████▎ | 1309/1572 [12:23<02:30,  1.75it/s] 83%|████████▎ | 1310/1572 [12:24<02:24,  1.82it/s] 83%|████████▎ | 1311/1572 [12:24<02:21,  1.84it/s] 83%|████████▎ | 1312/1572 [12:25<02:26,  1.78it/s] 84%|████████▎ | 1313/1572 [12:26<02:23,  1.80it/s] 84%|████████▎ | 1314/1572 [12:26<02:25,  1.77it/s] 84%|████████▎ | 1315/1572 [12:27<02:25,  1.77it/s] 84%|████████▎ | 1316/1572 [12:27<02:22,  1.80it/s] 84%|████████▍ | 1317/1572 [12:28<02:17,  1.86it/s] 84%|████████▍ | 1318/1572 [12:28<02:18,  1.83it/s] 84%|████████▍ | 1319/1572 [12:29<02:22,  1.78it/s] 84%|████████▍ | 1320/1572 [12:29<02:15,  1.86it/s] 84%|████████▍ | 1321/1572 [12:30<02:14,  1.87it/s] 84%|████████▍ | 1322/1572 [12:30<02:14,  1.86it/s] 84%|████████▍ | 1323/1572 [12:31<02:15,  1.83it/s] 84%|████████▍ | 1324/1572 [12:32<02:19,  1.78it/s] 84%|████████▍ | 1325/1572 [12:32<02:14,  1.84it/s] 84%|████████▍ | 1326/1572 [12:33<02:17,  1.79it/s] 84%|████████▍ | 1327/1572 [12:33<02:20,  1.75it/s] 84%|████████▍ | 1328/1572 [12:34<02:16,  1.78it/s] 85%|████████▍ | 1329/1572 [12:34<02:16,  1.78it/s] 85%|████████▍ | 1330/1572 [12:35<02:13,  1.81it/s] 85%|████████▍ | 1331/1572 [12:36<02:11,  1.83it/s] 85%|████████▍ | 1332/1572 [12:36<02:10,  1.84it/s] 85%|████████▍ | 1333/1572 [12:37<02:10,  1.83it/s] 85%|████████▍ | 1334/1572 [12:37<02:11,  1.81it/s] 85%|████████▍ | 1335/1572 [12:38<02:11,  1.80it/s] 85%|████████▍ | 1336/1572 [12:38<02:11,  1.79it/s] 85%|████████▌ | 1337/1572 [12:39<02:05,  1.87it/s] 85%|████████▌ | 1338/1572 [12:39<02:14,  1.74it/s] 85%|████████▌ | 1339/1572 [12:40<02:11,  1.78it/s] 85%|████████▌ | 1340/1572 [12:41<02:12,  1.75it/s] 85%|████████▌ | 1341/1572 [12:41<02:14,  1.71it/s] 85%|████████▌ | 1342/1572 [12:42<02:11,  1.75it/s] 85%|████████▌ | 1343/1572 [12:42<02:10,  1.76it/s] 85%|████████▌ | 1344/1572 [12:43<02:04,  1.83it/s] 86%|████████▌ | 1345/1572 [12:43<02:00,  1.88it/s] 86%|████████▌ | 1346/1572 [12:44<01:59,  1.89it/s] 86%|████████▌ | 1347/1572 [12:44<01:59,  1.89it/s] 86%|████████▌ | 1348/1572 [12:45<02:03,  1.81it/s] 86%|████████▌ | 1349/1572 [12:45<02:01,  1.83it/s] 86%|████████▌ | 1350/1572 [12:46<01:59,  1.85it/s] 86%|████████▌ | 1351/1572 [12:47<02:06,  1.75it/s] 86%|████████▌ | 1352/1572 [12:47<02:01,  1.82it/s] 86%|████████▌ | 1353/1572 [12:48<01:56,  1.87it/s] 86%|████████▌ | 1354/1572 [12:48<02:01,  1.80it/s] 86%|████████▌ | 1355/1572 [12:49<02:01,  1.79it/s] 86%|████████▋ | 1356/1572 [12:49<01:58,  1.82it/s] 86%|████████▋ | 1357/1572 [12:50<01:57,  1.83it/s] 86%|████████▋ | 1358/1572 [12:50<01:57,  1.82it/s] 86%|████████▋ | 1359/1572 [12:51<01:55,  1.84it/s] 87%|████████▋ | 1360/1572 [12:51<01:52,  1.88it/s] 87%|████████▋ | 1361/1572 [12:52<01:54,  1.85it/s] 87%|████████▋ | 1362/1572 [12:53<01:55,  1.82it/s] 87%|████████▋ | 1363/1572 [12:53<01:57,  1.77it/s] 87%|████████▋ | 1364/1572 [12:54<02:02,  1.70it/s] 87%|████████▋ | 1365/1572 [12:54<01:59,  1.73it/s] 87%|████████▋ | 1366/1572 [12:55<01:55,  1.78it/s] 87%|████████▋ | 1367/1572 [12:55<01:53,  1.81it/s] 87%|████████▋ | 1368/1572 [12:56<01:51,  1.84it/s] 87%|████████▋ | 1369/1572 [12:57<01:51,  1.82it/s] 87%|████████▋ | 1370/1572 [12:57<01:47,  1.87it/s] 87%|████████▋ | 1371/1572 [12:58<01:48,  1.85it/s] 87%|████████▋ | 1372/1572 [12:58<01:51,  1.80it/s] 87%|████████▋ | 1373/1572 [12:59<01:56,  1.71it/s] 87%|████████▋ | 1374/1572 [12:59<01:57,  1.69it/s] 87%|████████▋ | 1375/1572 [13:00<01:51,  1.77it/s] 88%|████████▊ | 1376/1572 [13:01<01:50,  1.77it/s] 88%|████████▊ | 1377/1572 [13:01<01:52,  1.74it/s] 88%|████████▊ | 1378/1572 [13:02<01:49,  1.78it/s] 88%|████████▊ | 1379/1572 [13:02<01:48,  1.78it/s] 88%|████████▊ | 1380/1572 [13:03<01:45,  1.81it/s] 88%|████████▊ | 1381/1572 [13:03<01:42,  1.87it/s] 88%|████████▊ | 1382/1572 [13:04<01:39,  1.90it/s] 88%|████████▊ | 1383/1572 [13:04<01:39,  1.90it/s] 88%|████████▊ | 1384/1572 [13:05<01:37,  1.93it/s] 88%|████████▊ | 1385/1572 [13:05<01:37,  1.91it/s] 88%|████████▊ | 1386/1572 [13:06<01:37,  1.91it/s] 88%|████████▊ | 1387/1572 [13:06<01:40,  1.83it/s] 88%|████████▊ | 1388/1572 [13:07<01:40,  1.82it/s] 88%|████████▊ | 1389/1572 [13:08<01:40,  1.82it/s] 88%|████████▊ | 1390/1572 [13:08<01:38,  1.84it/s] 88%|████████▊ | 1391/1572 [13:09<01:39,  1.82it/s] 89%|████████▊ | 1392/1572 [13:09<01:39,  1.80it/s] 89%|████████▊ | 1393/1572 [13:10<01:41,  1.76it/s] 89%|████████▊ | 1394/1572 [13:10<01:40,  1.76it/s] 89%|████████▊ | 1395/1572 [13:11<01:42,  1.73it/s] 89%|████████▉ | 1396/1572 [13:12<01:41,  1.74it/s] 89%|████████▉ | 1397/1572 [13:12<01:39,  1.76it/s] 89%|████████▉ | 1398/1572 [13:13<01:38,  1.77it/s] 89%|████████▉ | 1399/1572 [13:13<01:39,  1.74it/s] 89%|████████▉ | 1400/1572 [13:14<01:37,  1.76it/s] 89%|████████▉ | 1401/1572 [13:14<01:37,  1.76it/s] 89%|████████▉ | 1402/1572 [13:15<01:34,  1.79it/s] 89%|████████▉ | 1403/1572 [13:15<01:34,  1.80it/s] 89%|████████▉ | 1404/1572 [13:16<01:32,  1.82it/s] 89%|████████▉ | 1405/1572 [13:16<01:29,  1.86it/s] 89%|████████▉ | 1406/1572 [13:17<01:27,  1.90it/s] 90%|████████▉ | 1407/1572 [13:18<01:31,  1.81it/s] 90%|████████▉ | 1408/1572 [13:18<01:31,  1.80it/s] 90%|████████▉ | 1409/1572 [13:19<01:30,  1.80it/s] 90%|████████▉ | 1410/1572 [13:19<01:30,  1.79it/s] 90%|████████▉ | 1411/1572 [13:20<01:29,  1.79it/s] 90%|████████▉ | 1412/1572 [13:20<01:29,  1.80it/s] 90%|████████▉ | 1413/1572 [13:21<01:28,  1.80it/s] 90%|████████▉ | 1414/1572 [13:22<01:32,  1.72it/s] 90%|█████████ | 1415/1572 [13:22<01:27,  1.79it/s] 90%|█████████ | 1416/1572 [13:23<01:26,  1.80it/s] 90%|█████████ | 1417/1572 [13:23<01:25,  1.81it/s] 90%|█████████ | 1418/1572 [13:24<01:25,  1.81it/s] 90%|█████████ | 1419/1572 [13:24<01:26,  1.76it/s] 90%|█████████ | 1420/1572 [13:25<01:27,  1.73it/s] 90%|█████████ | 1421/1572 [13:26<01:27,  1.72it/s] 90%|█████████ | 1422/1572 [13:26<01:26,  1.74it/s] 91%|█████████ | 1423/1572 [13:27<01:24,  1.77it/s] 91%|█████████ | 1424/1572 [13:27<01:22,  1.79it/s] 91%|█████████ | 1425/1572 [13:28<01:21,  1.81it/s] 91%|█████████ | 1426/1572 [13:28<01:21,  1.80it/s] 91%|█████████ | 1427/1572 [13:29<01:26,  1.67it/s] 91%|█████████ | 1428/1572 [13:29<01:18,  1.82it/s] 91%|█████████ | 1429/1572 [13:30<01:17,  1.84it/s] 91%|█████████ | 1430/1572 [13:30<01:17,  1.82it/s] 91%|█████████ | 1431/1572 [13:31<01:19,  1.78it/s] 91%|█████████ | 1432/1572 [13:32<01:18,  1.78it/s] 91%|█████████ | 1433/1572 [13:32<01:21,  1.71it/s] 91%|█████████ | 1434/1572 [13:33<01:21,  1.70it/s] 91%|█████████▏| 1435/1572 [13:33<01:19,  1.73it/s] 91%|█████████▏| 1436/1572 [13:34<01:16,  1.77it/s] 91%|█████████▏| 1437/1572 [13:35<01:15,  1.78it/s] 91%|█████████▏| 1438/1572 [13:35<01:12,  1.86it/s] 92%|█████████▏| 1439/1572 [13:36<01:15,  1.76it/s] 92%|█████████▏| 1440/1572 [13:36<01:13,  1.79it/s] 92%|█████████▏| 1441/1572 [13:37<01:10,  1.87it/s] 92%|█████████▏| 1442/1572 [13:37<01:09,  1.87it/s] 92%|█████████▏| 1443/1572 [13:38<01:05,  1.97it/s] 92%|█████████▏| 1444/1572 [13:38<01:08,  1.86it/s] 92%|█████████▏| 1445/1572 [13:39<01:11,  1.78it/s] 92%|█████████▏| 1446/1572 [13:39<01:09,  1.81it/s] 92%|█████████▏| 1447/1572 [13:40<01:08,  1.82it/s] 92%|█████████▏| 1448/1572 [13:40<01:07,  1.84it/s] 92%|█████████▏| 1449/1572 [13:41<01:06,  1.85it/s] 92%|█████████▏| 1450/1572 [13:42<01:12,  1.69it/s] 92%|█████████▏| 1451/1572 [13:42<01:09,  1.75it/s] 92%|█████████▏| 1452/1572 [13:43<01:06,  1.81it/s] 92%|█████████▏| 1453/1572 [13:43<01:07,  1.77it/s] 92%|█████████▏| 1454/1572 [13:44<01:08,  1.73it/s] 93%|█████████▎| 1455/1572 [13:44<01:03,  1.85it/s] 93%|█████████▎| 1456/1572 [13:45<01:01,  1.90it/s] 93%|█████████▎| 1457/1572 [13:45<00:59,  1.93it/s] 93%|█████████▎| 1458/1572 [13:46<01:00,  1.88it/s] 93%|█████████▎| 1459/1572 [13:47<01:01,  1.85it/s] 93%|█████████▎| 1460/1572 [13:47<00:59,  1.89it/s] 93%|█████████▎| 1461/1572 [13:48<00:58,  1.89it/s] 93%|█████████▎| 1462/1572 [13:48<00:58,  1.87it/s] 93%|█████████▎| 1463/1572 [13:49<00:58,  1.87it/s] 93%|█████████▎| 1464/1572 [13:49<00:57,  1.88it/s] 93%|█████████▎| 1465/1572 [13:50<00:57,  1.86it/s] 93%|█████████▎| 1466/1572 [13:50<00:57,  1.83it/s] 93%|█████████▎| 1467/1572 [13:51<01:00,  1.74it/s] 93%|█████████▎| 1468/1572 [13:52<01:03,  1.65it/s] 93%|█████████▎| 1469/1572 [13:52<00:59,  1.74it/s] 94%|█████████▎| 1470/1572 [13:53<00:55,  1.83it/s] 94%|█████████▎| 1471/1572 [13:53<00:58,  1.74it/s] 94%|█████████▎| 1472/1572 [13:54<00:57,  1.74it/s] 94%|█████████▎| 1473/1572 [13:54<00:55,  1.78it/s] 94%|█████████▍| 1474/1572 [13:55<00:56,  1.75it/s] 94%|█████████▍| 1475/1572 [13:56<00:56,  1.71it/s] 94%|█████████▍| 1476/1572 [13:56<00:53,  1.78it/s] 94%|█████████▍| 1477/1572 [13:57<00:53,  1.79it/s] 94%|█████████▍| 1478/1572 [13:57<00:52,  1.80it/s] 94%|█████████▍| 1479/1572 [13:58<00:50,  1.85it/s] 94%|█████████▍| 1480/1572 [13:58<00:49,  1.87it/s] 94%|█████████▍| 1481/1572 [13:59<00:48,  1.87it/s] 94%|█████████▍| 1482/1572 [13:59<00:48,  1.85it/s] 94%|█████████▍| 1483/1572 [14:00<00:48,  1.83it/s] 94%|█████████▍| 1484/1572 [14:00<00:47,  1.84it/s] 94%|█████████▍| 1485/1572 [14:01<00:47,  1.85it/s] 95%|█████████▍| 1486/1572 [14:02<00:49,  1.75it/s] 95%|█████████▍| 1487/1572 [14:02<00:48,  1.76it/s] 95%|█████████▍| 1488/1572 [14:03<00:45,  1.83it/s] 95%|█████████▍| 1489/1572 [14:03<00:45,  1.84it/s] 95%|█████████▍| 1490/1572 [14:04<00:46,  1.75it/s] 95%|█████████▍| 1491/1572 [14:04<00:45,  1.79it/s] 95%|█████████▍| 1492/1572 [14:05<00:43,  1.83it/s] 95%|█████████▍| 1493/1572 [14:05<00:41,  1.90it/s] 95%|█████████▌| 1494/1572 [14:06<00:41,  1.90it/s] 95%|█████████▌| 1495/1572 [14:06<00:39,  1.95it/s] 95%|█████████▌| 1496/1572 [14:07<00:40,  1.89it/s] 95%|█████████▌| 1497/1572 [14:07<00:41,  1.81it/s] 95%|█████████▌| 1498/1572 [14:08<00:39,  1.86it/s] 95%|█████████▌| 1499/1572 [14:09<00:39,  1.83it/s] 95%|█████████▌| 1500/1572 [14:09<00:38,  1.86it/s] 95%|█████████▌| 1501/1572 [14:10<00:39,  1.80it/s] 96%|█████████▌| 1502/1572 [14:10<00:38,  1.80it/s] 96%|█████████▌| 1503/1572 [14:11<00:38,  1.82it/s] 96%|█████████▌| 1504/1572 [14:11<00:37,  1.82it/s] 96%|█████████▌| 1505/1572 [14:12<00:36,  1.84it/s] 96%|█████████▌| 1506/1572 [14:12<00:35,  1.87it/s] 96%|█████████▌| 1507/1572 [14:13<00:34,  1.87it/s] 96%|█████████▌| 1508/1572 [14:14<00:36,  1.76it/s] 96%|█████████▌| 1509/1572 [14:14<00:35,  1.76it/s] 96%|█████████▌| 1510/1572 [14:15<00:36,  1.69it/s] 96%|█████████▌| 1511/1572 [14:15<00:37,  1.64it/s] 96%|█████████▌| 1512/1572 [14:16<00:35,  1.71it/s] 96%|█████████▌| 1513/1572 [14:16<00:32,  1.79it/s] 96%|█████████▋| 1514/1572 [14:17<00:32,  1.79it/s] 96%|█████████▋| 1515/1572 [14:18<00:31,  1.79it/s] 96%|█████████▋| 1516/1572 [14:18<00:31,  1.80it/s] 97%|█████████▋| 1517/1572 [14:19<00:28,  1.91it/s] 97%|█████████▋| 1518/1572 [14:19<00:28,  1.91it/s] 97%|█████████▋| 1519/1572 [14:20<00:27,  1.91it/s] 97%|█████████▋| 1520/1572 [14:20<00:27,  1.91it/s] 97%|█████████▋| 1521/1572 [14:21<00:27,  1.88it/s] 97%|█████████▋| 1522/1572 [14:21<00:26,  1.85it/s] 97%|█████████▋| 1523/1572 [14:22<00:26,  1.83it/s] 97%|█████████▋| 1524/1572 [14:22<00:26,  1.81it/s] 97%|█████████▋| 1525/1572 [14:23<00:25,  1.83it/s] 97%|█████████▋| 1526/1572 [14:23<00:25,  1.78it/s] 97%|█████████▋| 1527/1572 [14:24<00:25,  1.77it/s] 97%|█████████▋| 1528/1572 [14:25<00:24,  1.77it/s] 97%|█████████▋| 1529/1572 [14:25<00:24,  1.77it/s] 97%|█████████▋| 1530/1572 [14:26<00:23,  1.78it/s] 97%|█████████▋| 1531/1572 [14:26<00:22,  1.84it/s] 97%|█████████▋| 1532/1572 [14:27<00:20,  1.91it/s] 98%|█████████▊| 1533/1572 [14:27<00:19,  2.00it/s] 98%|█████████▊| 1534/1572 [14:28<00:19,  2.00it/s] 98%|█████████▊| 1535/1572 [14:28<00:18,  1.96it/s] 98%|█████████▊| 1536/1572 [14:29<00:19,  1.85it/s] 98%|█████████▊| 1537/1572 [14:29<00:20,  1.75it/s] 98%|█████████▊| 1538/1572 [14:30<00:20,  1.67it/s] 98%|█████████▊| 1539/1572 [14:31<00:19,  1.67it/s] 98%|█████████▊| 1540/1572 [14:31<00:18,  1.75it/s] 98%|█████████▊| 1541/1572 [14:32<00:17,  1.77it/s] 98%|█████████▊| 1542/1572 [14:32<00:16,  1.77it/s] 98%|█████████▊| 1543/1572 [14:33<00:15,  1.85it/s] 98%|█████████▊| 1544/1572 [14:33<00:15,  1.75it/s] 98%|█████████▊| 1545/1572 [14:34<00:14,  1.84it/s] 98%|█████████▊| 1546/1572 [14:34<00:13,  1.86it/s] 98%|█████████▊| 1547/1572 [14:35<00:13,  1.87it/s] 98%|█████████▊| 1548/1572 [14:36<00:13,  1.84it/s] 99%|█████████▊| 1549/1572 [14:36<00:12,  1.78it/s] 99%|█████████▊| 1550/1572 [14:37<00:12,  1.81it/s] 99%|█████████▊| 1551/1572 [14:37<00:11,  1.80it/s] 99%|█████████▊| 1552/1572 [14:38<00:11,  1.76it/s] 99%|█████████▉| 1553/1572 [14:38<00:10,  1.79it/s] 99%|█████████▉| 1554/1572 [14:39<00:09,  1.85it/s] 99%|█████████▉| 1555/1572 [14:39<00:09,  1.83it/s] 99%|█████████▉| 1556/1572 [14:40<00:08,  1.90it/s] 99%|█████████▉| 1557/1572 [14:40<00:07,  1.95it/s] 99%|█████████▉| 1558/1572 [14:41<00:07,  1.82it/s] 99%|█████████▉| 1559/1572 [14:42<00:07,  1.75it/s] 99%|█████████▉| 1560/1572 [14:42<00:06,  1.82it/s] 99%|█████████▉| 1561/1572 [14:43<00:05,  1.87it/s] 99%|█████████▉| 1562/1572 [14:43<00:05,  1.84it/s] 99%|█████████▉| 1563/1572 [14:44<00:04,  1.83it/s] 99%|█████████▉| 1564/1572 [14:44<00:04,  1.84it/s]100%|█████████▉| 1565/1572 [14:45<00:03,  1.85it/s]100%|█████████▉| 1566/1572 [14:45<00:03,  1.85it/s]100%|█████████▉| 1567/1572 [14:46<00:02,  1.82it/s]100%|█████████▉| 1568/1572 [14:47<00:02,  1.77it/s]100%|█████████▉| 1569/1572 [14:47<00:01,  1.74it/s]100%|█████████▉| 1570/1572 [14:48<00:01,  1.77it/s]100%|█████████▉| 1571/1572 [14:48<00:00,  1.78it/s]100%|██████████| 1572/1572 [14:49<00:00,  1.81it/s]03/13/2024 21:23:36 - INFO - __main__ - epoch 2: {'accuracy': 0.7352941176470589, 'f1': 0.8193979933110368}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-03-13 21:23:36,849] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7/config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7/model.safetensors.index.json.
tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7/tokenizer_config.json
Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7/special_tokens_map.json
100%|██████████| 1572/1572 [15:19<00:00,  1.71it/s]
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: accelerate <command> [<args>]
Accelerate CLI tool: error: unrecognized arguments: --nproc_per_node
srun: error: v010: task 0: Exited with exit code 2
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 230, in main
    Accelerator(log_with=args.report_to, project_dir=args.output_dir, fp16=True, deepspeed_plugin="config.yaml") if args.with_tracking else Accelerator(fp16=True, deepspeed_plugin="config.yaml")
TypeError: __init__() got an unexpected keyword argument 'fp16'
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/jet/home/mmisra/miniconda3/envs/benchmark/bin/python', 'run_glue_no_trainer.py', '--model_name_or_path', 'datajuicer/LLaMA-1B-dj-refine-100B', '--task_name', 'mrpc', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--output_dir', '/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpcds8']' returned non-zero exit status 1.
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 665, in <module>
    main()
  File "run_glue_no_trainer.py", line 230, in main
    Accelerator(log_with=args.report_to, project_dir=args.output_dir, mixed_precision = 'fp16', deepspeed_plugin="config.yaml") if args.with_tracking else Accelerator(mixed_precision = 'fp16', deepspeed_plugin="config.yaml")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 296, in __init__
    assert isinstance(
AssertionError: `deepspeed_plugin` must be an `accelerate.utils.DeepSpeedPlugin` object.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/jet/home/mmisra/miniconda3/envs/benchmark/bin/python', 'run_glue_no_trainer.py', '--model_name_or_path', 'datajuicer/LLaMA-1B-dj-refine-100B', '--task_name', 'mrpc', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--output_dir', '/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpcds8']' returned non-zero exit status 1.
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:35:39 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 272, in main
    accelerator.wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 2351, in wait_for_everyone
    wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/other.py", line 118, in wait_for_everyone
    PartialState().wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 415, in wait_for_everyone
    torch.distributed.barrier()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 3428, in barrier
    opts.device = _get_pg_default_device(group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 644, in _get_pg_default_device
    group = group or _get_default_group()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 977, in _get_default_group
    raise ValueError(
ValueError: Default process group has not been initialized, please make sure to call init_process_group.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/jet/home/mmisra/miniconda3/envs/benchmark/bin/python', 'run_glue_no_trainer.py', '--model_name_or_path', 'datajuicer/LLaMA-1B-dj-refine-100B', '--task_name', 'mrpc', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--output_dir', '/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpcds8']' returned non-zero exit status 1.
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:37:38 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 272, in main
    accelerator.wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 2351, in wait_for_everyone
    wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/other.py", line 118, in wait_for_everyone
    PartialState().wait_for_everyone()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 415, in wait_for_everyone
    torch.distributed.barrier()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 3428, in barrier
    opts.device = _get_pg_default_device(group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 644, in _get_pg_default_device
    group = group or _get_default_group()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 977, in _get_default_group
    raise ValueError(
ValueError: Default process group has not been initialized, please make sure to call init_process_group.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1023, in launch_command
    simple_launcher(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/commands/launch.py", line 643, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/jet/home/mmisra/miniconda3/envs/benchmark/bin/python', 'run_glue_no_trainer.py', '--model_name_or_path', 'datajuicer/LLaMA-1B-dj-refine-100B', '--task_name', 'mrpc', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--output_dir', '/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpcds8']' returned non-zero exit status 1.
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:39:03,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:39:03,971] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:39:03,972] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:39:04 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 6730.58 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 7034.15 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 7182.18 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 7093.90 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6391.11 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 7089.15 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7154.93 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 7043.37 examples/s]
03/13/2024 21:39:10 - INFO - __main__ - Sample 2648 of the training set: {'input_ids': [1, 450, 2446, 8973, 4867, 674, 367, 746, 278, 2211, 29899, 17675, 479, 9434, 17507, 7475, 778, 967, 1147, 8977, 297, 7145, 29899, 29943, 3205, 653, 869, 1, 450, 8973, 525, 29879, 2211, 29899, 17675, 479, 9434, 17507, 471, 3806, 304, 2367, 967, 1147, 8977, 2446, 6339, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:39:10 - INFO - __main__ - Sample 2496 of the training set: {'input_ids': [1, 1205, 3271, 510, 886, 363, 1906, 13936, 1919, 408, 1532, 408, 278, 8542, 525, 29879, 29871, 29941, 5499, 26070, 1919, 29871, 29955, 386, 28398, 719, 17982, 1919, 505, 1286, 1063, 1400, 1112, 287, 297, 1753, 18639, 869, 1, 1205, 278, 28750, 310, 3271, 510, 886, 363, 1906, 13936, 1919, 408, 1532, 408, 278, 8542, 525, 29879, 29871, 29941, 5499, 26070, 1919, 29871, 29955, 386, 28398, 719, 17982, 1919, 338, 1286, 297, 25476, 568, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:39:10 - INFO - __main__ - Sample 3259 of the training set: {'input_ids': [1, 7803, 4038, 4307, 29885, 21079, 1497, 14189, 1446, 1795, 875, 625, 777, 8063, 550, 304, 2304, 263, 8818, 7251, 446, 869, 1, 7803, 512, 1049, 13939, 4307, 29885, 21079, 17845, 372, 1795, 367, 1950, 363, 14189, 1446, 304, 875, 625, 777, 8063, 550, 304, 2304, 263, 8818, 7251, 446, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:39:10 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1489, in _prepare_deepspeed
    raise ValueError(
ValueError: You cannot specify an optimizer in the config file and in the code at the same time. Please remove the optimizer from the config file or create `accelerate.utils.DummyOptim` in the code.
[2024-03-13 21:39:14,310] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31498) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:39:14
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31498)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:40:04,453] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:40:04,956] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:40:04,956] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:40:04 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 6128.20 examples/s]
03/13/2024 21:40:10 - INFO - __main__ - Sample 389 of the training set: {'input_ids': [1, 7849, 3064, 1919, 377, 6544, 284, 414, 5239, 278, 1139, 519, 5883, 3174, 304, 697, 310, 2211, 12176, 4797, 22965, 29560, 393, 11421, 4610, 1474, 599, 5883, 3174, 297, 278, 4234, 869, 1, 2180, 697, 1298, 297, 278, 9704, 1919, 263, 377, 6544, 18362, 723, 19417, 278, 1139, 519, 5883, 3174, 304, 697, 310, 2211, 12176, 4797, 14582, 393, 11421, 4610, 1474, 599, 5883, 3174, 297, 445, 4234, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:40:10 - INFO - __main__ - Sample 1867 of the training set: {'input_ids': [1, 13932, 3440, 1919, 278, 8973, 4845, 1312, 304, 8293, 8872, 2760, 26911, 14546, 17181, 15793, 18588, 525, 29879, 5929, 1338, 869, 1, 450, 20653, 363, 8872, 2760, 26911, 14546, 17181, 15793, 18588, 471, 3806, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:40:10 - INFO - __main__ - Sample 336 of the training set: {'input_ids': [1, 8011, 5337, 414, 750, 304, 3974, 1009, 7679, 1691, 515, 21006, 310, 17454, 3448, 1919, 411, 263, 3732, 29882, 2027, 6826, 261, 7934, 297, 263, 2011, 519, 12646, 15299, 869, 1, 8011, 5337, 414, 750, 304, 6826, 1009, 21283, 515, 21006, 310, 17963, 3448, 1919, 411, 263, 3732, 29882, 2027, 696, 3522, 6826, 261, 766, 2543, 3368, 408, 263, 2011, 519, 12646, 15299, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:40:10 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1489, in _prepare_deepspeed
    raise ValueError(
ValueError: You cannot specify an optimizer in the config file and in the code at the same time. Please remove the optimizer from the config file or create `accelerate.utils.DummyOptim` in the code.
[2024-03-13 21:40:15,594] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31661) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:40:15
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31661)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:40:44,287] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:40:44,791] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:40:44,791] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:40:44 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 6834.65 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 6953.95 examples/s]
03/13/2024 21:40:50 - INFO - __main__ - Sample 2228 of the training set: {'input_ids': [1, 6650, 1171, 8459, 304, 7910, 967, 12616, 368, 25227, 355, 304, 29871, 29906, 29945, 274, 1237, 263, 6232, 515, 29871, 29896, 29906, 274, 1237, 1919, 7537, 292, 7786, 8818, 13332, 362, 408, 263, 7601, 7329, 5742, 278, 4337, 869, 1, 512, 6124, 1919, 6650, 1171, 10425, 967, 12616, 368, 25227, 355, 304, 29871, 29906, 29945, 274, 1237, 263, 6232, 1919, 701, 515, 29871, 29896, 29941, 274, 1237, 9251, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:40:50 - INFO - __main__ - Sample 475 of the training set: {'input_ids': [1, 15733, 350, 4201, 10131, 1497, 670, 1487, 723, 679, 24081, 300, 541, 769, 2507, 2820, 322, 367, 18326, 670, 11062, 348, 2153, 869, 1, 2439, 1487, 723, 679, 24081, 300, 1919, 670, 16823, 1497, 1919, 541, 769, 2507, 2820, 322, 367, 18326, 670, 11062, 348, 2153, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:40:50 - INFO - __main__ - Sample 2527 of the training set: {'input_ids': [1, 739, 884, 298, 2859, 21353, 4481, 1132, 322, 20549, 21631, 304, 26987, 2143, 262, 19985, 470, 1791, 1247, 3864, 310, 395, 29871, 29896, 29955, 29945, 7284, 297, 16336, 11486, 2861, 2446, 1629, 869, 1, 319, 6040, 884, 756, 298, 2859, 21353, 4481, 1132, 322, 20549, 21631, 304, 14707, 3987, 363, 2143, 262, 19985, 470, 1791, 1247, 3864, 395, 29871, 29896, 29955, 29945, 7284, 310, 29871, 29896, 29900, 29889, 29945, 10151, 16336, 11486, 2861, 297, 3111, 29871, 29906, 29900, 29900, 29946, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:40:50 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1489, in _prepare_deepspeed
    raise ValueError(
ValueError: You cannot specify an optimizer in the config file and in the code at the same time. Please remove the optimizer from the config file or create `accelerate.utils.DummyOptim` in the code.
[2024-03-13 21:40:55,607] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31806) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:40:55
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31806)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 50, in <module>
    deepspeed_plugin = DeepSpeedPlugin("config.yaml")
  File "<string>", line 13, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/dataclasses.py", line 659, in __post_init__
    self.hf_ds_config = HfDeepSpeedConfig(self.hf_ds_config)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 49, in __init__
    config = json.load(f)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 11 column 5 (char 203)
[2024-03-13 21:41:42,788] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31942) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:41:42
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31942)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:42:03,072] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:42:04,017] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:42:04,018] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:42:04 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/13/2024 21:42:09 - INFO - __main__ - Sample 783 of the training set: {'input_ids': [1, 960, 19512, 5929, 1338, 1459, 29878, 728, 525, 29879, 364, 19478, 1919, 372, 723, 5040, 278, 17541, 328, 654, 1889, 322, 1033, 2125, 3196, 7378, 1919, 390, 548, 1497, 869, 1, 450, 25530, 723, 5040, 278, 17541, 328, 654, 1889, 322, 1033, 2125, 3196, 7378, 1919, 390, 548, 1497, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:42:09 - INFO - __main__ - Sample 1436 of the training set: {'input_ids': [1, 450, 22318, 1388, 29939, 20842, 2380, 8379, 29871, 29906, 29889, 29929, 29945, 1919, 470, 29871, 29900, 29889, 29906, 10151, 1919, 363, 278, 4723, 304, 29871, 29896, 29892, 29929, 29896, 29906, 29889, 29941, 29953, 1156, 380, 3774, 1847, 29871, 29941, 29955, 29889, 29955, 29947, 22600, 869, 1, 450, 22318, 1388, 29939, 321, 1463, 29871, 29900, 29889, 29896, 29945, 10151, 363, 278, 4723, 1156, 1023, 18942, 701, 11405, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:42:09 - INFO - __main__ - Sample 2794 of the training set: {'input_ids': [1, 24193, 1171, 5765, 12265, 514, 322, 383, 4174, 23056, 21628, 472, 278, 15050, 4515, 3250, 22514, 869, 1, 383, 4174, 9087, 5765, 12265, 514, 2225, 2247, 975, 22514, 27822, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:42:09 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1599, in _prepare_deepspeed
    optimizer = DeepSpeedCPUAdam(optimizer.param_groups, **defaults)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/adam/cpu_adam.py", line 94, in __init__
    self.ds_opt_adam = CPUAdamBuilder().load()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 479, in load
    return self.jit_load(verbose)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 510, in jit_load
    nvcc_args = self.strip_empty_entries(self.nvcc_args())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 713, in nvcc_args
    cuda_major, _ = installed_cuda_version()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 52, in installed_cuda_version
    output = subprocess.check_output([cuda_home + "/bin/nvcc", "-V"], universal_newlines=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 411, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 489, in run
    with Popen(*popenargs, **kwargs) as process:
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 854, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 1702, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'
Exception ignored in: <function DeepSpeedCPUAdam.__del__ at 0x147b59dcf8b0>
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/adam/cpu_adam.py", line 102, in __del__
    self.ds_opt_adam.destroy_adam(self.opt_id)
AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'
[2024-03-13 21:42:14,396] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32006) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:42:14
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32006)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:43:35,509] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:43:36,454] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:43:36,454] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:43:36 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'scheduler': {'type': 'WarmupLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/13/2024 21:43:43 - INFO - __main__ - Sample 741 of the training set: {'input_ids': [1, 376, 450, 27474, 526, 278, 9281, 297, 263, 3652, 310, 14231, 491, 278, 5874, 304, 4337, 1619, 273, 3034, 304, 2473, 29899, 22633, 1261, 25804, 322, 4797, 3022, 2638, 362, 869, 376, 1, 376, 450, 27474, 526, 278, 9281, 297, 263, 3652, 310, 14231, 491, 278, 5874, 304, 4337, 1619, 273, 3034, 17649, 304, 6674, 442, 29891, 1261, 25804, 322, 4797, 8265, 29883, 2638, 362, 1919, 376, 263, 5874, 3229, 1497, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:43:43 - INFO - __main__ - Sample 1162 of the training set: {'input_ids': [1, 16597, 3174, 526, 701, 304, 29871, 29945, 29900, 10151, 923, 7202, 297, 7400, 1135, 278, 3303, 3900, 1363, 310, 5874, 8666, 11761, 869, 1, 18007, 29899, 978, 5883, 3174, 297, 7400, 10331, 304, 367, 923, 7202, 1135, 297, 278, 3303, 3900, 1363, 310, 5874, 8666, 11761, 322, 263, 7853, 519, 14523, 6554, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:43:43 - INFO - __main__ - Sample 430 of the training set: {'input_ids': [1, 19573, 3391, 674, 7150, 263, 1629, 368, 4497, 653, 310, 395, 29871, 29896, 29900, 7284, 2298, 10814, 6394, 1919, 2326, 1076, 1075, 1048, 395, 29871, 29906, 29906, 29900, 29892, 29900, 29900, 29900, 263, 3748, 869, 1, 19573, 3391, 674, 24589, 263, 4723, 368, 4497, 653, 310, 1048, 395, 29871, 29906, 29896, 29941, 29892, 29900, 29900, 29900, 322, 2326, 29876, 395, 29871, 29896, 29900, 7284, 263, 1629, 1919, 2298, 10814, 6394, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:43:43 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1504, in _prepare_deepspeed
    raise ValueError(
ValueError: You cannot specify a scheduler in the config file and in the code at the same time. Please remove the scheduler from the config file or create `accelerate.utils.DummyScheduler` in the code.
[2024-03-13 21:43:46,833] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32210) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:43:46
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32210)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 50, in <module>
    deepspeed_plugin = DeepSpeedPlugin("config.yaml")
  File "<string>", line 13, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/dataclasses.py", line 659, in __post_init__
    self.hf_ds_config = HfDeepSpeedConfig(self.hf_ds_config)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 49, in __init__
    config = json.load(f)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 24 column 1 (char 565)
[2024-03-13 21:44:11,179] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32324) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:44:11
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32324)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 50, in <module>
    deepspeed_plugin = DeepSpeedPlugin("config.yaml")
  File "<string>", line 13, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/dataclasses.py", line 659, in __post_init__
    self.hf_ds_config = HfDeepSpeedConfig(self.hf_ds_config)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 49, in __init__
    config = json.load(f)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 24 column 1 (char 565)
[2024-03-13 21:44:33,308] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32393) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:44:33
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32393)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-03-13 21:45:18,388] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 21:45:18,883] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 21:45:18,883] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 21:45:18 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16
ds_config: {'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': True}, 'allgather_partitions': True, 'allgather_bucket_size': 200000000.0, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 200000000.0, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'bf16': {'enabled': False}}

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/13/2024 21:45:24 - INFO - __main__ - Sample 1944 of the training set: {'input_ids': [1, 376, 1205, 393, 947, 451, 2821, 963, 310, 278, 10788, 362, 304, 437, 4129, 1950, 304, 12566, 14175, 2638, 550, 1919, 322, 393, 338, 451, 825, 591, 525, 276, 8790, 869, 376, 1, 376, 1205, 393, 947, 451, 2821, 963, 310, 278, 23134, 304, 437, 4129, 1950, 304, 6260, 675, 7631, 713, 10311, 869, 376], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 21:45:24 - INFO - __main__ - Sample 2617 of the training set: {'input_ids': [1, 8502, 23306, 510, 1919, 402, 29923, 1192, 3847, 310, 405, 5371, 1192, 338, 884, 3595, 408, 263, 3109, 23644, 29875, 6288, 21000, 672, 9401, 304, 278, 4188, 267, 310, 14165, 29888, 1171, 470, 15225, 869, 1, 8502, 23306, 510, 1919, 4593, 26953, 338, 3595, 408, 263, 3109, 23644, 29875, 6288, 21000, 672, 9401, 411, 14165, 29888, 1171, 470, 15225, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 21:45:24 - INFO - __main__ - Sample 1671 of the training set: {'input_ids': [1, 1383, 1808, 29891, 525, 29879, 8034, 8967, 28728, 393, 29871, 29945, 29955, 29945, 29892, 29929, 29906, 29953, 1804, 3698, 505, 1063, 8967, 304, 1075, 869, 1, 1383, 1808, 29891, 525, 29879, 8034, 5492, 12608, 18139, 5683, 28728, 322, 1497, 2302, 583, 750, 8967, 21248, 29871, 29945, 29955, 29945, 29892, 29929, 29906, 29953, 1804, 3698, 577, 2215, 869], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 21:45:24 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (8).
Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 466, in main
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1220, in prepare
    result = self._prepare_deepspeed(*args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1599, in _prepare_deepspeed
    optimizer = DeepSpeedCPUAdam(optimizer.param_groups, **defaults)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/adam/cpu_adam.py", line 94, in __init__
    self.ds_opt_adam = CPUAdamBuilder().load()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 479, in load
    return self.jit_load(verbose)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 510, in jit_load
    nvcc_args = self.strip_empty_entries(self.nvcc_args())
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 713, in nvcc_args
    cuda_major, _ = installed_cuda_version()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/op_builder/builder.py", line 52, in installed_cuda_version
    output = subprocess.check_output([cuda_home + "/bin/nvcc", "-V"], universal_newlines=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 411, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 489, in run
    with Popen(*popenargs, **kwargs) as process:
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 854, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/subprocess.py", line 1702, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/cuda/bin/nvcc'
Exception ignored in: <function DeepSpeedCPUAdam.__del__ at 0x14d30eb0b8b0>
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/deepspeed/ops/adam/cpu_adam.py", line 102, in __del__
    self.ds_opt_adam.destroy_adam(self.opt_id)
AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'
[2024-03-13 21:45:29,235] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32580) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_21:45:29
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32580)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
