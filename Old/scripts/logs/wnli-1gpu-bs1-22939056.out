Wed Mar 13 22:51:38 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:16:00.0 Off |                    0 |
| N/A   33C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling
START TIME: Wed Mar 13 22:51:38 EDT 2024
MASTER_ADDR=v020 MASTER_PORT=6007 NUM_PROCESSES=1 GPUS_PER_NODE=1 NNODES=1 $SLURM_PROCID
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 22:51:52 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "wnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/635 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 635/635 [00:00<00:00, 2100.21 examples/s]Running tokenizer on dataset: 100%|██████████| 635/635 [00:00<00:00, 2074.85 examples/s]
Running tokenizer on dataset:   0%|          | 0/71 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 71/71 [00:00<00:00, 3185.90 examples/s]
Running tokenizer on dataset:   0%|          | 0/146 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 146/146 [00:00<00:00, 1440.16 examples/s]Running tokenizer on dataset: 100%|██████████| 146/146 [00:00<00:00, 1394.35 examples/s]
03/13/2024 22:53:51 - INFO - __main__ - Sample 19 of the training set: {'input_ids': [1, 3685, 20043, 701, 263, 11774, 304, 278, 11914, 29892, 541, 372, 471, 9391, 29892, 577, 540, 750, 304, 2317, 2012, 29889, 1, 450, 11914, 471, 9391, 29892, 577, 540, 750, 304, 2317, 2012, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 22:53:51 - INFO - __main__ - Sample 408 of the training set: {'input_ids': [1, 3685, 14455, 1716, 670, 385, 29895, 793, 322, 540, 29915, 29879, 22049, 411, 2181, 329, 6609, 29889, 1205, 263, 4098, 470, 577, 515, 1286, 896, 881, 367, 2253, 29889, 1, 450, 385, 29895, 793, 881, 367, 2253, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 22:53:51 - INFO - __main__ - Sample 295 of the training set: {'input_ids': [1, 306, 1304, 385, 2030, 17052, 304, 5941, 278, 889, 1607, 29892, 322, 769, 306, 1925, 372, 297, 278, 534, 1161, 29889, 1, 306, 1925, 278, 17052, 297, 278, 534, 1161, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 22:53:53 - INFO - __main__ - ***** Running training *****
03/13/2024 22:53:53 - INFO - __main__ -   Num examples = 635
03/13/2024 22:53:53 - INFO - __main__ -   Num Epochs = 3
03/13/2024 22:53:53 - INFO - __main__ -   Instantaneous batch size per device = 7
03/13/2024 22:53:53 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 7
03/13/2024 22:53:53 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 22:53:53 - INFO - __main__ -   Total optimization steps = 273
  0%|          | 0/273 [00:00<?, ?it/s]  0%|          | 1/273 [00:01<07:39,  1.69s/it]  1%|          | 2/273 [00:02<04:15,  1.06it/s]  1%|          | 3/273 [00:02<03:18,  1.36it/s]  1%|▏         | 4/273 [00:03<02:45,  1.62it/s]  2%|▏         | 5/273 [00:03<02:33,  1.75it/s]  2%|▏         | 6/273 [00:04<02:25,  1.84it/s]  3%|▎         | 7/273 [00:04<02:12,  2.01it/s]  3%|▎         | 8/273 [00:04<02:13,  1.98it/s]  3%|▎         | 9/273 [00:05<02:02,  2.16it/s]  4%|▎         | 10/273 [00:05<02:12,  1.98it/s]  4%|▍         | 11/273 [00:06<02:01,  2.15it/s]  4%|▍         | 12/273 [00:06<02:05,  2.08it/s]  5%|▍         | 13/273 [00:07<02:19,  1.86it/s]  5%|▌         | 14/273 [00:07<02:09,  1.99it/s]  5%|▌         | 15/273 [00:08<02:04,  2.08it/s]  6%|▌         | 16/273 [00:08<02:08,  2.00it/s]  6%|▌         | 17/273 [00:09<02:21,  1.81it/s]  7%|▋         | 18/273 [00:09<02:11,  1.94it/s]  7%|▋         | 19/273 [00:10<02:07,  1.99it/s]  7%|▋         | 20/273 [00:10<01:56,  2.16it/s]  8%|▊         | 21/273 [00:11<02:00,  2.09it/s]  8%|▊         | 22/273 [00:11<01:56,  2.16it/s]  8%|▊         | 23/273 [00:12<01:56,  2.15it/s]  9%|▉         | 24/273 [00:12<01:58,  2.11it/s]  9%|▉         | 25/273 [00:13<01:57,  2.11it/s] 10%|▉         | 26/273 [00:13<01:49,  2.25it/s] 10%|▉         | 27/273 [00:14<01:52,  2.19it/s] 10%|█         | 28/273 [00:14<01:44,  2.34it/s] 11%|█         | 29/273 [00:14<01:42,  2.38it/s] 11%|█         | 30/273 [00:15<01:58,  2.06it/s] 11%|█▏        | 31/273 [00:16<02:05,  1.92it/s] 12%|█▏        | 32/273 [00:16<02:03,  1.95it/s] 12%|█▏        | 33/273 [00:16<01:55,  2.08it/s] 12%|█▏        | 34/273 [00:17<01:51,  2.14it/s] 13%|█▎        | 35/273 [00:17<01:47,  2.20it/s] 13%|█▎        | 36/273 [00:18<01:50,  2.15it/s] 14%|█▎        | 37/273 [00:18<01:50,  2.14it/s] 14%|█▍        | 38/273 [00:19<01:45,  2.22it/s] 14%|█▍        | 39/273 [00:19<01:50,  2.12it/s] 15%|█▍        | 40/273 [00:20<01:47,  2.17it/s] 15%|█▌        | 41/273 [00:20<01:45,  2.20it/s] 15%|█▌        | 42/273 [00:21<01:43,  2.23it/s] 16%|█▌        | 43/273 [00:21<01:46,  2.17it/s] 16%|█▌        | 44/273 [00:21<01:39,  2.30it/s] 16%|█▋        | 45/273 [00:22<01:45,  2.17it/s] 17%|█▋        | 46/273 [00:22<01:42,  2.21it/s] 17%|█▋        | 47/273 [00:23<01:44,  2.15it/s] 18%|█▊        | 48/273 [00:23<01:37,  2.30it/s] 18%|█▊        | 49/273 [00:24<01:35,  2.35it/s] 18%|█▊        | 50/273 [00:24<01:39,  2.25it/s] 19%|█▊        | 51/273 [00:25<01:41,  2.20it/s] 19%|█▉        | 52/273 [00:25<01:52,  1.96it/s] 19%|█▉        | 53/273 [00:26<01:51,  1.97it/s] 20%|█▉        | 54/273 [00:26<01:52,  1.95it/s] 20%|██        | 55/273 [00:27<01:53,  1.93it/s] 21%|██        | 56/273 [00:27<01:47,  2.02it/s] 21%|██        | 57/273 [00:28<01:53,  1.90it/s] 21%|██        | 58/273 [00:28<01:53,  1.89it/s] 22%|██▏       | 59/273 [00:29<01:51,  1.93it/s] 22%|██▏       | 60/273 [00:29<01:52,  1.90it/s] 22%|██▏       | 61/273 [00:30<01:52,  1.89it/s] 23%|██▎       | 62/273 [00:30<01:42,  2.07it/s] 23%|██▎       | 63/273 [00:31<01:36,  2.18it/s] 23%|██▎       | 64/273 [00:31<01:41,  2.07it/s] 24%|██▍       | 65/273 [00:32<01:41,  2.04it/s] 24%|██▍       | 66/273 [00:32<01:41,  2.04it/s] 25%|██▍       | 67/273 [00:33<01:41,  2.03it/s] 25%|██▍       | 68/273 [00:33<01:37,  2.10it/s] 25%|██▌       | 69/273 [00:34<01:34,  2.15it/s] 26%|██▌       | 70/273 [00:34<01:38,  2.06it/s] 26%|██▌       | 71/273 [00:35<01:39,  2.02it/s] 26%|██▋       | 72/273 [00:35<01:42,  1.96it/s] 27%|██▋       | 73/273 [00:36<01:45,  1.90it/s] 27%|██▋       | 74/273 [00:36<01:44,  1.90it/s] 27%|██▋       | 75/273 [00:37<01:39,  1.99it/s] 28%|██▊       | 76/273 [00:37<01:40,  1.96it/s] 28%|██▊       | 77/273 [00:38<01:39,  1.97it/s] 29%|██▊       | 78/273 [00:38<01:41,  1.93it/s] 29%|██▉       | 79/273 [00:39<01:45,  1.83it/s] 29%|██▉       | 80/273 [00:39<01:35,  2.02it/s] 30%|██▉       | 81/273 [00:40<01:29,  2.14it/s] 30%|███       | 82/273 [00:40<01:27,  2.19it/s] 30%|███       | 83/273 [00:41<01:21,  2.32it/s] 31%|███       | 84/273 [00:41<01:21,  2.31it/s] 31%|███       | 85/273 [00:41<01:15,  2.49it/s] 32%|███▏      | 86/273 [00:42<01:13,  2.56it/s] 32%|███▏      | 87/273 [00:42<01:18,  2.38it/s] 32%|███▏      | 88/273 [00:43<01:24,  2.19it/s] 33%|███▎      | 89/273 [00:43<01:25,  2.14it/s] 33%|███▎      | 90/273 [00:44<01:26,  2.11it/s] 33%|███▎      | 91/273 [00:44<01:18,  2.32it/s]03/13/2024 22:54:39 - INFO - __main__ - epoch 0: {'accuracy': 0.4507042253521127}
 34%|███▎      | 92/273 [00:46<02:30,  1.20it/s] 34%|███▍      | 93/273 [00:46<02:11,  1.37it/s] 34%|███▍      | 94/273 [00:47<01:58,  1.51it/s] 35%|███▍      | 95/273 [00:47<01:56,  1.53it/s] 35%|███▌      | 96/273 [00:48<01:51,  1.59it/s] 36%|███▌      | 97/273 [00:48<01:43,  1.70it/s] 36%|███▌      | 98/273 [00:49<01:40,  1.74it/s] 36%|███▋      | 99/273 [00:49<01:35,  1.82it/s] 37%|███▋      | 100/273 [00:50<01:25,  2.02it/s] 37%|███▋      | 101/273 [00:50<01:17,  2.21it/s] 37%|███▋      | 102/273 [00:51<01:14,  2.28it/s] 38%|███▊      | 103/273 [00:51<01:10,  2.39it/s] 38%|███▊      | 104/273 [00:52<01:21,  2.07it/s] 38%|███▊      | 105/273 [00:52<01:24,  2.00it/s] 39%|███▉      | 106/273 [00:53<01:23,  2.00it/s] 39%|███▉      | 107/273 [00:53<01:24,  1.97it/s] 40%|███▉      | 108/273 [00:54<01:28,  1.86it/s] 40%|███▉      | 109/273 [00:54<01:27,  1.87it/s] 40%|████      | 110/273 [00:55<01:24,  1.93it/s] 41%|████      | 111/273 [00:55<01:22,  1.95it/s] 41%|████      | 112/273 [00:56<01:21,  1.97it/s] 41%|████▏     | 113/273 [00:56<01:19,  2.00it/s] 42%|████▏     | 114/273 [00:57<01:18,  2.01it/s] 42%|████▏     | 115/273 [00:57<01:15,  2.09it/s] 42%|████▏     | 116/273 [00:58<01:15,  2.07it/s] 43%|████▎     | 117/273 [00:58<01:10,  2.23it/s] 43%|████▎     | 118/273 [00:58<01:05,  2.35it/s] 44%|████▎     | 119/273 [00:59<01:04,  2.39it/s] 44%|████▍     | 120/273 [00:59<01:07,  2.28it/s] 44%|████▍     | 121/273 [01:00<01:05,  2.30it/s] 45%|████▍     | 122/273 [01:00<01:04,  2.35it/s] 45%|████▌     | 123/273 [01:01<01:11,  2.10it/s] 45%|████▌     | 124/273 [01:01<01:14,  2.01it/s] 46%|████▌     | 125/273 [01:02<01:16,  1.95it/s] 46%|████▌     | 126/273 [01:02<01:16,  1.92it/s] 47%|████▋     | 127/273 [01:03<01:09,  2.10it/s] 47%|████▋     | 128/273 [01:03<01:11,  2.02it/s] 47%|████▋     | 129/273 [01:04<01:08,  2.09it/s] 48%|████▊     | 130/273 [01:04<01:13,  1.94it/s] 48%|████▊     | 131/273 [01:05<01:11,  1.98it/s] 48%|████▊     | 132/273 [01:05<01:13,  1.93it/s] 49%|████▊     | 133/273 [01:06<01:11,  1.95it/s] 49%|████▉     | 134/273 [01:06<01:04,  2.14it/s] 49%|████▉     | 135/273 [01:07<01:06,  2.07it/s] 50%|████▉     | 136/273 [01:07<01:07,  2.03it/s] 50%|█████     | 137/273 [01:08<01:08,  1.99it/s] 51%|█████     | 138/273 [01:08<01:06,  2.02it/s] 51%|█████     | 139/273 [01:09<01:06,  2.02it/s] 51%|█████▏    | 140/273 [01:09<01:03,  2.09it/s] 52%|█████▏    | 141/273 [01:10<01:04,  2.03it/s] 52%|█████▏    | 142/273 [01:10<01:11,  1.83it/s] 52%|█████▏    | 143/273 [01:11<01:04,  2.03it/s] 53%|█████▎    | 144/273 [01:11<01:03,  2.03it/s] 53%|█████▎    | 145/273 [01:12<00:59,  2.14it/s] 53%|█████▎    | 146/273 [01:12<00:57,  2.21it/s] 54%|█████▍    | 147/273 [01:13<00:56,  2.23it/s] 54%|█████▍    | 148/273 [01:13<00:58,  2.13it/s] 55%|█████▍    | 149/273 [01:13<00:57,  2.16it/s] 55%|█████▍    | 150/273 [01:14<00:53,  2.31it/s] 55%|█████▌    | 151/273 [01:14<00:52,  2.31it/s] 56%|█████▌    | 152/273 [01:15<00:51,  2.35it/s] 56%|█████▌    | 153/273 [01:15<00:51,  2.34it/s] 56%|█████▋    | 154/273 [01:15<00:48,  2.45it/s] 57%|█████▋    | 155/273 [01:16<00:52,  2.25it/s] 57%|█████▋    | 156/273 [01:17<00:53,  2.17it/s] 58%|█████▊    | 157/273 [01:17<00:52,  2.22it/s] 58%|█████▊    | 158/273 [01:17<00:48,  2.35it/s] 58%|█████▊    | 159/273 [01:18<00:54,  2.10it/s] 59%|█████▊    | 160/273 [01:19<00:57,  1.95it/s] 59%|█████▉    | 161/273 [01:19<00:54,  2.04it/s] 59%|█████▉    | 162/273 [01:19<00:52,  2.11it/s] 60%|█████▉    | 163/273 [01:20<00:49,  2.21it/s] 60%|██████    | 164/273 [01:20<00:49,  2.22it/s] 60%|██████    | 165/273 [01:21<00:47,  2.29it/s] 61%|██████    | 166/273 [01:21<00:50,  2.13it/s] 61%|██████    | 167/273 [01:22<00:47,  2.21it/s] 62%|██████▏   | 168/273 [01:22<00:46,  2.26it/s] 62%|██████▏   | 169/273 [01:23<00:49,  2.09it/s] 62%|██████▏   | 170/273 [01:23<00:51,  2.01it/s] 63%|██████▎   | 171/273 [01:23<00:46,  2.19it/s] 63%|██████▎   | 172/273 [01:24<00:48,  2.08it/s] 63%|██████▎   | 173/273 [01:24<00:48,  2.06it/s] 64%|██████▎   | 174/273 [01:25<00:49,  2.02it/s] 64%|██████▍   | 175/273 [01:26<00:53,  1.82it/s] 64%|██████▍   | 176/273 [01:26<00:50,  1.93it/s] 65%|██████▍   | 177/273 [01:27<00:46,  2.06it/s] 65%|██████▌   | 178/273 [01:27<00:49,  1.92it/s] 66%|██████▌   | 179/273 [01:28<00:45,  2.04it/s] 66%|██████▌   | 180/273 [01:28<00:46,  1.99it/s] 66%|██████▋   | 181/273 [01:28<00:42,  2.16it/s] 67%|██████▋   | 182/273 [01:29<00:37,  2.44it/s]03/13/2024 22:55:23 - INFO - __main__ - epoch 1: {'accuracy': 0.49295774647887325}
 67%|██████▋   | 183/273 [01:30<01:11,  1.27it/s] 67%|██████▋   | 184/273 [01:31<01:03,  1.39it/s] 68%|██████▊   | 185/273 [01:32<00:58,  1.51it/s] 68%|██████▊   | 186/273 [01:32<00:53,  1.64it/s] 68%|██████▊   | 187/273 [01:32<00:46,  1.86it/s] 69%|██████▉   | 188/273 [01:33<00:47,  1.79it/s] 69%|██████▉   | 189/273 [01:33<00:43,  1.94it/s] 70%|██████▉   | 190/273 [01:34<00:40,  2.03it/s] 70%|██████▉   | 191/273 [01:34<00:37,  2.20it/s] 70%|███████   | 192/273 [01:35<00:38,  2.12it/s] 71%|███████   | 193/273 [01:35<00:39,  2.02it/s] 71%|███████   | 194/273 [01:36<00:39,  2.02it/s] 71%|███████▏  | 195/273 [01:36<00:39,  1.97it/s] 72%|███████▏  | 196/273 [01:37<00:38,  1.99it/s] 72%|███████▏  | 197/273 [01:37<00:36,  2.11it/s] 73%|███████▎  | 198/273 [01:38<00:34,  2.16it/s] 73%|███████▎  | 199/273 [01:38<00:38,  1.90it/s] 73%|███████▎  | 200/273 [01:39<00:38,  1.89it/s] 74%|███████▎  | 201/273 [01:39<00:38,  1.87it/s] 74%|███████▍  | 202/273 [01:40<00:37,  1.91it/s] 74%|███████▍  | 203/273 [01:40<00:38,  1.82it/s] 75%|███████▍  | 204/273 [01:41<00:35,  1.97it/s] 75%|███████▌  | 205/273 [01:41<00:34,  1.99it/s] 75%|███████▌  | 206/273 [01:42<00:34,  1.94it/s] 76%|███████▌  | 207/273 [01:42<00:34,  1.90it/s] 76%|███████▌  | 208/273 [01:43<00:31,  2.08it/s] 77%|███████▋  | 209/273 [01:43<00:28,  2.24it/s] 77%|███████▋  | 210/273 [01:44<00:28,  2.17it/s] 77%|███████▋  | 211/273 [01:44<00:28,  2.21it/s] 78%|███████▊  | 212/273 [01:45<00:29,  2.09it/s] 78%|███████▊  | 213/273 [01:45<00:28,  2.13it/s] 78%|███████▊  | 214/273 [01:46<00:25,  2.27it/s] 79%|███████▉  | 215/273 [01:46<00:26,  2.22it/s] 79%|███████▉  | 216/273 [01:46<00:24,  2.34it/s] 79%|███████▉  | 217/273 [01:47<00:22,  2.46it/s] 80%|███████▉  | 218/273 [01:47<00:21,  2.53it/s] 80%|████████  | 219/273 [01:48<00:25,  2.14it/s] 81%|████████  | 220/273 [01:48<00:24,  2.17it/s] 81%|████████  | 221/273 [01:49<00:25,  2.07it/s] 81%|████████▏ | 222/273 [01:49<00:25,  2.02it/s] 82%|████████▏ | 223/273 [01:50<00:24,  2.04it/s] 82%|████████▏ | 224/273 [01:50<00:24,  2.03it/s] 82%|████████▏ | 225/273 [01:51<00:25,  1.91it/s] 83%|████████▎ | 226/273 [01:51<00:25,  1.82it/s] 83%|████████▎ | 227/273 [01:52<00:23,  1.94it/s] 84%|████████▎ | 228/273 [01:52<00:21,  2.07it/s] 84%|████████▍ | 229/273 [01:53<00:22,  1.94it/s] 84%|████████▍ | 230/273 [01:53<00:22,  1.93it/s] 85%|████████▍ | 231/273 [01:54<00:21,  1.93it/s] 85%|████████▍ | 232/273 [01:54<00:19,  2.11it/s] 85%|████████▌ | 233/273 [01:55<00:19,  2.09it/s] 86%|████████▌ | 234/273 [01:55<00:17,  2.24it/s] 86%|████████▌ | 235/273 [01:56<00:19,  1.94it/s] 86%|████████▋ | 236/273 [01:56<00:19,  1.90it/s] 87%|████████▋ | 237/273 [01:57<00:18,  1.99it/s] 87%|████████▋ | 238/273 [01:57<00:17,  1.97it/s] 88%|████████▊ | 239/273 [01:58<00:17,  1.95it/s] 88%|████████▊ | 240/273 [01:58<00:16,  2.03it/s] 88%|████████▊ | 241/273 [01:59<00:14,  2.14it/s] 89%|████████▊ | 242/273 [01:59<00:13,  2.29it/s] 89%|████████▉ | 243/273 [02:00<00:13,  2.21it/s] 89%|████████▉ | 244/273 [02:00<00:13,  2.12it/s] 90%|████████▉ | 245/273 [02:00<00:12,  2.21it/s] 90%|█████████ | 246/273 [02:01<00:12,  2.15it/s] 90%|█████████ | 247/273 [02:01<00:12,  2.12it/s] 91%|█████████ | 248/273 [02:02<00:11,  2.16it/s] 91%|█████████ | 249/273 [02:02<00:11,  2.13it/s] 92%|█████████▏| 250/273 [02:03<00:10,  2.22it/s] 92%|█████████▏| 251/273 [02:03<00:10,  2.12it/s] 92%|█████████▏| 252/273 [02:04<00:10,  2.08it/s] 93%|█████████▎| 253/273 [02:04<00:08,  2.24it/s] 93%|█████████▎| 254/273 [02:05<00:08,  2.12it/s] 93%|█████████▎| 255/273 [02:05<00:08,  2.09it/s] 94%|█████████▍| 256/273 [02:06<00:07,  2.14it/s] 94%|█████████▍| 257/273 [02:06<00:07,  2.28it/s] 95%|█████████▍| 258/273 [02:07<00:06,  2.20it/s] 95%|█████████▍| 259/273 [02:07<00:05,  2.33it/s] 95%|█████████▌| 260/273 [02:07<00:06,  2.16it/s] 96%|█████████▌| 261/273 [02:08<00:05,  2.20it/s] 96%|█████████▌| 262/273 [02:08<00:05,  2.14it/s] 96%|█████████▋| 263/273 [02:09<00:04,  2.05it/s] 97%|█████████▋| 264/273 [02:10<00:04,  1.87it/s] 97%|█████████▋| 265/273 [02:10<00:03,  2.06it/s] 97%|█████████▋| 266/273 [02:10<00:03,  2.13it/s] 98%|█████████▊| 267/273 [02:11<00:02,  2.01it/s] 98%|█████████▊| 268/273 [02:11<00:02,  2.12it/s] 99%|█████████▊| 269/273 [02:12<00:01,  2.17it/s] 99%|█████████▉| 270/273 [02:12<00:01,  2.31it/s] 99%|█████████▉| 271/273 [02:13<00:00,  2.23it/s]100%|█████████▉| 272/273 [02:13<00:00,  2.12it/s]100%|██████████| 273/273 [02:14<00:00,  2.16it/s]03/13/2024 22:56:08 - INFO - __main__ - epoch 2: {'accuracy': 0.43661971830985913}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[2024-03-13 22:56:09,259] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli/config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli/model.safetensors.index.json.
tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli/tokenizer_config.json
Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli/special_tokens_map.json
100%|██████████| 273/273 [02:50<00:00,  1.60it/s]
END TIME: Wed Mar 13 22:56:46 EDT 2024
