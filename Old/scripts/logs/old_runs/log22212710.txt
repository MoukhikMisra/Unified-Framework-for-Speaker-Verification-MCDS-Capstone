WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 11:42:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 11:42:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/runs/Feb13_11-42-14_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 11:42:15 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 11:42:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:42:16 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 11:42:16 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:42:16 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 11:42:16,802 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 11:42:16,805 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:42:16,872 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:42:16,873 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:42:16,873 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:42:16,873 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:42:16,873 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 11:42:17,023 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-13 11:42:17,059 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 11:42:17,161 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-13 11:42:38,944 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 11:42:38,944 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-13 11:42:38,948 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-fa7ba0fd28b971a8.arrow
02/13/2024 11:42:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-fa7ba0fd28b971a8.arrow
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   1%|▏         | 1000/67349 [00:00<00:14, 4544.77 examples/s]Running tokenizer on dataset:   3%|▎         | 2000/67349 [00:00<00:14, 4606.72 examples/s]Running tokenizer on dataset:   6%|▌         | 4000/67349 [00:00<00:08, 7507.28 examples/s]Running tokenizer on dataset:   9%|▉         | 6000/67349 [00:00<00:07, 8544.89 examples/s]Running tokenizer on dataset:  12%|█▏        | 8000/67349 [00:00<00:06, 9672.78 examples/s]Running tokenizer on dataset:  13%|█▎        | 9000/67349 [00:01<00:06, 9651.70 examples/s]Running tokenizer on dataset:  16%|█▋        | 11000/67349 [00:01<00:05, 10337.34 examples/s]Running tokenizer on dataset:  19%|█▉        | 13000/67349 [00:01<00:05, 10747.72 examples/s]Running tokenizer on dataset:  22%|██▏       | 15000/67349 [00:01<00:04, 11056.78 examples/s]Running tokenizer on dataset:  25%|██▌       | 17000/67349 [00:01<00:04, 11189.43 examples/s]Running tokenizer on dataset:  28%|██▊       | 19000/dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
67349 [00:01<00:04, 11373.25 examples/s]Running tokenizer on dataset:  31%|███       | 21000/67349 [00:02<00:04, 11483.50 examples/s]Running tokenizer on dataset:  34%|███▍      | 23000/67349 [00:02<00:03, 11526.31 examples/s]Running tokenizer on dataset:  37%|███▋      | 25000/67349 [00:02<00:03, 11607.54 examples/s]Running tokenizer on dataset:  40%|████      | 27000/67349 [00:02<00:04, 9680.66 examples/s] Running tokenizer on dataset:  43%|████▎     | 29000/67349 [00:02<00:03, 10186.09 examples/s]Running tokenizer on dataset:  46%|████▌     | 31000/67349 [00:03<00:03, 10591.23 examples/s]Running tokenizer on dataset:  49%|████▉     | 33000/67349 [00:03<00:03, 10845.73 examples/s]Running tokenizer on dataset:  52%|█████▏    | 35000/67349 [00:03<00:02, 11063.24 examples/s]Running tokenizer on dataset:  55%|█████▍    | 37000/67349 [00:03<00:02, 11231.32 examples/s]Running tokenizer on dataset:  58%|█████▊dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
    | 39000/67349 [00:03<00:02, 11327.59 examples/s]Running tokenizer on dataset:  61%|██████    | 41000/67349 [00:03<00:02, 11459.51 examples/s]Running tokenizer on dataset:  64%|██████▍   | 43000/67349 [00:04<00:02, 11584.54 examples/s]Running tokenizer on dataset:  67%|██████▋   | 45000/67349 [00:04<00:01, 11544.59 examples/s]Running tokenizer on dataset:  70%|██████▉   | 47000/67349 [00:04<00:01, 11618.70 examples/s]Running tokenizer on dataset:  73%|███████▎  | 49000/67349 [00:04<00:01, 11618.22 examples/s]Running tokenizer on dataset:  76%|███████▌  | 51000/67349 [00:04<00:01, 9697.11 examples/s] Running tokenizer on dataset:  79%|███████▊  | 53000/67349 [00:05<00:01, 10236.05 examples/s]Running tokenizer on dataset:  82%|████████▏ | 55000/67349 [00:05<00:01, 10655.01 examples/s]Running tokenizer on dataset:  85%|████████▍ | 57000/67349 [00:05<00:00, 10898.46dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
 examples/s]Running tokenizer on dataset:  88%|████████▊ | 59000/67349 [00:05<00:00, 11089.58 examples/s]Running tokenizer on dataset:  91%|█████████ | 61000/67349 [00:05<00:00, 11208.00 examples/s]Running tokenizer on dataset:  94%|█████████▎| 63000/67349 [00:05<00:00, 11381.28 examples/s]Running tokenizer on dataset:  97%|█████████▋| 65000/67349 [00:06<00:00, 11485.72 examples/s]Running tokenizer on dataset:  99%|█████████▉| 67000/67349 [00:06<00:00, 11536.99 examples/s]Running tokenizer on dataset: 100%|██████████| 67349/67349 [00:06<00:00, 10662.16 examples/s]
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f293a0d77e57aef2.arrow
02/13/2024 11:42:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f293a0d77e57aef2.arrow
Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 8067.32 examples/s]Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 7714.38 examples/s]
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6c6c1111b3e71846.arrow
02/13/2024 11:42:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6c6c1111b3e71846.arrow
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:  55%|█████▍    | 1000/1821 [00:00<00:00, 7606.98 examples/s]Running tokenizer on dataset: 100%|██████████| 1821/1821 [00:00<00:00, 8001.57 examples/s]
02/13/2024 11:42:46 - INFO - __main__ - Sample 81 of the training set: {'sentence': 'generates ', 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 16785, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]}.
02/13/2024 11:42:46 - INFO - __main__ - Sample 14 of the training set: {'sentence': 'lend some dignity to a dumb story ', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 301, 355, 777, 18085, 537, 304, 263, 270, 3774, 5828, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 11:42:46 - INFO - __main__ - Sample 3 of the training set: {'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 9242, 14401, 368, 15787, 304, 3933, 278, 1021, 10106, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 7230.90 examples/s]Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 6934.63 examples/s]
02/13/2024 11:42:46 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 11:42:49,739 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 11:42:49,982 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 11:42:49,982 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-13 11:42:49,982 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 11:42:49,982 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 11:42:49,982 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 11:42:49,982 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 11:42:49,983 >>   Total optimization steps = 21
[INFO|trainer.py:1756] 2024-02-13 11:42:49,984 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/21 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  5%|▍         | 1/21 [00:03<01:04,  3.24s/it] 10%|▉         | 2/21 [00:04<00:33,  1.79s/it] 14%|█▍        | 3/21 [00:04<00:24,  1.37s/it] 19%|█▉        | 4/21 [00:05<00:20,  1.18s/it] 24%|██▍       | 5/21 [00:06<00:17,  1.06s/it] 29%|██▊       | 6/21 [00:07<00:14,  1.00it/s] 33%|███▎      | 7/21 [00:08<00:13,  1.04it/s] 38%|███▊      | 8/21 [00:09<00:12,  1.08it/s] 43%|████▎     | 9/21 [00:10<00:11,  1.09it/s] 48%|████▊     | 10/21 [00:11<00:09,  1.10it/s] 52%|█████▏    | 11/21 [00:11<00:08,  1.12it/s] 57%|█████▋    | 12/21 [00:12<00:07,  1.13it/s] 62%|██████▏   | 13/21 [00:13<00:07,  1.14it/s] 67%|██████▋   | 14/21 [00:14<00:06,  1.14it/s] 71%|███████▏  | 15/21 [00:15<00:05,  1.15it/s] 76%|███████▌  | 16/21 [00:16<00:04,  1.14it/s] 81%|████████  | 17/21 [00:17<00:03,  1.14it/s] 86%|████████▌ | 18/21 [00:17<00:02,  1.14it/s] 90%|█████████ | 19/21 [00:18<00:01,  1.14it/s] 95%|█████████▌| 20/21 [00:19<00:00,  1.14it/s]100%|██████████| 21/21 [00:20<00:00,  1.14it/s][INFO|trainer.py:1988] 2024-02-13 11:43:10,748 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 20.8828, 'train_samples_per_second': 14.366, 'train_steps_per_second': 1.006, 'train_loss': 1.6787356422061013, 'epoch': 3.0}
                                               100%|██████████| 21/21 [00:20<00:00,  1.14it/s]100%|██████████| 21/21 [00:20<00:00,  1.01it/s]
[INFO|trainer.py:2985] 2024-02-13 11:43:10,894 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial
[INFO|configuration_utils.py:473] 2024-02-13 11:43:10,897 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 11:43:38,625 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 11:43:38,628 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 11:43:38,629 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.6787
  train_runtime            = 0:00:20.88
  train_samples            =        100
  train_samples_per_second =     14.366
  train_steps_per_second   =      1.006
02/13/2024 11:43:38 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 11:43:38,749 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 11:43:38,751 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 11:43:38,751 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-13 11:43:38,751 >>   Batch size = 8
  0%|          | 0/7 [00:00<?, ?it/s] 29%|██▊       | 2/7 [00:00<00:00,  7.75it/s] 43%|████▎     | 3/7 [00:00<00:00,  5.38it/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.89it/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.52it/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.35it/s]100%|██████████| 7/7 [00:01<00:00,  4.22it/s]100%|██████████| 7/7 [00:01<00:00,  4.18it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.48
  eval_loss               =     0.8417
  eval_runtime            = 0:00:01.93
  eval_samples            =        100
  eval_samples_per_second =     51.647
  eval_steps_per_second   =      3.615
