WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:08:01 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:08:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-08-01_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:08:01 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:08:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:08:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:08:02 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:08:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caeb22-627c28541f61743d764c6408;41ba993b-aa08-4c98-9ef0-3462b96514d8)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
main()
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
  File "run_glue.py", line 384, in main
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caeb23-7817cbdd40e0ae8753523b6d;e48c7e93-1de0-4a9a-89c8-e638adfc355a)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-65caeb23-7817cbdd40e0ae8753523b6d;e48c7e93-1de0-4a9a-89c8-e638adfc355a)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-65caeb22-627c28541f61743d764c6408;41ba993b-aa08-4c98-9ef0-3462b96514d8)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.
[2024-02-12 23:08:07,526] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 28299) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:08:07
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 28300)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:08:07
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 28299)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:09:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:09:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-09-57_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:09:58 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:09:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:09:59 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:09:59 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:09:59 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caeb97-60ed25f66e32b02b14f854fc;b518db32-99f1-4f6a-9e08-662910be2970)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caeb97-60ed25f66e32b02b14f854fc;b518db32-99f1-4f6a-9e08-662910be2970)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caeb97-13395ba2634fee3f58d5f94c;9b20799e-4868-42f6-b231-28eae3c1572a)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caeb97-13395ba2634fee3f58d5f94c;9b20799e-4868-42f6-b231-28eae3c1572a)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
[2024-02-12 23:10:03,957] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 28399) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:10:03
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 28400)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:10:03
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 28399)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:28:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:28:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-28-06_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:28:07 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:28:08 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:28:08 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:28:08 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:28:08 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caefd8-5eaa9d81226f521a305c292b;a14e498b-7c3c-497c-97a1-2fcbd66ef8f0)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caefd8-5eaa9d81226f521a305c292b;a14e498b-7c3c-497c-97a1-2fcbd66ef8f0)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caefd9-3422f069417e84666a13f5bb;063b79e8-91e3-4de0-a346-9f72f311c4a2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caefd9-3422f069417e84666a13f5bb;063b79e8-91e3-4de0-a346-9f72f311c4a2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
[2024-02-12 23:28:13,518] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 29813) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:28:13
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 29814)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:28:13
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 29813)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:39:41 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:39:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-39-41_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:39:41 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:39:43 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:39:43 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:39:43 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:39:43 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caf28f-1f5eae305e8f94305e717fc4;b5673f2a-1586-4237-96a9-83d70a2aa838)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caf28f-1f5eae305e8f94305e717fc4;b5673f2a-1586-4237-96a9-83d70a2aa838)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caf28f-2fee5e1b4f0928410ce32b6a;1cdbbca2-ba24-457b-ae57-5a4f03216bd7)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b.
401 Client Error. (Request ID: Root=1-65caf28f-2fee5e1b4f0928410ce32b6a;1cdbbca2-ba24-457b-ae57-5a4f03216bd7)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b is gated. You must be authenticated to access it.
[2024-02-12 23:39:47,866] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 30810) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:39:47
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 30811)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:39:47
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 30810)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:40:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:40:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-40-29_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:40:30 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:40:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:40:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:40:31 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:40:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caf2bf-450edf4b4b2d9719269f64bb;cb1b3efc-d3e4-4317-9a65-77bc7edfa778)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-70b-chat-hf is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-70b-chat-hf.
401 Client Error. (Request ID: Root=1-65caf2bf-450edf4b4b2d9719269f64bb;cb1b3efc-d3e4-4317-9a65-77bc7edfa778)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-70b-chat-hf is gated. You must be authenticated to access it.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 286, in hf_raise_for_status
    response.raise_for_status()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1368, in hf_hub_download
    raise head_call_error
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1238, in hf_hub_download
    metadata = get_hf_file_metadata(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 1631, in get_hf_file_metadata
    r = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 385, in _request_wrapper
    response = _request_wrapper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/file_download.py", line 409, in _request_wrapper
    hf_raise_for_status(response)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py", line 302, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-65caf2c0-654198cf3991ad2f0a91b3d7;bf9e8a22-513c-4787-a510-422de03701a9)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-70b-chat-hf is gated. You must be authenticated to access it.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 384, in main
    config = AutoConfig.from_pretrained(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1103, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 634, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py", line 416, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-70b-chat-hf.
401 Client Error. (Request ID: Root=1-65caf2c0-654198cf3991ad2f0a91b3d7;bf9e8a22-513c-4787-a510-422de03701a9)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-70b-chat-hf is gated. You must be authenticated to access it.
[2024-02-12 23:40:33,101] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 30976) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:40:33
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 30977)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:40:33
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 30976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:43:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:43:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-43-24_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:43:24 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]config.json: 100%|| 578/578 [00:00<00:00, 188kB/s]
tokenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]tokenizer_config.json: 100%|| 745/745 [00:00<00:00, 384kB/s]
Overwrite dataset info from restored data version if exists.
02/12/2024 23:43:26 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:43:26 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:43:26 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:43:26 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 23:43:26,276 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/config.json
[INFO|configuration_utils.py:792] 2024-02-12 23:43:26,277 >> Model config LlamaConfig {
  "_name_or_path": "daryl149/llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]tokenizer.model: 100%|| 500k/500k [00:00<00:00, 21.3MB/s]
tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]tokenizer.json: 100%|| 1.84M/1.84M [00:00<00:00, 30.6MB/s]
special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]special_tokens_map.json: 100%|| 411/411 [00:00<00:00, 369kB/s]
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:43:26,681 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:43:26,681 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:43:26,681 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:43:26,681 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:43:26,682 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/tokenizer_config.json
pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]pytorch_model.bin.index.json: 100%|| 26.8k/26.8k [00:00<00:00, 8.53MB/s]
[INFO|modeling_utils.py:3259] 2024-02-12 23:43:27,490 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s][A
pytorch_model-00001-of-00002.bin:   0%|          | 10.5M/9.98G [00:00<01:36, 103MB/s][A
pytorch_model-00001-of-00002.bin:   0%|          | 21.0M/9.98G [00:00<03:37, 45.7MB/s][A
pytorch_model-00001-of-00002.bin:   0%|          | 31.5M/9.98G [00:00<03:44, 44.2MB/s][A
pytorch_model-00001-of-00002.bin:   0%|          | 41.9M/9.98G [00:00<03:22, 49.0MB/s][A
pytorch_model-00001-of-00002.bin:   1%|          | 52.4M/9.98G [00:01<03:22, 48.9MB/s][A
pytorch_model-00001-of-00002.bin:   1%|          | 73.4M/9.98G [00:01<02:28, 66.6MB/s][A
pytorch_model-00001-of-00002.bin:   1%|          | 83.9M/9.98G [00:01<02:49, 58.2MB/s][A
pytorch_model-00001-of-00002.bin:   1%|          | 105M/9.98G [00:01<02:27, 66.7MB/s] [A
pytorch_model-00001-of-00002.bin:   1%|          | 115M/9.98G [00:02<03:11, 51.4MB/s][A
pytorch_model-00001-of-00002.bin:   1%|         | 136M/9.98G [00:02<02:45, 59.5MB/s][A
pytorch_model-00001-of-00002.bin:   1%|         | 147M/9.98G [00:02<02:46, 59.2MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 168M/9.98G [00:02<02:50, 57.5MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 178M/9.98G [00:03<02:53, 56.6MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 199M/9.98G [00:03<02:36, 62.4MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 210M/9.98G [00:03<03:16, 49.8MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 231M/9.98G [00:04<03:20, 48.7MB/s][A
pytorch_model-00001-of-00002.bin:   2%|         | 241M/9.98G [00:04<03:08, 51.7MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 262M/9.98G [00:04<02:49, 57.3MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 273M/9.98G [00:04<03:04, 52.5MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 294M/9.98G [00:05<03:14, 49.8MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 304M/9.98G [00:05<02:59, 53.9MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 325M/9.98G [00:05<02:25, 66.4MB/s][A
pytorch_model-00001-of-00002.bin:   3%|         | 346M/9.98G [00:05<02:08, 75.1MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 357M/9.98G [00:06<02:20, 68.2MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 377M/9.98G [00:06<02:17, 69.6MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 388M/9.98G [00:06<02:25, 66.1MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 409M/9.98G [00:06<02:19, 68.4MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 419M/9.98G [00:07<02:44, 58.1MB/s][A
pytorch_model-00001-of-00002.bin:   4%|         | 440M/9.98G [00:07<02:37, 60.6MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 451M/9.98G [00:08<03:34, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 472M/9.98G [00:08<03:33, 44.4MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 482M/9.98G [00:09<04:37, 34.2MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 503M/9.98G [00:09<03:58, 39.8MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 514M/9.98G [00:09<04:17, 36.7MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 535M/9.98G [00:10<03:43, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:   5%|         | 545M/9.98G [00:10<03:43, 42.1MB/s][A
pytorch_model-00001-of-00002.bin:   6%|         | 566M/9.98G [00:10<03:42, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:   6%|         | 577M/9.98G [00:11<03:56, 39.8MB/s][A
pytorch_model-00001-of-00002.bin:   6%|         | 598M/9.98G [00:11<04:15, 36.7MB/s][A
pytorch_model-00001-of-00002.bin:   6%|         | 608M/9.98G [00:12<03:57, 39.5MB/s][A
pytorch_model-00001-of-00002.bin:   6%|         | 629M/9.98G [00:12<03:03, 50.9MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 650M/9.98G [00:12<02:33, 60.6MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 661M/9.98G [00:12<02:42, 57.3MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 682M/9.98G [00:13<02:50, 54.5MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 692M/9.98G [00:13<03:01, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 713M/9.98G [00:14<03:36, 42.8MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 724M/9.98G [00:14<03:45, 41.0MB/s][A
pytorch_model-00001-of-00002.bin:   7%|         | 744M/9.98G [00:14<03:34, 43.0MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 755M/9.98G [00:15<03:54, 39.3MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 776M/9.98G [00:15<03:23, 45.1MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 786M/9.98G [00:15<03:49, 40.0MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 807M/9.98G [00:16<03:51, 39.6MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 818M/9.98G [00:16<03:39, 41.8MB/s][A
pytorch_model-00001-of-00002.bin:   8%|         | 839M/9.98G [00:16<02:47, 54.7MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 849M/9.98G [00:17<02:56, 51.7MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 870M/9.98G [00:17<02:36, 58.1MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 881M/9.98G [00:17<03:04, 49.3MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 902M/9.98G [00:18<03:13, 47.0MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 912M/9.98G [00:18<03:39, 41.2MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 923M/9.98G [00:18<03:46, 39.9MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 933M/9.98G [00:19<04:31, 33.2MB/s][A
pytorch_model-00001-of-00002.bin:   9%|         | 944M/9.98G [00:19<03:56, 38.2MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 954M/9.98G [00:19<03:25, 44.0MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 965M/9.98G [00:19<03:00, 49.8MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 986M/9.98G [00:20<02:38, 56.7MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 996M/9.98G [00:20<02:30, 59.6MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 1.02G/9.98G [00:20<02:15, 65.9MB/s][A
pytorch_model-00001-of-00002.bin:  10%|         | 1.03G/9.98G [00:20<02:28, 60.3MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.05G/9.98G [00:20<02:18, 64.3MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.06G/9.98G [00:21<02:59, 49.6MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.08G/9.98G [00:21<02:44, 54.2MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.09G/9.98G [00:21<02:45, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.11G/9.98G [00:22<02:31, 58.4MB/s][A
pytorch_model-00001-of-00002.bin:  11%|         | 1.12G/9.98G [00:22<02:42, 54.5MB/s][A
pytorch_model-00001-of-00002.bin:  11%|        | 1.14G/9.98G [00:22<02:53, 50.9MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.15G/9.98G [00:23<03:41, 39.8MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.17G/9.98G [00:23<03:04, 47.7MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.18G/9.98G [00:23<03:23, 43.2MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.20G/9.98G [00:24<02:56, 49.8MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.21G/9.98G [00:24<02:51, 51.0MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.22G/9.98G [00:24<03:20, 43.8MB/s][A
pytorch_model-00001-of-00002.bin:  12%|        | 1.24G/9.98G [00:24<02:47, 52.3MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.25G/9.98G [00:24<02:27, 59.2MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.26G/9.98G [00:25<02:48, 51.6MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.27G/9.98G [00:25<03:16, 44.3MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.29G/9.98G [00:26<03:14, 44.6MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.30G/9.98G [00:26<03:16, 44.2MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.32G/9.98G [00:26<03:14, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.33G/9.98G [00:27<04:12, 34.2MB/s][A
pytorch_model-00001-of-00002.bin:  13%|        | 1.34G/9.98G [00:27<04:07, 34.9MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.35G/9.98G [00:27<03:49, 37.5MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.36G/9.98G [00:28<03:42, 38.7MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.38G/9.98G [00:28<02:51, 50.0MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.39G/9.98G [00:28<03:15, 43.9MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.42G/9.98G [00:29<03:55, 36.4MB/s][A
pytorch_model-00001-of-00002.bin:  14%|        | 1.43G/9.98G [00:29<03:55, 36.3MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.45G/9.98G [00:29<03:06, 45.7MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.46G/9.98G [00:30<03:58, 35.7MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.48G/9.98G [00:30<03:13, 43.9MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.49G/9.98G [00:31<03:09, 44.7MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.50G/9.98G [00:31<03:04, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.51G/9.98G [00:31<03:19, 42.4MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.52G/9.98G [00:31<03:16, 43.1MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.53G/9.98G [00:31<02:48, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  15%|        | 1.54G/9.98G [00:32<02:59, 47.0MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.56G/9.98G [00:32<02:20, 59.9MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.57G/9.98G [00:32<02:29, 56.1MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.58G/9.98G [00:32<02:34, 54.4MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.59G/9.98G [00:33<03:02, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.60G/9.98G [00:33<03:40, 38.0MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.63G/9.98G [00:34<03:54, 35.6MB/s][A
pytorch_model-00001-of-00002.bin:  16%|        | 1.64G/9.98G [00:34<04:22, 31.7MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.66G/9.98G [00:34<03:13, 42.9MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.67G/9.98G [00:35<03:03, 45.2MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.69G/9.98G [00:35<02:36, 52.8MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.70G/9.98G [00:35<02:35, 53.3MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.72G/9.98G [00:36<02:57, 46.6MB/s][A
pytorch_model-00001-of-00002.bin:  17%|        | 1.73G/9.98G [00:36<04:04, 33.7MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.75G/9.98G [00:37<03:41, 37.2MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.76G/9.98G [00:37<03:57, 34.6MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.78G/9.98G [00:37<03:14, 42.1MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.79G/9.98G [00:38<03:07, 43.6MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.81G/9.98G [00:38<02:57, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.82G/9.98G [00:39<03:48, 35.7MB/s][A
pytorch_model-00001-of-00002.bin:  18%|        | 1.85G/9.98G [00:39<03:00, 45.1MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.86G/9.98G [00:39<02:59, 45.2MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.87G/9.98G [00:39<02:58, 45.6MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.88G/9.98G [00:39<02:52, 47.0MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.90G/9.98G [00:40<02:18, 58.2MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.91G/9.98G [00:40<02:41, 49.9MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.92G/9.98G [00:40<02:20, 57.2MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.93G/9.98G [00:41<03:10, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  19%|        | 1.94G/9.98G [00:41<03:26, 38.9MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 1.96G/9.98G [00:41<03:01, 44.1MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 1.97G/9.98G [00:42<04:39, 28.7MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 1.99G/9.98G [00:42<03:39, 36.4MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 2.00G/9.98G [00:42<03:11, 41.6MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 2.02G/9.98G [00:43<03:18, 40.1MB/s][A
pytorch_model-00001-of-00002.bin:  20%|        | 2.03G/9.98G [00:44<05:23, 24.5MB/s][A
pytorch_model-00001-of-00002.bin:  21%|        | 2.06G/9.98G [00:44<04:05, 32.2MB/s][A
pytorch_model-00001-of-00002.bin:  21%|        | 2.07G/9.98G [00:45<04:08, 31.9MB/s][A
pytorch_model-00001-of-00002.bin:  21%|        | 2.09G/9.98G [00:45<03:07, 42.0MB/s][A
pytorch_model-00001-of-00002.bin:  21%|        | 2.10G/9.98G [00:45<03:03, 42.9MB/s][A
pytorch_model-00001-of-00002.bin:  21%|        | 2.12G/9.98G [00:45<02:37, 50.1MB/s][A
pytorch_model-00001-of-00002.bin:  21%|       | 2.13G/9.98G [00:46<02:43, 48.0MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.15G/9.98G [00:46<02:21, 55.2MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.16G/9.98G [00:46<02:38, 49.5MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.18G/9.98G [00:47<02:45, 47.1MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.20G/9.98G [00:47<02:19, 55.8MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.21G/9.98G [00:47<02:21, 54.9MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.22G/9.98G [00:47<02:07, 60.6MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.23G/9.98G [00:48<02:13, 57.8MB/s][A
pytorch_model-00001-of-00002.bin:  22%|       | 2.24G/9.98G [00:48<02:18, 55.9MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.26G/9.98G [00:48<02:14, 57.5MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.28G/9.98G [00:48<02:29, 51.6MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.30G/9.98G [00:49<02:05, 61.0MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.31G/9.98G [00:49<02:35, 49.4MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.33G/9.98G [00:49<02:01, 63.2MB/s][A
pytorch_model-00001-of-00002.bin:  23%|       | 2.34G/9.98G [00:49<02:02, 62.1MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.36G/9.98G [00:50<02:10, 58.2MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.37G/9.98G [00:50<02:22, 53.2MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.38G/9.98G [00:50<02:21, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.39G/9.98G [00:50<02:16, 55.7MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.40G/9.98G [00:51<02:36, 48.3MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.42G/9.98G [00:51<02:18, 54.7MB/s][A
pytorch_model-00001-of-00002.bin:  24%|       | 2.43G/9.98G [00:51<02:29, 50.5MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.45G/9.98G [00:52<02:39, 47.1MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.46G/9.98G [00:52<02:51, 43.8MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.49G/9.98G [00:52<02:34, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.50G/9.98G [00:53<02:18, 54.0MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.51G/9.98G [00:53<02:28, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.52G/9.98G [00:53<02:36, 47.6MB/s][A
pytorch_model-00001-of-00002.bin:  25%|       | 2.54G/9.98G [00:53<02:08, 58.0MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.55G/9.98G [00:54<02:25, 51.1MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.57G/9.98G [00:54<02:14, 55.2MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.58G/9.98G [00:54<02:45, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.60G/9.98G [00:54<02:05, 58.5MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.61G/9.98G [00:55<02:16, 53.8MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.63G/9.98G [00:55<02:24, 50.9MB/s][A
pytorch_model-00001-of-00002.bin:  26%|       | 2.64G/9.98G [00:55<02:19, 52.7MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.66G/9.98G [00:56<02:17, 53.4MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.67G/9.98G [00:56<02:30, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.69G/9.98G [00:56<02:05, 58.1MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.71G/9.98G [00:57<02:24, 50.5MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.73G/9.98G [00:57<02:14, 54.1MB/s][A
pytorch_model-00001-of-00002.bin:  27%|       | 2.74G/9.98G [00:57<02:07, 56.8MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.75G/9.98G [00:57<02:08, 56.2MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.76G/9.98G [00:57<02:14, 53.8MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.77G/9.98G [00:58<02:44, 43.9MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.79G/9.98G [00:58<02:20, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.80G/9.98G [00:58<02:04, 57.6MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.81G/9.98G [00:58<01:57, 61.2MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.82G/9.98G [00:59<02:43, 43.7MB/s][A
pytorch_model-00001-of-00002.bin:  28%|       | 2.84G/9.98G [00:59<02:35, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.85G/9.98G [01:00<02:50, 41.7MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.86G/9.98G [01:00<03:11, 37.1MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.87G/9.98G [01:00<03:06, 38.1MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.88G/9.98G [01:00<02:58, 39.7MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.90G/9.98G [01:01<03:05, 38.0MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.92G/9.98G [01:01<02:56, 39.9MB/s][A
pytorch_model-00001-of-00002.bin:  29%|       | 2.94G/9.98G [01:02<02:23, 49.2MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 2.95G/9.98G [01:02<02:10, 53.8MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 2.97G/9.98G [01:02<01:53, 61.5MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 2.98G/9.98G [01:02<02:22, 49.2MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 3.00G/9.98G [01:03<02:03, 56.7MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 3.01G/9.98G [01:03<02:09, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 3.03G/9.98G [01:03<02:01, 57.0MB/s][A
pytorch_model-00001-of-00002.bin:  30%|       | 3.04G/9.98G [01:04<02:59, 38.7MB/s][A
pytorch_model-00001-of-00002.bin:  31%|       | 3.06G/9.98G [01:04<02:19, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  31%|       | 3.07G/9.98G [01:04<02:29, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  31%|       | 3.09G/9.98G [01:05<02:33, 44.7MB/s][A
pytorch_model-00001-of-00002.bin:  31%|       | 3.11G/9.98G [01:05<02:01, 56.4MB/s][A
pytorch_model-00001-of-00002.bin:  31%|      | 3.12G/9.98G [01:05<01:56, 58.8MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.15G/9.98G [01:06<02:07, 53.8MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.16G/9.98G [01:06<02:15, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.18G/9.98G [01:06<01:49, 62.1MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.19G/9.98G [01:06<01:49, 62.1MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.21G/9.98G [01:07<01:52, 60.4MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.22G/9.98G [01:07<02:04, 54.3MB/s][A
pytorch_model-00001-of-00002.bin:  32%|      | 3.24G/9.98G [01:07<02:03, 54.5MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.25G/9.98G [01:08<02:29, 44.9MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.27G/9.98G [01:08<02:22, 47.0MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.28G/9.98G [01:08<02:28, 45.0MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.30G/9.98G [01:09<02:09, 51.5MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.31G/9.98G [01:09<02:33, 43.4MB/s][A
pytorch_model-00001-of-00002.bin:  33%|      | 3.33G/9.98G [01:09<02:07, 52.1MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.34G/9.98G [01:09<02:02, 53.9MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.37G/9.98G [01:10<02:26, 45.0MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.39G/9.98G [01:10<02:01, 54.1MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.40G/9.98G [01:10<02:13, 49.1MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.41G/9.98G [01:11<02:23, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.42G/9.98G [01:11<02:35, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  34%|      | 3.43G/9.98G [01:11<02:30, 43.4MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.45G/9.98G [01:12<02:01, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.46G/9.98G [01:12<02:29, 43.7MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.47G/9.98G [01:12<02:48, 38.7MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.48G/9.98G [01:13<02:40, 40.4MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.49G/9.98G [01:13<02:51, 37.8MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.51G/9.98G [01:13<02:16, 47.5MB/s][A
pytorch_model-00001-of-00002.bin:  35%|      | 3.52G/9.98G [01:13<02:26, 44.2MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.54G/9.98G [01:14<02:01, 53.0MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.55G/9.98G [01:14<02:06, 50.6MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.58G/9.98G [01:14<01:59, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.59G/9.98G [01:15<02:19, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.61G/9.98G [01:15<02:05, 50.7MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.62G/9.98G [01:15<02:13, 47.5MB/s][A
pytorch_model-00001-of-00002.bin:  36%|      | 3.64G/9.98G [01:16<02:03, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.65G/9.98G [01:16<02:00, 52.4MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.66G/9.98G [01:16<01:47, 59.0MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.67G/9.98G [01:16<01:44, 60.3MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.68G/9.98G [01:16<01:50, 56.9MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.70G/9.98G [01:17<01:39, 62.9MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.71G/9.98G [01:17<01:44, 59.9MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.72G/9.98G [01:17<01:46, 58.8MB/s][A
pytorch_model-00001-of-00002.bin:  37%|      | 3.73G/9.98G [01:17<01:46, 58.5MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.75G/9.98G [01:17<01:25, 72.9MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.76G/9.98G [01:18<01:31, 67.8MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.79G/9.98G [01:18<01:26, 71.8MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.80G/9.98G [01:18<01:25, 72.4MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.82G/9.98G [01:19<01:57, 52.5MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.83G/9.98G [01:19<02:19, 44.0MB/s][A
pytorch_model-00001-of-00002.bin:  38%|      | 3.84G/9.98G [01:19<02:35, 39.5MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.85G/9.98G [01:20<02:40, 38.2MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.86G/9.98G [01:20<02:31, 40.4MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.88G/9.98G [01:20<02:07, 48.0MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.89G/9.98G [01:20<01:58, 51.5MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.91G/9.98G [01:21<01:46, 56.9MB/s][A
pytorch_model-00001-of-00002.bin:  39%|      | 3.92G/9.98G [01:21<02:00, 50.3MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 3.94G/9.98G [01:21<01:54, 52.6MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 3.95G/9.98G [01:22<02:11, 45.9MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 3.97G/9.98G [01:22<02:06, 47.6MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 3.98G/9.98G [01:22<02:07, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 4.01G/9.98G [01:23<01:58, 50.6MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 4.02G/9.98G [01:23<02:08, 46.2MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 4.03G/9.98G [01:23<02:26, 40.5MB/s][A
pytorch_model-00001-of-00002.bin:  40%|      | 4.04G/9.98G [01:23<02:24, 41.0MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.05G/9.98G [01:24<02:07, 46.4MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.06G/9.98G [01:24<02:09, 45.7MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.07G/9.98G [01:24<02:22, 41.4MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.09G/9.98G [01:24<01:54, 51.4MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.10G/9.98G [01:25<01:45, 55.9MB/s][A
pytorch_model-00001-of-00002.bin:  41%|      | 4.11G/9.98G [01:25<01:37, 60.2MB/s][A
pytorch_model-00001-of-00002.bin:  41%|     | 4.12G/9.98G [01:25<01:41, 57.4MB/s][A
pytorch_model-00001-of-00002.bin:  41%|     | 4.13G/9.98G [01:25<01:50, 52.9MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.14G/9.98G [01:25<01:47, 54.3MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.15G/9.98G [01:26<01:42, 56.9MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.16G/9.98G [01:26<02:07, 45.4MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.18G/9.98G [01:26<01:49, 53.1MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.19G/9.98G [01:26<01:58, 48.7MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.22G/9.98G [01:27<02:07, 45.1MB/s][A
pytorch_model-00001-of-00002.bin:  42%|     | 4.23G/9.98G [01:27<02:02, 46.8MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.25G/9.98G [01:28<01:57, 48.8MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.26G/9.98G [01:28<02:15, 42.2MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.28G/9.98G [01:28<02:16, 41.6MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.29G/9.98G [01:29<02:09, 44.0MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.31G/9.98G [01:29<01:51, 50.8MB/s][A
pytorch_model-00001-of-00002.bin:  43%|     | 4.32G/9.98G [01:29<02:10, 43.3MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.34G/9.98G [01:30<01:56, 48.4MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.36G/9.98G [01:30<01:52, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.37G/9.98G [01:30<01:58, 47.1MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.38G/9.98G [01:31<01:51, 50.4MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.39G/9.98G [01:31<02:04, 45.0MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.40G/9.98G [01:31<02:10, 42.6MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.42G/9.98G [01:32<02:03, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  44%|     | 4.44G/9.98G [01:32<01:59, 46.5MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.46G/9.98G [01:32<01:37, 56.8MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.47G/9.98G [01:33<02:26, 37.6MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.48G/9.98G [01:33<02:43, 33.6MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.49G/9.98G [01:33<02:26, 37.5MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.50G/9.98G [01:34<02:32, 35.9MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.52G/9.98G [01:34<01:57, 46.4MB/s][A
pytorch_model-00001-of-00002.bin:  45%|     | 4.53G/9.98G [01:34<02:02, 44.6MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.55G/9.98G [01:34<01:41, 53.4MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.56G/9.98G [01:35<01:53, 47.6MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.58G/9.98G [01:35<01:37, 55.1MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.59G/9.98G [01:35<01:36, 55.5MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.61G/9.98G [01:35<01:25, 62.4MB/s][A
pytorch_model-00001-of-00002.bin:  46%|     | 4.62G/9.98G [01:36<01:30, 59.3MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.65G/9.98G [01:36<01:34, 56.4MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.66G/9.98G [01:36<01:32, 57.7MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.67G/9.98G [01:36<01:36, 54.8MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.68G/9.98G [01:37<01:30, 58.8MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.69G/9.98G [01:37<01:30, 58.7MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.70G/9.98G [01:37<01:36, 54.4MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.71G/9.98G [01:37<02:00, 43.7MB/s][A
pytorch_model-00001-of-00002.bin:  47%|     | 4.73G/9.98G [01:38<01:42, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.74G/9.98G [01:38<01:45, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.76G/9.98G [01:38<01:38, 52.7MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.77G/9.98G [01:38<01:37, 53.5MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.79G/9.98G [01:39<01:33, 55.3MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.80G/9.98G [01:39<01:27, 59.2MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.81G/9.98G [01:39<01:21, 63.0MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.82G/9.98G [01:39<01:22, 62.3MB/s][A
pytorch_model-00001-of-00002.bin:  48%|     | 4.83G/9.98G [01:39<01:18, 65.4MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.85G/9.98G [01:40<01:04, 79.5MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.87G/9.98G [01:40<01:12, 70.7MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.89G/9.98G [01:40<01:26, 59.1MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.90G/9.98G [01:40<01:41, 50.1MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.92G/9.98G [01:41<01:36, 52.5MB/s][A
pytorch_model-00001-of-00002.bin:  49%|     | 4.93G/9.98G [01:41<01:36, 52.1MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 4.94G/9.98G [01:41<01:35, 52.7MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 4.95G/9.98G [01:42<01:41, 49.4MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 4.97G/9.98G [01:42<01:29, 56.1MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 4.98G/9.98G [01:42<01:45, 47.6MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 4.99G/9.98G [01:42<01:42, 48.8MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 5.00G/9.98G [01:43<01:43, 48.1MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 5.01G/9.98G [01:43<01:39, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  50%|     | 5.03G/9.98G [01:43<01:23, 59.5MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.04G/9.98G [01:43<01:21, 60.6MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.05G/9.98G [01:43<01:16, 64.7MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.06G/9.98G [01:44<01:23, 59.1MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.08G/9.98G [01:44<01:42, 47.8MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.10G/9.98G [01:45<02:09, 37.8MB/s][A
pytorch_model-00001-of-00002.bin:  51%|     | 5.11G/9.98G [01:45<02:56, 27.6MB/s][A
pytorch_model-00001-of-00002.bin:  51%|    | 5.13G/9.98G [01:46<02:14, 36.0MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.14G/9.98G [01:46<02:15, 35.7MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.16G/9.98G [01:46<01:38, 48.8MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.17G/9.98G [01:46<01:31, 52.6MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.19G/9.98G [01:47<01:30, 53.0MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.20G/9.98G [01:47<01:37, 48.8MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.22G/9.98G [01:47<01:26, 54.9MB/s][A
pytorch_model-00001-of-00002.bin:  52%|    | 5.23G/9.98G [01:47<01:31, 51.8MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.25G/9.98G [01:48<01:17, 60.6MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.27G/9.98G [01:48<01:04, 72.7MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.28G/9.98G [01:48<01:08, 68.0MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.31G/9.98G [01:48<01:08, 68.5MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.32G/9.98G [01:49<01:16, 60.8MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.33G/9.98G [01:49<01:46, 43.7MB/s][A
pytorch_model-00001-of-00002.bin:  53%|    | 5.34G/9.98G [01:49<01:58, 39.0MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.35G/9.98G [01:50<01:44, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.37G/9.98G [01:50<01:27, 52.7MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.38G/9.98G [01:50<01:46, 43.1MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.40G/9.98G [01:51<01:47, 42.6MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.41G/9.98G [01:51<02:05, 36.3MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.42G/9.98G [01:51<01:56, 39.1MB/s][A
pytorch_model-00001-of-00002.bin:  54%|    | 5.43G/9.98G [01:52<01:51, 40.7MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.44G/9.98G [01:52<01:53, 39.9MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.46G/9.98G [01:52<01:28, 50.9MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.47G/9.98G [01:52<01:20, 56.1MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.48G/9.98G [01:52<01:15, 59.2MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.49G/9.98G [01:53<01:27, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.51G/9.98G [01:53<01:47, 41.6MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.53G/9.98G [01:53<01:33, 47.6MB/s][A
pytorch_model-00001-of-00002.bin:  55%|    | 5.54G/9.98G [01:54<01:42, 43.5MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.56G/9.98G [01:54<01:35, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.57G/9.98G [01:54<01:27, 50.5MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.58G/9.98G [01:55<01:23, 52.5MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.59G/9.98G [01:55<01:23, 52.2MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.61G/9.98G [01:55<01:30, 48.4MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.62G/9.98G [01:55<01:39, 43.9MB/s][A
pytorch_model-00001-of-00002.bin:  56%|    | 5.63G/9.98G [01:56<01:42, 42.4MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.64G/9.98G [01:56<01:38, 44.2MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.65G/9.98G [01:56<01:59, 36.3MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.67G/9.98G [01:57<01:45, 40.6MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.68G/9.98G [01:57<01:54, 37.6MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.70G/9.98G [01:58<01:53, 37.6MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.71G/9.98G [01:58<01:46, 40.0MB/s][A
pytorch_model-00001-of-00002.bin:  57%|    | 5.74G/9.98G [01:58<01:35, 44.3MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.75G/9.98G [01:59<02:13, 31.7MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.76G/9.98G [01:59<02:15, 31.1MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.77G/9.98G [02:00<02:15, 31.1MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.78G/9.98G [02:00<02:02, 34.2MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.80G/9.98G [02:00<01:42, 40.9MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.81G/9.98G [02:01<01:41, 40.9MB/s][A
pytorch_model-00001-of-00002.bin:  58%|    | 5.83G/9.98G [02:01<01:21, 51.0MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.84G/9.98G [02:01<01:36, 42.9MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.85G/9.98G [02:01<01:27, 46.9MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.86G/9.98G [02:02<01:40, 40.9MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.87G/9.98G [02:02<01:51, 36.8MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.89G/9.98G [02:02<01:20, 51.0MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.91G/9.98G [02:03<01:06, 61.5MB/s][A
pytorch_model-00001-of-00002.bin:  59%|    | 5.92G/9.98G [02:03<01:24, 47.7MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 5.95G/9.98G [02:03<01:10, 57.1MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 5.96G/9.98G [02:03<01:06, 60.2MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 5.98G/9.98G [02:04<00:56, 71.1MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 5.99G/9.98G [02:04<01:05, 60.5MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 6.01G/9.98G [02:04<01:01, 64.0MB/s][A
pytorch_model-00001-of-00002.bin:  60%|    | 6.02G/9.98G [02:04<01:08, 57.5MB/s][A
pytorch_model-00001-of-00002.bin:  61%|    | 6.04G/9.98G [02:05<01:05, 60.4MB/s][A
pytorch_model-00001-of-00002.bin:  61%|    | 6.05G/9.98G [02:05<01:08, 57.2MB/s][A
pytorch_model-00001-of-00002.bin:  61%|    | 6.07G/9.98G [02:05<01:02, 62.3MB/s][A
pytorch_model-00001-of-00002.bin:  61%|    | 6.08G/9.98G [02:06<01:17, 50.0MB/s][A
pytorch_model-00001-of-00002.bin:  61%|    | 6.10G/9.98G [02:06<01:09, 55.9MB/s][A
pytorch_model-00001-of-00002.bin:  61%|   | 6.11G/9.98G [02:06<01:16, 50.6MB/s][A
pytorch_model-00001-of-00002.bin:  61%|   | 6.13G/9.98G [02:06<01:01, 62.7MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.14G/9.98G [02:07<01:08, 56.1MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.17G/9.98G [02:07<01:12, 52.4MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.18G/9.98G [02:07<01:29, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.20G/9.98G [02:08<01:20, 47.1MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.21G/9.98G [02:08<01:12, 52.1MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.22G/9.98G [02:08<01:12, 51.6MB/s][A
pytorch_model-00001-of-00002.bin:  62%|   | 6.23G/9.98G [02:09<01:40, 37.4MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.24G/9.98G [02:09<01:50, 33.8MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.25G/9.98G [02:09<01:40, 37.0MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.26G/9.98G [02:10<01:56, 31.9MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.28G/9.98G [02:10<01:31, 40.6MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.29G/9.98G [02:10<01:31, 40.1MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.31G/9.98G [02:11<01:20, 45.5MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.32G/9.98G [02:11<01:21, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  63%|   | 6.33G/9.98G [02:11<01:15, 48.4MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.34G/9.98G [02:11<01:29, 40.8MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.35G/9.98G [02:12<01:52, 32.3MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.36G/9.98G [02:12<01:36, 37.6MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.38G/9.98G [02:13<01:55, 31.1MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.39G/9.98G [02:13<01:36, 37.2MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.41G/9.98G [02:13<01:17, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  64%|   | 6.42G/9.98G [02:13<01:29, 39.9MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.44G/9.98G [02:14<01:23, 42.6MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.45G/9.98G [02:14<01:18, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.47G/9.98G [02:15<01:17, 45.3MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.48G/9.98G [02:15<01:17, 45.0MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.50G/9.98G [02:15<01:17, 44.9MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.52G/9.98G [02:16<01:06, 52.0MB/s][A
pytorch_model-00001-of-00002.bin:  65%|   | 6.53G/9.98G [02:16<01:07, 51.1MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.55G/9.98G [02:16<00:57, 59.4MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.56G/9.98G [02:16<01:02, 54.3MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.59G/9.98G [02:16<00:53, 63.5MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.60G/9.98G [02:17<01:05, 51.4MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.62G/9.98G [02:17<00:58, 57.6MB/s][A
pytorch_model-00001-of-00002.bin:  66%|   | 6.63G/9.98G [02:17<01:08, 48.7MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.64G/9.98G [02:18<01:05, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.65G/9.98G [02:18<01:12, 45.7MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.66G/9.98G [02:18<01:21, 40.8MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.68G/9.98G [02:19<01:02, 53.0MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.69G/9.98G [02:19<00:58, 56.3MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.70G/9.98G [02:19<01:02, 52.8MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.71G/9.98G [02:19<01:02, 52.1MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.72G/9.98G [02:19<00:59, 54.4MB/s][A
pytorch_model-00001-of-00002.bin:  67%|   | 6.73G/9.98G [02:20<01:04, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.74G/9.98G [02:20<00:56, 57.3MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.75G/9.98G [02:20<01:02, 51.5MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.77G/9.98G [02:20<00:56, 57.0MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.78G/9.98G [02:20<01:00, 52.6MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.81G/9.98G [02:21<01:09, 45.5MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.82G/9.98G [02:21<01:03, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  68%|   | 6.83G/9.98G [02:21<01:00, 52.1MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.84G/9.98G [02:22<01:04, 48.5MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.86G/9.98G [02:22<01:08, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.87G/9.98G [02:22<01:18, 39.8MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.89G/9.98G [02:23<01:07, 45.4MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.90G/9.98G [02:23<01:05, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.92G/9.98G [02:23<01:04, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  69%|   | 6.93G/9.98G [02:24<01:03, 47.9MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 6.95G/9.98G [02:24<00:59, 51.1MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 6.96G/9.98G [02:24<01:03, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 6.98G/9.98G [02:25<00:53, 56.1MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 6.99G/9.98G [02:25<00:56, 53.2MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 7.01G/9.98G [02:25<01:04, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  70%|   | 7.03G/9.98G [02:26<01:24, 35.0MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.05G/9.98G [02:26<01:09, 42.2MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.06G/9.98G [02:27<01:09, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.07G/9.98G [02:27<01:00, 47.9MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.08G/9.98G [02:27<01:13, 39.6MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.09G/9.98G [02:27<01:04, 45.0MB/s][A
pytorch_model-00001-of-00002.bin:  71%|   | 7.10G/9.98G [02:27<01:02, 46.3MB/s][A
pytorch_model-00001-of-00002.bin:  71%|  | 7.11G/9.98G [02:28<01:08, 42.1MB/s][A
pytorch_model-00001-of-00002.bin:  71%|  | 7.12G/9.98G [02:28<00:58, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  71%|  | 7.13G/9.98G [02:28<00:54, 52.6MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.14G/9.98G [02:28<00:56, 50.1MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.16G/9.98G [02:28<00:45, 62.3MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.17G/9.98G [02:29<00:52, 53.5MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.18G/9.98G [02:29<00:46, 59.8MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.19G/9.98G [02:29<00:48, 57.1MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.20G/9.98G [02:29<00:57, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  72%|  | 7.22G/9.98G [02:30<00:54, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.24G/9.98G [02:30<01:04, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.26G/9.98G [02:31<01:04, 42.3MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.27G/9.98G [02:31<01:00, 44.6MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.29G/9.98G [02:31<00:47, 56.9MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.30G/9.98G [02:31<00:54, 49.2MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.32G/9.98G [02:32<00:48, 54.3MB/s][A
pytorch_model-00001-of-00002.bin:  73%|  | 7.33G/9.98G [02:32<00:46, 56.6MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.35G/9.98G [02:32<00:36, 72.8MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.36G/9.98G [02:32<00:35, 74.2MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.38G/9.98G [02:32<00:39, 66.5MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.39G/9.98G [02:33<01:01, 42.2MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.41G/9.98G [02:34<01:01, 41.9MB/s][A
pytorch_model-00001-of-00002.bin:  74%|  | 7.42G/9.98G [02:34<00:57, 44.3MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.43G/9.98G [02:34<00:55, 45.9MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.44G/9.98G [02:34<00:54, 46.2MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.47G/9.98G [02:34<00:42, 58.6MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.48G/9.98G [02:35<00:39, 63.4MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.50G/9.98G [02:35<00:35, 69.2MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.51G/9.98G [02:35<00:40, 61.0MB/s][A
pytorch_model-00001-of-00002.bin:  75%|  | 7.53G/9.98G [02:35<00:39, 61.3MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.54G/9.98G [02:36<00:49, 49.1MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.56G/9.98G [02:36<00:51, 47.3MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.57G/9.98G [02:36<00:48, 49.7MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.59G/9.98G [02:37<00:42, 55.8MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.60G/9.98G [02:37<00:58, 40.3MB/s][A
pytorch_model-00001-of-00002.bin:  76%|  | 7.62G/9.98G [02:38<00:51, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.63G/9.98G [02:38<00:54, 43.0MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.65G/9.98G [02:38<00:50, 46.4MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.67G/9.98G [02:38<00:47, 48.9MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.68G/9.98G [02:39<00:49, 46.5MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.69G/9.98G [02:39<00:48, 47.0MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.70G/9.98G [02:39<00:53, 42.8MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.72G/9.98G [02:40<00:43, 51.7MB/s][A
pytorch_model-00001-of-00002.bin:  77%|  | 7.73G/9.98G [02:40<00:52, 43.0MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.74G/9.98G [02:40<00:48, 46.1MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.75G/9.98G [02:40<00:58, 38.0MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.76G/9.98G [02:41<01:02, 35.3MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.77G/9.98G [02:41<00:59, 37.0MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.78G/9.98G [02:41<01:01, 35.5MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.80G/9.98G [02:42<00:58, 37.4MB/s][A
pytorch_model-00001-of-00002.bin:  78%|  | 7.81G/9.98G [02:42<00:57, 37.9MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.83G/9.98G [02:43<00:53, 40.3MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.84G/9.98G [02:43<00:54, 39.4MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.86G/9.98G [02:43<00:47, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.87G/9.98G [02:44<00:50, 41.5MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.90G/9.98G [02:44<00:42, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.91G/9.98G [02:44<00:44, 46.3MB/s][A
pytorch_model-00001-of-00002.bin:  79%|  | 7.93G/9.98G [02:44<00:37, 54.9MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 7.94G/9.98G [02:45<00:34, 58.9MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 7.95G/9.98G [02:45<00:42, 47.8MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 7.96G/9.98G [02:45<00:42, 47.7MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 7.97G/9.98G [02:45<00:41, 48.7MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 7.99G/9.98G [02:46<00:38, 51.1MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 8.00G/9.98G [02:46<00:39, 49.9MB/s][A
pytorch_model-00001-of-00002.bin:  80%|  | 8.02G/9.98G [02:46<00:40, 47.8MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.03G/9.98G [02:47<00:42, 45.4MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.04G/9.98G [02:47<01:05, 29.5MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.05G/9.98G [02:48<01:18, 24.4MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.06G/9.98G [02:48<01:08, 27.7MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.07G/9.98G [02:49<00:57, 33.3MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.08G/9.98G [02:49<01:00, 31.1MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.10G/9.98G [02:49<00:56, 33.3MB/s][A
pytorch_model-00001-of-00002.bin:  81%|  | 8.11G/9.98G [02:49<00:51, 36.2MB/s][A
pytorch_model-00001-of-00002.bin:  81%| | 8.12G/9.98G [02:50<00:55, 33.7MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.14G/9.98G [02:50<00:47, 38.5MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.15G/9.98G [02:51<01:02, 29.2MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.16G/9.98G [02:51<00:51, 35.1MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.17G/9.98G [02:51<00:48, 37.1MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.18G/9.98G [02:51<00:46, 38.3MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.20G/9.98G [02:52<00:42, 42.0MB/s][A
pytorch_model-00001-of-00002.bin:  82%| | 8.21G/9.98G [02:52<00:43, 40.6MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.23G/9.98G [02:52<00:33, 51.6MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.24G/9.98G [02:53<00:34, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.26G/9.98G [02:53<00:29, 58.5MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.27G/9.98G [02:53<00:28, 59.2MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.28G/9.98G [02:53<00:27, 62.4MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.29G/9.98G [02:54<00:35, 48.0MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.30G/9.98G [02:54<00:35, 47.5MB/s][A
pytorch_model-00001-of-00002.bin:  83%| | 8.33G/9.98G [02:54<00:27, 59.1MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.34G/9.98G [02:54<00:30, 53.3MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.36G/9.98G [02:55<00:28, 57.7MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.38G/9.98G [02:55<00:23, 68.0MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.39G/9.98G [02:55<00:27, 58.6MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.41G/9.98G [02:56<00:28, 55.2MB/s][A
pytorch_model-00001-of-00002.bin:  84%| | 8.42G/9.98G [02:56<00:27, 56.5MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.44G/9.98G [02:56<00:26, 58.0MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.45G/9.98G [02:56<00:28, 53.5MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.47G/9.98G [02:57<00:28, 53.2MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.48G/9.98G [02:57<00:27, 54.2MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.50G/9.98G [02:57<00:28, 51.7MB/s][A
pytorch_model-00001-of-00002.bin:  85%| | 8.51G/9.98G [02:58<00:29, 49.9MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.54G/9.98G [02:58<00:26, 54.7MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.55G/9.98G [02:58<00:32, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.56G/9.98G [02:59<00:38, 37.3MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.57G/9.98G [02:59<00:46, 30.5MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.58G/9.98G [02:59<00:39, 35.3MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.60G/9.98G [03:00<00:28, 49.0MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.61G/9.98G [03:00<00:35, 38.3MB/s][A
pytorch_model-00001-of-00002.bin:  86%| | 8.63G/9.98G [03:01<00:31, 42.7MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.64G/9.98G [03:01<00:29, 45.5MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.65G/9.98G [03:01<00:31, 41.4MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.66G/9.98G [03:01<00:32, 40.3MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.67G/9.98G [03:01<00:28, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.68G/9.98G [03:02<00:27, 46.6MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.69G/9.98G [03:02<00:28, 45.1MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.71G/9.98G [03:02<00:21, 58.1MB/s][A
pytorch_model-00001-of-00002.bin:  87%| | 8.72G/9.98G [03:02<00:22, 56.2MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.73G/9.98G [03:03<00:23, 53.0MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.75G/9.98G [03:03<00:26, 46.2MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.76G/9.98G [03:03<00:28, 43.5MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.78G/9.98G [03:03<00:21, 54.8MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.79G/9.98G [03:04<00:25, 46.2MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.81G/9.98G [03:04<00:22, 51.2MB/s][A
pytorch_model-00001-of-00002.bin:  88%| | 8.82G/9.98G [03:04<00:25, 46.0MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.84G/9.98G [03:05<00:24, 46.5MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.85G/9.98G [03:05<00:27, 40.3MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.87G/9.98G [03:06<00:31, 34.7MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.88G/9.98G [03:06<00:32, 34.2MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.90G/9.98G [03:07<00:28, 37.7MB/s][A
pytorch_model-00001-of-00002.bin:  89%| | 8.91G/9.98G [03:07<00:26, 39.8MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 8.93G/9.98G [03:07<00:22, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 8.94G/9.98G [03:07<00:21, 47.3MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 8.97G/9.98G [03:08<00:20, 50.5MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 8.98G/9.98G [03:08<00:19, 50.2MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 8.99G/9.98G [03:08<00:18, 53.4MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 9.00G/9.98G [03:08<00:19, 50.9MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 9.01G/9.98G [03:09<00:16, 57.8MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 9.02G/9.98G [03:09<00:21, 44.1MB/s][A
pytorch_model-00001-of-00002.bin:  90%| | 9.03G/9.98G [03:09<00:21, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  91%| | 9.05G/9.98G [03:09<00:15, 58.6MB/s][A
pytorch_model-00001-of-00002.bin:  91%| | 9.06G/9.98G [03:10<00:20, 44.1MB/s][A
pytorch_model-00001-of-00002.bin:  91%| | 9.08G/9.98G [03:10<00:16, 55.1MB/s][A
pytorch_model-00001-of-00002.bin:  91%| | 9.09G/9.98G [03:10<00:18, 48.3MB/s][A
pytorch_model-00001-of-00002.bin:  91%|| 9.11G/9.98G [03:11<00:21, 40.4MB/s][A
pytorch_model-00001-of-00002.bin:  91%|| 9.12G/9.98G [03:11<00:21, 39.3MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.14G/9.98G [03:12<00:23, 36.2MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.15G/9.98G [03:13<00:28, 29.1MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.16G/9.98G [03:13<00:24, 33.3MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.18G/9.98G [03:13<00:20, 38.5MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.19G/9.98G [03:13<00:18, 43.3MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.21G/9.98G [03:13<00:15, 49.3MB/s][A
pytorch_model-00001-of-00002.bin:  92%|| 9.22G/9.98G [03:14<00:15, 49.1MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.24G/9.98G [03:14<00:13, 56.6MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.25G/9.98G [03:14<00:17, 42.8MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.27G/9.98G [03:15<00:15, 45.8MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.28G/9.98G [03:15<00:14, 47.3MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.29G/9.98G [03:15<00:14, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.30G/9.98G [03:15<00:14, 45.3MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.31G/9.98G [03:16<00:14, 44.5MB/s][A
pytorch_model-00001-of-00002.bin:  93%|| 9.32G/9.98G [03:16<00:13, 48.6MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.33G/9.98G [03:16<00:13, 46.9MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.35G/9.98G [03:17<00:12, 48.4MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.36G/9.98G [03:17<00:13, 45.4MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.37G/9.98G [03:17<00:12, 46.5MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.38G/9.98G [03:17<00:11, 53.7MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.40G/9.98G [03:18<00:15, 37.8MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.41G/9.98G [03:18<00:13, 43.5MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.42G/9.98G [03:18<00:12, 46.7MB/s][A
pytorch_model-00001-of-00002.bin:  94%|| 9.43G/9.98G [03:18<00:11, 49.4MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.45G/9.98G [03:18<00:08, 64.5MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.46G/9.98G [03:19<00:11, 44.2MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.48G/9.98G [03:19<00:09, 53.8MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.49G/9.98G [03:19<00:10, 48.3MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.51G/9.98G [03:20<00:10, 43.0MB/s][A
pytorch_model-00001-of-00002.bin:  95%|| 9.52G/9.98G [03:20<00:09, 48.3MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.54G/9.98G [03:20<00:07, 61.5MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.55G/9.98G [03:20<00:07, 59.3MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.57G/9.98G [03:21<00:06, 64.0MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.59G/9.98G [03:21<00:05, 72.9MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.60G/9.98G [03:21<00:05, 70.7MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.62G/9.98G [03:21<00:06, 59.9MB/s][A
pytorch_model-00001-of-00002.bin:  96%|| 9.63G/9.98G [03:22<00:06, 54.6MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.64G/9.98G [03:22<00:06, 49.9MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.66G/9.98G [03:22<00:06, 49.5MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.67G/9.98G [03:23<00:06, 49.4MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.69G/9.98G [03:23<00:05, 57.1MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.70G/9.98G [03:23<00:05, 51.5MB/s][A
pytorch_model-00001-of-00002.bin:  97%|| 9.72G/9.98G [03:24<00:05, 51.0MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.73G/9.98G [03:24<00:05, 46.5MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.75G/9.98G [03:24<00:04, 48.5MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.76G/9.98G [03:25<00:04, 44.8MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.78G/9.98G [03:25<00:03, 57.8MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.79G/9.98G [03:25<00:03, 57.8MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.81G/9.98G [03:25<00:02, 55.0MB/s][A
pytorch_model-00001-of-00002.bin:  98%|| 9.83G/9.98G [03:26<00:03, 48.2MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.85G/9.98G [03:26<00:02, 50.0MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.86G/9.98G [03:26<00:02, 46.8MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.88G/9.98G [03:27<00:02, 47.2MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.89G/9.98G [03:27<00:02, 41.7MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.90G/9.98G [03:27<00:01, 44.7MB/s][A
pytorch_model-00001-of-00002.bin:  99%|| 9.91G/9.98G [03:28<00:01, 41.0MB/s][A
pytorch_model-00001-of-00002.bin: 100%|| 9.93G/9.98G [03:28<00:00, 50.7MB/s][A
pytorch_model-00001-of-00002.bin: 100%|| 9.94G/9.98G [03:28<00:00, 46.0MB/s][A
pytorch_model-00001-of-00002.bin: 100%|| 9.96G/9.98G [03:29<00:00, 42.8MB/s][A
pytorch_model-00001-of-00002.bin: 100%|| 9.97G/9.98G [03:29<00:00, 43.3MB/s][A
pytorch_model-00001-of-00002.bin: 100%|| 9.98G/9.98G [03:29<00:00, 40.4MB/s][Apytorch_model-00001-of-00002.bin: 100%|| 9.98G/9.98G [03:29<00:00, 47.6MB/s]
Downloading shards:  50%|     | 1/2 [03:29<03:29, 209.90s/it]Downloading shards:  50%|     | 1/2 [03:29<03:29, 209.92s/it]
pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s][A
pytorch_model-00002-of-00002.bin:   1%|          | 21.0M/3.50G [00:00<01:08, 51.1MB/s][A
pytorch_model-00002-of-00002.bin:   1%|          | 41.9M/3.50G [00:00<00:55, 61.8MB/s][A
pytorch_model-00002-of-00002.bin:   1%|         | 52.4M/3.50G [00:00<01:06, 51.6MB/s][A
pytorch_model-00002-of-00002.bin:   2%|         | 62.9M/3.50G [00:01<01:22, 41.6MB/s][A
pytorch_model-00002-of-00002.bin:   2%|         | 73.4M/3.50G [00:01<01:20, 42.8MB/s][A
pytorch_model-00002-of-00002.bin:   2%|         | 83.9M/3.50G [00:02<01:41, 33.6MB/s][A
pytorch_model-00002-of-00002.bin:   3%|         | 94.4M/3.50G [00:02<02:16, 24.9MB/s][A
pytorch_model-00002-of-00002.bin:   3%|         | 105M/3.50G [00:03<02:04, 27.3MB/s] [A
pytorch_model-00002-of-00002.bin:   3%|         | 115M/3.50G [00:03<01:51, 30.3MB/s][A
pytorch_model-00002-of-00002.bin:   4%|         | 136M/3.50G [00:03<01:24, 39.8MB/s][A
pytorch_model-00002-of-00002.bin:   4%|         | 147M/3.50G [00:03<01:24, 39.7MB/s][A
pytorch_model-00002-of-00002.bin:   5%|         | 168M/3.50G [00:04<01:15, 44.3MB/s][A
pytorch_model-00002-of-00002.bin:   5%|         | 178M/3.50G [00:04<01:29, 37.2MB/s][A
pytorch_model-00002-of-00002.bin:   6%|         | 199M/3.50G [00:04<01:09, 47.4MB/s][A
pytorch_model-00002-of-00002.bin:   6%|         | 210M/3.50G [00:05<01:09, 47.2MB/s][A
pytorch_model-00002-of-00002.bin:   7%|         | 231M/3.50G [00:05<01:05, 49.6MB/s][A
pytorch_model-00002-of-00002.bin:   7%|         | 241M/3.50G [00:05<01:07, 48.5MB/s][A
pytorch_model-00002-of-00002.bin:   7%|         | 262M/3.50G [00:06<01:10, 45.9MB/s][A
pytorch_model-00002-of-00002.bin:   8%|         | 273M/3.50G [00:06<01:12, 44.5MB/s][A
pytorch_model-00002-of-00002.bin:   8%|         | 294M/3.50G [00:06<00:55, 57.6MB/s][A
pytorch_model-00002-of-00002.bin:   9%|         | 304M/3.50G [00:06<00:57, 55.8MB/s][A
pytorch_model-00002-of-00002.bin:   9%|         | 325M/3.50G [00:07<00:52, 61.0MB/s][A
pytorch_model-00002-of-00002.bin:  10%|         | 336M/3.50G [00:07<00:50, 63.1MB/s][A
pytorch_model-00002-of-00002.bin:  10%|         | 346M/3.50G [00:07<00:49, 63.2MB/s][A
pytorch_model-00002-of-00002.bin:  10%|         | 357M/3.50G [00:08<01:09, 45.4MB/s][A
pytorch_model-00002-of-00002.bin:  10%|         | 367M/3.50G [00:08<01:02, 50.2MB/s][A
pytorch_model-00002-of-00002.bin:  11%|         | 377M/3.50G [00:08<01:12, 43.3MB/s][A
pytorch_model-00002-of-00002.bin:  11%|         | 388M/3.50G [00:08<01:18, 39.9MB/s][A
pytorch_model-00002-of-00002.bin:  12%|        | 409M/3.50G [00:09<00:58, 52.6MB/s][A
pytorch_model-00002-of-00002.bin:  12%|        | 419M/3.50G [00:09<00:59, 51.7MB/s][A
pytorch_model-00002-of-00002.bin:  13%|        | 440M/3.50G [00:09<00:59, 51.2MB/s][A
pytorch_model-00002-of-00002.bin:  13%|        | 451M/3.50G [00:09<01:00, 50.8MB/s][A
pytorch_model-00002-of-00002.bin:  13%|        | 472M/3.50G [00:10<00:57, 52.3MB/s][A
pytorch_model-00002-of-00002.bin:  14%|        | 482M/3.50G [00:10<01:09, 43.5MB/s][A
pytorch_model-00002-of-00002.bin:  14%|        | 503M/3.50G [00:10<01:01, 48.8MB/s][A
pytorch_model-00002-of-00002.bin:  15%|        | 514M/3.50G [00:11<01:14, 40.1MB/s][A
pytorch_model-00002-of-00002.bin:  15%|        | 524M/3.50G [00:11<01:05, 45.5MB/s][A
pytorch_model-00002-of-00002.bin:  15%|        | 535M/3.50G [00:11<01:19, 37.4MB/s][A
pytorch_model-00002-of-00002.bin:  16%|        | 545M/3.50G [00:12<01:17, 38.2MB/s][A
pytorch_model-00002-of-00002.bin:  16%|        | 566M/3.50G [00:12<01:12, 40.4MB/s][A
pytorch_model-00002-of-00002.bin:  16%|        | 577M/3.50G [00:12<01:09, 42.4MB/s][A
pytorch_model-00002-of-00002.bin:  17%|        | 598M/3.50G [00:13<00:54, 53.3MB/s][A
pytorch_model-00002-of-00002.bin:  17%|        | 608M/3.50G [00:13<00:50, 57.8MB/s][A
pytorch_model-00002-of-00002.bin:  18%|        | 619M/3.50G [00:13<00:52, 54.4MB/s][A
pytorch_model-00002-of-00002.bin:  18%|        | 629M/3.50G [00:13<00:58, 49.1MB/s][A
pytorch_model-00002-of-00002.bin:  18%|        | 640M/3.50G [00:14<01:07, 42.4MB/s][A
pytorch_model-00002-of-00002.bin:  19%|        | 650M/3.50G [00:14<01:00, 46.9MB/s][A
pytorch_model-00002-of-00002.bin:  19%|        | 661M/3.50G [00:14<01:06, 42.9MB/s][A
pytorch_model-00002-of-00002.bin:  19%|        | 682M/3.50G [00:14<00:56, 49.9MB/s][A
pytorch_model-00002-of-00002.bin:  20%|        | 692M/3.50G [00:15<00:57, 49.1MB/s][A
pytorch_model-00002-of-00002.bin:  20%|        | 713M/3.50G [00:15<00:54, 51.6MB/s][A
pytorch_model-00002-of-00002.bin:  21%|        | 724M/3.50G [00:15<00:52, 52.7MB/s][A
pytorch_model-00002-of-00002.bin:  21%|       | 744M/3.50G [00:16<00:52, 52.8MB/s][A
pytorch_model-00002-of-00002.bin:  22%|       | 755M/3.50G [00:16<00:53, 51.7MB/s][A
pytorch_model-00002-of-00002.bin:  22%|       | 776M/3.50G [00:16<00:49, 55.1MB/s][A
pytorch_model-00002-of-00002.bin:  22%|       | 786M/3.50G [00:16<00:45, 60.3MB/s][A
pytorch_model-00002-of-00002.bin:  23%|       | 807M/3.50G [00:17<00:41, 65.3MB/s][A
pytorch_model-00002-of-00002.bin:  23%|       | 818M/3.50G [00:17<00:52, 51.1MB/s][A
pytorch_model-00002-of-00002.bin:  24%|       | 828M/3.50G [00:17<00:54, 49.3MB/s][A
pytorch_model-00002-of-00002.bin:  24%|       | 839M/3.50G [00:18<01:06, 40.2MB/s][A
pytorch_model-00002-of-00002.bin:  24%|       | 849M/3.50G [00:18<01:21, 32.6MB/s][A
pytorch_model-00002-of-00002.bin:  25%|       | 860M/3.50G [00:18<01:08, 38.3MB/s][A
pytorch_model-00002-of-00002.bin:  25%|       | 870M/3.50G [00:19<01:12, 36.5MB/s][A
pytorch_model-00002-of-00002.bin:  25%|       | 881M/3.50G [00:19<01:37, 26.9MB/s][A
pytorch_model-00002-of-00002.bin:  26%|       | 902M/3.50G [00:19<01:07, 38.4MB/s][A
pytorch_model-00002-of-00002.bin:  26%|       | 912M/3.50G [00:20<01:03, 40.7MB/s][A
pytorch_model-00002-of-00002.bin:  27%|       | 933M/3.50G [00:20<00:49, 51.4MB/s][A
pytorch_model-00002-of-00002.bin:  27%|       | 954M/3.50G [00:20<00:42, 59.7MB/s][A
pytorch_model-00002-of-00002.bin:  28%|       | 965M/3.50G [00:20<00:45, 56.2MB/s][A
pytorch_model-00002-of-00002.bin:  28%|       | 986M/3.50G [00:21<00:41, 60.8MB/s][A
pytorch_model-00002-of-00002.bin:  28%|       | 996M/3.50G [00:21<00:53, 47.1MB/s][A
pytorch_model-00002-of-00002.bin:  29%|       | 1.02G/3.50G [00:21<00:48, 51.6MB/s][A
pytorch_model-00002-of-00002.bin:  29%|       | 1.03G/3.50G [00:22<00:56, 44.0MB/s][A
pytorch_model-00002-of-00002.bin:  30%|       | 1.04G/3.50G [00:22<00:51, 47.9MB/s][A
pytorch_model-00002-of-00002.bin:  30%|       | 1.05G/3.50G [00:22<00:51, 47.7MB/s][A
pytorch_model-00002-of-00002.bin:  30%|       | 1.06G/3.50G [00:23<00:57, 42.1MB/s][A
pytorch_model-00002-of-00002.bin:  31%|       | 1.08G/3.50G [00:23<00:58, 41.3MB/s][A
pytorch_model-00002-of-00002.bin:  31%|       | 1.09G/3.50G [00:23<01:06, 36.5MB/s][A
pytorch_model-00002-of-00002.bin:  32%|      | 1.11G/3.50G [00:24<00:49, 48.5MB/s][A
pytorch_model-00002-of-00002.bin:  32%|      | 1.12G/3.50G [00:24<00:44, 53.3MB/s][A
pytorch_model-00002-of-00002.bin:  33%|      | 1.14G/3.50G [00:24<00:38, 60.6MB/s][A
pytorch_model-00002-of-00002.bin:  33%|      | 1.15G/3.50G [00:24<00:46, 50.4MB/s][A
pytorch_model-00002-of-00002.bin:  34%|      | 1.17G/3.50G [00:25<00:40, 57.5MB/s][A
pytorch_model-00002-of-00002.bin:  34%|      | 1.18G/3.50G [00:25<00:37, 62.4MB/s][A
pytorch_model-00002-of-00002.bin:  34%|      | 1.21G/3.50G [00:25<00:34, 67.3MB/s][A
pytorch_model-00002-of-00002.bin:  35%|      | 1.22G/3.50G [00:25<00:33, 68.6MB/s][A
pytorch_model-00002-of-00002.bin:  35%|      | 1.24G/3.50G [00:26<00:35, 64.4MB/s][A
pytorch_model-00002-of-00002.bin:  36%|      | 1.25G/3.50G [00:26<00:36, 62.4MB/s][A
pytorch_model-00002-of-00002.bin:  36%|      | 1.26G/3.50G [00:26<00:35, 62.9MB/s][A
pytorch_model-00002-of-00002.bin:  36%|      | 1.27G/3.50G [00:26<00:37, 60.0MB/s][A
pytorch_model-00002-of-00002.bin:  37%|      | 1.29G/3.50G [00:26<00:35, 63.1MB/s][A
pytorch_model-00002-of-00002.bin:  37%|      | 1.30G/3.50G [00:27<00:51, 43.0MB/s][A
pytorch_model-00002-of-00002.bin:  37%|      | 1.31G/3.50G [00:28<01:24, 26.1MB/s][A
pytorch_model-00002-of-00002.bin:  38%|      | 1.32G/3.50G [00:28<01:10, 31.1MB/s][A
pytorch_model-00002-of-00002.bin:  38%|      | 1.33G/3.50G [00:28<00:58, 37.0MB/s][A
pytorch_model-00002-of-00002.bin:  39%|      | 1.35G/3.50G [00:28<00:45, 47.3MB/s][A
pytorch_model-00002-of-00002.bin:  39%|      | 1.36G/3.50G [00:29<00:56, 38.1MB/s][A
pytorch_model-00002-of-00002.bin:  40%|      | 1.38G/3.50G [00:29<00:41, 51.2MB/s][A
pytorch_model-00002-of-00002.bin:  40%|      | 1.39G/3.50G [00:29<00:42, 49.9MB/s][A
pytorch_model-00002-of-00002.bin:  40%|      | 1.41G/3.50G [00:30<00:46, 44.7MB/s][A
pytorch_model-00002-of-00002.bin:  40%|      | 1.42G/3.50G [00:30<01:00, 34.7MB/s][A
pytorch_model-00002-of-00002.bin:  41%|      | 1.43G/3.50G [00:30<01:05, 31.8MB/s][A
pytorch_model-00002-of-00002.bin:  41%|     | 1.45G/3.50G [00:31<00:52, 39.0MB/s][A
pytorch_model-00002-of-00002.bin:  42%|     | 1.46G/3.50G [00:31<00:50, 40.1MB/s][A
pytorch_model-00002-of-00002.bin:  42%|     | 1.48G/3.50G [00:31<00:43, 46.0MB/s][A
pytorch_model-00002-of-00002.bin:  43%|     | 1.49G/3.50G [00:32<00:45, 44.1MB/s][A
pytorch_model-00002-of-00002.bin:  43%|     | 1.51G/3.50G [00:32<00:32, 61.8MB/s][A
pytorch_model-00002-of-00002.bin:  43%|     | 1.52G/3.50G [00:32<00:31, 63.2MB/s][A
pytorch_model-00002-of-00002.bin:  44%|     | 1.53G/3.50G [00:32<00:28, 69.3MB/s][A
pytorch_model-00002-of-00002.bin:  44%|     | 1.54G/3.50G [00:33<00:42, 45.7MB/s][A
pytorch_model-00002-of-00002.bin:  44%|     | 1.55G/3.50G [00:33<00:37, 51.9MB/s][A
pytorch_model-00002-of-00002.bin:  45%|     | 1.56G/3.50G [00:33<00:36, 53.0MB/s][A
pytorch_model-00002-of-00002.bin:  45%|     | 1.57G/3.50G [00:33<00:49, 38.8MB/s][A
pytorch_model-00002-of-00002.bin:  45%|     | 1.58G/3.50G [00:34<00:47, 40.2MB/s][A
pytorch_model-00002-of-00002.bin:  46%|     | 1.59G/3.50G [00:34<00:50, 37.9MB/s][A
pytorch_model-00002-of-00002.bin:  46%|     | 1.60G/3.50G [00:34<00:51, 36.5MB/s][A
pytorch_model-00002-of-00002.bin:  46%|     | 1.61G/3.50G [00:34<00:44, 42.1MB/s][A
pytorch_model-00002-of-00002.bin:  46%|     | 1.63G/3.50G [00:34<00:37, 49.4MB/s][A
pytorch_model-00002-of-00002.bin:  47%|     | 1.64G/3.50G [00:35<00:36, 50.7MB/s][A
pytorch_model-00002-of-00002.bin:  47%|     | 1.66G/3.50G [00:35<00:30, 60.0MB/s][A
pytorch_model-00002-of-00002.bin:  48%|     | 1.67G/3.50G [00:35<00:33, 54.6MB/s][A
pytorch_model-00002-of-00002.bin:  48%|     | 1.69G/3.50G [00:36<00:34, 51.9MB/s][A
pytorch_model-00002-of-00002.bin:  49%|     | 1.70G/3.50G [00:36<00:35, 51.4MB/s][A
pytorch_model-00002-of-00002.bin:  49%|     | 1.72G/3.50G [00:36<00:34, 51.1MB/s][A
pytorch_model-00002-of-00002.bin:  49%|     | 1.73G/3.50G [00:37<00:41, 43.0MB/s][A
pytorch_model-00002-of-00002.bin:  50%|     | 1.74G/3.50G [00:37<00:40, 43.7MB/s][A
pytorch_model-00002-of-00002.bin:  50%|     | 1.75G/3.50G [00:37<00:41, 42.4MB/s][A
pytorch_model-00002-of-00002.bin:  50%|     | 1.76G/3.50G [00:37<00:41, 41.6MB/s][A
pytorch_model-00002-of-00002.bin:  51%|     | 1.78G/3.50G [00:38<00:36, 46.5MB/s][A
pytorch_model-00002-of-00002.bin:  51%|     | 1.79G/3.50G [00:38<00:33, 51.0MB/s][A
pytorch_model-00002-of-00002.bin:  52%|    | 1.81G/3.50G [00:38<00:30, 56.0MB/s][A
pytorch_model-00002-of-00002.bin:  52%|    | 1.82G/3.50G [00:39<00:33, 49.3MB/s][A
pytorch_model-00002-of-00002.bin:  53%|    | 1.85G/3.50G [00:39<00:28, 57.9MB/s][A
pytorch_model-00002-of-00002.bin:  53%|    | 1.87G/3.50G [00:39<00:23, 71.0MB/s][A
pytorch_model-00002-of-00002.bin:  54%|    | 1.88G/3.50G [00:39<00:23, 69.4MB/s][A
pytorch_model-00002-of-00002.bin:  54%|    | 1.90G/3.50G [00:39<00:24, 66.5MB/s][A
pytorch_model-00002-of-00002.bin:  55%|    | 1.91G/3.50G [00:40<00:32, 49.7MB/s][A
pytorch_model-00002-of-00002.bin:  55%|    | 1.93G/3.50G [00:40<00:28, 54.2MB/s][A
pytorch_model-00002-of-00002.bin:  55%|    | 1.94G/3.50G [00:40<00:27, 55.9MB/s][A
pytorch_model-00002-of-00002.bin:  56%|    | 1.95G/3.50G [00:41<00:26, 59.6MB/s][A
pytorch_model-00002-of-00002.bin:  56%|    | 1.96G/3.50G [00:41<00:27, 55.7MB/s][A
pytorch_model-00002-of-00002.bin:  56%|    | 1.97G/3.50G [00:41<00:31, 48.4MB/s][A
pytorch_model-00002-of-00002.bin:  57%|    | 1.99G/3.50G [00:41<00:24, 61.4MB/s][A
pytorch_model-00002-of-00002.bin:  57%|    | 2.00G/3.50G [00:42<00:35, 42.2MB/s][A
pytorch_model-00002-of-00002.bin:  58%|    | 2.02G/3.50G [00:42<00:28, 51.6MB/s][A
pytorch_model-00002-of-00002.bin:  58%|    | 2.03G/3.50G [00:42<00:30, 48.7MB/s][A
pytorch_model-00002-of-00002.bin:  58%|    | 2.04G/3.50G [00:43<00:34, 41.6MB/s][A
pytorch_model-00002-of-00002.bin:  59%|    | 2.06G/3.50G [00:43<00:39, 37.0MB/s][A
pytorch_model-00002-of-00002.bin:  59%|    | 2.07G/3.50G [00:43<00:36, 38.9MB/s][A
pytorch_model-00002-of-00002.bin:  60%|    | 2.09G/3.50G [00:44<00:32, 43.9MB/s][A
pytorch_model-00002-of-00002.bin:  60%|    | 2.10G/3.50G [00:44<00:31, 44.5MB/s][A
pytorch_model-00002-of-00002.bin:  61%|    | 2.12G/3.50G [00:44<00:25, 54.5MB/s][A
pytorch_model-00002-of-00002.bin:  61%|    | 2.13G/3.50G [00:44<00:24, 55.6MB/s][A
pytorch_model-00002-of-00002.bin:  61%|    | 2.14G/3.50G [00:45<00:25, 52.5MB/s][A
pytorch_model-00002-of-00002.bin:  61%|   | 2.15G/3.50G [00:45<00:26, 51.1MB/s][A
pytorch_model-00002-of-00002.bin:  62%|   | 2.16G/3.50G [00:45<00:27, 49.3MB/s][A
pytorch_model-00002-of-00002.bin:  62%|   | 2.18G/3.50G [00:45<00:23, 55.5MB/s][A
pytorch_model-00002-of-00002.bin:  63%|   | 2.19G/3.50G [00:45<00:22, 57.3MB/s][A
pytorch_model-00002-of-00002.bin:  63%|   | 2.20G/3.50G [00:46<00:23, 56.1MB/s][A
pytorch_model-00002-of-00002.bin:  63%|   | 2.21G/3.50G [00:46<00:28, 44.9MB/s][A
pytorch_model-00002-of-00002.bin:  64%|   | 2.23G/3.50G [00:47<00:43, 29.3MB/s][A
pytorch_model-00002-of-00002.bin:  64%|   | 2.24G/3.50G [00:47<00:42, 29.8MB/s][A
pytorch_model-00002-of-00002.bin:  65%|   | 2.26G/3.50G [00:48<00:32, 38.5MB/s][A
pytorch_model-00002-of-00002.bin:  65%|   | 2.28G/3.50G [00:48<00:32, 37.3MB/s][A
pytorch_model-00002-of-00002.bin:  66%|   | 2.30G/3.50G [00:48<00:24, 50.0MB/s][A
pytorch_model-00002-of-00002.bin:  66%|   | 2.31G/3.50G [00:49<00:25, 46.1MB/s][A
pytorch_model-00002-of-00002.bin:  66%|   | 2.32G/3.50G [00:49<00:22, 52.9MB/s][A
pytorch_model-00002-of-00002.bin:  67%|   | 2.33G/3.50G [00:49<00:25, 45.4MB/s][A
pytorch_model-00002-of-00002.bin:  67%|   | 2.34G/3.50G [00:49<00:26, 43.5MB/s][A
pytorch_model-00002-of-00002.bin:  67%|   | 2.36G/3.50G [00:50<00:21, 53.1MB/s][A
pytorch_model-00002-of-00002.bin:  68%|   | 2.37G/3.50G [00:50<00:24, 46.0MB/s][A
pytorch_model-00002-of-00002.bin:  68%|   | 2.39G/3.50G [00:50<00:19, 58.0MB/s][A
pytorch_model-00002-of-00002.bin:  69%|   | 2.40G/3.50G [00:50<00:19, 56.7MB/s][A
pytorch_model-00002-of-00002.bin:  69%|   | 2.42G/3.50G [00:51<00:19, 54.4MB/s][A
pytorch_model-00002-of-00002.bin:  69%|   | 2.43G/3.50G [00:51<00:21, 50.0MB/s][A
pytorch_model-00002-of-00002.bin:  70%|   | 2.45G/3.50G [00:51<00:18, 57.9MB/s][A
pytorch_model-00002-of-00002.bin:  70%|   | 2.46G/3.50G [00:51<00:19, 52.0MB/s][A
pytorch_model-00002-of-00002.bin:  71%|   | 2.49G/3.50G [00:52<00:17, 58.7MB/s][A
pytorch_model-00002-of-00002.bin:  71%|  | 2.50G/3.50G [00:52<00:15, 63.4MB/s][A
pytorch_model-00002-of-00002.bin:  72%|  | 2.51G/3.50G [00:52<00:20, 49.4MB/s][A
pytorch_model-00002-of-00002.bin:  72%|  | 2.52G/3.50G [00:52<00:19, 51.0MB/s][A
pytorch_model-00002-of-00002.bin:  72%|  | 2.53G/3.50G [00:53<00:19, 50.9MB/s][A
pytorch_model-00002-of-00002.bin:  72%|  | 2.54G/3.50G [00:53<00:23, 41.1MB/s][A
pytorch_model-00002-of-00002.bin:  73%|  | 2.55G/3.50G [00:53<00:25, 37.1MB/s][A
pytorch_model-00002-of-00002.bin:  73%|  | 2.57G/3.50G [00:54<00:19, 46.7MB/s][A
pytorch_model-00002-of-00002.bin:  74%|  | 2.58G/3.50G [00:54<00:20, 45.7MB/s][A
pytorch_model-00002-of-00002.bin:  74%|  | 2.60G/3.50G [00:54<00:18, 49.8MB/s][A
pytorch_model-00002-of-00002.bin:  75%|  | 2.61G/3.50G [00:54<00:17, 51.2MB/s][A
pytorch_model-00002-of-00002.bin:  75%|  | 2.63G/3.50G [00:55<00:17, 50.1MB/s][A
pytorch_model-00002-of-00002.bin:  75%|  | 2.64G/3.50G [00:55<00:19, 44.0MB/s][A
pytorch_model-00002-of-00002.bin:  76%|  | 2.66G/3.50G [00:56<00:16, 50.9MB/s][A
pytorch_model-00002-of-00002.bin:  76%|  | 2.67G/3.50G [00:56<00:20, 40.8MB/s][A
pytorch_model-00002-of-00002.bin:  77%|  | 2.69G/3.50G [00:57<00:21, 37.8MB/s][A
pytorch_model-00002-of-00002.bin:  77%|  | 2.71G/3.50G [00:57<00:20, 38.6MB/s][A
pytorch_model-00002-of-00002.bin:  78%|  | 2.73G/3.50G [00:57<00:17, 44.7MB/s][A
pytorch_model-00002-of-00002.bin:  78%|  | 2.74G/3.50G [00:57<00:16, 47.4MB/s][A
pytorch_model-00002-of-00002.bin:  78%|  | 2.75G/3.50G [00:58<00:15, 47.9MB/s][A
pytorch_model-00002-of-00002.bin:  79%|  | 2.76G/3.50G [00:58<00:16, 45.5MB/s][A
pytorch_model-00002-of-00002.bin:  79%|  | 2.77G/3.50G [00:58<00:15, 48.2MB/s][A
pytorch_model-00002-of-00002.bin:  80%|  | 2.79G/3.50G [00:58<00:11, 62.8MB/s][A
pytorch_model-00002-of-00002.bin:  80%|  | 2.81G/3.50G [00:59<00:11, 62.2MB/s][A
pytorch_model-00002-of-00002.bin:  81%|  | 2.82G/3.50G [00:59<00:12, 54.8MB/s][A
pytorch_model-00002-of-00002.bin:  81%|  | 2.84G/3.50G [00:59<00:10, 65.6MB/s][A
pytorch_model-00002-of-00002.bin:  81%| | 2.85G/3.50G [00:59<00:09, 64.9MB/s][A
pytorch_model-00002-of-00002.bin:  82%| | 2.87G/3.50G [01:00<00:09, 64.2MB/s][A
pytorch_model-00002-of-00002.bin:  82%| | 2.88G/3.50G [01:00<00:10, 56.4MB/s][A
pytorch_model-00002-of-00002.bin:  83%| | 2.90G/3.50G [01:00<00:11, 51.3MB/s][A
pytorch_model-00002-of-00002.bin:  83%| | 2.92G/3.50G [01:01<00:12, 46.4MB/s][A
pytorch_model-00002-of-00002.bin:  84%| | 2.94G/3.50G [01:01<00:10, 54.4MB/s][A
pytorch_model-00002-of-00002.bin:  84%| | 2.95G/3.50G [01:01<00:10, 51.8MB/s][A
pytorch_model-00002-of-00002.bin:  85%| | 2.97G/3.50G [01:01<00:09, 57.2MB/s][A
pytorch_model-00002-of-00002.bin:  85%| | 2.98G/3.50G [01:02<00:09, 55.3MB/s][A
pytorch_model-00002-of-00002.bin:  85%| | 2.99G/3.50G [01:02<00:10, 50.3MB/s][A
pytorch_model-00002-of-00002.bin:  86%| | 3.00G/3.50G [01:02<00:09, 51.9MB/s][A
pytorch_model-00002-of-00002.bin:  86%| | 3.01G/3.50G [01:02<00:09, 49.9MB/s][A
pytorch_model-00002-of-00002.bin:  87%| | 3.03G/3.50G [01:03<00:08, 54.6MB/s][A
pytorch_model-00002-of-00002.bin:  87%| | 3.04G/3.50G [01:03<00:08, 55.4MB/s][A
pytorch_model-00002-of-00002.bin:  87%| | 3.05G/3.50G [01:03<00:08, 54.6MB/s][A
pytorch_model-00002-of-00002.bin:  87%| | 3.06G/3.50G [01:03<00:10, 43.8MB/s][A
pytorch_model-00002-of-00002.bin:  88%| | 3.07G/3.50G [01:04<00:08, 47.8MB/s][A
pytorch_model-00002-of-00002.bin:  88%| | 3.09G/3.50G [01:04<00:08, 50.8MB/s][A
pytorch_model-00002-of-00002.bin:  89%| | 3.11G/3.50G [01:04<00:06, 60.4MB/s][A
pytorch_model-00002-of-00002.bin:  89%| | 3.12G/3.50G [01:04<00:06, 56.2MB/s][A
pytorch_model-00002-of-00002.bin:  90%| | 3.15G/3.50G [01:05<00:06, 55.3MB/s][A
pytorch_model-00002-of-00002.bin:  90%| | 3.16G/3.50G [01:05<00:06, 52.2MB/s][A
pytorch_model-00002-of-00002.bin:  91%| | 3.18G/3.50G [01:05<00:04, 65.5MB/s][A
pytorch_model-00002-of-00002.bin:  91%| | 3.19G/3.50G [01:06<00:05, 54.7MB/s][A
pytorch_model-00002-of-00002.bin:  92%|| 3.21G/3.50G [01:06<00:05, 57.2MB/s][A
pytorch_model-00002-of-00002.bin:  92%|| 3.22G/3.50G [01:06<00:05, 54.2MB/s][A
pytorch_model-00002-of-00002.bin:  93%|| 3.24G/3.50G [01:06<00:04, 59.7MB/s][A
pytorch_model-00002-of-00002.bin:  93%|| 3.25G/3.50G [01:07<00:04, 49.9MB/s][A
pytorch_model-00002-of-00002.bin:  93%|| 3.26G/3.50G [01:07<00:04, 50.2MB/s][A
pytorch_model-00002-of-00002.bin:  93%|| 3.27G/3.50G [01:07<00:04, 47.7MB/s][A
pytorch_model-00002-of-00002.bin:  94%|| 3.28G/3.50G [01:08<00:05, 43.2MB/s][A
pytorch_model-00002-of-00002.bin:  94%|| 3.29G/3.50G [01:08<00:04, 43.4MB/s][A
pytorch_model-00002-of-00002.bin:  94%|| 3.30G/3.50G [01:08<00:05, 35.9MB/s][A
pytorch_model-00002-of-00002.bin:  95%|| 3.31G/3.50G [01:08<00:04, 37.8MB/s][A
pytorch_model-00002-of-00002.bin:  95%|| 3.32G/3.50G [01:09<00:05, 31.1MB/s][A
pytorch_model-00002-of-00002.bin:  95%|| 3.33G/3.50G [01:09<00:05, 30.2MB/s][A
pytorch_model-00002-of-00002.bin:  96%|| 3.34G/3.50G [01:10<00:04, 31.2MB/s][A
pytorch_model-00002-of-00002.bin:  96%|| 3.37G/3.50G [01:10<00:03, 43.0MB/s][A
pytorch_model-00002-of-00002.bin:  96%|| 3.38G/3.50G [01:10<00:03, 41.3MB/s][A
pytorch_model-00002-of-00002.bin:  97%|| 3.40G/3.50G [01:10<00:02, 51.1MB/s][A
pytorch_model-00002-of-00002.bin:  97%|| 3.41G/3.50G [01:11<00:01, 55.6MB/s][A
pytorch_model-00002-of-00002.bin:  98%|| 3.42G/3.50G [01:11<00:01, 53.9MB/s][A
pytorch_model-00002-of-00002.bin:  98%|| 3.43G/3.50G [01:11<00:01, 43.4MB/s][A
pytorch_model-00002-of-00002.bin:  99%|| 3.45G/3.50G [01:11<00:00, 54.5MB/s][A
pytorch_model-00002-of-00002.bin:  99%|| 3.46G/3.50G [01:12<00:00, 49.4MB/s][A
pytorch_model-00002-of-00002.bin:  99%|| 3.48G/3.50G [01:12<00:00, 65.1MB/s][A
pytorch_model-00002-of-00002.bin: 100%|| 3.49G/3.50G [01:12<00:00, 51.8MB/s][A
pytorch_model-00002-of-00002.bin: 100%|| 3.50G/3.50G [01:12<00:00, 49.0MB/s][Apytorch_model-00002-of-00002.bin: 100%|| 3.50G/3.50G [01:12<00:00, 48.0MB/s]
Downloading shards: 100%|| 2/2 [04:43<00:00, 129.65s/it]Downloading shards: 100%|| 2/2 [04:43<00:00, 141.69s/it]
Downloading shards: 100%|| 2/2 [04:43<00:00, 129.64s/it]Downloading shards: 100%|| 2/2 [04:43<00:00, 141.68s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [00:16<00:16, 16.12s/it]Loading checkpoint shards:  50%|     | 1/2 [00:16<00:16, 16.73s/it]Loading checkpoint shards: 100%|| 2/2 [00:21<00:00,  9.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:21<00:00, 10.72s/it]
[INFO|modeling_utils.py:3984] 2024-02-12 23:48:32,580 >> Some weights of the model checkpoint at daryl149/llama-2-7b-hf were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 23:48:32,580 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at daryl149/llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Loading checkpoint shards: 100%|| 2/2 [00:22<00:00, 10.07s/it]Loading checkpoint shards: 100%|| 2/2 [00:22<00:00, 11.07s/it]
[WARNING|modeling_utils.py:3996] 2024-02-12 23:48:33,267 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at daryl149/llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
[2024-02-12 23:48:36,415] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 31616) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:48:36
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 31617)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:48:36
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31616)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:50:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:50:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b/runs/Feb12_23-50-45_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc7b,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:50:46 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:50:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:50:47 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[WARNING|logging.py:314] 2024-02-12 23:50:47,431 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:50:47 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:50:47 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 23:50:47,484 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/config.json
[INFO|configuration_utils.py:792] 2024-02-12 23:50:47,486 >> Model config LlamaConfig {
  "_name_or_path": "daryl149/llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:50:47,518 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:50:47,518 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:50:47,518 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:50:47,518 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:50:47,518 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|| 2/2 [00:00<00:00, 6569.00it/s]
[WARNING|logging.py:314] 2024-02-12 23:50:47,611 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-12 23:50:47,712 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--daryl149--llama-2-7b-hf/snapshots/142d0a5354ab12acdfff745a4d5c2ced307970dd/pytorch_model.bin.index.json
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|| 2/2 [00:00<00:00, 6915.59it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [00:16<00:16, 16.21s/it]Loading checkpoint shards:  50%|     | 1/2 [00:16<00:16, 16.06s/it]Loading checkpoint shards: 100%|| 2/2 [00:21<00:00,  9.78s/it]Loading checkpoint shards: 100%|| 2/2 [00:21<00:00,  9.71s/it]Loading checkpoint shards: 100%|| 2/2 [00:21<00:00, 10.75s/it]
[WARNING|modeling_utils.py:3996] 2024-02-12 23:51:09,214 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at daryl149/llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|| 2/2 [00:21<00:00, 10.67s/it]
[INFO|modeling_utils.py:3984] 2024-02-12 23:51:09,218 >> Some weights of the model checkpoint at daryl149/llama-2-7b-hf were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 23:51:09,218 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at daryl149/llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
02/12/2024 23:51:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
02/12/2024 23:51:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
02/12/2024 23:51:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
02/12/2024 23:51:10 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:51:10 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:51:10 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:51:10 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-12 23:51:23,202 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 560, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 560, in main
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1711, in _inner_training_loop
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1228, in prepare
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1711, in _inner_training_loop
    result = tuple(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1229, in <genexpr>
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1105, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1356, in prepare_model
    model = torch.nn.parallel.DistributedDataParallel(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1228, in prepare
    result = tuple(
    self._ddp_init_helper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1103, in _ddp_init_helper
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1229, in <genexpr>
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 35.31 MiB is free. Including non-PyTorch memory, this process has 31.70 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 104.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1105, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py", line 1356, in prepare_model
    model = torch.nn.parallel.DistributedDataParallel(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 812, in __init__
    self._ddp_init_helper(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1103, in _ddp_init_helper
    self.reducer = dist.Reducer(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 57.31 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 31.18 GiB is allocated by PyTorch, and 82.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-02-12 23:51:28,518] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 32164) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_23:51:28
  host      : v005.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 32165)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_23:51:28
  host      : v005.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 32164)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v005: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 23:53:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 23:53:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/runs/Feb12_23-53-13_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 23:53:14 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 23:53:15 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:53:15 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 23:53:15 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 23:53:15 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[WARNING|logging.py:314] 2024-02-12 23:53:15,529 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:729] 2024-02-12 23:53:15,551 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-12 23:53:15,552 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:53:15,583 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:53:15,583 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:53:15,583 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:53:15,583 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 23:53:15,584 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-12 23:53:15,649 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-12 23:53:15,699 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-12 23:53:37,033 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 23:53:37,035 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-12 23:53:37,042 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
02/12/2024 23:53:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
02/12/2024 23:53:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
02/12/2024 23:53:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
02/12/2024 23:53:38 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:53:38 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:53:38 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 23:53:38 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-12 23:53:41,145 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-12 23:53:41,284 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-12 23:53:41,286 >>   Num examples = 3,668
[INFO|trainer.py:1749] 2024-02-12 23:53:41,286 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-12 23:53:41,286 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-12 23:53:41,286 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-12 23:53:41,286 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-12 23:53:41,286 >>   Total optimization steps = 690
[INFO|trainer.py:1756] 2024-02-12 23:53:41,287 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/690 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/690 [00:03<38:59,  3.39s/it]  0%|          | 2/690 [00:04<21:42,  1.89s/it]  0%|          | 3/690 [00:05<16:24,  1.43s/it]  1%|          | 4/690 [00:06<13:56,  1.22s/it]  1%|          | 5/690 [00:06<12:33,  1.10s/it]  1%|          | 6/690 [00:07<11:40,  1.02s/it]  1%|          | 7/690 [00:08<11:08,  1.02it/s]  1%|          | 8/690 [00:09<10:48,  1.05it/s]  1%|         | 9/690 [00:10<10:32,  1.08it/s]  1%|         | 10/690 [00:11<10:25,  1.09it/s]  2%|         | 11/690 [00:12<10:21,  1.09it/s]  2%|         | 12/690 [00:13<10:09,  1.11it/s]  2%|         | 13/690 [00:13<10:07,  1.11it/s]  2%|         | 14/690 [00:14<10:07,  1.11it/s]  2%|         | 15/690 [00:15<10:02,  1.12it/s]  2%|         | 16/690 [00:16<09:58,  1.13it/s]  2%|         | 17/690 [00:17<09:57,  1.13it/s]  3%|         | 18/690 [00:18<09:54,  1.13it/s]  3%|         | 19/690 [00:19<09:56,  1.12it/s]  3%|         | 20/690 [00:20<09:55,  1.12it/s]  3%|         | 21/690 [00:21<09:56,  1.12it/s]  3%|         | 22/690 [00:21<09:50,  1.13it/s]  3%|         | 23/690 [00:22<09:53,  1.12it/s]  3%|         | 24/690 [00:23<09:49,  1.13it/s]  4%|         | 25/690 [00:24<09:54,  1.12it/s]  4%|         | 26/690 [00:25<09:54,  1.12it/s]  4%|         | 27/690 [00:26<09:53,  1.12it/s]  4%|         | 28/690 [00:27<09:51,  1.12it/s]  4%|         | 29/690 [00:28<09:46,  1.13it/s]  4%|         | 30/690 [00:29<09:50,  1.12it/s]  4%|         | 31/690 [00:30<09:48,  1.12it/s]  5%|         | 32/690 [00:30<09:46,  1.12it/s]  5%|         | 33/690 [00:31<09:45,  1.12it/s]  5%|         | 34/690 [00:32<09:45,  1.12it/s]  5%|         | 35/690 [00:33<09:43,  1.12it/s]  5%|         | 36/690 [00:34<09:42,  1.12it/s]  5%|         | 37/690 [00:35<09:41,  1.12it/s]  6%|         | 38/690 [00:36<09:36,  1.13it/s]  6%|         | 39/690 [00:37<09:37,  1.13it/s]  6%|         | 40/690 [00:38<09:34,  1.13it/s]  6%|         | 41/690 [00:38<09:33,  1.13it/s]  6%|         | 42/690 [00:39<09:33,  1.13it/s]  6%|         | 43/690 [00:40<09:36,  1.12it/s]  6%|         | 44/690 [00:41<09:34,  1.12it/s]  7%|         | 45/690 [00:42<09:33,  1.12it/s]  7%|         | 46/690 [00:43<09:36,  1.12it/s]  7%|         | 47/690 [00:44<09:35,  1.12it/s]  7%|         | 48/690 [00:45<09:32,  1.12it/s]  7%|         | 49/690 [00:46<09:30,  1.12it/s]  7%|         | 50/690 [00:46<09:30,  1.12it/s]  7%|         | 51/690 [00:47<09:25,  1.13it/s]  8%|         | 52/690 [00:48<09:26,  1.13it/s]  8%|         | 53/690 [00:49<09:32,  1.11it/s]  8%|         | 54/690 [00:50<09:28,  1.12it/s]  8%|         | 55/690 [00:51<09:26,  1.12it/s]  8%|         | 56/690 [00:52<09:23,  1.13it/s]  8%|         | 57/690 [00:53<09:22,  1.13it/s]  8%|         | 58/690 [00:54<09:21,  1.13it/s]  9%|         | 59/690 [00:54<09:21,  1.12it/s]  9%|         | 60/690 [00:55<09:21,  1.12it/s]  9%|         | 61/690 [00:56<09:24,  1.12it/s]  9%|         | 62/690 [00:57<09:20,  1.12it/s]  9%|         | 63/690 [00:58<09:18,  1.12it/s]  9%|         | 64/690 [00:59<09:18,  1.12it/s]  9%|         | 65/690 [01:00<09:17,  1.12it/s] 10%|         | 66/690 [01:01<09:15,  1.12it/s] 10%|         | 67/690 [01:02<09:18,  1.12it/s] 10%|         | 68/690 [01:02<09:14,  1.12it/s] 10%|         | 69/690 [01:03<09:13,  1.12it/s] 10%|         | 70/690 [01:04<09:12,  1.12it/s] 10%|         | 71/690 [01:05<09:10,  1.12it/s] 10%|         | 72/690 [01:06<09:10,  1.12it/s] 11%|         | 73/690 [01:07<09:08,  1.12it/s] 11%|         | 74/690 [01:08<09:10,  1.12it/s] 11%|         | 75/690 [01:09<09:06,  1.13it/s] 11%|         | 76/690 [01:10<09:08,  1.12it/s] 11%|         | 77/690 [01:10<09:07,  1.12it/s] 11%|        | 78/690 [01:11<09:05,  1.12it/s] 11%|        | 79/690 [01:12<09:06,  1.12it/s] 12%|        | 80/690 [01:13<09:02,  1.12it/s] 12%|        | 81/690 [01:14<09:00,  1.13it/s] 12%|        | 82/690 [01:15<09:01,  1.12it/s] 12%|        | 83/690 [01:16<09:03,  1.12it/s] 12%|        | 84/690 [01:17<09:00,  1.12it/s] 12%|        | 85/690 [01:18<09:01,  1.12it/s] 12%|        | 86/690 [01:19<09:01,  1.12it/s] 13%|        | 87/690 [01:19<08:56,  1.12it/s] 13%|        | 88/690 [01:20<08:59,  1.12it/s] 13%|        | 89/690 [01:21<08:57,  1.12it/s] 13%|        | 90/690 [01:22<08:55,  1.12it/s] 13%|        | 91/690 [01:23<09:03,  1.10it/s] 13%|        | 92/690 [01:24<08:58,  1.11it/s] 13%|        | 93/690 [01:25<08:54,  1.12it/s] 14%|        | 94/690 [01:26<08:53,  1.12it/s] 14%|        | 95/690 [01:27<08:48,  1.13it/s] 14%|        | 96/690 [01:27<08:50,  1.12it/s] 14%|        | 97/690 [01:28<08:49,  1.12it/s] 14%|        | 98/690 [01:29<08:47,  1.12it/s] 14%|        | 99/690 [01:30<08:46,  1.12it/s] 14%|        | 100/690 [01:31<08:47,  1.12it/s] 15%|        | 101/690 [01:32<08:45,  1.12it/s] 15%|        | 102/690 [01:33<08:50,  1.11it/s] 15%|        | 103/690 [01:34<08:46,  1.12it/s] 15%|        | 104/690 [01:35<08:46,  1.11it/s] 15%|        | 105/690 [01:36<08:45,  1.11it/s] 15%|        | 106/690 [01:36<08:41,  1.12it/s] 16%|        | 107/690 [01:37<08:41,  1.12it/s] 16%|        | 108/690 [01:38<08:42,  1.11it/s] 16%|        | 109/690 [01:39<08:41,  1.11it/s] 16%|        | 110/690 [01:40<08:37,  1.12it/s] 16%|        | 111/690 [01:41<08:35,  1.12it/s] 16%|        | 112/690 [01:42<08:35,  1.12it/s] 16%|        | 113/690 [01:43<08:37,  1.11it/s] 17%|        | 114/690 [01:44<08:36,  1.12it/s] 17%|        | 115/690 [01:44<08:33,  1.12it/s] 17%|        | 116/690 [01:45<08:31,  1.12it/s] 17%|        | 117/690 [01:46<08:30,  1.12it/s] 17%|        | 118/690 [01:47<08:28,  1.12it/s] 17%|        | 119/690 [01:48<08:25,  1.13it/s] 17%|        | 120/690 [01:49<08:29,  1.12it/s] 18%|        | 121/690 [01:50<08:29,  1.12it/s] 18%|        | 122/690 [01:51<08:22,  1.13it/s] 18%|        | 123/690 [01:52<08:23,  1.13it/s] 18%|        | 124/690 [01:52<08:21,  1.13it/s] 18%|        | 125/690 [01:53<08:24,  1.12it/s] 18%|        | 126/690 [01:54<08:23,  1.12it/s] 18%|        | 127/690 [01:55<08:22,  1.12it/s] 19%|        | 128/690 [01:56<08:20,  1.12it/s] 19%|        | 129/690 [01:57<08:18,  1.12it/s] 19%|        | 130/690 [01:58<08:18,  1.12it/s] 19%|        | 131/690 [01:59<08:17,  1.12it/s] 19%|        | 132/690 [02:00<08:15,  1.13it/s] 19%|        | 133/690 [02:00<08:14,  1.13it/s] 19%|        | 134/690 [02:01<08:12,  1.13it/s] 20%|        | 135/690 [02:02<08:11,  1.13it/s] 20%|        | 136/690 [02:03<08:11,  1.13it/s] 20%|        | 137/690 [02:04<08:11,  1.13it/s] 20%|        | 138/690 [02:05<08:10,  1.12it/s] 20%|        | 139/690 [02:06<08:09,  1.13it/s] 20%|        | 140/690 [02:07<08:08,  1.13it/s] 20%|        | 141/690 [02:08<08:07,  1.13it/s] 21%|        | 142/690 [02:08<08:06,  1.13it/s] 21%|        | 143/690 [02:09<08:05,  1.13it/s] 21%|        | 144/690 [02:10<08:04,  1.13it/s] 21%|        | 145/690 [02:11<08:04,  1.12it/s] 21%|        | 146/690 [02:12<08:05,  1.12it/s] 21%|       | 147/690 [02:13<08:02,  1.13it/s] 21%|       | 148/690 [02:14<08:05,  1.12it/s] 22%|       | 149/690 [02:15<08:03,  1.12it/s] 22%|       | 150/690 [02:16<08:01,  1.12it/s] 22%|       | 151/690 [02:16<08:01,  1.12it/s] 22%|       | 152/690 [02:17<08:04,  1.11it/s] 22%|       | 153/690 [02:18<08:02,  1.11it/s] 22%|       | 154/690 [02:19<07:58,  1.12it/s] 22%|       | 155/690 [02:20<08:00,  1.11it/s] 23%|       | 156/690 [02:21<07:58,  1.12it/s] 23%|       | 157/690 [02:22<07:56,  1.12it/s] 23%|       | 158/690 [02:23<07:54,  1.12it/s] 23%|       | 159/690 [02:24<07:56,  1.11it/s] 23%|       | 160/690 [02:25<07:54,  1.12it/s] 23%|       | 161/690 [02:25<07:51,  1.12it/s] 23%|       | 162/690 [02:26<07:50,  1.12it/s] 24%|       | 163/690 [02:27<07:46,  1.13it/s] 24%|       | 164/690 [02:28<07:46,  1.13it/s] 24%|       | 165/690 [02:29<07:44,  1.13it/s] 24%|       | 166/690 [02:30<07:45,  1.13it/s] 24%|       | 167/690 [02:31<07:44,  1.13it/s] 24%|       | 168/690 [02:32<07:44,  1.12it/s] 24%|       | 169/690 [02:33<07:43,  1.12it/s] 25%|       | 170/690 [02:33<07:42,  1.13it/s] 25%|       | 171/690 [02:34<07:43,  1.12it/s] 25%|       | 172/690 [02:35<07:41,  1.12it/s] 25%|       | 173/690 [02:36<07:41,  1.12it/s] 25%|       | 174/690 [02:37<07:39,  1.12it/s] 25%|       | 175/690 [02:38<07:39,  1.12it/s] 26%|       | 176/690 [02:39<07:39,  1.12it/s] 26%|       | 177/690 [02:40<07:35,  1.13it/s] 26%|       | 178/690 [02:41<07:38,  1.12it/s] 26%|       | 179/690 [02:41<07:34,  1.12it/s] 26%|       | 180/690 [02:42<07:35,  1.12it/s] 26%|       | 181/690 [02:43<07:36,  1.11it/s] 26%|       | 182/690 [02:44<07:33,  1.12it/s] 27%|       | 183/690 [02:45<07:32,  1.12it/s] 27%|       | 184/690 [02:46<07:32,  1.12it/s] 27%|       | 185/690 [02:47<07:32,  1.12it/s] 27%|       | 186/690 [02:48<07:31,  1.12it/s] 27%|       | 187/690 [02:49<07:29,  1.12it/s] 27%|       | 188/690 [02:50<07:29,  1.12it/s] 27%|       | 189/690 [02:50<07:29,  1.11it/s] 28%|       | 190/690 [02:51<07:26,  1.12it/s] 28%|       | 191/690 [02:52<07:26,  1.12it/s] 28%|       | 192/690 [02:53<07:24,  1.12it/s] 28%|       | 193/690 [02:54<07:27,  1.11it/s] 28%|       | 194/690 [02:55<07:23,  1.12it/s] 28%|       | 195/690 [02:56<07:20,  1.12it/s] 28%|       | 196/690 [02:57<07:20,  1.12it/s] 29%|       | 197/690 [02:58<07:19,  1.12it/s] 29%|       | 198/690 [02:58<07:17,  1.12it/s] 29%|       | 199/690 [02:59<07:19,  1.12it/s] 29%|       | 200/690 [03:00<07:19,  1.11it/s] 29%|       | 201/690 [03:01<07:19,  1.11it/s] 29%|       | 202/690 [03:02<07:16,  1.12it/s] 29%|       | 203/690 [03:03<07:13,  1.12it/s] 30%|       | 204/690 [03:04<07:18,  1.11it/s] 30%|       | 205/690 [03:05<07:14,  1.12it/s] 30%|       | 206/690 [03:06<07:12,  1.12it/s] 30%|       | 207/690 [03:07<07:16,  1.11it/s] 30%|       | 208/690 [03:07<07:11,  1.12it/s] 30%|       | 209/690 [03:08<07:09,  1.12it/s] 30%|       | 210/690 [03:09<07:08,  1.12it/s] 31%|       | 211/690 [03:10<07:09,  1.12it/s] 31%|       | 212/690 [03:11<07:05,  1.12it/s] 31%|       | 213/690 [03:12<07:04,  1.12it/s] 31%|       | 214/690 [03:13<07:05,  1.12it/s] 31%|       | 215/690 [03:14<07:06,  1.11it/s] 31%|      | 216/690 [03:15<07:03,  1.12it/s] 31%|      | 217/690 [03:15<07:03,  1.12it/s] 32%|      | 218/690 [03:16<07:06,  1.11it/s] 32%|      | 219/690 [03:17<07:03,  1.11it/s] 32%|      | 220/690 [03:18<07:03,  1.11it/s] 32%|      | 221/690 [03:19<07:00,  1.11it/s] 32%|      | 222/690 [03:20<06:56,  1.12it/s] 32%|      | 223/690 [03:21<06:59,  1.11it/s] 32%|      | 224/690 [03:22<06:57,  1.12it/s] 33%|      | 225/690 [03:23<06:56,  1.12it/s] 33%|      | 226/690 [03:24<06:53,  1.12it/s] 33%|      | 227/690 [03:24<06:54,  1.12it/s] 33%|      | 228/690 [03:25<06:53,  1.12it/s] 33%|      | 229/690 [03:26<06:52,  1.12it/s] 33%|      | 230/690 [03:27<06:49,  1.12it/s] 33%|      | 231/690 [03:28<06:49,  1.12it/s] 34%|      | 232/690 [03:29<06:49,  1.12it/s] 34%|      | 233/690 [03:30<06:50,  1.11it/s] 34%|      | 234/690 [03:31<06:47,  1.12it/s] 34%|      | 235/690 [03:32<06:44,  1.13it/s] 34%|      | 236/690 [03:32<06:45,  1.12it/s] 34%|      | 237/690 [03:33<06:48,  1.11it/s] 34%|      | 238/690 [03:34<06:44,  1.12it/s] 35%|      | 239/690 [03:35<06:46,  1.11it/s] 35%|      | 240/690 [03:36<06:44,  1.11it/s] 35%|      | 241/690 [03:37<06:44,  1.11it/s] 35%|      | 242/690 [03:38<06:40,  1.12it/s] 35%|      | 243/690 [03:39<06:39,  1.12it/s] 35%|      | 244/690 [03:40<06:40,  1.11it/s] 36%|      | 245/690 [03:41<06:38,  1.12it/s] 36%|      | 246/690 [03:41<06:38,  1.12it/s] 36%|      | 247/690 [03:42<06:35,  1.12it/s] 36%|      | 248/690 [03:43<06:36,  1.11it/s] 36%|      | 249/690 [03:44<06:35,  1.12it/s] 36%|      | 250/690 [03:45<06:30,  1.13it/s] 36%|      | 251/690 [03:46<06:33,  1.12it/s] 37%|      | 252/690 [03:47<06:34,  1.11it/s] 37%|      | 253/690 [03:48<06:34,  1.11it/s] 37%|      | 254/690 [03:49<06:32,  1.11it/s] 37%|      | 255/690 [03:50<06:29,  1.12it/s] 37%|      | 256/690 [03:50<06:27,  1.12it/s] 37%|      | 257/690 [03:51<06:27,  1.12it/s] 37%|      | 258/690 [03:52<06:26,  1.12it/s] 38%|      | 259/690 [03:53<06:23,  1.12it/s] 38%|      | 260/690 [03:54<06:23,  1.12it/s] 38%|      | 261/690 [03:55<06:22,  1.12it/s] 38%|      | 262/690 [03:56<06:18,  1.13it/s] 38%|      | 263/690 [03:57<06:21,  1.12it/s] 38%|      | 264/690 [03:58<06:19,  1.12it/s] 38%|      | 265/690 [03:58<06:18,  1.12it/s] 39%|      | 266/690 [03:59<06:19,  1.12it/s] 39%|      | 267/690 [04:00<06:17,  1.12it/s] 39%|      | 268/690 [04:01<06:17,  1.12it/s] 39%|      | 269/690 [04:02<06:14,  1.12it/s] 39%|      | 270/690 [04:03<06:13,  1.12it/s] 39%|      | 271/690 [04:04<06:11,  1.13it/s] 39%|      | 272/690 [04:05<06:11,  1.12it/s] 40%|      | 273/690 [04:06<06:12,  1.12it/s] 40%|      | 274/690 [04:06<06:11,  1.12it/s] 40%|      | 275/690 [04:07<06:10,  1.12it/s] 40%|      | 276/690 [04:08<06:10,  1.12it/s] 40%|      | 277/690 [04:09<06:07,  1.12it/s] 40%|      | 278/690 [04:10<06:06,  1.13it/s] 40%|      | 279/690 [04:11<06:04,  1.13it/s] 41%|      | 280/690 [04:12<06:05,  1.12it/s] 41%|      | 281/690 [04:13<06:04,  1.12it/s] 41%|      | 282/690 [04:14<06:04,  1.12it/s] 41%|      | 283/690 [04:14<06:03,  1.12it/s] 41%|      | 284/690 [04:15<06:02,  1.12it/s] 41%|     | 285/690 [04:16<05:59,  1.13it/s] 41%|     | 286/690 [04:17<06:01,  1.12it/s] 42%|     | 287/690 [04:18<06:00,  1.12it/s] 42%|     | 288/690 [04:19<05:58,  1.12it/s] 42%|     | 289/690 [04:20<05:58,  1.12it/s] 42%|     | 290/690 [04:21<05:59,  1.11it/s] 42%|     | 291/690 [04:22<05:59,  1.11it/s] 42%|     | 292/690 [04:23<05:55,  1.12it/s] 42%|     | 293/690 [04:23<05:55,  1.12it/s] 43%|     | 294/690 [04:24<05:54,  1.12it/s] 43%|     | 295/690 [04:25<05:51,  1.13it/s] 43%|     | 296/690 [04:26<05:50,  1.13it/s] 43%|     | 297/690 [04:27<05:49,  1.13it/s] 43%|     | 298/690 [04:28<05:51,  1.12it/s] 43%|     | 299/690 [04:29<05:49,  1.12it/s] 43%|     | 300/690 [04:30<05:50,  1.11it/s] 44%|     | 301/690 [04:31<05:47,  1.12it/s] 44%|     | 302/690 [04:31<05:45,  1.12it/s] 44%|     | 303/690 [04:32<05:46,  1.12it/s] 44%|     | 304/690 [04:33<05:44,  1.12it/s] 44%|     | 305/690 [04:34<05:45,  1.12it/s] 44%|     | 306/690 [04:35<05:43,  1.12it/s] 44%|     | 307/690 [04:36<05:41,  1.12it/s] 45%|     | 308/690 [04:37<05:40,  1.12it/s] 45%|     | 309/690 [04:38<05:40,  1.12it/s] 45%|     | 310/690 [04:39<05:37,  1.12it/s] 45%|     | 311/690 [04:39<05:39,  1.12it/s] 45%|     | 312/690 [04:40<05:38,  1.12it/s] 45%|     | 313/690 [04:41<05:36,  1.12it/s] 46%|     | 314/690 [04:42<05:36,  1.12it/s] 46%|     | 315/690 [04:43<05:34,  1.12it/s] 46%|     | 316/690 [04:44<05:34,  1.12it/s] 46%|     | 317/690 [04:45<05:31,  1.13it/s] 46%|     | 318/690 [04:46<05:33,  1.12it/s] 46%|     | 319/690 [04:47<05:30,  1.12it/s] 46%|     | 320/690 [04:48<05:29,  1.12it/s] 47%|     | 321/690 [04:48<05:30,  1.12it/s] 47%|     | 322/690 [04:49<05:30,  1.11it/s] 47%|     | 323/690 [04:50<05:28,  1.12it/s] 47%|     | 324/690 [04:51<05:27,  1.12it/s] 47%|     | 325/690 [04:52<05:25,  1.12it/s] 47%|     | 326/690 [04:53<05:23,  1.13it/s] 47%|     | 327/690 [04:54<05:23,  1.12it/s] 48%|     | 328/690 [04:55<05:22,  1.12it/s] 48%|     | 329/690 [04:56<05:21,  1.12it/s] 48%|     | 330/690 [04:56<05:20,  1.12it/s] 48%|     | 331/690 [04:57<05:20,  1.12it/s] 48%|     | 332/690 [04:58<05:18,  1.12it/s] 48%|     | 333/690 [04:59<05:17,  1.12it/s] 48%|     | 334/690 [05:00<05:17,  1.12it/s] 49%|     | 335/690 [05:01<05:17,  1.12it/s] 49%|     | 336/690 [05:02<05:15,  1.12it/s] 49%|     | 337/690 [05:03<05:12,  1.13it/s] 49%|     | 338/690 [05:04<05:14,  1.12it/s] 49%|     | 339/690 [05:04<05:13,  1.12it/s] 49%|     | 340/690 [05:05<05:14,  1.11it/s] 49%|     | 341/690 [05:06<05:10,  1.12it/s] 50%|     | 342/690 [05:07<05:10,  1.12it/s] 50%|     | 343/690 [05:08<05:11,  1.11it/s] 50%|     | 344/690 [05:09<05:09,  1.12it/s] 50%|     | 345/690 [05:10<05:08,  1.12it/s] 50%|     | 346/690 [05:11<05:05,  1.13it/s] 50%|     | 347/690 [05:12<05:06,  1.12it/s] 50%|     | 348/690 [05:13<05:04,  1.12it/s] 51%|     | 349/690 [05:13<05:05,  1.12it/s] 51%|     | 350/690 [05:14<05:03,  1.12it/s] 51%|     | 351/690 [05:15<05:01,  1.13it/s] 51%|     | 352/690 [05:16<05:01,  1.12it/s] 51%|     | 353/690 [05:17<04:59,  1.12it/s] 51%|    | 354/690 [05:18<04:59,  1.12it/s] 51%|    | 355/690 [05:19<04:59,  1.12it/s] 52%|    | 356/690 [05:20<04:58,  1.12it/s] 52%|    | 357/690 [05:21<04:57,  1.12it/s] 52%|    | 358/690 [05:21<04:57,  1.11it/s] 52%|    | 359/690 [05:22<04:56,  1.12it/s] 52%|    | 360/690 [05:23<04:52,  1.13it/s] 52%|    | 361/690 [05:24<04:54,  1.12it/s] 52%|    | 362/690 [05:25<04:52,  1.12it/s] 53%|    | 363/690 [05:26<04:51,  1.12it/s] 53%|    | 364/690 [05:27<04:50,  1.12it/s] 53%|    | 365/690 [05:28<04:49,  1.12it/s] 53%|    | 366/690 [05:29<04:48,  1.12it/s] 53%|    | 367/690 [05:29<04:51,  1.11it/s] 53%|    | 368/690 [05:30<04:51,  1.11it/s] 53%|    | 369/690 [05:31<04:49,  1.11it/s] 54%|    | 370/690 [05:32<04:48,  1.11it/s] 54%|    | 371/690 [05:33<04:46,  1.11it/s] 54%|    | 372/690 [05:34<04:45,  1.11it/s] 54%|    | 373/690 [05:35<04:42,  1.12it/s] 54%|    | 374/690 [05:36<04:42,  1.12it/s] 54%|    | 375/690 [05:37<04:41,  1.12it/s] 54%|    | 376/690 [05:38<04:40,  1.12it/s] 55%|    | 377/690 [05:38<04:37,  1.13it/s] 55%|    | 378/690 [05:39<04:38,  1.12it/s] 55%|    | 379/690 [05:40<04:36,  1.12it/s] 55%|    | 380/690 [05:41<04:36,  1.12it/s] 55%|    | 381/690 [05:42<04:36,  1.12it/s] 55%|    | 382/690 [05:43<04:35,  1.12it/s] 56%|    | 383/690 [05:44<04:33,  1.12it/s] 56%|    | 384/690 [05:45<04:34,  1.11it/s] 56%|    | 385/690 [05:46<04:33,  1.12it/s] 56%|    | 386/690 [05:46<04:31,  1.12it/s] 56%|    | 387/690 [05:47<04:31,  1.12it/s] 56%|    | 388/690 [05:48<04:29,  1.12it/s] 56%|    | 389/690 [05:49<04:28,  1.12it/s] 57%|    | 390/690 [05:50<04:27,  1.12it/s] 57%|    | 391/690 [05:51<04:27,  1.12it/s] 57%|    | 392/690 [05:52<04:26,  1.12it/s] 57%|    | 393/690 [05:53<04:25,  1.12it/s] 57%|    | 394/690 [05:54<04:23,  1.12it/s] 57%|    | 395/690 [05:55<04:22,  1.12it/s] 57%|    | 396/690 [05:55<04:20,  1.13it/s] 58%|    | 397/690 [05:56<04:23,  1.11it/s] 58%|    | 398/690 [05:57<04:22,  1.11it/s] 58%|    | 399/690 [05:58<04:20,  1.12it/s] 58%|    | 400/690 [05:59<04:20,  1.11it/s] 58%|    | 401/690 [06:00<04:18,  1.12it/s] 58%|    | 402/690 [06:01<04:16,  1.12it/s] 58%|    | 403/690 [06:02<04:16,  1.12it/s] 59%|    | 404/690 [06:03<04:16,  1.12it/s] 59%|    | 405/690 [06:03<04:14,  1.12it/s] 59%|    | 406/690 [06:04<04:13,  1.12it/s] 59%|    | 407/690 [06:05<04:12,  1.12it/s] 59%|    | 408/690 [06:06<04:11,  1.12it/s] 59%|    | 409/690 [06:07<04:11,  1.12it/s] 59%|    | 410/690 [06:08<04:11,  1.11it/s] 60%|    | 411/690 [06:09<04:11,  1.11it/s] 60%|    | 412/690 [06:10<04:10,  1.11it/s] 60%|    | 413/690 [06:11<04:09,  1.11it/s] 60%|    | 414/690 [06:12<04:09,  1.11it/s] 60%|    | 415/690 [06:12<04:09,  1.10it/s] 60%|    | 416/690 [06:13<04:05,  1.11it/s] 60%|    | 417/690 [06:14<04:04,  1.12it/s] 61%|    | 418/690 [06:15<04:03,  1.12it/s] 61%|    | 419/690 [06:16<04:03,  1.11it/s] 61%|    | 420/690 [06:17<04:01,  1.12it/s] 61%|    | 421/690 [06:18<04:01,  1.11it/s] 61%|    | 422/690 [06:19<03:58,  1.12it/s] 61%|   | 423/690 [06:20<03:59,  1.12it/s] 61%|   | 424/690 [06:21<03:58,  1.12it/s] 62%|   | 425/690 [06:21<03:56,  1.12it/s] 62%|   | 426/690 [06:22<03:55,  1.12it/s] 62%|   | 427/690 [06:23<03:55,  1.12it/s] 62%|   | 428/690 [06:24<03:54,  1.12it/s] 62%|   | 429/690 [06:25<03:53,  1.12it/s] 62%|   | 430/690 [06:26<03:52,  1.12it/s] 62%|   | 431/690 [06:27<03:50,  1.12it/s] 63%|   | 432/690 [06:28<03:50,  1.12it/s] 63%|   | 433/690 [06:29<03:49,  1.12it/s] 63%|   | 434/690 [06:29<03:49,  1.12it/s] 63%|   | 435/690 [06:30<03:47,  1.12it/s] 63%|   | 436/690 [06:31<03:47,  1.12it/s] 63%|   | 437/690 [06:32<03:46,  1.12it/s] 63%|   | 438/690 [06:33<03:44,  1.12it/s] 64%|   | 439/690 [06:34<03:44,  1.12it/s] 64%|   | 440/690 [06:35<03:45,  1.11it/s] 64%|   | 441/690 [06:36<03:44,  1.11it/s] 64%|   | 442/690 [06:37<03:43,  1.11it/s] 64%|   | 443/690 [06:38<03:41,  1.12it/s] 64%|   | 444/690 [06:38<03:40,  1.12it/s] 64%|   | 445/690 [06:39<03:40,  1.11it/s] 65%|   | 446/690 [06:40<03:36,  1.13it/s] 65%|   | 447/690 [06:41<03:36,  1.12it/s] 65%|   | 448/690 [06:42<03:34,  1.13it/s] 65%|   | 449/690 [06:43<03:34,  1.13it/s] 65%|   | 450/690 [06:44<03:33,  1.12it/s] 65%|   | 451/690 [06:45<03:32,  1.13it/s] 66%|   | 452/690 [06:46<03:33,  1.11it/s] 66%|   | 453/690 [06:46<03:32,  1.11it/s] 66%|   | 454/690 [06:47<03:30,  1.12it/s] 66%|   | 455/690 [06:48<03:29,  1.12it/s] 66%|   | 456/690 [06:49<03:28,  1.12it/s] 66%|   | 457/690 [06:50<03:29,  1.11it/s] 66%|   | 458/690 [06:51<03:27,  1.12it/s] 67%|   | 459/690 [06:52<03:26,  1.12it/s] 67%|   | 460/690 [06:53<03:25,  1.12it/s] 67%|   | 461/690 [06:54<03:24,  1.12it/s] 67%|   | 462/690 [06:54<03:24,  1.12it/s] 67%|   | 463/690 [06:55<03:22,  1.12it/s] 67%|   | 464/690 [06:56<03:20,  1.13it/s] 67%|   | 465/690 [06:57<03:21,  1.12it/s] 68%|   | 466/690 [06:58<03:19,  1.12it/s] 68%|   | 467/690 [06:59<03:20,  1.11it/s] 68%|   | 468/690 [07:00<03:17,  1.12it/s] 68%|   | 469/690 [07:01<03:18,  1.11it/s] 68%|   | 470/690 [07:02<03:16,  1.12it/s] 68%|   | 471/690 [07:03<03:15,  1.12it/s] 68%|   | 472/690 [07:03<03:13,  1.13it/s] 69%|   | 473/690 [07:04<03:14,  1.12it/s] 69%|   | 474/690 [07:05<03:13,  1.12it/s] 69%|   | 475/690 [07:06<03:11,  1.12it/s] 69%|   | 476/690 [07:07<03:10,  1.12it/s] 69%|   | 477/690 [07:08<03:10,  1.12it/s] 69%|   | 478/690 [07:09<03:09,  1.12it/s] 69%|   | 479/690 [07:10<03:07,  1.13it/s] 70%|   | 480/690 [07:11<03:07,  1.12it/s] 70%|   | 481/690 [07:11<03:06,  1.12it/s] 70%|   | 482/690 [07:12<03:06,  1.12it/s] 70%|   | 483/690 [07:13<03:04,  1.12it/s] 70%|   | 484/690 [07:14<03:04,  1.12it/s] 70%|   | 485/690 [07:15<03:02,  1.12it/s] 70%|   | 486/690 [07:16<03:03,  1.11it/s] 71%|   | 487/690 [07:17<03:01,  1.12it/s] 71%|   | 488/690 [07:18<03:00,  1.12it/s] 71%|   | 489/690 [07:19<02:58,  1.12it/s] 71%|   | 490/690 [07:19<02:58,  1.12it/s] 71%|   | 491/690 [07:20<02:57,  1.12it/s] 71%|  | 492/690 [07:21<02:57,  1.11it/s] 71%|  | 493/690 [07:22<02:56,  1.12it/s] 72%|  | 494/690 [07:23<02:56,  1.11it/s] 72%|  | 495/690 [07:24<02:54,  1.12it/s] 72%|  | 496/690 [07:25<02:54,  1.11it/s] 72%|  | 49{'loss': 0.6291, 'learning_rate': 1.3768115942028985e-05, 'epoch': 2.17}
7/690 [07:26<02:52,  1.12it/s] 72%|  | 498/690 [07:27<02:51,  1.12it/s] 72%|  | 499/690 [07:28<02:50,  1.12it/s] 72%|  | 500/690 [07:28<02:50,  1.12it/s]                                                  72%|  | 500/690 [07:29<02:50,  1.12it/s][INFO|trainer.py:2985] 2024-02-13 00:01:10,528 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-13 00:01:10,531 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:01:36,905 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:01:36,907 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:01:36,908 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tmp-checkpoint-500/special_tokens_map.json
 73%|  | 501/690 [08:45<1:14:13, 23.56s/it] 73%|  | 502/690 [08:46<52:31, 16.76s/it]   73%|  | 503/690 [08:47<37:23, 12.00s/it] 73%|  | 504/690 [08:48<26:51,  8.66s/it] 73%|  | 505/690 [08:48<19:30,  6.33s/it] 73%|  | 506/690 [08:49<14:23,  4.69s/it] 73%|  | 507/690 [08:50<10:51,  3.56s/it] 74%|  | 508/690 [08:51<08:21,  2.76s/it] 74%|  | 509/690 [08:52<06:37,  2.19s/it] 74%|  | 510/690 [08:53<05:24,  1.80s/it] 74%|  | 511/690 [08:54<04:33,  1.53s/it] 74%|  | 512/690 [08:55<03:57,  1.33s/it] 74%|  | 513/690 [08:56<03:31,  1.20s/it] 74%|  | 514/690 [08:56<03:14,  1.10s/it] 75%|  | 515/690 [08:57<03:02,  1.04s/it] 75%|  | 516/690 [08:58<02:52,  1.01it/s] 75%|  | 517/690 [08:59<02:46,  1.04it/s] 75%|  | 518/690 [09:00<02:41,  1.06it/s] 75%|  | 519/690 [09:01<02:38,  1.08it/s] 75%|  | 520/690 [09:02<02:35,  1.10it/s] 76%|  | 521/690 [09:03<02:34,  1.10it/s] 76%|  | 522/690 [09:04<02:32,  1.10it/s] 76%|  | 523/690 [09:04<02:31,  1.10it/s] 76%|  | 524/690 [09:05<02:28,  1.12it/s] 76%|  | 525/690 [09:06<02:27,  1.12it/s] 76%|  | 526/690 [09:07<02:26,  1.12it/s] 76%|  | 527/690 [09:08<02:25,  1.12it/s] 77%|  | 528/690 [09:09<02:24,  1.12it/s] 77%|  | 529/690 [09:10<02:23,  1.12it/s] 77%|  | 530/690 [09:11<02:22,  1.12it/s] 77%|  | 531/690 [09:12<02:21,  1.13it/s] 77%|  | 532/690 [09:12<02:20,  1.12it/s] 77%|  | 533/690 [09:13<02:19,  1.12it/s] 77%|  | 534/690 [09:14<02:20,  1.11it/s] 78%|  | 535/690 [09:15<02:18,  1.12it/s] 78%|  | 536/690 [09:16<02:17,  1.12it/s] 78%|  | 537/690 [09:17<02:16,  1.12it/s] 78%|  | 538/690 [09:18<02:16,  1.12it/s] 78%|  | 539/690 [09:19<02:14,  1.12it/s] 78%|  | 540/690 [09:20<02:13,  1.12it/s] 78%|  | 541/690 [09:20<02:13,  1.12it/s] 79%|  | 542/690 [09:21<02:11,  1.12it/s] 79%|  | 543/690 [09:22<02:10,  1.13it/s] 79%|  | 544/690 [09:23<02:09,  1.13it/s] 79%|  | 545/690 [09:24<02:08,  1.13it/s] 79%|  | 546/690 [09:25<02:08,  1.12it/s] 79%|  | 547/690 [09:26<02:07,  1.12it/s] 79%|  | 548/690 [09:27<02:05,  1.13it/s] 80%|  | 549/690 [09:28<02:06,  1.12it/s] 80%|  | 550/690 [09:28<02:05,  1.12it/s] 80%|  | 551/690 [09:29<02:04,  1.12it/s] 80%|  | 552/690 [09:30<02:03,  1.12it/s] 80%|  | 553/690 [09:31<02:02,  1.12it/s] 80%|  | 554/690 [09:32<02:01,  1.12it/s] 80%|  | 555/690 [09:33<01:59,  1.13it/s] 81%|  | 556/690 [09:34<01:59,  1.12it/s] 81%|  | 557/690 [09:35<02:00,  1.11it/s] 81%|  | 558/690 [09:36<01:58,  1.11it/s] 81%|  | 559/690 [09:37<01:57,  1.12it/s] 81%|  | 560/690 [09:37<01:56,  1.12it/s] 81%| | 561/690 [09:38<01:55,  1.12it/s] 81%| | 562/690 [09:39<01:54,  1.12it/s] 82%| | 563/690 [09:40<01:52,  1.12it/s] 82%| | 564/690 [09:41<01:52,  1.12it/s] 82%| | 565/690 [09:42<01:51,  1.12it/s] 82%| | 566/690 [09:43<01:51,  1.12it/s] 82%| | 567/690 [09:44<01:49,  1.12it/s] 82%| | 568/690 [09:45<01:49,  1.11it/s] 82%| | 569/690 [09:45<01:48,  1.11it/s] 83%| | 570/690 [09:46<01:47,  1.12it/s] 83%| | 571/690 [09:47<01:46,  1.12it/s] 83%| | 572/690 [09:48<01:45,  1.12it/s] 83%| | 573/690 [09:49<01:44,  1.12it/s] 83%| | 574/690 [09:50<01:43,  1.12it/s] 83%| | 575/690 [09:51<01:41,  1.13it/s] 83%| | 576/690 [09:52<01:41,  1.13it/s] 84%| | 577/690 [09:53<01:40,  1.12it/s] 84%| | 578/690 [09:53<01:39,  1.12it/s] 84%| | 579/690 [09:54<01:39,  1.12it/s] 84%| | 580/690 [09:55<01:38,  1.12it/s] 84%| | 581/690 [09:56<01:37,  1.12it/s] 84%| | 582/690 [09:57<01:36,  1.12it/s] 84%| | 583/690 [09:58<01:35,  1.13it/s] 85%| | 584/690 [09:59<01:33,  1.13it/s] 85%| | 585/690 [10:00<01:33,  1.12it/s] 85%| | 586/690 [10:01<01:32,  1.13it/s] 85%| | 587/690 [10:02<01:31,  1.12it/s] 85%| | 588/690 [10:02<01:30,  1.12it/s] 85%| | 589/690 [10:03<01:29,  1.13it/s] 86%| | 590/690 [10:04<01:28,  1.13it/s] 86%| | 591/690 [10:05<01:27,  1.13it/s] 86%| | 592/690 [10:06<01:27,  1.12it/s] 86%| | 593/690 [10:07<01:26,  1.12it/s] 86%| | 594/690 [10:08<01:25,  1.13it/s] 86%| | 595/690 [10:09<01:24,  1.13it/s] 86%| | 596/690 [10:10<01:23,  1.12it/s] 87%| | 597/690 [10:10<01:22,  1.12it/s] 87%| | 598/690 [10:11<01:22,  1.12it/s] 87%| | 599/690 [10:12<01:21,  1.12it/s] 87%| | 600/690 [10:13<01:20,  1.12it/s] 87%| | 601/690 [10:14<01:19,  1.12it/s] 87%| | 602/690 [10:15<01:18,  1.12it/s] 87%| | 603/690 [10:16<01:17,  1.12it/s] 88%| | 604/690 [10:17<01:16,  1.13it/s] 88%| | 605/690 [10:18<01:15,  1.12it/s] 88%| | 606/690 [10:18<01:14,  1.12it/s] 88%| | 607/690 [10:19<01:13,  1.13it/s] 88%| | 608/690 [10:20<01:13,  1.12it/s] 88%| | 609/690 [10:21<01:11,  1.13it/s] 88%| | 610/690 [10:22<01:10,  1.13it/s] 89%| | 611/690 [10:23<01:10,  1.12it/s] 89%| | 612/690 [10:24<01:09,  1.12it/s] 89%| | 613/690 [10:25<01:08,  1.13it/s] 89%| | 614/690 [10:26<01:08,  1.12it/s] 89%| | 615/690 [10:26<01:07,  1.12it/s] 89%| | 616/690 [10:27<01:06,  1.12it/s] 89%| | 617/690 [10:28<01:05,  1.12it/s] 90%| | 618/690 [10:29<01:04,  1.12it/s] 90%| | 619/690 [10:30<01:03,  1.11it/s] 90%| | 620/690 [10:31<01:02,  1.12it/s] 90%| | 621/690 [10:32<01:01,  1.12it/s] 90%| | 622/690 [10:33<01:00,  1.13it/s] 90%| | 623/690 [10:34<00:59,  1.13it/s] 90%| | 624/690 [10:34<00:58,  1.12it/s] 91%| | 625/690 [10:35<00:57,  1.13it/s] 91%| | 626/690 [10:36<00:57,  1.12it/s] 91%| | 627/690 [10:37<00:56,  1.12it/s] 91%| | 628/690 [10:38<00:55,  1.12it/s] 91%| | 629/690 [10:39<00:54,  1.12it/s] 91%|| 630/690 [10:40<00:53,  1.12it/s] 91%|| 631/690 [10:41<00:52,  1.12it/s] 92%|| 632/690 [10:42<00:51,  1.12it/s] 92%|| 633/690 [10:42<00:50,  1.12it/s] 92%|| 634/690 [10:43<00:49,  1.12it/s] 92%|| 635/690 [10:44<00:49,  1.12it/s] 92%|| 636/690 [10:45<00:48,  1.12it/s] 92%|| 637/690 [10:46<00:47,  1.12it/s] 92%|| 638/690 [10:47<00:46,  1.13it/s] 93%|| 639/690 [10:48<00:45,  1.11it/s] 93%|| 640/690 [10:49<00:44,  1.12it/s] 93%|| 641/690 [10:50<00:43,  1.12it/s] 93%|| 642/690 [10:51<00:42,  1.12it/s] 93%|| 643/690 [10:51<00:41,  1.12it/s] 93%|| 644/690 [10:52<00:40,  1.12it/s] 93%|| 645/690 [10:53<00:39,  1.13it/s] 94%|| 646/690 [10:54<00:38,  1.13it/s] 94%|| 647/690 [10:55<00:38,  1.12it/s] 94%|| 648/690 [10:56<00:37,  1.12it/s] 94%|| 649/690 [10:57<00:36,  1.12it/s] 94%|| 650/690 [10:58<00:35,  1.12it/s] 94%|| 651/690 [10:59<00:34,  1.13it/s] 94%|| 652/690 [10:59<00:33,  1.12it/s] 95%|| 653/690 [11:00<00:32,  1.12it/s] 95%|| 654/690 [11:01<00:32,  1.12it/s] 95%|| 655/690 [11:02<00:31,  1.11it/s] 95%|| 656/690 [11:03<00:30,  1.12it/s] 95%|| 657/690 [11:04<00:29,  1.11it/s] 95%|| 658/690 [11:05<00:28,  1.12it/s] 96%|| 659/690 [11:06<00:27,  1.12it/s] 96%|| 660/690 [11:07<00:26,  1.12it/s] 96%|| 661/690 [11:07<00:25,  1.13it/s] 96%|| 662/690 [11:08<00:24,  1.12it/s] 96%|| 663/690 [11:09<00:24,  1.12it/s] 96%|| 664/690 [11:10<00:23,  1.12it/s] 96%|| 665/690 [11:11<00:22,  1.13it/s] 97%|| 666/690 [11:12<00:21,  1.13it/s] 97%|| 667/690 [11:13<00:20,  1.13it/s] 97%|| 668/690 [11:14<00:19,  1.12it/s] 97%|| 669/690 [11:15<00:18,  1.12it/s] 97%|| 670/690 [11:15<00:17,  1.13it/s] 97%|| 671/690 [11:16<00:17,  1.12it/s] 97%|| 672/690 [11:17<00:16,  1.12it/s] 98%|| 673/690 [11:18<00:15,  1.12it/s] 98%|| 674/690 [11:19<00:14,  1.12it/s] 98%|| 675/690 [11:20<00:13,  1.13it/s] 98%|| 676/690 [11:21<00:12,  1.12it/s] 98%|| 677/690 [11:22<00:11,  1.12it/s] 98%|| 678/690 [11:23<00:10,  1.12it/s] 98%|| 679/690 [11:23<00:09,  1.13it/s] 99%|| 680/690 [11:24<00:08,  1.13it/s] 99%|| 681/690 [11:25<00:08,  1.12it/s] 99%|| 682/690 [11:26<00:07,  1.12it/s] 99%|| 683/690 [11:27<00:06,  1.11it/s] 99%|| 684/690 [11:28<00:05,  1.12it/s] 99%|| 685/690 [11:29<00:04,  1.11it/s] 99%|| 686/690 [11:30<00:03,  1.12it/s]100%|| 687/690 [11:31<00:02,  1.11it/s]100%|| 688/690 [11:32<00:01,  1.11it/s]100%|| 689/690 [11:32<00:00,  1.12it/s]100%|| 690/690 [11:33<00:00,  1.12it/s][INFO|trainer.py:1988] 2024-02-13 00:05:15,258 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 694.086, 'train_samples_per_second': 15.854, 'train_steps_per_second': 0.994, 'train_loss': 0.5416578209918478, 'epoch': 3.0}
                                                 100%|| 690/690 [11:33<00:00,  1.12it/s]100%|| 690/690 [11:33<00:00,  1.01s/it]
[INFO|trainer.py:2985] 2024-02-13 00:05:15,392 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc
[INFO|configuration_utils.py:473] 2024-02-13 00:05:15,396 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:05:38,819 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:05:38,821 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:05:38,822 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.5417
  train_runtime            = 0:11:34.08
  train_samples            =       3668
  train_samples_per_second =     15.854
  train_steps_per_second   =      0.994
02/13/2024 00:05:38 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 00:05:38,874 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 00:05:38,877 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 00:05:38,877 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-13 00:05:38,877 >>   Batch size = 8
  0%|          | 0/26 [00:00<?, ?it/s]  8%|         | 2/26 [00:00<00:02,  8.15it/s] 12%|        | 3/26 [00:00<00:04,  5.54it/s] 15%|        | 4/26 [00:00<00:04,  4.83it/s] 19%|        | 5/26 [00:01<00:04,  4.34it/s] 23%|       | 6/26 [00:01<00:04,  4.29it/s] 27%|       | 7/26 [00:01<00:04,  4.14it/s] 31%|       | 8/26 [00:01<00:04,  4.07it/s] 35%|      | 9/26 [00:02<00:04,  4.06it/s] 38%|      | 10/26 [00:02<00:03,  4.02it/s] 42%|     | 11/26 [00:02<00:03,  3.86it/s] 46%|     | 12/26 [00:02<00:03,  3.89it/s] 50%|     | 13/26 [00:03<00:03,  3.77it/s] 54%|    | 14/26 [00:03<00:03,  3.83it/s] 58%|    | 15/26 [00:03<00:03,  2.97it/s] 62%|   | 16/26 [00:04<00:03,  3.21it/s] 65%|   | 17/26 [00:04<00:02,  3.42it/s] 69%|   | 18/26 [00:04<00:02,  3.45it/s] 73%|  | 19/26 [00:04<00:01,  3.68it/s] 77%|  | 20/26 [00:05<00:01,  3.75it/s] 81%|  | 21/26 [00:05<00:01,  3.90it/s] 85%| | 22/26 [00:05<00:01,  3.91it/s] 88%| | 23/26 [00:05<00:00,  3.90it/s] 92%|| 24/26 [00:06<00:00,  3.90it/s] 96%|| 25/26 [00:06<00:00,  3.96it/s]100%|| 26/26 [00:06<00:00,  3.87it/s]100%|| 26/26 [00:06<00:00,  3.72it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.7132
  eval_combined_score     =     0.7576
  eval_f1                 =      0.802
  eval_loss               =     0.8239
  eval_runtime            = 0:00:07.25
  eval_samples            =        408
  eval_samples_per_second =     56.213
  eval_steps_per_second   =      3.582
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 00:09:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 00:09:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/runs/Feb13_00-09-40_v005.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 00:09:40 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]Downloading data: 100%|| 3.11M/3.11M [00:00<00:00, 19.2MB/s]Downloading data: 100%|| 3.11M/3.11M [00:00<00:00, 18.9MB/s]
Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]Downloading data: 100%|| 72.8k/72.8k [00:00<00:00, 1.05MB/s]
Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]Downloading data: 100%|| 148k/148k [00:00<00:00, 4.05MB/s]
Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]Generating train split: 100%|| 67349/67349 [00:00<00:00, 535068.99 examples/s]
Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]Generating validation split: 100%|| 872/872 [00:00<00:00, 300503.91 examples/s]
Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]Generating test split: 100%|| 1821/1821 [00:00<00:00, 564844.52 examples/s]
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 00:09:43 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 00:09:43 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 00:09:43,296 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 00:09:43,298 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 00:09:43,330 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 00:09:43,330 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 00:09:43,330 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 00:09:43,330 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 00:09:43,331 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 00:09:43,426 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-13 00:09:43,437 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 00:09:43,530 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|modeling_utils.py:3996] 2024-02-13 00:09:49,387 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3984] 2024-02-13 00:09:49,391 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 00:09:49,392 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9d90d7dab54fe7e1.arrow
02/13/2024 00:09:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9d90d7dab54fe7e1.arrow
Running tokenizer on dataset:   1%|         | 1000/67349 [00:00<00:21, 3110.90 examples/s]Running tokenizer on dataset:   3%|         | 2000/67349 [00:00<00:16, 3937.32 examples/s]Running tokenizer on dataset:   6%|         | 4000/67349 [00:00<00:09, 6986.66 examples/s]Running tokenizer on dataset:   9%|         | 6000/67349 [00:00<00:06, 8892.01 examples/s]Running tokenizer on dataset:  12%|        | 8000/67349 [00:00<00:05, 10249.37 examples/s]Running tokenizer on dataset:  15%|        | 10000/67349 [00:01<00:05, 11016.39 examples/s]Running tokenizer on dataset:  18%|        | 12000/67349 [00:01<00:04, 11592.14 examples/s]Running tokenizer on dataset:  21%|        | 14000/67349 [00:01<00:04, 12029.07 examples/s]Running tokenizer on dataset:  24%|       | 16000/67349 [00:01<00:04, 12349.12 examples/s]Running tokenizer on dataset:  27%|       | 18000/67349 [00:01<00:03, 12523.06 examples/s]Running tokenizer on dataset:  30%|       | 20000/67349 [00:01<00:03, 12803.50 examples/s]Running tokenizer on dataset:  33%|      | 22000/67349 [00:02<00:03, 12978.67 examples/s]Running tokenizer on dataset:  36%|      | 24000/67349 [00:02<00:03, 13085.12 examples/s]Running tokenizer on dataset:  39%|      | 26000/67349 [00:02<00:03, 10944.22 examples/s]Running tokenizer on dataset:  42%|     | 28000/67349 [00:02<00:03, 11502.82 examples/s]Running tokenizer on dataset:  45%|     | 30000/67349 [00:02<00:03, 12063.05 examples/s]Running tokenizer on dataset:  48%|     | 32000/67349 [00:02<00:02, 12447.29 examples/s]Running tokenizer on dataset:  50%|     | 34000/67349 [00:03<00:02, 12582.65 examples/s]Running tokenizer on dataset:  53%|    | 36000/67349 [00:03<00:02, 12643.47 examples/s]Running tokenizer on dataset:  56%|    | 38000/67349 [00:03<00:02, 12838.60 examples/s]Running tokenizer on dataset:  59%|    | 40000/67349 [00:03<00:02, 12965.65 examples/s]Running tokenizer on dataset:  62%|   | 42000/67349 [00:03<00:01, 13057.92 examples/s]Running tokenizer on dataset:  65%|   | 44000/67349 [00:03<00:01, 13151.14 examples/s]Running tokenizer on dataset:  68%|   | 46000/67349 [00:03<00:01, 13079.52 examples/s]Running tokenizer on dataset:  71%|  | 48000/67349 [00:04<00:01, 13106.70 examples/s]Running tokenizer on dataset:  74%|  | 50000/67349 [00:04<00:01, 10857.90 examples/s]Running tokenizer on dataset:  77%|  | 52000/67349 [00:04<00:01, 11328.74 examples/s]Running tokenizer on dataset:  80%|  | 54000/67349 [00:04<00:01, 11776.38 examples/s]Running tokenizer on dataset:  83%| | 56000/67349 [00:04<00:00, 12110.49 examples/s]Running tokenizer on dataset:  86%| | 58000/67349 [00:05<00:00, 12429.40 examples/s]Running tokenizer on dataset:  89%| | 60000/67349 [00:05<00:00, 12548.45 examples/s]Running tokenizer on dataset:  92%|| 62000/67349 [00:05<00:00, 12694.43 examples/s]Running tokenizer on dataset:  95%|| 64000/67349 [00:05<00:00, 12961.64 examples/s]Running tokenizer on dataset:  98%|| 66000/67349 [00:05<00:00, 13086.47 examples/s]Running tokenizer on dataset: 100%|| 67349/67349 [00:05<00:00, 13013.06 examples/s]Running tokenizer on dataset: 100%|| 67349/67349 [00:05<00:00, 11761.72 examples/s]
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-102b700b1eb14291.arrow
02/13/2024 00:09:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-102b700b1eb14291.arrow
Running tokenizer on dataset: 100%|| 872/872 [00:00<00:00, 9788.66 examples/s]
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9e489da059060cfb.arrow
02/13/2024 00:09:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9e489da059060cfb.arrow
Running tokenizer on dataset: 100%|| 1821/1821 [00:00<00:00, 10451.55 examples/s]Running tokenizer on dataset: 100%|| 1821/1821 [00:00<00:00, 10175.07 examples/s]
02/13/2024 00:09:56 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 263, 2107, 14064, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]}.
02/13/2024 00:09:56 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 22684, 292, 1919, 565, 10579, 3918, 1891, 1919, 3158, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 00:09:56 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1584, 746, 727, 526, 301, 913, 29879, 1919, 278, 23023, 1080, 2833, 15585, 1919, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|| 872/872 [00:00<00:00, 8436.36 examples/s]Running tokenizer on dataset: 100%|| 872/872 [00:00<00:00, 8074.62 examples/s]
02/13/2024 00:09:56 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 00:09:59,244 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 00:09:59,673 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 00:09:59,674 >>   Num examples = 67,349
[INFO|trainer.py:1749] 2024-02-13 00:09:59,674 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 00:09:59,674 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 00:09:59,674 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 00:09:59,674 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 00:09:59,674 >>   Total optimization steps = 12,630
[INFO|trainer.py:1756] 2024-02-13 00:09:59,675 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/12630 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/12630 [00:02<9:19:29,  2.66s/it]  0%|          | 2/12630 [00:03<5:29:02,  1.56s/it]  0%|          | 3/12630 [00:04<4:23:06,  1.25s/it]  0%|          | 4/12630 [00:05<3:53:09,  1.11s/it]  0%|          | 5/12630 [00:06<3:36:29,  1.03s/it]  0%|          | 6/12630 [00:07<3:27:56,  1.01it/s]  0%|          | 7/12630 [00:07<3:21:04,  1.05it/s]  0%|          | 8/12630 [00:08<3:15:30,  1.08it/s]  0%|          | 9/12630 [00:09<3:12:34,  1.09it/s]  0%|          | 10/12630 [00:10<3:09:39,  1.11it/s]  0%|          | 11/12630 [00:11<3:08:08,  1.12it/s]  0%|          | 12/12630 [00:12<3:07:29,  1.12it/s]  0%|          | 13/12630 [00:13<3:07:43,  1.12it/s]  0%|          | 14/12630 [00:14<3:05:38,  1.13it/s]  0%|          | 15/12630 [00:14<3:05:42,  1.13it/s]  0%|          | 16/12630 [00:15<3:04:41,  1.14it/s]  0%|          | 17/12630 [00:16<3:05:05,  1.14it/s]  0%|          | 18/12630 [00:17<3:04:27,  1.14it/s]  0%|          | 19/12630 [00:18<3:04:05,  1.14it/s]  0%|          | 20/12630 [00:19<3:04:52,  1.14it/s]  0%|          | 21/12630 [00:20<3:05:19,  1.13it/s]  0%|          | 22/12630 [00:21<3:04:38,  1.14it/s]  0%|          | 23/12630 [00:21<3:03:21,  1.15it/s]  0%|          | 24/12630 [00:22<3:03:47,  1.14it/s]  0%|          | 25/12630 [00:23<3:05:33,  1.13it/s]  0%|          | 26/12630 [00:24<3:05:59,  1.13it/s]  0%|          | 27/12630 [00:25<3:04:13,  1.14it/s]  0%|          | 28/12630 [00:26<3:05:20,  1.13it/s]  0%|          | 29/12630 [00:27<3:05:19,  1.13it/s]  0%|          | 30/12630 [00:28<3:04:38,  1.14it/s]  0%|          | 31/12630 [00:28<3:04:32,  1.14it/s]  0%|          | 32/12630 [00:29<3:05:14,  1.13it/s]  0%|          | 33/12630 [00:30<3:04:47,  1.14it/s]  0%|          | 34/12630 [00:31<3:06:45,  1.12it/s]  0%|          | 35/12630 [00:32<3:05:02,  1.13it/s]  0%|          | 36/12630 [00:33<3:05:18,  1.13it/s]  0%|          | 37/12630 [00:34<3:05:53,  1.13it/s]  0%|          | 38/12630 [00:35<3:06:02,  1.13it/s]  0%|          | 39/12630 [00:36<3:05:52,  1.13it/s]  0%|          | 40/12630 [00:36<3:05:04,  1.13it/s]  0%|          | 41/12630 [00:37<3:04:56,  1.13it/s]  0%|          | 42/12630 [00:38<3:05:19,  1.13it/s]  0%|          | 43/12630 [00:39<3:07:11,  1.12it/s]  0%|          | 44/12630 [00:40<3:06:11,  1.13it/s]  0%|          | 45/12630 [00:41<3:08:09,  1.11it/s]  0%|          | 46/12630 [00:42<3:07:07,  1.12it/s]  0%|          | 47/12630 [00:43<3:07:23,  1.12it/s]  0%|          | 48/12630 [00:44<3:06:22,  1.13it/s]  0%|          | 49/12630 [00:44<3:05:55,  1.13it/s]  0%|          | 50/12630 [00:45<3:05:54,  1.13it/s]  0%|          | 51/12630 [00:46<3:05:57,  1.13it/s]  0%|          | 52/12630 [00:47<3:05:58,  1.13it/s]  0%|          | 53/12630 [00:48<3:05:12,  1.13it/s]  0%|          | 54/12630 [00:49<3:04:06,  1.14it/s]  0%|          | 55/12630 [00:50<3:06:21,  1.12it/s]  0%|          | 56/12630 [00:51<3:06:14,  1.13it/s]  0%|          | 57/12630 [00:52<3:07:01,  1.12it/s]  0%|          | 58/12630 [00:52<3:05:52,  1.13it/s]  0%|          | 59/12630 [00:53<3:05:16,  1.13it/s]  0%|          | 60/12630 [00:54<3:06:13,  1.13it/s]  0%|          | 61/12630 [00:55<3:06:45,  1.12it/s]  0%|          | 62/12630 [00:56<3:05:36,  1.13it/s]  0%|          | 63/12630 [00:57<3:06:39,  1.12it/s]  1%|          | 64/12630 [00:58<3:05:25,  1.13it/s]  1%|          | 65/12630 [00:59<3:05:28,  1.13it/s]  1%|          | 66/12630 [01:00<3:04:50,  1.13it/s]  1%|          | 67/12630 [01:00<3:04:51,  1.13it/s]  1%|          | 68/12630 [01:01<3:04:19,  1.14it/s]  1%|          | 69/12630 [01:02<3:05:50,  1.13it/s]  1%|          | 70/12630 [01:03<3:06:14,  1.12it/s]  1%|          | 71/12630 [01:04<3:05:28,  1.13it/s]  1%|          | 72/12630 [01:05<3:06:40,  1.12it/s]  1%|          | 73/12630 [01:06<3:05:51,  1.13it/s]  1%|          | 74/12630 [01:07<3:05:17,  1.13it/s]  1%|          | 75/12630 [01:08<3:05:16,  1.13it/s]  1%|          | 76/12630 [01:08<3:04:28,  1.13it/s]  1%|          | 77/12630 [01:09<3:04:54,  1.13it/s]  1%|          | 78/12630 [01:10<3:04:46,  1.13it/s]  1%|          | 79/12630 [01:11<3:04:13,  1.14it/s]  1%|          | 80/12630 [01:12<3:05:16,  1.13it/s]  1%|          | 81/12630 [01:13<3:04:26,  1.13it/s]  1%|          | 82/12630 [01:14<3:03:21,  1.14it/s]  1%|          | 83/12630 [01:15<3:05:51,  1.13it/s]  1%|          | 84/12630 [01:15<3:06:22,  1.12it/s]  1%|          | 85/12630 [01:16<3:05:14,  1.13it/s]  1%|          | 86/12630 [01:17<3:04:39,  1.13it/s]  1%|          | 87/12630 [01:18<3:05:06,  1.13it/s]  1%|          | 88/12630 [01:19<3:04:53,  1.13it/s]  1%|          | 89/12630 [01:20<3:04:40,  1.13it/s]  1%|          | 90/12630 [01:21<3:05:27,  1.13it/s]  1%|          | 91/12630 [01:22<3:05:59,  1.12it/s]  1%|          | 92/12630 [01:23<3:05:47,  1.12it/s]  1%|          | 93/12630 [01:23<3:06:40,  1.12it/s]  1%|          | 94/12630 [01:24<3:05:28,  1.13it/s]  1%|          | 95/12630 [01:25<3:04:38,  1.13it/s]  1%|          | 96/12630 [01:26<3:04:53,  1.13it/s]  1%|          | 97/12630 [01:27<3:05:15,  1.13it/s]  1%|          | 98/12630 [01:28<3:04:27,  1.13it/s]  1%|          | 99/12630 [01:29<3:05:29,  1.13it/s]  1%|          | 100/12630 [01:30<3:04:25,  1.13it/s]  1%|          | 101/12630 [01:31<3:04:32,  1.13it/s]  1%|          | 102/12630 [01:31<3:05:50,  1.12it/s]  1%|          | 103/12630 [01:32<3:05:12,  1.13it/s]  1%|          | 104/12630 [01:33<3:03:40,  1.14it/s]  1%|          | 105/12630 [01:34<3:04:14,  1.13it/s]  1%|          | 106/12630 [01:35<3:03:39,  1.14it/s]  1%|          | 107/12630 [01:36<3:03:15,  1.14it/s]  1%|          | 108/12630 [01:37<3:03:43,  1.14it/s]  1%|          | 109/12630 [01:38<3:04:43,  1.13it/s]  1%|          | 110/12630 [01:38<3:03:25,  1.14it/s]  1%|          | 111/12630 [01:39<3:04:35,  1.13it/s]  1%|          | 112/12630 [01:40<3:04:39,  1.13it/s]  1%|          | 113/12630 [01:41<3:05:40,  1.12it/s]  1%|          | 114/12630 [01:42<3:05:02,  1.13it/s]  1%|          | 115/12630 [01:43<3:04:38,  1.13it/s]  1%|          | 116/12630 [01:44<3:04:39,  1.13it/s]  1%|          | 117/12630 [01:45<3:04:51,  1.13it/s]  1%|          | 118/12630 [01:46<3:04:02,  1.13it/s]  1%|          | 119/12630 [01:46<3:03:51,  1.13it/s]  1%|          | 120/12630 [01:47<3:04:40,  1.13it/s]  1%|          | 121/12630 [01:48<3:04:30,  1.13it/s]  1%|          | 122/12630 [01:49<3:05:07,  1.13it/s]  1%|          | 123/12630 [01:50<3:04:41,  1.13it/s]  1%|          | 124/12630 [01:51<3:04:41,  1.13it/s]  1%|          | 125/12630 [01:52<3:05:26,  1.12it/s]  1%|          | 126/12630 [01:53<3:05:31,  1.12it/s]  1%|          | 127/12630 [01:54<3:04:29,  1.13it/s]  1%|          | 128/12630 [01:54<3:05:31,  1.12it/s]  1%|          | 129/12630 [01:55<3:04:41,  1.13it/s]  1%|          | 130/12630 [01:56<3:05:48,  1.12it/s]  1%|          | 131/12630 [01:57<3:03:37,  1.13it/s]  1%|          | 132/12630 [01:58<3:04:22,  1.13it/s]  1%|          | 133/12630 [01:59<3:04:28,  1.13it/s]  1%|          | 134/12630 [02:00<3:05:34,  1.12it/s]  1%|          | 135/12630 [02:01<3:05:46,  1.12it/s]  1%|          | 136/12630 [02:02<3:04:21,  1.13it/s]  1%|          | 137/12630 [02:02<3:04:19,  1.13it/s]  1%|          | 138/12630 [02:03<3:03:41,  1.13it/s]  1%|          | 139/12630 [02:04<3:04:26,  1.13it/s]  1%|          | 140/12630 [02:05<3:04:56,  1.13it/s]  1%|          | 141/12630 [02:06<3:05:11,  1.12it/s]  1%|          | 142/12630 [02:07<3:04:43,  1.13it/s]  1%|          | 143/12630 [02:08<3:04:52,  1.13it/s]  1%|          | 144/12630 [02:09<3:05:27,  1.12it/s]  1%|          | 145/12630 [02:10<3:04:45,  1.13it/s]  1%|          | 146/12630 [02:10<3:05:37,  1.12it/s]  1%|          | 147/12630 [02:11<3:04:39,  1.13it/s]  1%|          | 148/12630 [02:12<3:05:15,  1.12it/s]  1%|          | 149/12630 [02:13<3:05:06,  1.12it/s]  1%|          | 150/12630 [02:14<3:05:20,  1.12it/s]  1%|          | 151/12630 [02:15<3:05:28,  1.12it/s]  1%|          | 152/12630 [02:16<3:04:18,  1.13it/s]  1%|          | 153/12630 [02:17<3:03:47,  1.13it/s]  1%|          | 154/12630 [02:18<3:03:50,  1.13it/s]  1%|          | 155/12630 [02:18<3:04:03,  1.13it/s]  1%|          | 156/12630 [02:19<3:04:14,  1.13it/s]  1%|          | 157/12630 [02:20<3:03:04,  1.14it/s]  1%|         | 158/12630 [02:21<3:05:13,  1.12it/s]  1%|         | 159/12630 [02:22<3:04:10,  1.13it/s]  1%|         | 160/12630 [02:23<3:03:34,  1.13it/s]  1%|         | 161/12630 [02:24<3:04:19,  1.13it/s]  1%|         | 162/12630 [02:25<3:03:50,  1.13it/s]  1%|         | 163/12630 [02:26<3:04:48,  1.12it/s]  1%|         | 164/12630 [02:26<3:06:05,  1.12it/s]  1%|         | 165/12630 [02:27<3:04:50,  1.12it/s]  1%|         | 166/12630 [02:28<3:04:59,  1.12it/s]  1%|         | 167/12630 [02:29<3:04:58,  1.12it/s]  1%|         | 168/12630 [02:30<3:04:01,  1.13it/s]  1%|         | 169/12630 [02:31<3:05:27,  1.12it/s]  1%|         | 170/12630 [02:32<3:04:15,  1.13it/s]  1%|         | 171/12630 [02:33<3:03:30,  1.13it/s]  1%|         | 172/12630 [02:33<3:03:21,  1.13it/s]  1%|         | 173/12630 [02:34<3:02:57,  1.13it/s]  1%|         | 174/12630 [02:35<3:03:11,  1.13it/s]  1%|         | 175/12630 [02:36<3:03:36,  1.13it/s]  1%|         | 176/12630 [02:37<3:03:52,  1.13it/s]  1%|         | 177/12630 [02:38<3:03:59,  1.13it/s]  1%|         | 178/12630 [02:39<3:03:20,  1.13it/s]  1%|         | 179/12630 [02:40<3:02:56,  1.13it/s]  1%|         | 180/12630 [02:41<3:04:08,  1.13it/s]  1%|         | 181/12630 [02:41<3:03:33,  1.13it/s]  1%|         | 182/12630 [02:42<3:04:00,  1.13it/s]  1%|         | 183/12630 [02:43<3:03:36,  1.13it/s]  1%|         | 184/12630 [02:44<3:03:14,  1.13it/s]  1%|         | 185/12630 [02:45<3:03:06,  1.13it/s]  1%|         | 186/12630 [02:46<3:02:59,  1.13it/s]  1%|         | 187/12630 [02:47<3:02:33,  1.14it/s]  1%|         | 188/12630 [02:48<3:02:32,  1.14it/s]  1%|         | 189/12630 [02:48<3:01:27,  1.14it/s]  2%|         | 190/12630 [02:49<3:03:50,  1.13it/s]  2%|         | 191/12630 [02:50<3:03:42,  1.13it/s]  2%|         | 192/12630 [02:51<3:03:41,  1.13it/s]  2%|         | 193/12630 [02:52<3:04:31,  1.12it/s]  2%|         | 194/12630 [02:53<3:03:21,  1.13it/s]  2%|         | 195/12630 [02:54<3:02:58,  1.13it/s]  2%|         | 196/12630 [02:55<3:02:09,  1.14it/s]  2%|         | 197/12630 [02:56<3:02:35,  1.13it/s]  2%|         | 198/12630 [02:56<3:02:14,  1.14it/s]  2%|         | 199/12630 [02:57<3:03:22,  1.13it/s]  2%|         | 200/12630 [02:58<3:03:03,  1.13it/s]  2%|         | 201/12630 [02:59<3:02:32,  1.13it/s]  2%|         | 202/12630 [03:00<3:02:54,  1.13it/s]  2%|         | 203/12630 [03:01<3:01:36,  1.14it/s]  2%|         | 204/12630 [03:02<3:02:53,  1.13it/s]  2%|         | 205/12630 [03:03<3:02:48,  1.13it/s]  2%|         | 206/12630 [03:04<3:02:14,  1.14it/s]  2%|         | 207/12630 [03:04<3:03:31,  1.13it/s]  2%|         | 208/12630 [03:05<3:02:57,  1.13it/s]  2%|         | 209/12630 [03:06<3:04:21,  1.12it/s]  2%|         | 210/12630 [03:07<3:03:52,  1.13it/s]  2%|         | 211/12630 [03:08<3:03:48,  1.13it/s]  2%|         | 212/12630 [03:09<3:04:02,  1.12it/s]  2%|         | 213/12630 [03:10<3:03:09,  1.13it/s]  2%|         | 214/12630 [03:11<3:03:16,  1.13it/s]  2%|         | 215/12630 [03:12<3:03:22,  1.13it/s]  2%|         | 216/12630 [03:12<3:02:41,  1.13it/s]  2%|         | 217/12630 [03:13<3:02:58,  1.13it/s]  2%|         | 218/12630 [03:14<3:02:40,  1.13it/s]  2%|         | 219/12630 [03:15<3:03:09,  1.13it/s]  2%|         | 220/12630 [03:16<3:02:16,  1.13it/s]  2%|         | 221/12630 [03:17<3:03:17,  1.13it/s]  2%|         | 222/12630 [03:18<3:02:18,  1.13it/s]  2%|         | 223/12630 [03:19<3:02:30,  1.13it/s]  2%|         | 224/12630 [03:19<3:03:31,  1.13it/s]  2%|         | 225/12630 [03:20<3:02:30,  1.13it/s]  2%|         | 226/12630 [03:21<3:02:26,  1.13it/s]  2%|         | 227/12630 [03:22<3:02:18,  1.13it/s]  2%|         | 228/12630 [03:23<3:01:31,  1.14it/s]  2%|         | 229/12630 [03:24<3:04:16,  1.12it/s]  2%|         | 230/12630 [03:25<3:03:18,  1.13it/s]  2%|         | 231/12630 [03:26<3:03:46,  1.12it/s]  2%|         | 232/12630 [03:27<3:03:24,  1.13it/s]  2%|         | 233/12630 [03:27<3:03:26,  1.13it/s]  2%|         | 234/12630 [03:28<3:02:53,  1.13it/s]  2%|         | 235/12630 [03:29<3:04:09,  1.12it/s]  2%|         | 236/12630 [03:30<3:03:21,  1.13it/s]  2%|         | 237/12630 [03:31<3:02:10,  1.13it/s]  2%|         | 238/12630 [03:32<3:02:51,  1.13it/s]  2%|         | 239/12630 [03:33<3:02:00,  1.13it/s]  2%|         | 240/12630 [03:34<3:02:22,  1.13it/s]  2%|         | 241/12630 [03:35<3:02:44,  1.13it/s]  2%|         | 242/12630 [03:35<3:03:18,  1.13it/s]  2%|         | 243/12630 [03:36<3:01:48,  1.14it/s]  2%|         | 244/12630 [03:37<3:04:05,  1.12it/s]  2%|         | 245/12630 [03:38<3:03:46,  1.12it/s]  2%|         | 246/12630 [03:39<3:03:29,  1.12it/s]  2%|         | 247/12630 [03:40<3:04:08,  1.12it/s]  2%|         | 248/12630 [03:41<3:04:17,  1.12it/s]  2%|         | 249/12630 [03:42<3:04:36,  1.12it/s]  2%|         | 250/12630 [03:43<3:04:20,  1.12it/s]  2%|         | 251/12630 [03:43<3:04:02,  1.12it/s]  2%|         | 252/12630 [03:44<3:03:13,  1.13it/s]  2%|         | 253/12630 [03:45<3:03:00,  1.13it/s]  2%|         | 254/12630 [03:46<3:01:46,  1.13it/s]  2%|         | 255/12630 [03:47<3:02:26,  1.13it/s]  2%|         | 256/12630 [03:48<3:04:08,  1.12it/s]  2%|         | 257/12630 [03:49<3:04:28,  1.12it/s]  2%|         | 258/12630 [03:50<3:04:20,  1.12it/s]  2%|         | 259/12630 [03:51<3:03:34,  1.12it/s]  2%|         | 260/12630 [03:51<3:03:21,  1.12it/s]  2%|         | 261/12630 [03:52<3:02:57,  1.13it/s]  2%|         | 262/12630 [03:53<3:03:14,  1.12it/s]  2%|         | 263/12630 [03:54<3:02:01,  1.13it/s]  2%|         | 264/12630 [03:55<3:03:12,  1.12it/s]  2%|         | 265/12630 [03:56<3:03:36,  1.12it/s]  2%|         | 266/12630 [03:57<3:03:38,  1.12it/s]  2%|         | 267/12630 [03:58<3:02:47,  1.13it/s]  2%|         | 268/12630 [03:59<3:03:32,  1.12it/s]  2%|         | 269/12630 [03:59<3:02:44,  1.13it/s]  2%|         | 270/12630 [04:00<3:02:45,  1.13it/s]  2%|         | 271/12630 [04:01<3:02:53,  1.13it/s]  2%|         | 272/12630 [04:02<3:03:22,  1.12it/s]  2%|         | 273/12630 [04:03<3:01:38,  1.13it/s]  2%|         | 274/12630 [04:04<3:03:16,  1.12it/s]  2%|         | 275/12630 [04:05<3:03:08,  1.12it/s]  2%|         | 276/12630 [04:06<3:02:32,  1.13it/s]  2%|         | 277/12630 [04:07<3:02:20,  1.13it/s]  2%|         | 278/12630 [04:07<3:02:04,  1.13it/s]  2%|         | 279/12630 [04:08<3:02:19,  1.13it/s]  2%|         | 280/12630 [04:09<3:03:53,  1.12it/s]  2%|         | 281/12630 [04:10<3:02:52,  1.13it/s]  2%|         | 282/12630 [04:11<3:03:14,  1.12it/s]  2%|         | 283/12630 [04:12<3:02:06,  1.13it/s]  2%|         | 284/12630 [04:13<3:01:44,  1.13it/s]  2%|         | 285/12630 [04:14<3:01:51,  1.13it/s]  2%|         | 286/12630 [04:15<3:03:15,  1.12it/s]  2%|         | 287/12630 [04:15<3:02:43,  1.13it/s]  2%|         | 288/12630 [04:16<3:01:48,  1.13it/s]  2%|         | 289/12630 [04:17<3:02:40,  1.13it/s]  2%|         | 290/12630 [04:18<3:03:49,  1.12it/s]  2%|         | 291/12630 [04:19<3:03:09,  1.12it/s]  2%|         | 292/12630 [04:20<3:01:51,  1.13it/s]  2%|         | 293/12630 [04:21<3:02:36,  1.13it/s]  2%|         | 294/12630 [04:22<3:01:22,  1.13it/s]  2%|         | 295/12630 [04:23<3:02:54,  1.12it/s]  2%|         | 296/12630 [04:23<3:02:06,  1.13it/s]  2%|         | 297/12630 [04:24<3:01:39,  1.13it/s]  2%|         | 298/12630 [04:25<3:01:09,  1.13it/s]  2%|         | 299/12630 [04:26<3:00:51,  1.14it/s]  2%|         | 300/12630 [04:27<3:01:49,  1.13it/s]  2%|         | 301/12630 [04:28<3:00:36,  1.14it/s]  2%|         | 302/12630 [04:29<3:01:31,  1.13it/s]  2%|         | 303/12630 [04:30<3:00:45,  1.14it/s]  2%|         | 304/12630 [04:30<3:00:42,  1.14it/s]  2%|         | 305/12630 [04:31<3:01:09,  1.13it/s]  2%|         | 306/12630 [04:32<3:01:46,  1.13it/s]  2%|         | 307/12630 [04:33<3:00:23,  1.14it/s]  2%|         | 308/12630 [04:34<3:00:21,  1.14it/s]  2%|         | 309/12630 [04:35<3:02:00,  1.13it/s]  2%|         | 310/12630 [04:36<3:02:52,  1.12it/s]  2%|         | 311/12630 [04:37<3:02:17,  1.13it/s]  2%|         | 312/12630 [04:37<3:01:26,  1.13it/s]  2%|         | 313/12630 [04:38<3:01:21,  1.13it/s]  2%|         | 314/12630 [04:39<3:00:43,  1.14it/s]  2%|         | 315/12630 [04:40<3:02:02,  1.13it/s]  3%|         | 316/12630 [04:41<3:02:20,  1.13it/s]  3%|         | 317/12630 [04:42<3:02:38,  1.12it/s]  3%|         | 318/12630 [04:43<3:01:25,  1.13it/s]  3%|         | 319/12630 [04:44<3:01:20,  1.13it/s]  3%|         | 320/12630 [04:45<3:01:35,  1.13it/s]  3%|         | 321/12630 [04:45<3:01:39,  1.13it/s]  3%|         | 322/12630 [04:46<3:01:13,  1.13it/s]  3%|         | 323/12630 [04:47<3:00:43,  1.13it/s]  3%|         | 324/12630 [04:48<3:00:33,  1.14it/s]  3%|         | 325/12630 [04:49<3:00:56,  1.13it/s]  3%|         | 326/12630 [04:50<3:01:30,  1.13it/s]  3%|         | 327/12630 [04:51<3:00:49,  1.13it/s]  3%|         | 328/12630 [04:52<3:02:05,  1.13it/s]  3%|         | 329/12630 [04:53<3:02:11,  1.13it/s]  3%|         | 330/12630 [04:53<3:00:49,  1.13it/s]  3%|         | 331/12630 [04:54<3:01:28,  1.13it/s]  3%|         | 332/12630 [04:55<3:01:48,  1.13it/s]  3%|         | 333/12630 [04:56<3:01:47,  1.13it/s]  3%|         | 334/12630 [04:57<3:00:36,  1.13it/s]  3%|         | 335/12630 [04:58<3:00:02,  1.14it/s]  3%|         | 336/12630 [04:59<3:00:13,  1.14it/s]  3%|         | 337/12630 [05:00<3:01:46,  1.13it/s]  3%|         | 338/12630 [05:00<3:01:17,  1.13it/s]  3%|         | 339/12630 [05:01<3:03:18,  1.12it/s]  3%|         | 340/12630 [05:02<3:02:50,  1.12it/s]  3%|         | 341/12630 [05:03<3:01:34,  1.13it/s]  3%|         | 342/12630 [05:04<3:00:57,  1.13it/s]  3%|         | 343/12630 [05:05<3:02:28,  1.12it/s]  3%|         | 344/12630 [05:06<3:01:22,  1.13it/s]  3%|         | 345/12630 [05:07<3:00:36,  1.13it/s]  3%|         | 346/12630 [05:08<3:01:23,  1.13it/s]  3%|         | 347/12630 [05:08<3:00:57,  1.13it/s]  3%|         | 348/12630 [05:09<3:00:31,  1.13it/s]  3%|         | 349/12630 [05:10<3:01:05,  1.13it/s]  3%|         | 350/12630 [05:11<3:01:21,  1.13it/s]  3%|         | 351/12630 [05:12<3:01:56,  1.12it/s]  3%|         | 352/12630 [05:13<3:01:45,  1.13it/s]  3%|         | 353/12630 [05:14<3:01:35,  1.13it/s]  3%|         | 354/12630 [05:15<3:01:24,  1.13it/s]  3%|         | 355/12630 [05:16<3:01:56,  1.12it/s]  3%|         | 356/12630 [05:16<3:01:45,  1.13it/s]  3%|         | 357/12630 [05:17<3:00:18,  1.13it/s]  3%|         | 358/12630 [05:18<3:00:02,  1.14it/s]  3%|         | 359/12630 [05:19<3:00:42,  1.13it/s]  3%|         | 360/12630 [05:20<3:00:53,  1.13it/s]  3%|         | 361/12630 [05:21<3:01:19,  1.13it/s]  3%|         | 362/12630 [05:22<3:01:38,  1.13it/s]  3%|         | 363/12630 [05:23<3:01:19,  1.13it/s]  3%|         | 364/12630 [05:24<3:00:10,  1.13it/s]  3%|         | 365/12630 [05:24<3:01:58,  1.12it/s]  3%|         | 366/12630 [05:25<3:01:28,  1.13it/s]  3%|         | 367/12630 [05:26<3:01:49,  1.12it/s]  3%|         | 368/12630 [05:27<3:01:59,  1.12it/s]  3%|         | 369/12630 [05:28<3:00:56,  1.13it/s]  3%|         | 370/12630 [05:29<3:01:37,  1.13it/s]  3%|         | 371/12630 [05:30<3:00:44,  1.13it/s]  3%|         | 372/12630 [05:31<3:01:07,  1.13it/s]  3%|         | 373/12630 [05:32<3:01:49,  1.12it/s]  3%|         | 374/12630 [05:32<3:00:56,  1.13it/s]  3%|         | 375/12630 [05:33<3:02:53,  1.12it/s]  3%|         | 376/12630 [05:34<3:01:27,  1.13it/s]  3%|         | 377/12630 [05:35<3:01:30,  1.13it/s]  3%|         | 378/12630 [05:36<3:00:52,  1.13it/s]  3%|         | 379/12630 [05:37<3:01:03,  1.13it/s]  3%|         | 380/12630 [05:38<3:00:11,  1.13it/s]  3%|         | 381/12630 [05:39<3:01:31,  1.12it/s]  3%|         | 382/12630 [05:40<3:01:15,  1.13it/s]  3%|         | 383/12630 [05:40<3:01:11,  1.13it/s]  3%|         | 384/12630 [05:41<3:00:38,  1.13it/s]  3%|         | 385/12630 [05:42<3:01:29,  1.12it/s]  3%|         | 386/12630 [05:43<3:00:41,  1.13it/s]  3%|         | 387/12630 [05:44<2:59:51,  1.13it/s]  3%|         | 388/12630 [05:45<3:00:34,  1.13it/s]  3%|         | 389/12630 [05:46<3:01:07,  1.13it/s]  3%|         | 390/12630 [05:47<3:01:28,  1.12it/s]  3%|         | 391/12630 [05:48<3:01:02,  1.13it/s]  3%|         | 392/12630 [05:48<3:01:11,  1.13it/s]  3%|         | 393/12630 [05:49<3:00:48,  1.13it/s]  3%|         | 394/12630 [05:50<3:01:07,  1.13it/s]  3%|         | 395/12630 [05:51<3:01:27,  1.12it/s]  3%|         | 396/12630 [05:52<3:01:34,  1.12it/s]  3%|         | 397/12630 [05:53<3:01:48,  1.12it/s]  3%|         | 398/12630 [05:54<3:00:29,  1.13it/s]  3%|         | 399/12630 [05:55<3:02:13,  1.12it/s]  3%|         | 400/12630 [05:56<3:01:41,  1.12it/s]  3%|         | 401/12630 [05:56<3:01:32,  1.12it/s]  3%|         | 402/12630 [05:57<3:00:16,  1.13it/s]  3%|         | 403/12630 [05:58<3:00:41,  1.13it/s]  3%|         | 404/12630 [05:59<3:01:37,  1.12it/s]  3%|         | 405/12630 [06:00<3:03:15,  1.11it/s]  3%|         | 406/12630 [06:01<3:02:01,  1.12it/s]  3%|         | 407/12630 [06:02<3:01:44,  1.12it/s]  3%|         | 408/12630 [06:03<3:01:30,  1.12it/s]  3%|         | 409/12630 [06:04<3:00:58,  1.13it/s]  3%|         | 410/12630 [06:04<3:00:42,  1.13it/s]  3%|         | 411/12630 [06:05<3:00:42,  1.13it/s]  3%|         | 412/12630 [06:06<3:00:34,  1.13it/s]  3%|         | 413/12630 [06:07<3:00:49,  1.13it/s]  3%|         | 414/12630 [06:08<3:00:05,  1.13it/s]  3%|         | 415/12630 [06:09<3:01:03,  1.12it/s]  3%|         | 416/12630 [06:10<3:02:06,  1.12it/s]  3%|         | 417/12630 [06:11<3:01:18,  1.12it/s]  3%|         | 418/12630 [06:12<3:00:21,  1.13it/s]  3%|         | 419/12630 [06:12<3:01:30,  1.12it/s]  3%|         | 420/12630 [06:13<3:01:03,  1.12it/s]  3%|         | 421/12630 [06:14<3:00:42,  1.13it/s]  3%|         | 422/12630 [06:15<2:59:30,  1.13it/s]  3%|         | 423/12630 [06:16<2:59:56,  1.13it/s]  3%|         | 424/12630 [06:17<2:59:54,  1.13it/s]  3%|         | 425/12630 [06:18<2:58:56,  1.14it/s]  3%|         | 426/12630 [06:19<3:00:08,  1.13it/s]  3%|         | 427/12630 [06:19<2:59:55,  1.13it/s]  3%|         | 428/12630 [06:20<2:58:44,  1.14it/s]  3%|         | 429/12630 [06:21<2:59:28,  1.13it/s]  3%|         | 430/12630 [06:22<2:59:02,  1.14it/s]  3%|         | 431/12630 [06:23<2:59:33,  1.13it/s]  3%|         | 432/12630 [06:24<2:59:29,  1.13it/s]  3%|         | 433/12630 [06:25<2:58:21,  1.14it/s]  3%|         | 434/12630 [06:26<3:00:36,  1.13it/s]  3%|         | 435/12630 [06:27<2:59:41,  1.13it/s]  3%|         | 436/12630 [06:27<2:59:51,  1.13it/s]  3%|         | 437/12630 [06:28<3:00:04,  1.13it/s]  3%|         | 438/12630 [06:29<2:58:59,  1.14it/s]  3%|         | 439/12630 [06:30<2:59:51,  1.13it/s]  3%|         | 440/12630 [06:31<2:58:58,  1.14it/s]  3%|         | 441/12630 [06:32<2:58:32,  1.14it/s]  3%|         | 442/12630 [06:33<2:59:15,  1.13it/s]  4%|         | 443/12630 [06:34<2:59:56,  1.13it/s]  4%|         | 444/12630 [06:34<2:59:27,  1.13it/s]  4%|         | 445/12630 [06:35<3:00:06,  1.13it/s]  4%|         | 446/12630 [06:36<2:59:48,  1.13it/s]  4%|         | 447/12630 [06:37<2:59:46,  1.13it/s]  4%|         | 448/12630 [06:38<2:58:31,  1.14it/s]  4%|         | 449/12630 [06:39<2:59:51,  1.13it/s]  4%|         | 450/12630 [06:40<2:59:08,  1.13it/s]  4%|         | 451/12630 [06:41<3:00:14,  1.13it/s]  4%|         | 452/12630 [06:42<2:59:31,  1.13it/s]  4%|         | 453/12630 [06:42<2:59:04,  1.13it/s]  4%|         | 454/12630 [06:43<2:59:16,  1.13it/s]  4%|         | 455/12630 [06:44<2:58:53,  1.13it/s]  4%|         | 456/12630 [06:45<2:58:25,  1.14it/s]  4%|         | 457/12630 [06:46<2:59:31,  1.13it/s]  4%|         | 458/12630 [06:47<2:59:44,  1.13it/s]  4%|         | 459/12630 [06:48<3:00:06,  1.13it/s]  4%|         | 460/12630 [06:49<2:58:52,  1.13it/s]  4%|         | 461/12630 [06:50<2:59:46,  1.13it/s]  4%|         | 462/12630 [06:50<3:00:22,  1.12it/s]  4%|         | 463/12630 [06:51<2:59:41,  1.13it/s]  4%|         | 464/12630 [06:52<2:58:37,  1.14it/s]  4%|         | 465/12630 [06:53<2:57:40,  1.14it/s]  4%|         | 466/12630 [06:54<2:59:06,  1.13it/s]  4%|         | 467/12630 [06:55<2:58:27,  1.14it/s]  4%|         | 468/12630 [06:56<2:58:11,  1.14it/s]  4%|         | 469/12630 [06:57<2:57:55,  1.14it/s]  4%|         | 470/12630 [06:57<2:57:54,  1.14it/s]  4%|         | 471/12630 [06:58<2:58:30,  1.14it/s]  4%|         | 472/12630 [06:59<2:58:55,  1.13it/s]  4%|         | 473/12630 [07:00<2:58:44,  1.13it/s]  4%|         | 474/12630 [07:01<2:58:09,  1.14it/s]  4%|         | 475/12630 [07:02<2:58:51,  1.13it/s]  4%|         | 476/12630 [07:03<2:58:21,  1.14it/s]  4%|         | 477/12630 [07:04<3:00:01,  1.13it/s]  4%|         | 478/12630 [07:05<2:58:21,  1.14it/s]  4%|         | 479/12630 [07:05<2:58:48,  1.13it/s]  4%|         | 480/12630 [07:06<2:59:07,  1.13it/s]  4%|         | 481/12630 [07:07<2:59:57,  1.13it/s]  4%|         | 482/12630 [07:08<2:58:27,  1.13it/s]  4%|         | 483/12630 [07:09<2:58:06,  1.14it/{'loss': 0.6194, 'learning_rate': 4.802058590657166e-05, 'epoch': 0.12}
s]  4%|         | 484/12630 [07:10<2:58:46,  1.13it/s]  4%|         | 485/12630 [07:11<3:00:23,  1.12it/s]  4%|         | 486/12630 [07:12<2:59:38,  1.13it/s]  4%|         | 487/12630 [07:12<2:59:02,  1.13it/s]  4%|         | 488/12630 [07:13<2:59:02,  1.13it/s]  4%|         | 489/12630 [07:14<2:57:43,  1.14it/s]  4%|         | 490/12630 [07:15<2:58:24,  1.13it/s]  4%|         | 491/12630 [07:16<3:00:27,  1.12it/s]  4%|         | 492/12630 [07:17<2:59:18,  1.13it/s]  4%|         | 493/12630 [07:18<2:58:42,  1.13it/s]  4%|         | 494/12630 [07:19<3:00:00,  1.12it/s]  4%|         | 495/12630 [07:20<3:00:03,  1.12it/s]  4%|         | 496/12630 [07:20<2:59:09,  1.13it/s]  4%|         | 497/12630 [07:21<2:58:08,  1.14it/s]  4%|         | 498/12630 [07:22<2:57:45,  1.14it/s]  4%|         | 499/12630 [07:23<2:58:18,  1.13it/s]  4%|         | 500/12630 [07:24<2:57:51,  1.14it/s]                                                       4%|         | 500/12630 [07:24<2:57:51,  1.14it/s][INFO|trainer.py:2985] 2024-02-13 00:17:24,444 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-13 00:17:24,463 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:17:48,166 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:17:48,168 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:17:48,169 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/special_tokens_map.json
  4%|         | 501/12630 [08:13<51:49:58, 15.38s/it]  4%|         | 502/12630 [08:14<37:09:37, 11.03s/it]  4%|         | 503/12630 [08:15<26:55:18,  7.99s/it]  4%|         | 504/12630 [08:16<19:45:29,  5.87s/it]  4%|         | 505/12630 [08:17<14:43:19,  4.37s/it]  4%|         | 506/12630 [08:18<11:12:03,  3.33s/it]  4%|         | 507/12630 [08:19<8:43:13,  2.59s/it]   4%|         | 508/12630 [08:19<6:59:26,  2.08s/it]  4%|         | 509/12630 [08:20<5:47:01,  1.72s/it]  4%|         | 510/12630 [08:21<4:54:58,  1.46s/it]  4%|         | 511/12630 [08:22<4:19:19,  1.28s/it]  4%|         | 512/12630 [08:23<3:55:05,  1.16s/it]  4%|         | 513/12630 [08:24<3:37:56,  1.08s/it]  4%|         | 514/12630 [08:25<3:27:07,  1.03s/it]  4%|         | 515/12630 [08:26<3:18:38,  1.02it/s]  4%|         | 516/12630 [08:26<3:12:28,  1.05it/s]  4%|         | 517/12630 [08:27<3:07:39,  1.08it/s]  4%|         | 518/12630 [08:28<3:04:03,  1.10it/s]  4%|         | 519/12630 [08:29<3:02:54,  1.10it/s]  4%|         | 520/12630 [08:30<3:01:49,  1.11it/s]  4%|         | 521/12630 [08:31<3:00:35,  1.12it/s]  4%|         | 522/12630 [08:32<2:58:45,  1.13it/s]  4%|         | 523/12630 [08:33<2:59:18,  1.13it/s]  4%|         | 524/12630 [08:33<2:57:50,  1.13it/s]  4%|         | 525/12630 [08:34<2:58:43,  1.13it/s]  4%|         | 526/12630 [08:35<2:58:14,  1.13it/s]  4%|         | 527/12630 [08:36<2:57:42,  1.14it/s]  4%|         | 528/12630 [08:37<2:58:30,  1.13it/s]  4%|         | 529/12630 [08:38<2:58:06,  1.13it/s]  4%|         | 530/12630 [08:39<2:57:49,  1.13it/s]  4%|         | 531/12630 [08:40<2:58:12,  1.13it/s]  4%|         | 532/12630 [08:41<2:58:58,  1.13it/s]  4%|         | 533/12630 [08:41<2:57:37,  1.14it/s]  4%|         | 534/12630 [08:42<2:58:04,  1.13it/s]  4%|         | 535/12630 [08:43<2:58:28,  1.13it/s]  4%|         | 536/12630 [08:44<2:58:31,  1.13it/s]  4%|         | 537/12630 [08:45<2:58:04,  1.13it/s]  4%|         | 538/12630 [08:46<2:56:42,  1.14it/s]  4%|         | 539/12630 [08:47<2:57:01,  1.14it/s]  4%|         | 540/12630 [08:48<2:56:21,  1.14it/s]  4%|         | 541/12630 [08:48<2:56:21,  1.14it/s]  4%|         | 542/12630 [08:49<2:56:14,  1.14it/s]  4%|         | 543/12630 [08:50<2:56:31,  1.14it/s]  4%|         | 544/12630 [08:51<2:57:13,  1.14it/s]  4%|         | 545/12630 [08:52<2:57:12,  1.14it/s]  4%|         | 546/12630 [08:53<2:57:22,  1.14it/s]  4%|         | 547/12630 [08:54<2:56:44,  1.14it/s]  4%|         | 548/12630 [08:55<2:57:02,  1.14it/s]  4%|         | 549/12630 [08:55<2:56:54,  1.14it/s]  4%|         | 550/12630 [08:56<2:57:38,  1.13it/s]  4%|         | 551/12630 [08:57<2:57:11,  1.14it/s]  4%|         | 552/12630 [08:58<2:57:39,  1.13it/s]  4%|         | 553/12630 [08:59<2:57:16,  1.14it/s]  4%|         | 554/12630 [09:00<2:58:42,  1.13it/s]  4%|         | 555/12630 [09:01<2:58:24,  1.13it/s]  4%|         | 556/12630 [09:02<2:59:13,  1.12it/s]  4%|         | 557/12630 [09:03<2:57:43,  1.13it/s]  4%|         | 558/12630 [09:03<2:58:36,  1.13it/s]  4%|         | 559/12630 [09:04<2:57:15,  1.14it/s]  4%|         | 560/12630 [09:05<2:58:32,  1.13it/s]  4%|         | 561/12630 [09:06<2:57:56,  1.13it/s]  4%|         | 562/12630 [09:07<2:59:00,  1.12it/s]  4%|         | 563/12630 [09:08<2:59:36,  1.12it/s]  4%|         | 564/12630 [09:09<2:58:28,  1.13it/s]  4%|         | 565/12630 [09:10<2:59:45,  1.12it/s]  4%|         | 566/12630 [09:11<3:00:44,  1.11it/s]  4%|         | 567/12630 [09:11<2:58:52,  1.12it/s]  4%|         | 568/12630 [09:12<2:59:48,  1.12it/s]  5%|         | 569/12630 [09:13<2:59:13,  1.12it/s]  5%|         | 570/12630 [09:14<2:58:51,  1.12it/s]  5%|         | 571/12630 [09:15<2:59:52,  1.12it/s]  5%|         | 572/12630 [09:16<2:59:11,  1.12it/s]  5%|         | 573/12630 [09:17<2:58:10,  1.13it/s]  5%|         | 574/12630 [09:18<2:57:42,  1.13it/s]  5%|         | 575/12630 [09:19<2:57:06,  1.13it/s]  5%|         | 576/12630 [09:19<2:57:30,  1.13it/s]  5%|         | 577/12630 [09:20<2:57:44,  1.13it/s]  5%|         | 578/12630 [09:21<2:57:54,  1.13it/s]  5%|         | 579/12630 [09:22<2:57:16,  1.13it/s]  5%|         | 580/12630 [09:23<2:58:41,  1.12it/s]  5%|         | 581/12630 [09:24<2:58:22,  1.13it/s]  5%|         | 582/12630 [09:25<2:58:21,  1.13it/s]  5%|         | 583/12630 [09:26<2:56:58,  1.13it/s]  5%|         | 584/12630 [09:27<2:57:10,  1.13it/s]  5%|         | 585/12630 [09:27<2:56:53,  1.13it/s]  5%|         | 586/12630 [09:28<2:57:30,  1.13it/s]  5%|         | 587/12630 [09:29<2:57:00,  1.13it/s]  5%|         | 588/12630 [09:30<2:57:11,  1.13it/s]  5%|         | 589/12630 [09:31<2:57:37,  1.13it/s]  5%|         | 590/12630 [09:32<2:57:51,  1.13it/s]  5%|         | 591/12630 [09:33<2:58:10,  1.13it/s]  5%|         | 592/12630 [09:34<3:00:28,  1.11it/s]  5%|         | 593/12630 [09:35<2:58:58,  1.12it/s]  5%|         | 594/12630 [09:35<2:57:25,  1.13it/s]  5%|         | 595/12630 [09:36<2:57:42,  1.13it/s]  5%|         | 596/12630 [09:37<2:57:27,  1.13it/s]  5%|         | 597/12630 [09:38<2:56:33,  1.14it/s]  5%|         | 598/12630 [09:39<2:57:07,  1.13it/s]  5%|         | 599/12630 [09:40<2:58:44,  1.12it/s]  5%|         | 600/12630 [09:41<2:57:21,  1.13it/s]  5%|         | 601/12630 [09:42<2:58:19,  1.12it/s]  5%|         | 602/12630 [09:43<2:57:51,  1.13it/s]  5%|         | 603/12630 [09:43<2:58:18,  1.12it/s]  5%|         | 604/12630 [09:44<2:57:59,  1.13it/s]  5%|         | 605/12630 [09:45<2:58:38,  1.12it/s]  5%|         | 606/12630 [09:46<2:57:53,  1.13it/s]  5%|         | 607/12630 [09:47<2:57:53,  1.13it/s]  5%|         | 608/12630 [09:48<2:57:22,  1.13it/s]  5%|         | 609/12630 [09:49<2:57:19,  1.13it/s]  5%|         | 610/12630 [09:50<2:57:38,  1.13it/s]  5%|         | 611/12630 [09:50<2:57:50,  1.13it/s]  5%|         | 612/12630 [09:51<2:57:04,  1.13it/s]  5%|         | 613/12630 [09:52<2:56:37,  1.13it/s]  5%|         | 614/12630 [09:53<2:56:53,  1.13it/s]  5%|         | 615/12630 [09:54<2:57:09,  1.13it/s]  5%|         | 616/12630 [09:55<2:56:40,  1.13it/s]  5%|         | 617/12630 [09:56<2:57:01,  1.13it/s]  5%|         | 618/12630 [09:57<2:56:29,  1.13it/s]  5%|         | 619/12630 [09:58<2:56:12,  1.14it/s]  5%|         | 620/12630 [09:58<2:57:37,  1.13it/s]  5%|         | 621/12630 [09:59<2:56:51,  1.13it/s]  5%|         | 622/12630 [10:00<2:56:06,  1.14it/s]  5%|         | 623/12630 [10:01<2:56:30,  1.13it/s]  5%|         | 624/12630 [10:02<2:55:52,  1.14it/s]  5%|         | 625/12630 [10:03<2:56:28,  1.13it/s]  5%|         | 626/12630 [10:04<2:56:25,  1.13it/s]  5%|         | 627/12630 [10:05<2:55:29,  1.14it/s]  5%|         | 628/12630 [10:05<2:55:36,  1.14it/s]  5%|         | 629/12630 [10:06<2:55:25,  1.14it/s]  5%|         | 630/12630 [10:07<2:57:15,  1.13it/s]  5%|         | 631/12630 [10:08<2:56:54,  1.13it/s]  5%|         | 632/12630 [10:09<2:56:25,  1.13it/s]  5%|         | 633/12630 [10:10<2:56:25,  1.13it/s]  5%|         | 634/12630 [10:11<2:56:49,  1.13it/s]  5%|         | 635/12630 [10:12<2:58:09,  1.12it/s]  5%|         | 636/12630 [10:13<2:57:30,  1.13it/s]  5%|         | 637/12630 [10:13<2:56:45,  1.13it/s]  5%|         | 638/12630 [10:14<2:58:02,  1.12it/s]  5%|         | 639/12630 [10:15<2:56:43,  1.13it/s]  5%|         | 640/12630 [10:16<2:57:39,  1.12it/s]  5%|         | 641/12630 [10:17<2:57:24,  1.13it/s]  5%|         | 642/12630 [10:18<2:56:37,  1.13it/s]  5%|         | 643/12630 [10:19<2:56:04,  1.13it/s]  5%|         | 644/12630 [10:20<2:57:13,  1.13it/s]  5%|         | 645/12630 [10:21<2:57:22,  1.13it/s]  5%|         | 646/12630 [10:21<2:56:30,  1.13it/s]  5%|         | 647/12630 [10:22<2:56:16,  1.13it/s]  5%|         | 648/12630 [10:23<2:56:31,  1.13it/s]  5%|         | 649/12630 [10:24<2:56:41,  1.13it/s]  5%|         | 650/12630 [10:25<2:56:44,  1.13it/s]  5%|         | 651/12630 [10:26<2:56:29,  1.13it/s]  5%|         | 652/12630 [10:27<2:56:10,  1.13it/s]  5%|         | 653/12630 [10:28<2:55:36,  1.14it/s]  5%|         | 654/12630 [10:28<2:55:19,  1.14it/s]  5%|         | 655/12630 [10:29<2:56:10,  1.13it/s]  5%|         | 656/12630 [10:30<2:56:16,  1.13it/s]  5%|         | 657/12630 [10:31<2:57:09,  1.13it/s]  5%|         | 658/12630 [10:32<2:56:18,  1.13it/s]  5%|         | 659/12630 [10:33<2:56:21,  1.13it/s]  5%|         | 660/12630 [10:34<2:57:08,  1.13it/s]  5%|         | 661/12630 [10:35<2:55:51,  1.13it/s]  5%|         | 662/12630 [10:36<2:56:38,  1.13it/s]  5%|         | 663/12630 [10:36<2:56:49,  1.13it/s]  5%|         | 664/12630 [10:37<2:56:26,  1.13it/s]  5%|         | 665/12630 [10:38<2:56:06,  1.13it/s]  5%|         | 666/12630 [10:39<2:56:09,  1.13it/s]  5%|         | 667/12630 [10:40<2:56:34,  1.13it/s]  5%|         | 668/12630 [10:41<2:56:41,  1.13it/s]  5%|         | 669/12630 [10:42<2:55:52,  1.13it/s]  5%|         | 670/12630 [10:43<2:56:34,  1.13it/s]  5%|         | 671/12630 [10:44<2:56:03,  1.13it/s]  5%|         | 672/12630 [10:44<2:56:23,  1.13it/s]  5%|         | 673/12630 [10:45<2:55:41,  1.13it/s]  5%|         | 674/12630 [10:46<2:57:10,  1.12it/s]  5%|         | 675/12630 [10:47<2:56:34,  1.13it/s]  5%|         | 676/12630 [10:48<2:56:32,  1.13it/s]  5%|         | 677/12630 [10:49<2:56:09,  1.13it/s]  5%|         | 678/12630 [10:50<2:55:34,  1.13it/s]  5%|         | 679/12630 [10:51<2:56:45,  1.13it/s]  5%|         | 680/12630 [10:51<2:55:55,  1.13it/s]  5%|         | 681/12630 [10:52<2:58:05,  1.12it/s]  5%|         | 682/12630 [10:53<2:57:25,  1.12it/s]  5%|         | 683/12630 [10:54<2:57:17,  1.12it/s]  5%|         | 684/12630 [10:55<2:55:44,  1.13it/s]  5%|         | 685/12630 [10:56<2:57:32,  1.12it/s]  5%|         | 686/12630 [10:57<2:55:56,  1.13it/s]  5%|         | 687/12630 [10:58<2:56:27,  1.13it/s]  5%|         | 688/12630 [10:59<2:57:00,  1.12it/s]  5%|         | 689/12630 [10:59<2:56:47,  1.13it/s]  5%|         | 690/12630 [11:00<2:57:39,  1.12it/s]  5%|         | 691/12630 [11:01<2:56:51,  1.13it/s]  5%|         | 692/12630 [11:02<2:56:24,  1.13it/s]  5%|         | 693/12630 [11:03<2:56:37,  1.13it/s]  5%|         | 694/12630 [11:04<2:55:30,  1.13it/s]  6%|         | 695/12630 [11:05<2:56:37,  1.13it/s]  6%|         | 696/12630 [11:06<2:56:28,  1.13it/s]  6%|         | 697/12630 [11:07<2:55:54,  1.13it/s]  6%|         | 698/12630 [11:07<2:54:35,  1.14it/s]  6%|         | 699/12630 [11:08<2:56:06,  1.13it/s]  6%|         | 700/12630 [11:09<2:55:49,  1.13it/s]  6%|         | 701/12630 [11:10<2:54:52,  1.14it/s]  6%|         | 702/12630 [11:11<2:54:50,  1.14it/s]  6%|         | 703/12630 [11:12<2:54:32,  1.14it/s]  6%|         | 704/12630 [11:13<2:55:24,  1.13it/s]  6%|         | 705/12630 [11:14<2:54:33,  1.14it/s]  6%|         | 706/12630 [11:14<2:55:01,  1.14it/s]  6%|         | 707/12630 [11:15<2:55:06,  1.13it/s]  6%|         | 708/12630 [11:16<2:55:19,  1.13it/s]  6%|         | 709/12630 [11:17<2:54:29,  1.14it/s]  6%|         | 710/12630 [11:18<2:56:44,  1.12it/s]  6%|         | 711/12630 [11:19<2:57:09,  1.12it/s]  6%|         | 712/12630 [11:20<2:57:22,  1.12it/s]  6%|         | 713/12630 [11:21<2:57:24,  1.12it/s]  6%|         | 714/12630 [11:22<2:57:23,  1.12it/s]  6%|         | 715/12630 [11:22<2:55:37,  1.13it/s]  6%|         | 716/12630 [11:23<2:55:30,  1.13it/s]  6%|         | 717/12630 [11:24<2:56:11,  1.13it/s]  6%|         | 718/12630 [11:25<2:55:41,  1.13it/s]  6%|         | 719/12630 [11:26<2:55:09,  1.13it/s]  6%|         | 720/12630 [11:27<2:55:53,  1.13it/s]  6%|         | 721/12630 [11:28<2:55:18,  1.13it/s]  6%|         | 722/12630 [11:29<2:55:48,  1.13it/s]  6%|         | 723/12630 [11:30<2:56:58,  1.12it/s]  6%|         | 724/12630 [11:30<2:56:14,  1.13it/s]  6%|         | 725/12630 [11:31<2:56:03,  1.13it/s]  6%|         | 726/12630 [11:32<2:55:18,  1.13it/s]  6%|         | 727/12630 [11:33<2:55:28,  1.13it/s]  6%|         | 728/12630 [11:34<2:55:12,  1.13it/s]  6%|         | 729/12630 [11:35<2:57:14,  1.12it/s]  6%|         | 730/12630 [11:36<2:55:19,  1.13it/s]  6%|         | 731/12630 [11:37<2:55:29,  1.13it/s]  6%|         | 732/12630 [11:38<2:55:25,  1.13it/s]  6%|         | 733/12630 [11:38<2:55:22,  1.13it/s]  6%|         | 734/12630 [11:39<2:54:08,  1.14it/s]  6%|         | 735/12630 [11:40<2:54:35,  1.14it/s]  6%|         | 736/12630 [11:41<2:54:20,  1.14it/s]  6%|         | 737/12630 [11:42<2:55:36,  1.13it/s]  6%|         | 738/12630 [11:43<2:54:21,  1.14it/s]  6%|         | 739/12630 [11:44<2:55:45,  1.13it/s]  6%|         | 740/12630 [11:45<2:55:41,  1.13it/s]  6%|         | 741/12630 [11:46<2:55:32,  1.13it/s]  6%|         | 742/12630 [11:46<2:54:29,  1.14it/s]  6%|         | 743/12630 [11:47<2:56:21,  1.12it/s]  6%|         | 744/12630 [11:48<2:56:29,  1.12it/s]  6%|         | 745/12630 [11:49<2:56:01,  1.13it/s]  6%|         | 746/12630 [11:50<2:55:47,  1.13it/s]  6%|         | 747/12630 [11:51<2:54:41,  1.13it/s]  6%|         | 748/12630 [11:52<2:54:15,  1.14it/s]  6%|         | 749/12630 [11:53<2:54:41,  1.13it/s]  6%|         | 750/12630 [11:53<2:54:18,  1.14it/s]  6%|         | 751/12630 [11:54<2:53:33,  1.14it/s]  6%|         | 752/12630 [11:55<2:54:39,  1.13it/s]  6%|         | 753/12630 [11:56<2:54:09,  1.14it/s]  6%|         | 754/12630 [11:57<2:54:05,  1.14it/s]  6%|         | 755/12630 [11:58<2:54:32,  1.13it/s]  6%|         | 756/12630 [11:59<2:54:55,  1.13it/s]  6%|         | 757/12630 [12:00<2:55:23,  1.13it/s]  6%|         | 758/12630 [12:01<2:55:35,  1.13it/s]  6%|         | 759/12630 [12:01<2:55:20,  1.13it/s]  6%|         | 760/12630 [12:02<2:55:34,  1.13it/s]  6%|         | 761/12630 [12:03<2:54:35,  1.13it/s]  6%|         | 762/12630 [12:04<2:55:07,  1.13it/s]  6%|         | 763/12630 [12:05<2:54:52,  1.13it/s]  6%|         | 764/12630 [12:06<2:54:52,  1.13it/s]  6%|         | 765/12630 [12:07<2:54:20,  1.13it/s]  6%|         | 766/12630 [12:08<2:53:54,  1.14it/s]  6%|         | 767/12630 [12:08<2:54:59,  1.13it/s]  6%|         | 768/12630 [12:09<2:54:37,  1.13it/s]  6%|         | 769/12630 [12:10<2:54:43,  1.13it/s]  6%|         | 770/12630 [12:11<2:53:56,  1.14it/s]  6%|         | 771/12630 [12:12<2:54:20,  1.13it/s]  6%|         | 772/12630 [12:13<2:55:21,  1.13it/s]  6%|         | 773/12630 [12:14<2:56:55,  1.12it/s]  6%|         | 774/12630 [12:15<2:57:10,  1.12it/s]  6%|         | 775/12630 [12:16<2:56:21,  1.12it/s]  6%|         | 776/12630 [12:16<2:55:40,  1.12it/s]  6%|         | 777/12630 [12:17<2:54:54,  1.13it/s]  6%|         | 778/12630 [12:18<2:55:25,  1.13it/s]  6%|         | 779/12630 [12:19<2:54:35,  1.13it/s]  6%|         | 780/12630 [12:20<2:53:22,  1.14it/s]  6%|         | 781/12630 [12:21<2:53:52,  1.14it/s]  6%|         | 782/12630 [12:22<2:55:50,  1.12it/s]  6%|         | 783/12630 [12:23<2:56:25,  1.12it/s]  6%|         | 784/12630 [12:24<2:55:30,  1.12it/s]  6%|         | 785/12630 [12:24<2:55:00,  1.13it/s]  6%|         | 786/12630 [12:25<2:54:49,  1.13it/s]  6%|         | 787/12630 [12:26<2:54:09,  1.13it/s]  6%|         | 788/12630 [12:27<2:54:46,  1.13it/s]  6%|         | 789/12630 [12:28<2:55:29,  1.12it/s]  6%|         | 790/12630 [12:29<2:54:48,  1.13it/s]  6%|         | 791/12630 [12:30<2:54:16,  1.13it/s]  6%|         | 792/12630 [12:31<2:53:24,  1.14it/s]  6%|         | 793/12630 [12:32<2:54:07,  1.13it/s]  6%|         | 794/12630 [12:32<2:54:24,  1.13it/s]  6%|         | 795/12630 [12:33<2:53:29,  1.14it/s]  6%|         | 796/12630 [12:34<2:53:46,  1.14it/s]  6%|         | 797/12630 [12:35<2:54:10,  1.13it/s]  6%|         | 798/12630 [12:36<2:54:40,  1.13it/s]  6%|         | 799/12630 [12:37<2:54:54,  1.13it/s]  6%|         | 800/12630 [12:38<2:54:35,  1.13it/s]  6%|         | 801/12630 [12:39<2:55:21,  1.12it/s]  6%|         | 802/12630 [12:39<2:54:24,  1.13it/s]  6%|         | 803/12630 [12:40<2:53:32,  1.14it/s]  6%|         | 804/12630 [12:41<2:54:10,  1.13it/s]  6%|         | 805/12630 [12:42<2:53:37,  1.14it/s]  6%|         | 806/12630 [12:43<2:54:30,  1.13it/s]  6%|         | 807/12630 [12:44<2:55:08,  1.13it/s]  6%|         | 808/12630 [12:45<2:56:00,  1.12it/s]  6%|         | 809/12630 [12:46<2:55:22,  1.12it/s]  6%|         | 810/12630 [12:47<2:54:36,  1.13it/s]  6%|         | 811/12630 [12:47<2:54:42,  1.13it/s]  6%|         | 812/12630 [12:48<2:55:17,  1.12it/s]  6%|         | 813/12630 [12:49<2:54:10,  1.13it/s]  6%|         | 814/12630 [12:50<2:54:42,  1.13it/s]  6%|         | 815/12630 [12:51<2:54:39,  1.13it/s]  6%|         | 816/12630 [12:52<2:53:34,  1.13it/s]  6%|         | 817/12630 [12:53<2:53:50,  1.13it/s]  6%|         | 818/12630 [12:54<2:54:09,  1.13it/s]  6%|         | 819/12630 [12:55<2:54:24,  1.13it/s]  6%|         | 820/12630 [12:55<2:53:39,  1.13it/s]  7%|         | 821/12630 [12:56<2:53:19,  1.14it/s]  7%|         | 822/12630 [12:57<2:54:17,  1.13it/s]  7%|         | 823/12630 [12:58<2:54:03,  1.13it/s]  7%|         | 824/12630 [12:59<2:54:24,  1.13it/s]  7%|         | 825/12630 [13:00<2:55:32,  1.12it/s]  7%|         | 826/12630 [13:01<2:54:04,  1.13it/s]  7%|         | 827/12630 [13:02<2:54:15,  1.13it/s]  7%|         | 828/12630 [13:03<2:54:01,  1.13it/s]  7%|         | 829/12630 [13:03<2:53:51,  1.13it/s]  7%|         | 830/12630 [13:04<2:53:25,  1.13it/s]  7%|         | 831/12630 [13:05<2:53:11,  1.14it/s]  7%|         | 832/12630 [13:06<2:54:37,  1.13it/s]  7%|         | 833/12630 [13:07<2:54:12,  1.13it/s]  7%|         | 834/12630 [13:08<2:53:35,  1.13it/s]  7%|         | 835/12630 [13:09<2:54:23,  1.13it/s]  7%|         | 836/12630 [13:10<2:54:04,  1.13it/s]  7%|         | 837/12630 [13:10<2:54:18,  1.13it/s]  7%|         | 838/12630 [13:11<2:53:29,  1.13it/s]  7%|         | 839/12630 [13:12<2:53:47,  1.13it/s]  7%|         | 840/12630 [13:13<2:54:12,  1.13it/s]  7%|         | 841/12630 [13:14<2:52:55,  1.14it/s]  7%|         | 842/12630 [13:15<2:54:15,  1.13it/s]  7%|         | 843/12630 [13:16<2:53:36,  1.13it/s]  7%|         | 844/12630 [13:17<2:54:21,  1.13it/s]  7%|         | 845/12630 [13:18<2:54:48,  1.12it/s]  7%|         | 846/12630 [13:18<2:55:13,  1.12it/s]  7%|         | 847/12630 [13:19<2:55:29,  1.12it/s]  7%|         | 848/12630 [13:20<2:53:26,  1.13it/s]  7%|         | 849/12630 [13:21<2:54:38,  1.12it/s]  7%|         | 850/12630 [13:22<2:54:17,  1.13it/s]  7%|         | 851/12630 [13:23<2:53:28,  1.13it/s]  7%|         | 852/12630 [13:24<2:53:44,  1.13it/s]  7%|         | 853/12630 [13:25<2:54:18,  1.13it/s]  7%|         | 854/12630 [13:26<2:54:06,  1.13it/s]  7%|         | 855/12630 [13:26<2:54:13,  1.13it/s]  7%|         | 856/12630 [13:27<2:53:39,  1.13it/s]  7%|         | 857/12630 [13:28<2:54:48,  1.12it/s]  7%|         | 858/12630 [13:29<2:54:57,  1.12it/s]  7%|         | 859/12630 [13:30<2:54:26,  1.12it/s]  7%|         | 860/12630 [13:31<2:54:38,  1.12it/s]  7%|         | 861/12630 [13:32<2:54:18,  1.13it/s]  7%|         | 862/12630 [13:33<2:55:12,  1.12it/s]  7%|         | 863/12630 [13:34<2:53:45,  1.13it/s]  7%|         | 864/12630 [13:34<2:53:43,  1.13it/s]  7%|         | 865/12630 [13:35<2:53:02,  1.13it/s]  7%|         | 866/12630 [13:36<2:54:01,  1.13it/s]  7%|         | 867/12630 [13:37<2:53:36,  1.13it/s]  7%|         | 868/12630 [13:38<2:53:46,  1.13it/s]  7%|         | 869/12630 [13:39<2:54:36,  1.12it/s]  7%|         | 870/12630 [13:40<2:54:16,  1.12it/s]  7%|         | 871/12630 [13:41<2:53:34,  1.13it/s]  7%|         | 872/12630 [13:42<2:54:08,  1.13it/s]  7%|         | 873/12630 [13:42<2:53:32,  1.13it/s]  7%|         | 874/12630 [13:43<2:53:38,  1.13it/s]  7%|         | 875/12630 [13:44<2:53:21,  1.13it/s]  7%|         | 876/12630 [13:45<2:52:00,  1.14it/s]  7%|         | 877/12630 [13:46<2:52:46,  1.13it/s]  7%|         | 878/12630 [13:47<2:52:45,  1.13it/s]  7%|         | 879/12630 [13:48<2:53:05,  1.13it/s]  7%|         | 880/12630 [13:49<2:54:05,  1.12it/s]  7%|         | 881/12630 [13:49<2:54:11,  1.12it/s]  7%|         | 882/12630 [13:50<2:54:08,  1.12it/s]  7%|         | 883/12630 [13:51<2:53:21,  1.13it/s]  7%|         | 884/12630 [13:52<2:52:04,  1.14it/s]  7%|         | 885/12630 [13:53<2:52:35,  1.13it/s]  7%|         | 886/12630 [13:54<2:53:13,  1.13it/s]  7%|         | 887/12630 [13:55<2:52:34,  1.13it/s]  7%|         | 888/12630 [13:56<2:52:55,  1.13it/s]  7%|         | 889/12630 [13:57<2:53:09,  1.13it/s]  7%|         | 890/12630 [13:57<2:54:09,  1.12it/s]  7%|         | 891/12630 [13:58<2:54:12,  1.12it/s]  7%|         | 892/12630 [13:59<2:53:01,  1.13it/s]  7%|         | 893/12630 [14:00<2:53:14,  1.13it/s]  7%|         | 894/12630 [14:01<2:53:59,  1.12it/s]  7%|         | 895/12630 [14:02<2:54:34,  1.12it/s]  7%|         | 896/12630 [14:03<2:53:55,  1.12it/s]  7%|         | 897/12630 [14:04<2:53:31,  1.13it/s]  7%|         | 898/12630 [14:05<2:52:13,  1.14it/s]  7%|         | 899/12630 [14:05<2:52:05,  1.14it/s]  7%|         | 900/12630 [14:06<2:53:13,  1.13it/s]  7%|         | 901/12630 [14:07<2:52:19,  1.13it/s]  7%|         | 902/12630 [14:08<2:53:11,  1.13it/s]  7%|         | 903/12630 [14:09<2:52:51,  1.13it/s]  7%|         | 904/12630 [14:10<2:54:03,  1.12it/s]  7%|         | 905/12630 [14:11<2:53:44,  1.12it/s]  7%|         | 906/12630 [14:12<2:53:39,  1.13it/s]  7%|         | 907/12630 [14:13<2:53:13,  1.13it/s]  7%|         | 908/12630 [14:13<2:53:16,  1.13it/s]  7%|         | 909/12630 [14:14<2:52:26,  1.13it/s]  7%|         | 910/12630 [14:15<2:52:31,  1.13it/s]  7%|         | 911/12630 [14:16<2:52:24,  1.13it/s]  7%|         | 912/12630 [14:17<2:52:48,  1.13it/s]  7%|         | 913/12630 [14:18<2:53:44,  1.12it/s]  7%|         | 914/12630 [14:19<2:52:49,  1.13it/s]  7%|         | 915/12630 [14:20<2:52:59,  1.13it/s]  7%|         | 916/12630 [14:20<2:52:22,  1.13it/s]  7%|         | 917/12630 [14:21<2:52:54,  1.13it/s]  7%|         | 918/12630 [14:22<2:52:46,  1.13it/s]  7%|         | 919/12630 [14:23<2:52:57,  1.13it/s]  7%|         | 920/12630 [14:24<2:52:28,  1.13it/s]  7%|         | 921/12630 [14:25<2:52:52,  1.13it/s]  7%|         | 922/12630 [14:26<2:52:53,  1.13it/s]  7%|         | 923/12630 [14:27<2:52:14,  1.13it/s]  7%|         | 924/12630 [14:28<2:54:03,  1.12it/s]  7%|         | 925/12630 [14:28<2:53:20,  1.13it/s]  7%|         | 926/12630 [14:29<2:54:15,  1.12it/s]  7%|         | 927/12630 [14:30<2:54:08,  1.12it/s]  7%|         | 928/12630 [14:31<2:52:53,  1.13it/s]  7%|         | 929/12630 [14:32<2:52:57,  1.13it/s]  7%|         | 930/12630 [14:33<2:53:02,  1.13it/s]  7%|         | 931/12630 [14:34<2:52:55,  1.13it/s]  7%|         | 932/12630 [14:35<2:52:53,  1.13it/s]  7%|         | 933/12630 [14:36<2:51:53,  1.13it/s]  7%|         | 934/12630 [14:36<2:51:32,  1.14it/s]  7%|         | 935/12630 [14:37<2:51:37,  1.14it/s]  7%|         | 936/12630 [14:38<2:52:50,  1.13it/s]  7%|         | 937/12630 [14:39<2:52:14,  1.13it/s]  7%|         | 938/12630 [14:40<2:52:32,  1.13it/s]  7%|         | 939/12630 [14:41<2:53:02,  1.13it/s]  7%|         | 940/12630 [14:42<2:51:30,  1.14it/s]  7%|         | 941/12630 [14:43<2:51:54,  1.13it/s]  7%|         | 942/12630 [14:44<2:53:16,  1.12it/s]  7%|         | 943/12630 [14:44<2:53:38,  1.12it/s]  7%|         | 944/12630 [14:45<2:52:46,  1.13it/s]  7%|         | 945/12630 [14:46<2:52:36,  1.13it/s]  7%|         | 946/12630 [14:47<2:52:06,  1.13it/s]  7%|         | 947/12630 [14:48<2:52:39,  1.13it/s]  8%|         | 948/12630 [14:49<2:52:32,  1.13it/s]  8%|         | 949/12630 [14:50<2:52:26,  1.13it/s]  8%|         | 950/12630 [14:51<2:52:31,  1.13it/s]  8%|         | 951/12630 [14:52<2:52:41,  1.13it/s]  8%|         | 952/12630 [14:52<2:53:12,  1.12it/s]  8%|         | 953/12630 [14:53<2:52:05,  1.13it/s]  8%|         | 954/12630 [14:54<2:52:23,  1.13it/s]  8%|         | 955/12630 [14:55<2:52:46,  1.13it/s]  8%|         | 956/12630 [14:56<2:52:32,  1.13it/s]  8%|         | 957/12630 [14:57<2:52:47,  1.13it/s]  8%|         | 958/12630 [14:58<2:51:49,  1.13it/s]  8%|         | 959/12630 [14:59<2:54:16,  1.12it/s]  8%|         | 960/12630 [15:00<2:53:16,  1.12it/s]  8%|         | 961/12630 [15:00<2:54:23,  1.12it/s]  8%|         | 962/12630 [15:01<2:53:40,  1.12it/s]  8%|         | 963/12630 [15:02<2:53:41,  1.12it/s]  8%|         | 964/12630 [15:03<2:53:25,  1.12it/s]  8%|         | 965/12630 [15:04<2:52:46,  1.13it/s]  8%|         | 966/12630 [15:05<2:52:42,  1.13it/s]  8%|         | 967/12630 [15:06<2:52:14,  1.13it/s]  8%|         | 968/12630 [15:07<2:53:13,  1.12it/s]  8%|         | 969/12630 [15:08<2:52:36,  1.13it/s]  8%|         | 970/12630 [15:08<2:53:39,  1.12it/s]  8%|         | 971/12630 [15:09<2:53:23,  1.12it/s]  8%|         | 972/12630 [15:10<2:53:26,  1.12it/s]  8%|         | 973/12630 [15:11<2:52:10,  1.13it/s]  8%|         | 974/12630 [15:12<2:52:59,  1.12it/s]  8%|         | 975/12630 [15:13<2:52:50,  1.12it/s]  8%|         | 976/12630 [15:14<2:52:20,  1.13it/s]  8%|         | 977/12630 [15:15<2:52:03,  1.13it/s]  8%|         | 978/12630 [15:16<2:52:03,  1.13it/s]  8%|         | 979/12630 [15:16<2:50:52,  1.14it/s]  8%|         | 980/12630 [15:17<2:52:29,  1.13it/s]  8%|         | 981/12630 [15:18<2:52:22,  1.13it/s]  8%|         | 982/12630 [15:19<2:51:29,  1.13it/s]  8%|         | 983/12630 [15:20<2:51:57,  1.13it/s]  8%|         | 984/12630 [15:21<2:50:36,  1.14it/s]  8%|         | 985/12630 [15:22<2:51:05,  1.13it/s]  8%|         | 986/12630 [15:23<2:50:45,  1.14it/s]  8%|         | 987/12630 [15:23<2:52:43,  1.12it/s]  8%|         | 988/12630 [15:24<2:52:03,  1.13it/s]  8%|         | 989/12630 [15:25<2:52:37,  1.12it/s]  8%|         | 990/12630 [15:26<2:52:16,  1.13it/s]  8%|         | 991/12630 [15:27<2:52:20,  1.13it/s]  8%|         | 992/12630 [15:28<2:51:59,  1.13it/s]  8%|         | 993/12630 [15:29<2:52:01,  1.13it/s]  8%|         | 994/12630 [15{'loss': 0.4164, 'learning_rate': 4.604117181314331e-05, 'epoch': 0.24}
:30<2:51:44,  1.13it/s]  8%|         | 995/12630 [15:31<2:51:25,  1.13it/s]  8%|         | 996/12630 [15:31<2:51:27,  1.13it/s]  8%|         | 997/12630 [15:32<2:50:41,  1.14it/s]  8%|         | 998/12630 [15:33<2:52:49,  1.12it/s]  8%|         | 999/12630 [15:34<2:53:06,  1.12it/s]  8%|         | 1000/12630 [15:35<2:52:11,  1.13it/s]                                                        8%|         | 1000/12630 [15:35<2:52:11,  1.13it/s][INFO|trainer.py:2985] 2024-02-13 00:25:35,500 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000
[INFO|configuration_utils.py:473] 2024-02-13 00:25:35,515 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:26:01,343 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:26:01,345 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:26:01,347 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/special_tokens_map.json
  8%|         | 1001/12630 [16:50<74:53:01, 23.18s/it]  8%|         | 1002/12630 [16:51<53:15:31, 16.49s/it]  8%|         | 1003/12630 [16:52<38:09:36, 11.82s/it]  8%|         | 1004/12630 [16:53<27:33:14,  8.53s/it]  8%|         | 1005/12630 [16:54<20:08:43,  6.24s/it]  8%|         | 1006/12630 [16:55<14:57:04,  4.63s/it]  8%|         | 1007/12630 [16:56<11:19:36,  3.51s/it]  8%|         | 1008/12630 [16:56<8:47:12,  2.72s/it]   8%|         | 1009/12630 [16:57<7:00:32,  2.17s/it]  8%|         | 1010/12630 [16:58<5:45:57,  1.79s/it]  8%|         | 1011/12630 [16:59<4:52:59,  1.51s/it]  8%|         | 1012/12630 [17:00<4:15:57,  1.32s/it]  8%|         | 1013/12630 [17:01<3:50:16,  1.19s/it]  8%|         | 1014/12630 [17:02<3:32:38,  1.10s/it]  8%|         | 1015/12630 [17:03<3:22:34,  1.05s/it]  8%|         | 1016/12630 [17:04<3:12:51,  1.00it/s]  8%|         | 1017/12630 [17:04<3:05:47,  1.04it/s]  8%|         | 1018/12630 [17:05<3:02:11,  1.06it/s]  8%|         | 1019/12630 [17:06<2:58:13,  1.09it/s]  8%|         | 1020/12630 [17:07<2:56:13,  1.10it/s]  8%|         | 1021/12630 [17:08<2:54:38,  1.11it/s]  8%|         | 1022/12630 [17:09<2:53:34,  1.11it/s]  8%|         | 1023/12630 [17:10<2:54:51,  1.11it/s]  8%|         | 1024/12630 [17:11<2:53:19,  1.12it/s]  8%|         | 1025/12630 [17:11<2:51:50,  1.13it/s]  8%|         | 1026/12630 [17:12<2:51:02,  1.13it/s]  8%|         | 1027/12630 [17:13<2:50:30,  1.13it/s]  8%|         | 1028/12630 [17:14<2:50:57,  1.13it/s]  8%|         | 1029/12630 [17:15<2:51:39,  1.13it/s]  8%|         | 1030/12630 [17:16<2:51:52,  1.12it/s]  8%|         | 1031/12630 [17:17<2:51:57,  1.12it/s]  8%|         | 1032/12630 [17:18<2:52:03,  1.12it/s]  8%|         | 1033/12630 [17:19<2:51:45,  1.13it/s]  8%|         | 1034/12630 [17:19<2:51:57,  1.12it/s]  8%|         | 1035/12630 [17:20<2:51:21,  1.13it/s]  8%|         | 1036/12630 [17:21<2:52:08,  1.12it/s]  8%|         | 1037/12630 [17:22<2:50:39,  1.13it/s]  8%|         | 1038/12630 [17:23<2:50:19,  1.13it/s]  8%|         | 1039/12630 [17:24<2:51:55,  1.12it/s]  8%|         | 1040/12630 [17:25<2:50:47,  1.13it/s]  8%|         | 1041/12630 [17:26<2:51:32,  1.13it/s]  8%|         | 1042/12630 [17:27<2:51:47,  1.12it/s]  8%|         | 1043/12630 [17:27<2:51:42,  1.12it/s]  8%|         | 1044/12630 [17:28<2:52:33,  1.12it/s]  8%|         | 1045/12630 [17:29<2:51:47,  1.12it/s]  8%|         | 1046/12630 [17:30<2:51:07,  1.13it/s]  8%|         | 1047/12630 [17:31<2:50:58,  1.13it/s]  8%|         | 1048/12630 [17:32<2:50:24,  1.13it/s]  8%|         | 1049/12630 [17:33<2:50:16,  1.13it/s]  8%|         | 1050/12630 [17:34<2:50:04,  1.13it/s]  8%|         | 1051/12630 [17:35<2:50:30,  1.13it/s]  8%|         | 1052/12630 [17:35<2:50:03,  1.13it/s]  8%|         | 1053/12630 [17:36<2:50:25,  1.13it/s]  8%|         | 1054/12630 [17:37<2:50:39,  1.13it/s]  8%|         | 1055/12630 [17:38<2:50:24,  1.13it/s]  8%|         | 1056/12630 [17:39<2:50:40,  1.13it/s]  8%|         | 1057/12630 [17:40<2:50:27,  1.13it/s]  8%|         | 1058/12630 [17:41<2:50:11,  1.13it/s]  8%|         | 1059/12630 [17:42<2:49:33,  1.14it/s]  8%|         | 1060/12630 [17:42<2:50:18,  1.13it/s]  8%|         | 1061/12630 [17:43<2:50:36,  1.13it/s]  8%|         | 1062/12630 [17:44<2:49:38,  1.14it/s]  8%|         | 1063/12630 [17:45<2:51:01,  1.13it/s]  8%|         | 1064/12630 [17:46<2:51:16,  1.13it/s]  8%|         | 1065/12630 [17:47<2:50:21,  1.13it/s]  8%|         | 1066/12630 [17:48<2:52:13,  1.12it/s]  8%|         | 1067/12630 [17:49<2:50:58,  1.13it/s]  8%|         | 1068/12630 [17:50<2:52:22,  1.12it/s]  8%|         | 1069/12630 [17:50<2:51:11,  1.13it/s]  8%|         | 1070/12630 [17:51<2:49:52,  1.13it/s]  8%|         | 1071/12630 [17:52<2:52:58,  1.11it/s]  8%|         | 1072/12630 [17:53<2:50:26,  1.13it/s]  8%|         | 1073/12630 [17:54<2:51:28,  1.12it/s]  9%|         | 1074/12630 [17:55<2:51:49,  1.12it/s]  9%|         | 1075/12630 [17:56<2:50:19,  1.13it/s]  9%|         | 1076/12630 [17:57<2:49:49,  1.13it/s]  9%|         | 1077/12630 [17:58<2:50:18,  1.13it/s]  9%|         | 1078/12630 [17:58<2:49:47,  1.13it/s]  9%|         | 1079/12630 [17:59<2:49:38,  1.13it/s]  9%|         | 1080/12630 [18:00<2:51:03,  1.13it/s]  9%|         | 1081/12630 [18:01<2:51:28,  1.12it/s]  9%|         | 1082/12630 [18:02<2:50:32,  1.13it/s]  9%|         | 1083/12630 [18:03<2:50:13,  1.13it/s]  9%|         | 1084/12630 [18:04<2:49:39,  1.13it/s]  9%|         | 1085/12630 [18:05<2:49:59,  1.13it/s]  9%|         | 1086/12630 [18:06<2:49:40,  1.13it/s]  9%|         | 1087/12630 [18:06<2:49:16,  1.14it/s]  9%|         | 1088/12630 [18:07<2:50:14,  1.13it/s]  9%|         | 1089/12630 [18:08<2:50:34,  1.13it/s]  9%|         | 1090/12630 [18:09<2:49:22,  1.14it/s]  9%|         | 1091/12630 [18:10<2:50:32,  1.13it/s]  9%|         | 1092/12630 [18:11<2:50:28,  1.13it/s]  9%|         | 1093/12630 [18:12<2:50:39,  1.13it/s]  9%|         | 1094/12630 [18:13<2:49:57,  1.13it/s]  9%|         | 1095/12630 [18:13<2:49:33,  1.13it/s]  9%|         | 1096/12630 [18:14<2:49:53,  1.13it/s]  9%|         | 1097/12630 [18:15<2:50:07,  1.13it/s]  9%|         | 1098/12630 [18:16<2:49:48,  1.13it/s]  9%|         | 1099/12630 [18:17<2:49:14,  1.14it/s]  9%|         | 1100/12630 [18:18<2:49:52,  1.13it/s]  9%|         | 1101/12630 [18:19<2:50:38,  1.13it/s]  9%|         | 1102/12630 [18:20<2:50:07,  1.13it/s]  9%|         | 1103/12630 [18:21<2:50:16,  1.13it/s]  9%|         | 1104/12630 [18:21<2:49:42,  1.13it/s]  9%|         | 1105/12630 [18:22<2:50:38,  1.13it/s]  9%|         | 1106/12630 [18:23<2:50:41,  1.13it/s]  9%|         | 1107/12630 [18:24<2:52:36,  1.11it/s]  9%|         | 1108/12630 [18:25<2:51:25,  1.12it/s]  9%|         | 1109/12630 [18:26<2:51:06,  1.12it/s]  9%|         | 1110/12630 [18:27<2:50:34,  1.13it/s]  9%|         | 1111/12630 [18:28<2:51:05,  1.12it/s]  9%|         | 1112/12630 [18:29<2:50:24,  1.13it/s]  9%|         | 1113/12630 [18:29<2:51:04,  1.12it/s]  9%|         | 1114/12630 [18:30<2:50:07,  1.13it/s]  9%|         | 1115/12630 [18:31<2:51:23,  1.12it/s]  9%|         | 1116/12630 [18:32<2:50:38,  1.12it/s]  9%|         | 1117/12630 [18:33<2:51:18,  1.12it/s]  9%|         | 1118/12630 [18:34<2:51:33,  1.12it/s]  9%|         | 1119/12630 [18:35<2:50:24,  1.13it/s]  9%|         | 1120/12630 [18:36<2:50:05,  1.13it/s]  9%|         | 1121/12630 [18:37<2:51:11,  1.12it/s]  9%|         | 1122/12630 [18:37<2:50:44,  1.12it/s]  9%|         | 1123/12630 [18:38<2:50:01,  1.13it/s]  9%|         | 1124/12630 [18:39<2:50:22,  1.13it/s]  9%|         | 1125/12630 [18:40<2:49:38,  1.13it/s]  9%|         | 1126/12630 [18:41<2:49:14,  1.13it/s]  9%|         | 1127/12630 [18:42<2:49:15,  1.13it/s]  9%|         | 1128/12630 [18:43<2:51:00,  1.12it/s]  9%|         | 1129/12630 [18:44<2:50:40,  1.12it/s]  9%|         | 1130/12630 [18:45<2:50:14,  1.13it/s]  9%|         | 1131/12630 [18:45<2:50:11,  1.13it/s]  9%|         | 1132/12630 [18:46<2:49:52,  1.13it/s]  9%|         | 1133/12630 [18:47<2:49:18,  1.13it/s]  9%|         | 1134/12630 [18:48<2:50:21,  1.12it/s]  9%|         | 1135/12630 [18:49<2:50:22,  1.12it/s]  9%|         | 1136/12630 [18:50<2:50:19,  1.12it/s]  9%|         | 1137/12630 [18:51<2:49:33,  1.13it/s]  9%|         | 1138/12630 [18:52<2:49:44,  1.13it/s]  9%|         | 1139/12630 [18:53<2:49:02,  1.13it/s]  9%|         | 1140/12630 [18:53<2:49:41,  1.13it/s]  9%|         | 1141/12630 [18:54<2:49:45,  1.13it/s]  9%|         | 1142/12630 [18:55<2:49:40,  1.13it/s]  9%|         | 1143/12630 [18:56<2:49:59,  1.13it/s]  9%|         | 1144/12630 [18:57<2:49:15,  1.13it/s]  9%|         | 1145/12630 [18:58<2:49:40,  1.13it/s]  9%|         | 1146/12630 [18:59<2:48:56,  1.13it/s]  9%|         | 1147/12630 [19:00<2:50:30,  1.12it/s]  9%|         | 1148/12630 [19:01<2:50:20,  1.12it/s]  9%|         | 1149/12630 [19:01<2:50:13,  1.12it/s]  9%|         | 1150/12630 [19:02<2:49:26,  1.13it/s]  9%|         | 1151/12630 [19:03<2:49:43,  1.13it/s]  9%|         | 1152/12630 [19:04<2:49:06,  1.13it/s]  9%|         | 1153/12630 [19:05<2:51:17,  1.12it/s]  9%|         | 1154/12630 [19:06<2:50:19,  1.12it/s]  9%|         | 1155/12630 [19:07<2:52:10,  1.11it/s]  9%|         | 1156/12630 [19:08<2:52:11,  1.11it/s]  9%|         | 1157/12630 [19:09<2:50:26,  1.12it/s]  9%|         | 1158/12630 [19:09<2:50:46,  1.12it/s]  9%|         | 1159/12630 [19:10<2:50:27,  1.12it/s]  9%|         | 1160/12630 [19:11<2:49:36,  1.13it/s]  9%|         | 1161/12630 [19:12<2:51:10,  1.12it/s]  9%|         | 1162/12630 [19:13<2:49:56,  1.12it/s]  9%|         | 1163/12630 [19:14<2:50:33,  1.12it/s]  9%|         | 1164/12630 [19:15<2:50:23,  1.12it/s]  9%|         | 1165/12630 [19:16<2:50:11,  1.12it/s]  9%|         | 1166/12630 [19:17<2:49:28,  1.13it/s]  9%|         | 1167/12630 [19:17<2:49:32,  1.13it/s]  9%|         | 1168/12630 [19:18<2:49:01,  1.13it/s]  9%|         | 1169/12630 [19:19<2:49:41,  1.13it/s]  9%|         | 1170/12630 [19:20<2:49:45,  1.13it/s]  9%|         | 1171/12630 [19:21<2:49:37,  1.13it/s]  9%|         | 1172/12630 [19:22<2:48:39,  1.13it/s]  9%|         | 1173/12630 [19:23<2:48:43,  1.13it/s]  9%|         | 1174/12630 [19:24<2:48:42,  1.13it/s]  9%|         | 1175/12630 [19:25<2:47:53,  1.14it/s]  9%|         | 1176/12630 [19:25<2:48:23,  1.13it/s]  9%|         | 1177/12630 [19:26<2:49:32,  1.13it/s]  9%|         | 1178/12630 [19:27<2:49:53,  1.12it/s]  9%|         | 1179/12630 [19:28<2:48:19,  1.13it/s]  9%|         | 1180/12630 [19:29<2:49:10,  1.13it/s]  9%|         | 1181/12630 [19:30<2:49:20,  1.13it/s]  9%|         | 1182/12630 [19:31<2:48:46,  1.13it/s]  9%|         | 1183/12630 [19:32<2:50:12,  1.12it/s]  9%|         | 1184/12630 [19:33<2:49:40,  1.12it/s]  9%|         | 1185/12630 [19:33<2:49:02,  1.13it/s]  9%|         | 1186/12630 [19:34<2:49:07,  1.13it/s]  9%|         | 1187/12630 [19:35<2:50:05,  1.12it/s]  9%|         | 1188/12630 [19:36<2:50:03,  1.12it/s]  9%|         | 1189/12630 [19:37<2:49:46,  1.12it/s]  9%|         | 1190/12630 [19:38<2:48:58,  1.13it/s]  9%|         | 1191/12630 [19:39<2:48:18,  1.13it/s]  9%|         | 1192/12630 [19:40<2:48:09,  1.13it/s]  9%|         | 1193/12630 [19:40<2:47:46,  1.14it/s]  9%|         | 1194/12630 [19:41<2:49:48,  1.12it/s]  9%|         | 1195/12630 [19:42<2:49:26,  1.12it/s]  9%|         | 1196/12630 [19:43<2:48:45,  1.13it/s]  9%|         | 1197/12630 [19:44<2:48:29,  1.13it/s]  9%|         | 1198/12630 [19:45<2:48:32,  1.13it/s]  9%|         | 1199/12630 [19:46<2:49:07,  1.13it/s] 10%|         | 1200/12630 [19:47<2:49:15,  1.13it/s] 10%|         | 1201/12630 [19:48<2:49:05,  1.13it/s] 10%|         | 1202/12630 [19:48<2:48:54,  1.13it/s] 10%|         | 1203/12630 [19:49<2:49:36,  1.12it/s] 10%|         | 1204/12630 [19:50<2:48:30,  1.13it/s] 10%|         | 1205/12630 [19:51<2:48:17,  1.13it/s] 10%|         | 1206/12630 [19:52<2:48:41,  1.13it/s] 10%|         | 1207/12630 [19:53<2:47:46,  1.13it/s] 10%|         | 1208/12630 [19:54<2:47:39,  1.14it/s] 10%|         | 1209/12630 [19:55<2:49:11,  1.13it/s] 10%|         | 1210/12630 [19:56<2:49:14,  1.12it/s] 10%|         | 1211/12630 [19:56<2:48:58,  1.13it/s] 10%|         | 1212/12630 [19:57<2:48:48,  1.13it/s] 10%|         | 1213/12630 [19:58<2:48:33,  1.13it/s] 10%|         | 1214/12630 [19:59<2:48:33,  1.13it/s] 10%|         | 1215/12630 [20:00<2:48:38,  1.13it/s] 10%|         | 1216/12630 [20:01<2:48:46,  1.13it/s] 10%|         | 1217/12630 [20:02<2:49:53,  1.12it/s] 10%|         | 1218/12630 [20:03<2:48:53,  1.13it/s] 10%|         | 1219/12630 [20:04<2:48:34,  1.13it/s] 10%|         | 1220/12630 [20:04<2:48:00,  1.13it/s] 10%|         | 1221/12630 [20:05<2:49:01,  1.12it/s] 10%|         | 1222/12630 [20:06<2:48:09,  1.13it/s] 10%|         | 1223/12630 [20:07<2:49:38,  1.12it/s] 10%|         | 1224/12630 [20:08<2:48:46,  1.13it/s] 10%|         | 1225/12630 [20:09<2:48:35,  1.13it/s] 10%|         | 1226/12630 [20:10<2:48:30,  1.13it/s] 10%|         | 1227/12630 [20:11<2:48:37,  1.13it/s] 10%|         | 1228/12630 [20:12<2:48:56,  1.12it/s] 10%|         | 1229/12630 [20:12<2:47:50,  1.13it/s] 10%|         | 1230/12630 [20:13<2:48:08,  1.13it/s] 10%|         | 1231/12630 [20:14<2:48:38,  1.13it/s] 10%|         | 1232/12630 [20:15<2:48:28,  1.13it/s] 10%|         | 1233/12630 [20:16<2:48:31,  1.13it/s] 10%|         | 1234/12630 [20:17<2:48:43,  1.13it/s] 10%|         | 1235/12630 [20:18<2:49:17,  1.12it/s] 10%|         | 1236/12630 [20:19<2:48:35,  1.13it/s] 10%|         | 1237/12630 [20:20<2:48:08,  1.13it/s] 10%|         | 1238/12630 [20:20<2:48:38,  1.13it/s] 10%|         | 1239/12630 [20:21<2:48:14,  1.13it/s] 10%|         | 1240/12630 [20:22<2:48:10,  1.13it/s] 10%|         | 1241/12630 [20:23<2:47:43,  1.13it/s] 10%|         | 1242/12630 [20:24<2:47:45,  1.13it/s] 10%|         | 1243/12630 [20:25<2:47:27,  1.13it/s] 10%|         | 1244/12630 [20:26<2:46:40,  1.14it/s] 10%|         | 1245/12630 [20:27<2:47:30,  1.13it/s] 10%|         | 1246/12630 [20:27<2:48:28,  1.13it/s] 10%|         | 1247/12630 [20:28<2:47:05,  1.14it/s] 10%|         | 1248/12630 [20:29<2:47:55,  1.13it/s] 10%|         | 1249/12630 [20:30<2:47:37,  1.13it/s] 10%|         | 1250/12630 [20:31<2:48:11,  1.13it/s] 10%|         | 1251/12630 [20:32<2:48:00,  1.13it/s] 10%|         | 1252/12630 [20:33<2:48:57,  1.12it/s] 10%|         | 1253/12630 [20:34<2:47:57,  1.13it/s] 10%|         | 1254/12630 [20:35<2:48:01,  1.13it/s] 10%|         | 1255/12630 [20:35<2:47:24,  1.13it/s] 10%|         | 1256/12630 [20:36<2:46:39,  1.14it/s] 10%|         | 1257/12630 [20:37<2:46:01,  1.14it/s] 10%|         | 1258/12630 [20:38<2:45:55,  1.14it/s] 10%|         | 1259/12630 [20:39<2:47:15,  1.13it/s] 10%|         | 1260/12630 [20:40<2:46:11,  1.14it/s] 10%|         | 1261/12630 [20:41<2:47:09,  1.13it/s] 10%|         | 1262/12630 [20:42<2:46:44,  1.14it/s] 10%|         | 1263/12630 [20:42<2:47:14,  1.13it/s] 10%|         | 1264/12630 [20:43<2:46:20,  1.14it/s] 10%|         | 1265/12630 [20:44<2:47:21,  1.13it/s] 10%|         | 1266/12630 [20:45<2:47:52,  1.13it/s] 10%|         | 1267/12630 [20:46<2:49:20,  1.12it/s] 10%|         | 1268/12630 [20:47<2:48:42,  1.12it/s] 10%|         | 1269/12630 [20:48<2:47:33,  1.13it/s] 10%|         | 1270/12630 [20:49<2:47:31,  1.13it/s] 10%|         | 1271/12630 [20:50<2:47:10,  1.13it/s] 10%|         | 1272/12630 [20:50<2:47:50,  1.13it/s] 10%|         | 1273/12630 [20:51<2:46:52,  1.13it/s] 10%|         | 1274/12630 [20:52<2:48:19,  1.12it/s] 10%|         | 1275/12630 [20:53<2:47:13,  1.13it/s] 10%|         | 1276/12630 [20:54<2:47:28,  1.13it/s] 10%|         | 1277/12630 [20:55<2:47:49,  1.13it/s] 10%|         | 1278/12630 [20:56<2:48:21,  1.12it/s] 10%|         | 1279/12630 [20:57<2:49:44,  1.11it/s] 10%|         | 1280/12630 [20:58<2:49:45,  1.11it/s] 10%|         | 1281/12630 [20:58<2:48:11,  1.12it/s] 10%|         | 1282/12630 [20:59<2:47:17,  1.13it/s] 10%|         | 1283/12630 [21:00<2:47:37,  1.13it/s] 10%|         | 1284/12630 [21:01<2:48:19,  1.12it/s] 10%|         | 1285/12630 [21:02<2:48:06,  1.12it/s] 10%|         | 1286/12630 [21:03<2:48:27,  1.12it/s] 10%|         | 1287/12630 [21:04<2:47:14,  1.13it/s] 10%|         | 1288/12630 [21:05<2:46:09,  1.14it/s] 10%|         | 1289/12630 [21:06<2:47:21,  1.13it/s] 10%|         | 1290/12630 [21:06<2:47:10,  1.13it/s] 10%|         | 1291/12630 [21:07<2:46:12,  1.14it/s] 10%|         | 1292/12630 [21:08<2:45:52,  1.14it/s] 10%|         | 1293/12630 [21:09<2:47:44,  1.13it/s] 10%|         | 1294/12630 [21:10<2:47:01,  1.13it/s] 10%|         | 1295/12630 [21:11<2:48:14,  1.12it/s] 10%|         | 1296/12630 [21:12<2:47:48,  1.13it/s] 10%|         | 1297/12630 [21:13<2:49:13,  1.12it/s] 10%|         | 1298/12630 [21:14<2:48:45,  1.12it/s] 10%|         | 1299/12630 [21:14<2:47:01,  1.13it/s] 10%|         | 1300/12630 [21:15<2:47:21,  1.13it/s] 10%|         | 1301/12630 [21:16<2:46:33,  1.13it/s] 10%|         | 1302/12630 [21:17<2:48:08,  1.12it/s] 10%|         | 1303/12630 [21:18<2:46:52,  1.13it/s] 10%|         | 1304/12630 [21:19<2:47:08,  1.13it/s] 10%|         | 1305/12630 [21:20<2:48:08,  1.12it/s] 10%|         | 1306/12630 [21:21<2:48:19,  1.12it/s] 10%|         | 1307/12630 [21:22<2:47:59,  1.12it/s] 10%|         | 1308/12630 [21:22<2:46:38,  1.13it/s] 10%|         | 1309/12630 [21:23<2:48:03,  1.12it/s] 10%|         | 1310/12630 [21:24<2:47:56,  1.12it/s] 10%|         | 1311/12630 [21:25<2:48:06,  1.12it/s] 10%|         | 1312/12630 [21:26<2:47:21,  1.13it/s] 10%|         | 1313/12630 [21:27<2:48:26,  1.12it/s] 10%|         | 1314/12630 [21:28<2:47:48,  1.12it/s] 10%|         | 1315/12630 [21:29<2:47:08,  1.13it/s] 10%|         | 1316/12630 [21:30<2:48:26,  1.12it/s] 10%|         | 1317/12630 [21:30<2:47:17,  1.13it/s] 10%|         | 1318/12630 [21:31<2:47:05,  1.13it/s] 10%|         | 1319/12630 [21:32<2:46:33,  1.13it/s] 10%|         | 1320/12630 [21:33<2:46:26,  1.13it/s] 10%|         | 1321/12630 [21:34<2:47:13,  1.13it/s] 10%|         | 1322/12630 [21:35<2:47:58,  1.12it/s] 10%|         | 1323/12630 [21:36<2:47:17,  1.13it/s] 10%|         | 1324/12630 [21:37<2:47:06,  1.13it/s] 10%|         | 1325/12630 [21:37<2:45:59,  1.14it/s] 10%|         | 1326/12630 [21:38<2:47:12,  1.13it/s] 11%|         | 1327/12630 [21:39<2:46:59,  1.13it/s] 11%|         | 1328/12630 [21:40<2:48:33,  1.12it/s] 11%|         | 1329/12630 [21:41<2:47:23,  1.13it/s] 11%|         | 1330/12630 [21:42<2:47:21,  1.13it/s] 11%|         | 1331/12630 [21:43<2:46:46,  1.13it/s] 11%|         | 1332/12630 [21:44<2:47:23,  1.12it/s] 11%|         | 1333/12630 [21:45<2:47:05,  1.13it/s] 11%|         | 1334/12630 [21:45<2:46:23,  1.13it/s] 11%|         | 1335/12630 [21:46<2:46:21,  1.13it/s] 11%|         | 1336/12630 [21:47<2:46:34,  1.13it/s] 11%|         | 1337/12630 [21:48<2:46:44,  1.13it/s] 11%|         | 1338/12630 [21:49<2:46:14,  1.13it/s] 11%|         | 1339/12630 [21:50<2:45:12,  1.14it/s] 11%|         | 1340/12630 [21:51<2:45:46,  1.14it/s] 11%|         | 1341/12630 [21:52<2:46:38,  1.13it/s] 11%|         | 1342/12630 [21:53<2:44:44,  1.14it/s] 11%|         | 1343/12630 [21:53<2:46:36,  1.13it/s] 11%|         | 1344/12630 [21:54<2:45:55,  1.13it/s] 11%|         | 1345/12630 [21:55<2:46:04,  1.13it/s] 11%|         | 1346/12630 [21:56<2:45:46,  1.13it/s] 11%|         | 1347/12630 [21:57<2:47:06,  1.13it/s] 11%|         | 1348/12630 [21:58<2:46:09,  1.13it/s] 11%|         | 1349/12630 [21:59<2:46:12,  1.13it/s] 11%|         | 1350/12630 [22:00<2:46:03,  1.13it/s] 11%|         | 1351/12630 [22:01<2:47:03,  1.13it/s] 11%|         | 1352/12630 [22:01<2:47:21,  1.12it/s] 11%|         | 1353/12630 [22:02<2:47:05,  1.12it/s] 11%|         | 1354/12630 [22:03<2:46:30,  1.13it/s] 11%|         | 1355/12630 [22:04<2:46:06,  1.13it/s] 11%|         | 1356/12630 [22:05<2:46:31,  1.13it/s] 11%|         | 1357/12630 [22:06<2:45:05,  1.14it/s] 11%|         | 1358/12630 [22:07<2:44:55,  1.14it/s] 11%|         | 1359/12630 [22:08<2:45:39,  1.13it/s] 11%|         | 1360/12630 [22:08<2:45:15,  1.14it/s] 11%|         | 1361/12630 [22:09<2:46:26,  1.13it/s] 11%|         | 1362/12630 [22:10<2:46:11,  1.13it/s] 11%|         | 1363/12630 [22:11<2:45:34,  1.13it/s] 11%|         | 1364/12630 [22:12<2:44:59,  1.14it/s] 11%|         | 1365/12630 [22:13<2:45:22,  1.14it/s] 11%|         | 1366/12630 [22:14<2:45:41,  1.13it/s] 11%|         | 1367/12630 [22:15<2:44:50,  1.14it/s] 11%|         | 1368/12630 [22:16<2:45:59,  1.13it/s] 11%|         | 1369/12630 [22:16<2:46:04,  1.13it/s] 11%|         | 1370/12630 [22:17<2:46:07,  1.13it/s] 11%|         | 1371/12630 [22:18<2:45:27,  1.13it/s] 11%|         | 1372/12630 [22:19<2:45:42,  1.13it/s] 11%|         | 1373/12630 [22:20<2:47:31,  1.12it/s] 11%|         | 1374/12630 [22:21<2:46:16,  1.13it/s] 11%|         | 1375/12630 [22:22<2:46:21,  1.13it/s] 11%|         | 1376/12630 [22:23<2:45:10,  1.14it/s] 11%|         | 1377/12630 [22:23<2:46:00,  1.13it/s] 11%|         | 1378/12630 [22:24<2:46:26,  1.13it/s] 11%|         | 1379/12630 [22:25<2:46:08,  1.13it/s] 11%|         | 1380/12630 [22:26<2:45:17,  1.13it/s] 11%|         | 1381/12630 [22:27<2:44:56,  1.14it/s] 11%|         | 1382/12630 [22:28<2:46:26,  1.13it/s] 11%|         | 1383/12630 [22:29<2:46:16,  1.13it/s] 11%|         | 1384/12630 [22:30<2:45:36,  1.13it/s] 11%|         | 1385/12630 [22:31<2:46:33,  1.13it/s] 11%|         | 1386/12630 [22:31<2:46:09,  1.13it/s] 11%|         | 1387/12630 [22:32<2:45:49,  1.13it/s] 11%|         | 1388/12630 [22:33<2:45:59,  1.13it/s] 11%|         | 1389/12630 [22:34<2:45:34,  1.13it/s] 11%|         | 1390/12630 [22:35<2:46:55,  1.12it/s] 11%|         | 1391/12630 [22:36<2:46:10,  1.13it/s] 11%|         | 1392/12630 [22:37<2:45:14,  1.13it/s] 11%|         | 1393/12630 [22:38<2:45:13,  1.13it/s] 11%|         | 1394/12630 [22:39<2:45:01,  1.13it/s] 11%|         | 1395/12630 [22:39<2:44:45,  1.14it/s] 11%|         | 1396/12630 [22:40<2:45:47,  1.13it/s] 11%|         | 1397/12630 [22:41<2:46:08,  1.13it/s] 11%|         | 1398/12630 [22:42<2:45:16,  1.13it/s] 11%|         | 1399/12630 [22:43<2:45:28,  1.13it/s] 11%|         | 1400/12630 [22:44<2:45:43,  1.13it/s] 11%|         | 1401/12630 [22:45<2:45:49,  1.13it/s] 11%|         | 1402/12630 [22:46<2:44:39,  1.14it/s] 11%|         | 1403/12630 [22:46<2:45:00,  1.13it/s] 11%|         | 1404/12630 [22:47<2:44:01,  1.14it/s] 11%|         | 1405/12630 [22:48<2:45:17,  1.13it/s] 11%|         | 1406/12630 [22:49<2:44:31,  1.14it/s] 11%|         | 1407/12630 [22:50<2:44:51,  1.13it/s] 11%|         | 1408/12630 [22:51<2:44:28,  1.14it/s] 11%|         | 1409/12630 [22:52<2:45:07,  1.13it/s] 11%|         | 1410/12630 [22:53<2:47:30,  1.12it/s] 11%|         | 1411/12630 [22:54<2:46:21,  1.12it/s] 11%|         | 1412/12630 [22:54<2:45:58,  1.13it/s] 11%|         | 1413/12630 [22:55<2:45:42,  1.13it/s] 11%|         | 1414/12630 [22:56<2:46:17,  1.12it/s] 11%|         | 1415/12630 [22:57<2:46:05,  1.13it/s] 11%|         | 1416/12630 [22:58<2:44:49,  1.13it/s] 11%|         | 1417/12630 [22:59<2:45:02,  1.13it/s] 11%|         | 1418/12630 [23:00<2:44:43,  1.13it/s] 11%|         | 1419/12630 [23:01<2:44:33,  1.14it/s] 11%|         | 1420/12630 [23:02<2:45:11,  1.13it/s] 11%|        | 1421/12630 [23:02<2:44:28,  1.14it/s] 11%|        | 1422/12630 [23:03<2:45:33,  1.13it/s] 11%|        | 1423/12630 [23:04<2:45:15,  1.13it/s] 11%|        | 1424/12630 [23:05<2:45:51,  1.13it/s] 11%|        | 1425/12630 [23:06<2:46:51,  1.12it/s] 11%|        | 1426/12630 [23:07<2:45:12,  1.13it/s] 11%|        | 1427/12630 [23:08<2:45:07,  1.13it/s] 11%|        | 1428/12630 [23:09<2:45:24,  1.13it/s] 11%|        | 1429/12630 [23:10<2:46:39,  1.12it/s] 11%|        | 1430/12630 [23:10<2:46:09,  1.12it/s] 11%|        | 1431/12630 [23:11<2:45:22,  1.13it/s] 11%|        | 1432/12630 [23:12<2:45:16,  1.13it/s] 11%|        | 1433/12630 [23:13<2:45:12,  1.13it/s] 11%|        | 1434/12630 [23:14<2:45:02,  1.13it/s] 11%|        | 1435/12630 [23:15<2:44:29,  1.13it/s] 11%|        | 1436/12630 [23:16<2:45:29,  1.13it/s] 11%|        | 1437/12630 [23:17<2:45:38,  1.13it/s] 11%|        | 1438/12630 [23:17<2:46:07,  1.12it/s] 11%|        | 1439/12630 [23:18<2:45:54,  1.12it/s] 11%|        | 1440/12630 [23:19<2:44:14,  1.14it/s] 11%|        | 1441/12630 [23:20<2:45:02,  1.13it/s] 11%|        | 1442/12630 [23:21<2:44:35,  1.13it/s] 11%|        | 1443/12630 [23:22<2:44:51,  1.13it/s] 11%|        | 1444/12630 [23:23<2:43:51,  1.14it/s] 11%|        | 1445/12630 [23:24<2:45:25,  1.13it/s] 11%|        | 1446/12630 [23:25<2:44:40,  1.13it/s] 11%|        | 1447/12630 [23:25<2:45:12,  1.13it/s] 11%|        | 1448/12630 [23:26<2:44:41,  1.13it/s] 11%|        | 1449/12630 [23:27<2:44:18,  1.13it/s] 11%|        | 1450/12630 [23:28<2:44:39,  1.13it/s] 11%|        | 1451/12630 [23:29<2:45:18,  1.13it/s] 11%|        | 1452/12630 [23:30<2:45:04,  1.13it/s] 12%|        | 1453/12630 [23:31<2:45:40,  1.12it/s] 12%|        | 1454/12630 [23:32<2:44:21,  1.13it/s] 12%|        | 1455/12630 [23:32<2:44:28,  1.13it/s] 12%|        | 1456/12630 [23:33<2:45:16,  1.13it/s] 12%|        | 1457/12630 [23:34<2:45:03,  1.13it/s] 12%|        | 1458/12630 [23:35<2:45:58,  1.12it/s] 12%|        | 1459/12630 [23:36<2:45:44,  1.12it/s] 12%|        | 1460/12630 [23:37<2:44:53,  1.13it/s] 12%|        | 1461/12630 [23:38<2:44:41,  1.13it/s] 12%|        | 1462/12630 [23:39<2:44:03,  1.13it/s] 12%|        | 1463/12630 [23:40<2:44:34,  1.13it/s] 12%|        | 1464/12630 [23:40<2:44:07,  1.13it/s] 12%|        | 1465/12630 [23:41<2:44:06,  1.13it/s] 12%|        | 1466/12630 [23:42<2:44:02,  1.13it/s] 12%|        | 1467/12630 [23:43<2:45:09,  1.13it/s] 12%|        | 1468/12630 [23:44<2:44:45,  1.13it/s] 12%|        | 1469/12630 [23:45<2:46:19,  1.12it/s] 12%|        | 1470/12630 [23:46<2:45:27,  1.12it/s] 12%|        | 1471/12630 [23:47<2:44:43,  1.13it/s] 12%|        | 1472/12630 [23:48<2:44:48,  1.13it/s] 12%|        | 1473/12630 [23:48<2:45:35,  1.12it/s] 12%|        | 1474/12630 [23:49<2:44:45,  1.13it/s] 12%|        | 1475/12630 [23:50<2:44:47,  1.13it/s] 12%|        | 1476/12630 [23:51<2:43:59,  1.13it/s] 12%|        | 1477/12630 [23:52<2:44:15,  1.13it/s] 12%|        | 1478/12630 [23:53<2:44:11,  1.13it/s] 12%|        | 1479/12630 [23:54<2:44:05,  1.13it/s] 12%|        | 1480/12630 [23:55<2:44:07,  1.13it/s] 12%|        | 1481/12630 [23:56<2:43:57,  1.13it/s] 12%|        | 1482/12630 [23:56<2:42:58,  1.14it/s] 12%|        | 1483/12630 [23:57<2:43:25,  1.14it/s] 12%|        | 1484/12630 [23:58<2:45:01,  1.13it/s] 12%|        | 1485/12630 [23:59<2:45:05,  1.13it/s] 12%|        | 1486/12630 [24:00<2:43:28,  1.14it/s] 12%|        | 1487/12630 [24:01<2:44:40,  1.13it/s] 12%|        | 1488/12630 [24:02<2:44:09,  1.13it/s] 12%|        | 1489/12630 [24:03<2:44:29,  1.13it/s] 12%|        | 1490/12630 [24:03<2:44:21,  1.13it/s] 12%|        | 1491/12630 [24:04<2:44:15,  1.13it/s] 12%|        | 1492/12630 [24:05<2:43:41,  1.13it/s] 12%|        | 1493/12630 [24:06<2:43:31,  1.14it/s] 12%|        | 1494/12630 [24:07<2:43:53,  1.13it/s] 12%|        | 1495/12630 [24:08<2:43:05,  1.14it/s] 12%|        | 1496/12630 [24:09<2:43:55,  1.13it/s] 12%|        | 1497/12630 [24:10<2:44:10,  1.13it/s] 12%|        | 1498/12630 [24:11<2:43:47,  1.13it/s] 12%|        | 1499/12630 [24:11<2:43:19,  1.14it/s] 12%|        | 1500/12630 [24:12<2:43:00,  1.14it/s]   {'loss': 0.3796, 'learning_rate': 4.406175771971497e-05, 'epoch': 0.36}
                                                    12%|        | 1500/12630 [24:12<2:43:00,  1.14it/s][INFO|trainer.py:2985] 2024-02-13 00:34:12,781 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500
[INFO|configuration_utils.py:473] 2024-02-13 00:34:12,803 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:34:38,843 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:34:38,845 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:34:39,363 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/special_tokens_map.json
 12%|        | 1501/12630 [25:29<72:46:57, 23.54s/it] 12%|        | 1502/12630 [25:30<51:45:01, 16.74s/it] 12%|        | 1503/12630 [25:30<37:02:43, 11.99s/it] 12%|        | 1504/12630 [25:31<26:45:11,  8.66s/it] 12%|        | 1505/12630 [25:32<19:33:36,  6.33s/it] 12%|        | 1506/12630 [25:33<14:30:23,  4.69s/it] 12%|        | 1507/12630 [25:34<10:59:10,  3.56s/it] 12%|        | 1508/12630 [25:35<8:29:36,  2.75s/it]  12%|        | 1509/12630 [25:36<6:44:47,  2.18s/it] 12%|        | 1510/12630 [25:37<5:32:17,  1.79s/it] 12%|        | 1511/12630 [25:38<4:41:30,  1.52s/it] 12%|        | 1512/12630 [25:38<4:06:39,  1.33s/it] 12%|        | 1513/12630 [25:39<3:41:56,  1.20s/it] 12%|        | 1514/12630 [25:40<3:25:05,  1.11s/it] 12%|        | 1515/12630 [25:41<3:12:10,  1.04s/it] 12%|        | 1516/12630 [25:42<3:03:57,  1.01it/s] 12%|        | 1517/12630 [25:43<2:58:27,  1.04it/s] 12%|        | 1518/12630 [25:44<2:53:47,  1.07it/s] 12%|        | 1519/12630 [25:45<2:50:59,  1.08it/s] 12%|        | 1520/12630 [25:46<2:47:43,  1.10it/s] 12%|        | 1521/12630 [25:46<2:46:53,  1.11it/s] 12%|        | 1522/12630 [25:47<2:46:44,  1.11it/s] 12%|        | 1523/12630 [25:48<2:45:59,  1.12it/s] 12%|        | 1524/12630 [25:49<2:44:45,  1.12it/s] 12%|        | 1525/12630 [25:50<2:43:45,  1.13it/s] 12%|        | 1526/12630 [25:51<2:43:49,  1.13it/s] 12%|        | 1527/12630 [25:52<2:44:28,  1.13it/s] 12%|        | 1528/12630 [25:53<2:44:01,  1.13it/s] 12%|        | 1529/12630 [25:53<2:43:03,  1.13it/s] 12%|        | 1530/12630 [25:54<2:43:20,  1.13it/s] 12%|        | 1531/12630 [25:55<2:43:10,  1.13it/s] 12%|        | 1532/12630 [25:56<2:42:18,  1.14it/s] 12%|        | 1533/12630 [25:57<2:42:27,  1.14it/s] 12%|        | 1534/12630 [25:58<2:42:03,  1.14it/s] 12%|        | 1535/12630 [25:59<2:42:43,  1.14it/s] 12%|        | 1536/12630 [26:00<2:43:57,  1.13it/s] 12%|        | 1537/12630 [26:01<2:43:52,  1.13it/s] 12%|        | 1538/12630 [26:01<2:43:47,  1.13it/s] 12%|        | 1539/12630 [26:02<2:43:37,  1.13it/s] 12%|        | 1540/12630 [26:03<2:43:17,  1.13it/s] 12%|        | 1541/12630 [26:04<2:42:47,  1.14it/s] 12%|        | 1542/12630 [26:05<2:42:08,  1.14it/s] 12%|        | 1543/12630 [26:06<2:42:24,  1.14it/s] 12%|        | 1544/12630 [26:07<2:43:29,  1.13it/s] 12%|        | 1545/12630 [26:08<2:42:57,  1.13it/s] 12%|        | 1546/12630 [26:08<2:42:03,  1.14it/s] 12%|        | 1547/12630 [26:09<2:43:20,  1.13it/s] 12%|        | 1548/12630 [26:10<2:43:12,  1.13it/s] 12%|        | 1549/12630 [26:11<2:43:30,  1.13it/s] 12%|        | 1550/12630 [26:12<2:43:18,  1.13it/s] 12%|        | 1551/12630 [26:13<2:43:37,  1.13it/s] 12%|        | 1552/12630 [26:14<2:43:20,  1.13it/s] 12%|        | 1553/12630 [26:15<2:42:44,  1.13it/s] 12%|        | 1554/12630 [26:16<2:42:31,  1.14it/s] 12%|        | 1555/12630 [26:16<2:43:29,  1.13it/s] 12%|        | 1556/12630 [26:17<2:42:18,  1.14it/s] 12%|        | 1557/12630 [26:18<2:43:49,  1.13it/s] 12%|        | 1558/12630 [26:19<2:42:50,  1.13it/s] 12%|        | 1559/12630 [26:20<2:42:59,  1.13it/s] 12%|        | 1560/12630 [26:21<2:43:22,  1.13it/s] 12%|        | 1561/12630 [26:22<2:43:30,  1.13it/s] 12%|        | 1562/12630 [26:23<2:43:34,  1.13it/s] 12%|        | 1563/12630 [26:23<2:43:38,  1.13it/s] 12%|        | 1564/12630 [26:24<2:42:48,  1.13it/s] 12%|        | 1565/12630 [26:25<2:42:54,  1.13it/s] 12%|        | 1566/12630 [26:26<2:42:58,  1.13it/s] 12%|        | 1567/12630 [26:27<2:43:36,  1.13it/s] 12%|        | 1568/12630 [26:28<2:42:59,  1.13it/s] 12%|        | 1569/12630 [26:29<2:42:22,  1.14it/s] 12%|        | 1570/12630 [26:30<2:42:23,  1.14it/s] 12%|        | 1571/12630 [26:31<2:42:55,  1.13it/s] 12%|        | 1572/12630 [26:31<2:42:06,  1.14it/s] 12%|        | 1573/12630 [26:32<2:42:33,  1.13it/s] 12%|        | 1574/12630 [26:33<2:41:49,  1.14it/s] 12%|        | 1575/12630 [26:34<2:43:53,  1.12it/s] 12%|        | 1576/12630 [26:35<2:43:30,  1.13it/s] 12%|        | 1577/12630 [26:36<2:43:05,  1.13it/s] 12%|        | 1578/12630 [26:37<2:43:13,  1.13it/s] 13%|        | 1579/12630 [26:38<2:43:16,  1.13it/s] 13%|        | 1580/12630 [26:39<2:43:10,  1.13it/s] 13%|        | 1581/12630 [26:39<2:42:54,  1.13it/s] 13%|        | 1582/12630 [26:40<2:41:50,  1.14it/s] 13%|        | 1583/12630 [26:41<2:42:58,  1.13it/s] 13%|        | 1584/12630 [26:42<2:42:52,  1.13it/s] 13%|        | 1585/12630 [26:43<2:42:40,  1.13it/s] 13%|        | 1586/12630 [26:44<2:42:42,  1.13it/s] 13%|        | 1587/12630 [26:45<2:41:59,  1.14it/s] 13%|        | 1588/12630 [26:46<2:43:11,  1.13it/s] 13%|        | 1589/12630 [26:46<2:42:49,  1.13it/s] 13%|        | 1590/12630 [26:47<2:42:21,  1.13it/s] 13%|        | 1591/12630 [26:48<2:42:39,  1.13it/s] 13%|        | 1592/12630 [26:49<2:42:50,  1.13it/s] 13%|        | 1593/12630 [26:50<2:42:19,  1.13it/s] 13%|        | 1594/12630 [26:51<2:42:04,  1.13it/s] 13%|        | 1595/12630 [26:52<2:42:26,  1.13it/s] 13%|        | 1596/12630 [26:53<2:42:43,  1.13it/s] 13%|        | 1597/12630 [26:54<2:42:44,  1.13it/s] 13%|        | 1598/12630 [26:54<2:42:29,  1.13it/s] 13%|        | 1599/12630 [26:55<2:42:09,  1.13it/s] 13%|        | 1600/12630 [26:56<2:43:01,  1.13it/s] 13%|        | 1601/12630 [26:57<2:42:11,  1.13it/s] 13%|        | 1602/12630 [26:58<2:41:14,  1.14it/s] 13%|        | 1603/12630 [26:59<2:42:30,  1.13it/s] 13%|        | 1604/12630 [27:00<2:42:21,  1.13it/s] 13%|        | 1605/12630 [27:01<2:41:40,  1.14it/s] 13%|        | 1606/12630 [27:01<2:41:24,  1.14it/s] 13%|        | 1607/12630 [27:02<2:41:54,  1.13it/s] 13%|        | 1608/12630 [27:03<2:41:20,  1.14it/s] 13%|        | 1609/12630 [27:04<2:42:10,  1.13it/s] 13%|        | 1610/12630 [27:05<2:41:37,  1.14it/s] 13%|        | 1611/12630 [27:06<2:40:56,  1.14it/s] 13%|        | 1612/12630 [27:07<2:42:10,  1.13it/s] 13%|        | 1613/12630 [27:08<2:41:38,  1.14it/s] 13%|        | 1614/12630 [27:09<2:41:47,  1.13it/s] 13%|        | 1615/12630 [27:09<2:42:49,  1.13it/s] 13%|        | 1616/12630 [27:10<2:42:01,  1.13it/s] 13%|        | 1617/12630 [27:11<2:41:45,  1.13it/s] 13%|        | 1618/12630 [27:12<2:41:28,  1.14it/s] 13%|        | 1619/12630 [27:13<2:42:09,  1.13it/s] 13%|        | 1620/12630 [27:14<2:42:01,  1.13it/s] 13%|        | 1621/12630 [27:15<2:43:03,  1.13it/s] 13%|        | 1622/12630 [27:16<2:41:37,  1.14it/s] 13%|        | 1623/12630 [27:16<2:41:58,  1.13it/s] 13%|        | 1624/12630 [27:17<2:41:28,  1.14it/s] 13%|        | 1625/12630 [27:18<2:41:40,  1.13it/s] 13%|        | 1626/12630 [27:19<2:42:11,  1.13it/s] 13%|        | 1627/12630 [27:20<2:42:02,  1.13it/s] 13%|        | 1628/12630 [27:21<2:43:02,  1.12it/s] 13%|        | 1629/12630 [27:22<2:42:53,  1.13it/s] 13%|        | 1630/12630 [27:23<2:42:52,  1.13it/s] 13%|        | 1631/12630 [27:24<2:42:48,  1.13it/s] 13%|        | 1632/12630 [27:24<2:42:17,  1.13it/s] 13%|        | 1633/12630 [27:25<2:41:53,  1.13it/s] 13%|        | 1634/12630 [27:26<2:42:12,  1.13it/s] 13%|        | 1635/12630 [27:27<2:42:32,  1.13it/s] 13%|        | 1636/12630 [27:28<2:41:44,  1.13it/s] 13%|        | 1637/12630 [27:29<2:40:37,  1.14it/s] 13%|        | 1638/12630 [27:30<2:40:58,  1.14it/s] 13%|        | 1639/12630 [27:31<2:41:35,  1.13it/s] 13%|        | 1640/12630 [27:31<2:41:22,  1.14it/s] 13%|        | 1641/12630 [27:32<2:41:24,  1.13it/s] 13%|        | 1642/12630 [27:33<2:42:04,  1.13it/s] 13%|        | 1643/12630 [27:34<2:40:50,  1.14it/s] 13%|        | 1644/12630 [27:35<2:42:06,  1.13it/s] 13%|        | 1645/12630 [27:36<2:41:41,  1.13it/s] 13%|        | 1646/12630 [27:37<2:41:51,  1.13it/s] 13%|        | 1647/12630 [27:38<2:41:39,  1.13it/s] 13%|        | 1648/12630 [27:39<2:40:41,  1.14it/s] 13%|        | 1649/12630 [27:39<2:41:54,  1.13it/s] 13%|        | 1650/12630 [27:40<2:41:59,  1.13it/s] 13%|        | 1651/12630 [27:41<2:41:32,  1.13it/s] 13%|        | 1652/12630 [27:42<2:42:19,  1.13it/s] 13%|        | 1653/12630 [27:43<2:42:54,  1.12it/s] 13%|        | 1654/12630 [27:44<2:41:53,  1.13it/s] 13%|        | 1655/12630 [27:45<2:41:38,  1.13it/s] 13%|        | 1656/12630 [27:46<2:41:19,  1.13it/s] 13%|        | 1657/12630 [27:47<2:40:36,  1.14it/s] 13%|        | 1658/12630 [27:47<2:40:38,  1.14it/s] 13%|        | 1659/12630 [27:48<2:40:34,  1.14it/s] 13%|        | 1660/12630 [27:49<2:40:13,  1.14it/s] 13%|        | 1661/12630 [27:50<2:40:50,  1.14it/s] 13%|        | 1662/12630 [27:51<2:40:48,  1.14it/s] 13%|        | 1663/12630 [27:52<2:41:08,  1.13it/s] 13%|        | 1664/12630 [27:53<2:40:43,  1.14it/s] 13%|        | 1665/12630 [27:54<2:40:17,  1.14it/s] 13%|        | 1666/12630 [27:54<2:41:10,  1.13it/s] 13%|        | 1667/12630 [27:55<2:41:49,  1.13it/s] 13%|        | 1668/12630 [27:56<2:41:18,  1.13it/s] 13%|        | 1669/12630 [27:57<2:40:37,  1.14it/s] 13%|        | 1670/12630 [27:58<2:41:20,  1.13it/s] 13%|        | 1671/12630 [27:59<2:42:25,  1.12it/s] 13%|        | 1672/12630 [28:00<2:41:14,  1.13it/s] 13%|        | 1673/12630 [28:01<2:41:11,  1.13it/s] 13%|        | 1674/12630 [28:01<2:41:21,  1.13it/s] 13%|        | 1675/12630 [28:02<2:41:47,  1.13it/s] 13%|        | 1676/12630 [28:03<2:40:52,  1.13it/s] 13%|        | 1677/12630 [28:04<2:42:55,  1.12it/s] 13%|        | 1678/12630 [28:05<2:42:51,  1.12it/s] 13%|        | 1679/12630 [28:06<2:42:09,  1.13it/s] 13%|        | 1680/12630 [28:07<2:42:07,  1.13it/s] 13%|        | 1681/12630 [28:08<2:41:43,  1.13it/s] 13%|        | 1682/12630 [28:09<2:41:28,  1.13it/s] 13%|        | 1683/12630 [28:09<2:41:43,  1.13it/s] 13%|        | 1684/12630 [28:10<2:41:06,  1.13it/s] 13%|        | 1685/12630 [28:11<2:41:55,  1.13it/s] 13%|        | 1686/12630 [28:12<2:41:36,  1.13it/s] 13%|        | 1687/12630 [28:13<2:41:12,  1.13it/s] 13%|        | 1688/12630 [28:14<2:40:17,  1.14it/s] 13%|        | 1689/12630 [28:15<2:40:44,  1.13it/s] 13%|        | 1690/12630 [28:16<2:40:53,  1.13it/s] 13%|        | 1691/12630 [28:17<2:41:00,  1.13it/s] 13%|        | 1692/12630 [28:17<2:41:23,  1.13it/s] 13%|        | 1693/12630 [28:18<2:41:43,  1.13it/s] 13%|        | 1694/12630 [28:19<2:41:42,  1.13it/s] 13%|        | 1695/12630 [28:20<2:41:16,  1.13it/s] 13%|        | 1696/12630 [28:21<2:41:25,  1.13it/s] 13%|        | 1697/12630 [28:22<2:41:19,  1.13it/s] 13%|        | 1698/12630 [28:23<2:41:59,  1.12it/s] 13%|        | 1699/12630 [28:24<2:42:18,  1.12it/s] 13%|        | 1700/12630 [28:25<2:41:38,  1.13it/s] 13%|        | 1701/12630 [28:25<2:41:09,  1.13it/s] 13%|        | 1702/12630 [28:26<2:40:35,  1.13it/s] 13%|        | 1703/12630 [28:27<2:41:27,  1.13it/s] 13%|        | 1704/12630 [28:28<2:40:16,  1.14it/s] 13%|        | 1705/12630 [28:29<2:41:44,  1.13it/s] 14%|        | 1706/12630 [28:30<2:40:08,  1.14it/s] 14%|        | 1707/12630 [28:31<2:40:35,  1.13it/s] 14%|        | 1708/12630 [28:32<2:40:49,  1.13it/s] 14%|        | 1709/12630 [28:32<2:39:45,  1.14it/s] 14%|        | 1710/12630 [28:33<2:40:25,  1.13it/s] 14%|        | 1711/12630 [28:34<2:40:26,  1.13it/s] 14%|        | 1712/12630 [28:35<2:40:23,  1.13it/s] 14%|        | 1713/12630 [28:36<2:40:16,  1.14it/s] 14%|        | 1714/12630 [28:37<2:41:30,  1.13it/s] 14%|        | 1715/12630 [28:38<2:41:15,  1.13it/s] 14%|        | 1716/12630 [28:39<2:40:50,  1.13it/s] 14%|        | 1717/12630 [28:40<2:41:14,  1.13it/s] 14%|        | 1718/12630 [28:40<2:40:23,  1.13it/s] 14%|        | 1719/12630 [28:41<2:41:03,  1.13it/s] 14%|        | 1720/12630 [28:42<2:40:19,  1.13it/s] 14%|        | 1721/12630 [28:43<2:40:41,  1.13it/s] 14%|        | 1722/12630 [28:44<2:41:37,  1.12it/s] 14%|        | 1723/12630 [28:45<2:40:42,  1.13it/s] 14%|        | 1724/12630 [28:46<2:40:42,  1.13it/s] 14%|        | 1725/12630 [28:47<2:40:28,  1.13it/s] 14%|        | 1726/12630 [28:48<2:40:23,  1.13it/s] 14%|        | 1727/12630 [28:48<2:39:16,  1.14it/s] 14%|        | 1728/12630 [28:49<2:39:56,  1.14it/s] 14%|        | 1729/12630 [28:50<2:40:29,  1.13it/s] 14%|        | 1730/12630 [28:51<2:40:38,  1.13it/s] 14%|        | 1731/12630 [28:52<2:40:59,  1.13it/s] 14%|        | 1732/12630 [28:53<2:41:21,  1.13it/s] 14%|        | 1733/12630 [28:54<2:40:45,  1.13it/s] 14%|        | 1734/12630 [28:55<2:40:44,  1.13it/s] 14%|        | 1735/12630 [28:55<2:41:46,  1.12it/s] 14%|        | 1736/12630 [28:56<2:40:13,  1.13it/s] 14%|        | 1737/12630 [28:57<2:39:35,  1.14it/s] 14%|        | 1738/12630 [28:58<2:40:37,  1.13it/s] 14%|        | 1739/12630 [28:59<2:40:09,  1.13it/s] 14%|        | 1740/12630 [29:00<2:39:45,  1.14it/s] 14%|        | 1741/12630 [29:01<2:40:29,  1.13it/s] 14%|        | 1742/12630 [29:02<2:41:14,  1.13it/s] 14%|        | 1743/12630 [29:03<2:41:00,  1.13it/s] 14%|        | 1744/12630 [29:03<2:42:25,  1.12it/s] 14%|        | 1745/12630 [29:04<2:41:26,  1.12it/s] 14%|        | 1746/12630 [29:05<2:41:08,  1.13it/s] 14%|        | 1747/12630 [29:06<2:40:38,  1.13it/s] 14%|        | 1748/12630 [29:07<2:39:25,  1.14it/s] 14%|        | 1749/12630 [29:08<2:41:00,  1.13it/s] 14%|        | 1750/12630 [29:09<2:41:24,  1.12it/s] 14%|        | 1751/12630 [29:10<2:40:49,  1.13it/s] 14%|        | 1752/12630 [29:11<2:40:52,  1.13it/s] 14%|        | 1753/12630 [29:11<2:39:59,  1.13it/s] 14%|        | 1754/12630 [29:12<2:39:07,  1.14it/s] 14%|        | 1755/12630 [29:13<2:40:23,  1.13it/s] 14%|        | 1756/12630 [29:14<2:39:46,  1.13it/s] 14%|        | 1757/12630 [29:15<2:40:42,  1.13it/s] 14%|        | 1758/12630 [29:16<2:40:12,  1.13it/s] 14%|        | 1759/12630 [29:17<2:40:33,  1.13it/s] 14%|        | 1760/12630 [29:18<2:40:11,  1.13it/s] 14%|        | 1761/12630 [29:18<2:39:59,  1.13it/s] 14%|        | 1762/12630 [29:19<2:39:03,  1.14it/s] 14%|        | 1763/12630 [29:20<2:41:21,  1.12it/s] 14%|        | 1764/12630 [29:21<2:40:50,  1.13it/s] 14%|        | 1765/12630 [29:22<2:40:19,  1.13it/s] 14%|        | 1766/12630 [29:23<2:40:43,  1.13it/s] 14%|        | 1767/12630 [29:24<2:40:51,  1.13it/s] 14%|        | 1768/12630 [29:25<2:40:11,  1.13it/s] 14%|        | 1769/12630 [29:26<2:39:55,  1.13it/s] 14%|        | 1770/12630 [29:26<2:40:02,  1.13it/s] 14%|        | 1771/12630 [29:27<2:40:21,  1.13it/s] 14%|        | 1772/12630 [29:28<2:40:19,  1.13it/s] 14%|        | 1773/12630 [29:29<2:40:17,  1.13it/s] 14%|        | 1774/12630 [29:30<2:40:04,  1.13it/s] 14%|        | 1775/12630 [29:31<2:39:48,  1.13it/s] 14%|        | 1776/12630 [29:32<2:40:03,  1.13it/s] 14%|        | 1777/12630 [29:33<2:40:10,  1.13it/s] 14%|        | 1778/12630 [29:34<2:39:27,  1.13it/s] 14%|        | 1779/12630 [29:34<2:40:07,  1.13it/s] 14%|        | 1780/12630 [29:35<2:39:37,  1.13it/s] 14%|        | 1781/12630 [29:36<2:40:57,  1.12it/s] 14%|        | 1782/12630 [29:37<2:41:06,  1.12it/s] 14%|        | 1783/12630 [29:38<2:40:49,  1.12it/s] 14%|        | 1784/12630 [29:39<2:40:32,  1.13it/s] 14%|        | 1785/12630 [29:40<2:39:45,  1.13it/s] 14%|        | 1786/12630 [29:41<2:39:48,  1.13it/s] 14%|        | 1787/12630 [29:42<2:40:22,  1.13it/s] 14%|        | 1788/12630 [29:42<2:40:33,  1.13it/s] 14%|        | 1789/12630 [29:43<2:40:36,  1.12it/s] 14%|        | 1790/12630 [29:44<2:39:52,  1.13it/s] 14%|        | 1791/12630 [29:45<2:40:12,  1.13it/s] 14%|        | 1792/12630 [29:46<2:40:42,  1.12it/s] 14%|        | 1793/12630 [29:47<2:40:13,  1.13it/s] 14%|        | 1794/12630 [29:48<2:39:33,  1.13it/s] 14%|        | 1795/12630 [29:49<2:39:05,  1.14it/s] 14%|        | 1796/12630 [29:49<2:39:31,  1.13it/s] 14%|        | 1797/12630 [29:50<2:40:47,  1.12it/s] 14%|        | 1798/12630 [29:51<2:39:50,  1.13it/s] 14%|        | 1799/12630 [29:52<2:39:15,  1.13it/s] 14%|        | 1800/12630 [29:53<2:39:58,  1.13it/s] 14%|        | 1801/12630 [29:54<2:39:11,  1.13it/s] 14%|        | 1802/12630 [29:55<2:39:18,  1.13it/s] 14%|        | 1803/12630 [29:56<2:40:07,  1.13it/s] 14%|        | 1804/12630 [29:57<2:39:20,  1.13it/s] 14%|        | 1805/12630 [29:57<2:39:09,  1.13it/s] 14%|        | 1806/12630 [29:58<2:40:28,  1.12it/s] 14%|        | 1807/12630 [29:59<2:39:45,  1.13it/s] 14%|        | 1808/12630 [30:00<2:40:28,  1.12it/s] 14%|        | 1809/12630 [30:01<2:40:40,  1.12it/s] 14%|        | 1810/12630 [30:02<2:39:30,  1.13it/s] 14%|        | 1811/12630 [30:03<2:39:43,  1.13it/s] 14%|        | 1812/12630 [30:04<2:39:24,  1.13it/s] 14%|        | 1813/12630 [30:05<2:39:32,  1.13it/s] 14%|        | 1814/12630 [30:05<2:38:53,  1.13it/s] 14%|        | 1815/12630 [30:06<2:39:19,  1.13it/s] 14%|        | 1816/12630 [30:07<2:39:09,  1.13it/s] 14%|        | 1817/12630 [30:08<2:40:00,  1.13it/s] 14%|        | 1818/12630 [30:09<2:39:50,  1.13it/s] 14%|        | 1819/12630 [30:10<2:39:28,  1.13it/s] 14%|        | 1820/12630 [30:11<2:38:46,  1.13it/s] 14%|        | 1821/12630 [30:12<2:39:21,  1.13it/s] 14%|        | 1822/12630 [30:12<2:38:51,  1.13it/s] 14%|        | 1823/12630 [30:13<2:39:10,  1.13it/s] 14%|        | 1824/12630 [30:14<2:38:52,  1.13it/s] 14%|        | 1825/12630 [30:15<2:39:41,  1.13it/s] 14%|        | 1826/12630 [30:16<2:39:56,  1.13it/s] 14%|        | 1827/12630 [30:17<2:41:08,  1.12it/s] 14%|        | 1828/12630 [30:18<2:40:02,  1.12it/s] 14%|        | 1829/12630 [30:19<2:39:57,  1.13it/s] 14%|        | 1830/12630 [30:20<2:38:43,  1.13it/s] 14%|        | 1831/12630 [30:20<2:39:02,  1.13it/s] 15%|        | 1832/12630 [30:21<2:39:57,  1.13it/s] 15%|        | 1833/12630 [30:22<2:39:05,  1.13it/s] 15%|        | 1834/12630 [30:23<2:38:49,  1.13it/s] 15%|        | 1835/12630 [30:24<2:40:22,  1.12it/s] 15%|        | 1836/12630 [30:25<2:40:25,  1.12it/s] 15%|        | 1837/12630 [30:26<2:40:26,  1.12it/s] 15%|        | 1838/12630 [30:27<2:39:11,  1.13it/s] 15%|        | 1839/12630 [30:28<2:38:41,  1.13it/s] 15%|        | 1840/12630 [30:28<2:40:23,  1.12it/s] 15%|        | 1841/12630 [30:29<2:39:23,  1.13it/s] 15%|        | 1842/12630 [30:30<2:39:08,  1.13it/s] 15%|        | 1843/12630 [30:31<2:39:05,  1.13it/s] 15%|        | 1844/12630 [30:32<2:39:13,  1.13it/s] 15%|        | 1845/12630 [30:33<2:39:46,  1.13it/s] 15%|        | 1846/12630 [30:34<2:39:00,  1.13it/s] 15%|        | 1847/12630 [30:35<2:39:01,  1.13it/s] 15%|        | 1848/12630 [30:36<2:38:45,  1.13it/s] 15%|        | 1849/12630 [30:36<2:40:11,  1.12it/s] 15%|        | 1850/12630 [30:37<2:38:55,  1.13it/s] 15%|        | 1851/12630 [30:38<2:38:52,  1.13it/s] 15%|        | 1852/12630 [30:39<2:38:08,  1.14it/s] 15%|        | 1853/12630 [30:40<2:39:07,  1.13it/s] 15%|        | 1854/12630 [30:41<2:38:39,  1.13it/s] 15%|        | 1855/12630 [30:42<2:38:39,  1.13it/s] 15%|        | 1856/12630 [30:43<2:39:10,  1.13it/s] 15%|        | 1857/12630 [30:44<2:38:57,  1.13it/s] 15%|        | 1858/12630 [30:44<2:38:56,  1.13it/s] 15%|        | 1859/12630 [30:45<2:38:44,  1.13it/s] 15%|        | 1860/12630 [30:46<2:38:54,  1.13it/s] 15%|        | 1861/12630 [30:47<2:39:47,  1.12it/s] 15%|        | 1862/12630 [30:48<2:39:36,  1.12it/s] 15%|        | 1863/12630 [30:49<2:39:31,  1.12it/s] 15%|        | 1864/12630 [30:50<2:39:30,  1.12it/s] 15%|        | 1865/12630 [30:51<2:38:09,  1.13it/s] 15%|        | 1866/12630 [30:52<2:39:33,  1.12it/s] 15%|        | 1867/12630 [30:52<2:39:47,  1.12it/s] 15%|        | 1868/12630 [30:53<2:39:37,  1.12it/s] 15%|        | 1869/12630 [30:54<2:38:33,  1.13it/s] 15%|        | 1870/12630 [30:55<2:39:03,  1.13it/s] 15%|        | 1871/12630 [30:56<2:38:25,  1.13it/s] 15%|        | 1872/12630 [30:57<2:38:40,  1.13it/s] 15%|        | 1873/12630 [30:58<2:38:56,  1.13it/s] 15%|        | 1874/12630 [30:59<2:38:09,  1.13it/s] 15%|        | 1875/12630 [30:59<2:38:29,  1.13it/s] 15%|        | 1876/12630 [31:00<2:38:14,  1.13it/s] 15%|        | 1877/12630 [31:01<2:38:08,  1.13it/s] 15%|        | 1878/12630 [31:02<2:38:09,  1.13it/s] 15%|        | 1879/12630 [31:03<2:37:29,  1.14it/s] 15%|        | 1880/12630 [31:04<2:38:02,  1.13it/s] 15%|        | 1881/12630 [31:05<2:38:06,  1.13it/s] 15%|        | 1882/12630 [31:06<2:37:38,  1.14it/s] 15%|        | 1883/12630 [31:07<2:39:23,  1.12it/s] 15%|        | 1884/12630 [31:07<2:38:49,  1.13it/s] 15%|        | 1885/12630 [31:08<2:39:10,  1.13it/s] 15%|        | 1886/12630 [31:09<2:37:32,  1.14it/s] 15%|        | 1887/12630 [31:10<2:38:43,  1.13it/s] 15%|        | 1888/12630 [31:11<2:38:00,  1.13it/s] 15%|        | 1889/12630 [31:12<2:37:29,  1.14it/s] 15%|        | 1890/12630 [31:13<2:38:59,  1.13it/s] 15%|        | 1891/12630 [31:14<2:38:02,  1.13it/s] 15%|        | 1892/12630 [31:14<2:38:18,  1.13it/s] 15%|        | 1893/12630 [31:15<2:38:30,  1.13it/s] 15%|        | 1894/12630 [31:16<2:38:05,  1.13it/s] 15%|        | 1895/12630 [31:17<2:37:47,  1.13it/s] 15%|        | 1896/12630 [31:18<2:37:57,  1.13it/s] 15%|        | 1897/12630 [31:19<2:38:56,  1.13it/s] 15%|        | 1898/12630 [31:20<2:39:07,  1.12it/s] 15%|        | 1899/12630 [31:21<2:39:17,  1.12it/s] 15%|        | 1900/12630 [31:22<2:38:04,  1.13it/s] 15%|        | 1901/12630 [31:22<2:37:32,  1.14it/s] 15%|        | 1902/12630 [31:23<2:37:55,  1.13it/s] 15%|        | 1903/12630 [31:24<2:39:49,  1.12it/s] 15%|        | 1904/12630 [31:25<2:40:07,  1.12it/s] 15%|        | 1905/12630 [31:26<2:38:06,  1.13it/s] 15%|        | 1906/12630 [31:27<2:38:13,  1.13it/s] 15%|        | 1907/12630 [31:28<2:38:27,  1.13it/s] 15%|        | 1908/12630 [31:29<2:37:48,  1.13it/s] 15%|        | 1909/12630 [31:30<2:37:56,  1.13it/s] 15%|        | 1910/12630 [31:30<2:37:34,  1.13it/s] 15%|        | 1911/12630 [31:31<2:37:15,  1.14it/s] 15%|        | 1912/12630 [31:32<2:36:56,  1.14it/s] 15%|        | 1913/12630 [31:33<2:37:52,  1.13it/s] 15%|        | 1914/12630 [31:34<2:38:03,  1.13it/s] 15%|        | 1915/12630 [31:35<2:38:06,  1.13it/s] 15%|        | 1916/12630 [31:36<2:39:30,  1.12it/s] 15%|        | 1917/12630 [31:37<2:38:31,  1.13it/s] 15%|        | 1918/12630 [31:38<2:37:49,  1.13it/s] 15%|        | 1919/12630 [31:38<2:38:26,  1.13it/s] 15%|        | 1920/12630 [31:39<2:38:25,  1.13it/s] 15%|        | 1921/12630 [31:40<2:38:22,  1.13it/s] 15%|        | 1922/12630 [31:41<2:38:29,  1.13it/s] 15%|        | 1923/12630 [31:42<2:37:47,  1.13it/s] 15%|        | 1924/12630 [31:43<2:38:39,  1.12it/s] 15%|        | 1925/12630 [31:44<2:38:48,  1.12it/s] 15%|        | 1926/12630 [31:45<2:38:09,  1.13it/s] 15%|        | 1927/12630 [31:45<2:37:07,  1.14it/s] 15%|        | 1928/12630 [31:46<2:37:27,  1.13it/s] 15%|        | 1929/12630 [31:47<2:38:36,  1.12it/s] 15%|        | 1930/12630 [31:48<2:38:25,  1.13it/s] 15%|        | 1931/12630 [31:49<2:38:52,  1.12it/s] 15%|        | 1932/12630 [31:50<2:38:04,  1.13it/s] 15%|        | 1933/12630 [31:51<2:38:14,  1.13it/s] 15%|        | 1934/12630 [31:52<2:39:03,  1.12it/s] 15%|        | 1935/12630 [31:53<2:38:11,  1.13it/s] 15%|        | 1936/12630 [31:54<2:39:04,  1.12it/s] 15%|        | 1937/12630 [31:54<2:38:09,  1.13it/s] 15%|        | 1938/12630 [31:55<2:39:22,  1.12it/s] 15%|        | 1939/12630 [31:56<2:38:51,  1.12it/s] 15%|        | 1940/12630 [31:57<2:37:28,  1.13it/s] 15%|        | 1941/12630 [31:58<2:37:42,  1.13it/s] 15%|        | 1942/12630 [31:59<2:37:23,  1.13it/s] 15%|        | 1943/12630 [32:00<2:37:00,  1.13it/s] 15%|        | 1944/12630 [32:01<2:37:11,  1.13it/s] 15%|        | 1945/12630 [32:01<2:37:33,  1.13it/s] 15%|        | 1946/12630 [32:02<2:36:30,  1.14it/s] 15%|        | 1947/12630 [32:03<2:37:35,  1.13it/s] 15%|        | 1948/12630 [32:04<2:37:34,  1.13it/s] 15%|        | 1949/12630 [32:05<2:37:22,  1.13it/s] 15%|        | 1950/12630 [32:06<2:37:29,  1.13it/s] 15%|        | 1951/12630 [32:07<2:36:41,  1.14it/s] 15%|        | 1952/12630 [32:08<2:36:50,  1.13it/s] 15%|        | 1953/12630 [32:09<2:36:25,  1.14it/s] 15%|        | 1954/12630 [32:09<2:37:38,  1.13it/s] 15%|        | 1955/12630 [32:10<2:37:44,  1.13it/s] 15%|        | 1956/12630 [32:11<2:36:40,  1.14it/s] 15%|        | 1957/12630 [32:12<2:37:12,  1.13it/s] 16%|        | 1958/12630 [32:13<2:37:35,  1.13it/s] 16%|        | 1959/12630 [32:14<2:37:18,  1.13it/s] 16%|        | 1960/12630 [32:15<2:38:10,  1.12it/s] 16%|        | 1961/12630 [32:16<2:37:23,  1.13it/s] 16%|        | 1962/12630 [32:17<2:38:22,  1.12it/s] 16%|        | 1963/12630 [32:17<2:38:03,  1.12it/s] 16%|        | 1964/12630 [32:18<2:37:20,  1.13it/s] 16%|        | 1965/12630 [32:19<2:38:09,  1.12it/s] 16%|        | 1966/12630 [32:20<2:38:20,  1.12it/s] 16%|        | 1967/12630 [32:21<2:37:57,  1.13it/s] 16%|        | 1968/12630 [32:22<2:38:00,  1.12it/s] 16%|        | 1969/12630 [32:23<2:37:47,  1.13it/s] 16%|        | 1970/12630 [32:24<2:37:29,  1.13it/s] 16%|        | 1971/12630 [32:24<2:37:06,  1.13it/s] 16%|        | 1972/12630 [32:25<2:36:32,  1.13it/s] 16%|        | 1973/12630 [32:26<2:37:38,  1.13it/s] 16%|        | 1974/12630 [32:27<2:37:15,  1.13it/s] 16%|        | 1975/12630 [32:28<2:36:45,  1.13it/s] 16%|        | 1976/12630 [32:29<2:38:46,  1.12it/s] 16%|        | 1977/12630 [32:30<2:38:46,  1.12it/s] 16%|        | 1978/12630 [32:31<2:37:43,  1.13it/s] 16%|        | 1979/12630 [32:32<2:37:59,  1.12it/s] 16%|        | 1980/12630 [32:32<2:37:28,  1.13it/s] 16%|        | 1981/12630 [32:33<2:36:49,  1.13it/s] 16%|        | 1982/12630 [32:34<2:36:01,  1.14it/s] 16%|        | 1983/12630 [32:35<2:37:33,  1.13it/s] 16%|        | 1984/12630 [32:36<2:36:29,  1.13it/s] 16%|        | 1985/12630 [32:37<2:37:40,  1.13it/s] 16%|        | 1986/12630 [32:38<2:38:00, {'loss': 0.3408, 'learning_rate': 4.208234362628662e-05, 'epoch': 0.48}
 1.12it/s] 16%|        | 1987/12630 [32:39<2:37:27,  1.13it/s] 16%|        | 1988/12630 [32:40<2:36:47,  1.13it/s] 16%|        | 1989/12630 [32:40<2:36:44,  1.13it/s] 16%|        | 1990/12630 [32:41<2:37:10,  1.13it/s] 16%|        | 1991/12630 [32:42<2:37:10,  1.13it/s] 16%|        | 1992/12630 [32:43<2:37:20,  1.13it/s] 16%|        | 1993/12630 [32:44<2:36:44,  1.13it/s] 16%|        | 1994/12630 [32:45<2:35:41,  1.14it/s] 16%|        | 1995/12630 [32:46<2:37:02,  1.13it/s] 16%|        | 1996/12630 [32:47<2:36:24,  1.13it/s] 16%|        | 1997/12630 [32:48<2:36:41,  1.13it/s] 16%|        | 1998/12630 [32:48<2:37:30,  1.13it/s] 16%|        | 1999/12630 [32:49<2:36:51,  1.13it/s] 16%|        | 2000/12630 [32:50<2:36:33,  1.13it/s]                                                       16%|        | 2000/12630 [32:50<2:36:33,  1.13it/s][INFO|trainer.py:2985] 2024-02-13 00:42:50,655 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2000
[INFO|configuration_utils.py:473] 2024-02-13 00:42:50,671 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2000/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:43:14,290 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:43:14,292 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:43:14,293 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2000/special_tokens_map.json
 16%|        | 2001/12630 [33:55<58:49:58, 19.93s/it] 16%|        | 2002/12630 [33:55<41:58:05, 14.22s/it] 16%|        | 2003/12630 [33:56<30:09:04, 10.21s/it] 16%|        | 2004/12630 [33:57<21:52:45,  7.41s/it] 16%|        | 2005/12630 [33:58<16:06:11,  5.46s/it] 16%|        | 2006/12630 [33:59<12:03:15,  4.08s/it] 16%|        | 2007/12630 [34:00<9:11:55,  3.12s/it]  16%|        | 2008/12630 [34:01<7:14:25,  2.45s/it] 16%|        | 2009/12630 [34:02<5:50:10,  1.98s/it] 16%|        | 2010/12630 [34:02<4:52:11,  1.65s/it] 16%|        | 2011/12630 [34:03<4:11:20,  1.42s/it] 16%|        | 2012/12630 [34:04<3:42:16,  1.26s/it] 16%|        | 2013/12630 [34:05<3:22:06,  1.14s/it] 16%|        | 2014/12630 [34:06<3:08:32,  1.07s/it] 16%|        | 2015/12630 [34:07<2:59:05,  1.01s/it] 16%|        | 2016/12630 [34:08<2:52:28,  1.03it/s] 16%|        | 2017/12630 [34:09<2:48:11,  1.05it/s] 16%|        | 2018/12630 [34:10<2:43:23,  1.08it/s] 16%|        | 2019/12630 [34:10<2:41:24,  1.10it/s] 16%|        | 2020/12630 [34:11<2:39:55,  1.11it/s] 16%|        | 2021/12630 [34:12<2:37:46,  1.12it/s] 16%|        | 2022/12630 [34:13<2:37:43,  1.12it/s] 16%|        | 2023/12630 [34:14<2:37:07,  1.13it/s] 16%|        | 2024/12630 [34:15<2:36:22,  1.13it/s] 16%|        | 2025/12630 [34:16<2:36:12,  1.13it/s] 16%|        | 2026/12630 [34:17<2:36:36,  1.13it/s] 16%|        | 2027/12630 [34:17<2:36:14,  1.13it/s] 16%|        | 2028/12630 [34:18<2:35:42,  1.13it/s] 16%|        | 2029/12630 [34:19<2:35:43,  1.13it/s] 16%|        | 2030/12630 [34:20<2:35:53,  1.13it/s] 16%|        | 2031/12630 [34:21<2:35:52,  1.13it/s] 16%|        | 2032/12630 [34:22<2:37:09,  1.12it/s] 16%|        | 2033/12630 [34:23<2:36:46,  1.13it/s] 16%|        | 2034/12630 [34:24<2:36:23,  1.13it/s] 16%|        | 2035/12630 [34:25<2:36:25,  1.13it/s] 16%|        | 2036/12630 [34:25<2:36:34,  1.13it/s] 16%|        | 2037/12630 [34:26<2:36:32,  1.13it/s] 16%|        | 2038/12630 [34:27<2:36:37,  1.13it/s] 16%|        | 2039/12630 [34:28<2:35:29,  1.14it/s] 16%|        | 2040/12630 [34:29<2:35:57,  1.13it/s] 16%|        | 2041/12630 [34:30<2:35:24,  1.14it/s] 16%|        | 2042/12630 [34:31<2:35:58,  1.13it/s] 16%|        | 2043/12630 [34:32<2:35:59,  1.13it/s] 16%|        | 2044/12630 [34:33<2:36:25,  1.13it/s] 16%|        | 2045/12630 [34:33<2:36:05,  1.13it/s] 16%|        | 2046/12630 [34:34<2:36:57,  1.12it/s] 16%|        | 2047/12630 [34:35<2:36:22,  1.13it/s] 16%|        | 2048/12630 [34:36<2:36:40,  1.13it/s] 16%|        | 2049/12630 [34:37<2:36:45,  1.12it/s] 16%|        | 2050/12630 [34:38<2:36:48,  1.12it/s] 16%|        | 2051/12630 [34:39<2:36:59,  1.12it/s] 16%|        | 2052/12630 [34:40<2:36:37,  1.13it/s] 16%|        | 2053/12630 [34:41<2:37:05,  1.12it/s] 16%|        | 2054/12630 [34:41<2:35:59,  1.13it/s] 16%|        | 2055/12630 [34:42<2:36:03,  1.13it/s] 16%|        | 2056/12630 [34:43<2:36:59,  1.12it/s] 16%|        | 2057/12630 [34:44<2:36:07,  1.13it/s] 16%|        | 2058/12630 [34:45<2:36:43,  1.12it/s] 16%|        | 2059/12630 [34:46<2:35:18,  1.13it/s] 16%|        | 2060/12630 [34:47<2:35:54,  1.13it/s] 16%|        | 2061/12630 [34:48<2:35:41,  1.13it/s] 16%|        | 2062/12630 [34:48<2:36:15,  1.13it/s] 16%|        | 2063/12630 [34:49<2:35:30,  1.13it/s] 16%|        | 2064/12630 [34:50<2:35:24,  1.13it/s] 16%|        | 2065/12630 [34:51<2:35:31,  1.13it/s] 16%|        | 2066/12630 [34:52<2:36:02,  1.13it/s] 16%|        | 2067/12630 [34:53<2:35:30,  1.13it/s] 16%|        | 2068/12630 [34:54<2:35:51,  1.13it/s] 16%|        | 2069/12630 [34:55<2:35:17,  1.13it/s] 16%|        | 2070/12630 [34:56<2:35:48,  1.13it/s] 16%|        | 2071/12630 [34:56<2:34:34,  1.14it/s] 16%|        | 2072/12630 [34:57<2:35:02,  1.13it/s] 16%|        | 2073/12630 [34:58<2:34:30,  1.14it/s] 16%|        | 2074/12630 [34:59<2:35:22,  1.13it/s] 16%|        | 2075/12630 [35:00<2:36:00,  1.13it/s] 16%|        | 2076/12630 [35:01<2:36:04,  1.13it/s] 16%|        | 2077/12630 [35:02<2:35:43,  1.13it/s] 16%|        | 2078/12630 [35:03<2:35:17,  1.13it/s] 16%|        | 2079/12630 [35:03<2:35:14,  1.13it/s] 16%|        | 2080/12630 [35:04<2:36:13,  1.13it/s] 16%|        | 2081/12630 [35:05<2:36:07,  1.13it/s] 16%|        | 2082/12630 [35:06<2:36:08,  1.13it/s] 16%|        | 2083/12630 [35:07<2:36:22,  1.12it/s] 17%|        | 2084/12630 [35:08<2:36:05,  1.13it/s] 17%|        | 2085/12630 [35:09<2:35:49,  1.13it/s] 17%|        | 2086/12630 [35:10<2:36:55,  1.12it/s] 17%|        | 2087/12630 [35:11<2:35:04,  1.13it/s] 17%|        | 2088/12630 [35:11<2:36:14,  1.12it/s] 17%|        | 2089/12630 [35:12<2:35:34,  1.13it/s] 17%|        | 2090/12630 [35:13<2:35:57,  1.13it/s] 17%|        | 2091/12630 [35:14<2:34:47,  1.13it/s] 17%|        | 2092/12630 [35:15<2:34:29,  1.14it/s] 17%|        | 2093/12630 [35:16<2:34:54,  1.13it/s] 17%|        | 2094/12630 [35:17<2:35:09,  1.13it/s] 17%|        | 2095/12630 [35:18<2:35:29,  1.13it/s] 17%|        | 2096/12630 [35:19<2:35:04,  1.13it/s] 17%|        | 2097/12630 [35:19<2:34:34,  1.14it/s] 17%|        | 2098/12630 [35:20<2:34:54,  1.13it/s] 17%|        | 2099/12630 [35:21<2:34:51,  1.13it/s] 17%|        | 2100/12630 [35:22<2:34:59,  1.13it/s] 17%|        | 2101/12630 [35:23<2:35:36,  1.13it/s] 17%|        | 2102/12630 [35:24<2:35:16,  1.13it/s] 17%|        | 2103/12630 [35:25<2:35:02,  1.13it/s] 17%|        | 2104/12630 [35:26<2:35:10,  1.13it/s] 17%|        | 2105/12630 [35:27<2:35:07,  1.13it/s] 17%|        | 2106/12630 [35:27<2:34:58,  1.13it/s] 17%|        | 2107/12630 [35:28<2:37:27,  1.11it/s] 17%|        | 2108/12630 [35:29<2:36:24,  1.12it/s] 17%|        | 2109/12630 [35:30<2:35:34,  1.13it/s] 17%|        | 2110/12630 [35:31<2:35:11,  1.13it/s] 17%|        | 2111/12630 [35:32<2:34:51,  1.13it/s] 17%|        | 2112/12630 [35:33<2:35:03,  1.13it/s] 17%|        | 2113/12630 [35:34<2:34:52,  1.13it/s] 17%|        | 2114/12630 [35:34<2:34:50,  1.13it/s] 17%|        | 2115/12630 [35:35<2:35:19,  1.13it/s] 17%|        | 2116/12630 [35:36<2:35:28,  1.13it/s] 17%|        | 2117/12630 [35:37<2:35:49,  1.12it/s] 17%|        | 2118/12630 [35:38<2:35:12,  1.13it/s] 17%|        | 2119/12630 [35:39<2:35:08,  1.13it/s] 17%|        | 2120/12630 [35:40<2:34:34,  1.13it/s] 17%|        | 2121/12630 [35:41<2:34:31,  1.13it/s] 17%|        | 2122/12630 [35:42<2:34:09,  1.14it/s] 17%|        | 2123/12630 [35:42<2:34:31,  1.13it/s] 17%|        | 2124/12630 [35:43<2:35:27,  1.13it/s] 17%|        | 2125/12630 [35:44<2:35:29,  1.13it/s] 17%|        | 2126/12630 [35:45<2:36:22,  1.12it/s] 17%|        | 2127/12630 [35:46<2:35:20,  1.13it/s] 17%|        | 2128/12630 [35:47<2:36:02,  1.12it/s] 17%|        | 2129/12630 [35:48<2:35:45,  1.12it/s] 17%|        | 2130/12630 [35:49<2:35:45,  1.12it/s] 17%|        | 2131/12630 [35:50<2:35:06,  1.13it/s] 17%|        | 2132/12630 [35:50<2:36:21,  1.12it/s] 17%|        | 2133/12630 [35:51<2:35:14,  1.13it/s] 17%|        | 2134/12630 [35:52<2:35:34,  1.12it/s] 17%|        | 2135/12630 [35:53<2:35:29,  1.12it/s] 17%|        | 2136/12630 [35:54<2:34:46,  1.13it/s] 17%|        | 2137/12630 [35:55<2:35:06,  1.13it/s] 17%|        | 2138/12630 [35:56<2:34:26,  1.13it/s] 17%|        | 2139/12630 [35:57<2:34:53,  1.13it/s] 17%|        | 2140/12630 [35:58<2:35:29,  1.12it/s] 17%|        | 2141/12630 [35:58<2:34:42,  1.13it/s] 17%|        | 2142/12630 [35:59<2:35:07,  1.13it/s] 17%|        | 2143/12630 [36:00<2:35:07,  1.13it/s] 17%|        | 2144/12630 [36:01<2:33:40,  1.14it/s] 17%|        | 2145/12630 [36:02<2:34:15,  1.13it/s] 17%|        | 2146/12630 [36:03<2:33:48,  1.14it/s] 17%|        | 2147/12630 [36:04<2:34:30,  1.13it/s] 17%|        | 2148/12630 [36:05<2:33:47,  1.14it/s] 17%|        | 2149/12630 [36:05<2:34:07,  1.13it/s] 17%|        | 2150/12630 [36:06<2:34:12,  1.13it/s] 17%|        | 2151/12630 [36:07<2:34:02,  1.13it/s] 17%|        | 2152/12630 [36:08<2:34:17,  1.13it/s] 17%|        | 2153/12630 [36:09<2:33:24,  1.14it/s] 17%|        | 2154/12630 [36:10<2:33:53,  1.13it/s] 17%|        | 2155/12630 [36:11<2:34:22,  1.13it/s] 17%|        | 2156/12630 [36:12<2:34:50,  1.13it/s] 17%|        | 2157/12630 [36:13<2:33:57,  1.13it/s] 17%|        | 2158/12630 [36:13<2:34:15,  1.13it/s] 17%|        | 2159/12630 [36:14<2:33:36,  1.14it/s] 17%|        | 2160/12630 [36:15<2:34:08,  1.13it/s] 17%|        | 2161/12630 [36:16<2:33:56,  1.13it/s] 17%|        | 2162/12630 [36:17<2:33:54,  1.13it/s] 17%|        | 2163/12630 [36:18<2:33:50,  1.13it/s] 17%|        | 2164/12630 [36:19<2:33:53,  1.13it/s] 17%|        | 2165/12630 [36:20<2:33:21,  1.14it/s] 17%|        | 2166/12630 [36:21<2:34:27,  1.13it/s] 17%|        | 2167/12630 [36:21<2:33:55,  1.13it/s] 17%|        | 2168/12630 [36:22<2:36:05,  1.12it/s] 17%|        | 2169/12630 [36:23<2:34:57,  1.13it/s] 17%|        | 2170/12630 [36:24<2:34:19,  1.13it/s] 17%|        | 2171/12630 [36:25<2:34:16,  1.13it/s] 17%|        | 2172/12630 [36:26<2:34:20,  1.13it/s] 17%|        | 2173/12630 [36:27<2:34:11,  1.13it/s] 17%|        | 2174/12630 [36:28<2:33:26,  1.14it/s] 17%|        | 2175/12630 [36:28<2:33:51,  1.13it/s] 17%|        | 2176/12630 [36:29<2:34:13,  1.13it/s] 17%|        | 2177/12630 [36:30<2:34:06,  1.13it/s] 17%|        | 2178/12630 [36:31<2:33:44,  1.13it/s] 17%|        | 2179/12630 [36:32<2:34:47,  1.13it/s] 17%|        | 2180/12630 [36:33<2:34:23,  1.13it/s] 17%|        | 2181/12630 [36:34<2:34:11,  1.13it/s] 17%|        | 2182/12630 [36:35<2:33:58,  1.13it/s] 17%|        | 2183/12630 [36:36<2:34:25,  1.13it/s] 17%|        | 2184/12630 [36:36<2:34:01,  1.13it/s] 17%|        | 2185/12630 [36:37<2:33:57,  1.13it/s] 17%|        | 2186/12630 [36:38<2:33:36,  1.13it/s] 17%|        | 2187/12630 [36:39<2:35:08,  1.12it/s] 17%|        | 2188/12630 [36:40<2:33:52,  1.13it/s] 17%|        | 2189/12630 [36:41<2:34:26,  1.13it/s] 17%|        | 2190/12630 [36:42<2:34:51,  1.12it/s] 17%|        | 2191/12630 [36:43<2:33:22,  1.13it/s] 17%|        | 2192/12630 [36:44<2:34:04,  1.13it/s] 17%|        | 2193/12630 [36:44<2:34:11,  1.13it/s] 17%|        | 2194/12630 [36:45<2:34:49,  1.12it/s] 17%|        | 2195/12630 [36:46<2:35:18,  1.12it/s] 17%|        | 2196/12630 [36:47<2:34:10,  1.13it/s] 17%|        | 2197/12630 [36:48<2:34:15,  1.13it/s] 17%|        | 2198/12630 [36:49<2:35:20,  1.12it/s] 17%|        | 2199/12630 [36:50<2:33:40,  1.13it/s] 17%|        | 2200/12630 [36:51<2:33:36,  1.13it/s] 17%|        | 2201/12630 [36:52<2:32:42,  1.14it/s] 17%|        | 2202/12630 [36:52<2:33:33,  1.13it/s] 17%|        | 2203/12630 [36:53<2:33:08,  1.13it/s] 17%|        | 2204/12630 [36:54<2:33:45,  1.13it/s] 17%|        | 2205/12630 [36:55<2:34:06,  1.13it/s] 17%|        | 2206/12630 [36:56<2:33:34,  1.13it/s] 17%|        | 2207/12630 [36:57<2:33:47,  1.13it/s] 17%|        | 2208/12630 [36:58<2:33:47,  1.13it/s] 17%|        | 2209/12630 [36:59<2:33:36,  1.13it/s] 17%|        | 2210/12630 [36:59<2:34:14,  1.13it/s] 18%|        | 2211/12630 [37:00<2:33:08,  1.13it/s] 18%|        | 2212/12630 [37:01<2:33:45,  1.13it/s] 18%|        | 2213/12630 [37:02<2:33:48,  1.13it/s] 18%|        | 2214/12630 [37:03<2:34:56,  1.12it/s] 18%|        | 2215/12630 [37:04<2:34:01,  1.13it/s] 18%|        | 2216/12630 [37:05<2:35:21,  1.12it/s] 18%|        | 2217/12630 [37:06<2:34:27,  1.12it/s] 18%|        | 2218/12630 [37:07<2:34:43,  1.12it/s] 18%|        | 2219/12630 [37:07<2:34:12,  1.13it/s] 18%|        | 2220/12630 [37:08<2:34:17,  1.12it/s] 18%|        | 2221/12630 [37:09<2:33:37,  1.13it/s] 18%|        | 2222/12630 [37:10<2:34:51,  1.12it/s] 18%|        | 2223/12630 [37:11<2:34:17,  1.12it/s] 18%|        | 2224/12630 [37:12<2:33:43,  1.13it/s] 18%|        | 2225/12630 [37:13<2:33:47,  1.13it/s] 18%|        | 2226/12630 [37:14<2:33:03,  1.13it/s] 18%|        | 2227/12630 [37:15<2:33:36,  1.13it/s] 18%|        | 2228/12630 [37:15<2:33:43,  1.13it/s] 18%|        | 2229/12630 [37:16<2:32:58,  1.13it/s] 18%|        | 2230/12630 [37:17<2:33:10,  1.13it/s] 18%|        | 2231/12630 [37:18<2:32:57,  1.13it/s] 18%|        | 2232/12630 [37:19<2:33:06,  1.13it/s] 18%|        | 2233/12630 [37:20<2:32:43,  1.13it/s] 18%|        | 2234/12630 [37:21<2:34:27,  1.12it/s] 18%|        | 2235/12630 [37:22<2:33:35,  1.13it/s] 18%|        | 2236/12630 [37:23<2:33:04,  1.13it/s] 18%|        | 2237/12630 [37:23<2:33:45,  1.13it/s] 18%|        | 2238/12630 [37:24<2:34:04,  1.12it/s] 18%|        | 2239/12630 [37:25<2:32:58,  1.13it/s] 18%|        | 2240/12630 [37:26<2:33:03,  1.13it/s] 18%|        | 2241/12630 [37:27<2:33:58,  1.12it/s] 18%|        | 2242/12630 [37:28<2:33:12,  1.13it/s] 18%|        | 2243/12630 [37:29<2:33:36,  1.13it/s] 18%|        | 2244/12630 [37:30<2:32:23,  1.14it/s] 18%|        | 2245/12630 [37:31<2:32:38,  1.13it/s] 18%|        | 2246/12630 [37:31<2:32:56,  1.13it/s] 18%|        | 2247/12630 [37:32<2:33:12,  1.13it/s] 18%|        | 2248/12630 [37:33<2:32:35,  1.13it/s] 18%|        | 2249/12630 [37:34<2:32:25,  1.14it/s] 18%|        | 2250/12630 [37:35<2:33:01,  1.13it/s] 18%|        | 2251/12630 [37:36<2:32:46,  1.13it/s] 18%|        | 2252/12630 [37:37<2:32:49,  1.13it/s] 18%|        | 2253/12630 [37:38<2:32:12,  1.14it/s] 18%|        | 2254/12630 [37:38<2:31:59,  1.14it/s] 18%|        | 2255/12630 [37:39<2:32:48,  1.13it/s] 18%|        | 2256/12630 [37:40<2:32:42,  1.13it/s] 18%|        | 2257/12630 [37:41<2:31:50,  1.14it/s] 18%|        | 2258/12630 [37:42<2:32:52,  1.13it/s] 18%|        | 2259/12630 [37:43<2:31:53,  1.14it/s] 18%|        | 2260/12630 [37:44<2:33:00,  1.13it/s] 18%|        | 2261/12630 [37:45<2:32:37,  1.13it/s] 18%|        | 2262/12630 [37:46<2:33:00,  1.13it/s] 18%|        | 2263/12630 [37:46<2:33:27,  1.13it/s] 18%|        | 2264/12630 [37:47<2:32:50,  1.13it/s] 18%|        | 2265/12630 [37:48<2:32:02,  1.14it/s] 18%|        | 2266/12630 [37:49<2:32:07,  1.14it/s] 18%|        | 2267/12630 [37:50<2:32:55,  1.13it/s] 18%|        | 2268/12630 [37:51<2:33:38,  1.12it/s] 18%|        | 2269/12630 [37:52<2:32:38,  1.13it/s] 18%|        | 2270/12630 [37:53<2:32:49,  1.13it/s] 18%|        | 2271/12630 [37:53<2:32:08,  1.13it/s] 18%|        | 2272/12630 [37:54<2:32:08,  1.13it/s] 18%|        | 2273/12630 [37:55<2:32:15,  1.13it/s] 18%|        | 2274/12630 [37:56<2:33:09,  1.13it/s] 18%|        | 2275/12630 [37:57<2:32:37,  1.13it/s] 18%|        | 2276/12630 [37:58<2:32:18,  1.13it/s] 18%|        | 2277/12630 [37:59<2:32:43,  1.13it/s] 18%|        | 2278/12630 [38:00<2:31:39,  1.14it/s] 18%|        | 2279/12630 [38:01<2:32:41,  1.13it/s] 18%|        | 2280/12630 [38:01<2:32:26,  1.13it/s] 18%|        | 2281/12630 [38:02<2:32:27,  1.13it/s] 18%|        | 2282/12630 [38:03<2:32:40,  1.13it/s] 18%|        | 2283/12630 [38:04<2:32:23,  1.13it/s] 18%|        | 2284/12630 [38:05<2:32:48,  1.13it/s] 18%|        | 2285/12630 [38:06<2:32:36,  1.13it/s] 18%|        | 2286/12630 [38:07<2:32:58,  1.13it/s] 18%|        | 2287/12630 [38:08<2:32:10,  1.13it/s] 18%|        | 2288/12630 [38:08<2:31:49,  1.14it/s] 18%|        | 2289/12630 [38:09<2:32:00,  1.13it/s] 18%|        | 2290/12630 [38:10<2:31:21,  1.14it/s] 18%|        | 2291/12630 [38:11<2:31:14,  1.14it/s] 18%|        | 2292/12630 [38:12<2:32:36,  1.13it/s] 18%|        | 2293/12630 [38:13<2:32:08,  1.13it/s] 18%|        | 2294/12630 [38:14<2:32:12,  1.13it/s] 18%|        | 2295/12630 [38:15<2:32:06,  1.13it/s] 18%|        | 2296/12630 [38:16<2:31:58,  1.13it/s] 18%|        | 2297/12630 [38:16<2:33:34,  1.12it/s] 18%|        | 2298/12630 [38:17<2:33:15,  1.12it/s] 18%|        | 2299/12630 [38:18<2:33:41,  1.12it/s] 18%|        | 2300/12630 [38:19<2:32:22,  1.13it/s] 18%|        | 2301/12630 [38:20<2:32:37,  1.13it/s] 18%|        | 2302/12630 [38:21<2:32:12,  1.13it/s] 18%|        | 2303/12630 [38:22<2:31:50,  1.13it/s] 18%|        | 2304/12630 [38:23<2:32:00,  1.13it/s] 18%|        | 2305/12630 [38:24<2:32:14,  1.13it/s] 18%|        | 2306/12630 [38:24<2:31:53,  1.13it/s] 18%|        | 2307/12630 [38:25<2:31:33,  1.14it/s] 18%|        | 2308/12630 [38:26<2:32:16,  1.13it/s] 18%|        | 2309/12630 [38:27<2:32:01,  1.13it/s] 18%|        | 2310/12630 [38:28<2:33:18,  1.12it/s] 18%|        | 2311/12630 [38:29<2:32:59,  1.12it/s] 18%|        | 2312/12630 [38:30<2:32:06,  1.13it/s] 18%|        | 2313/12630 [38:31<2:32:14,  1.13it/s] 18%|        | 2314/12630 [38:31<2:31:40,  1.13it/s] 18%|        | 2315/12630 [38:32<2:32:13,  1.13it/s] 18%|        | 2316/12630 [38:33<2:31:25,  1.14it/s] 18%|        | 2317/12630 [38:34<2:32:03,  1.13it/s] 18%|        | 2318/12630 [38:35<2:32:25,  1.13it/s] 18%|        | 2319/12630 [38:36<2:32:08,  1.13it/s] 18%|        | 2320/12630 [38:37<2:32:24,  1.13it/s] 18%|        | 2321/12630 [38:38<2:32:10,  1.13it/s] 18%|        | 2322/12630 [38:39<2:32:01,  1.13it/s] 18%|        | 2323/12630 [38:39<2:31:39,  1.13it/s] 18%|        | 2324/12630 [38:40<2:32:31,  1.13it/s] 18%|        | 2325/12630 [38:41<2:32:17,  1.13it/s] 18%|        | 2326/12630 [38:42<2:32:40,  1.12it/s] 18%|        | 2327/12630 [38:43<2:32:28,  1.13it/s] 18%|        | 2328/12630 [38:44<2:32:13,  1.13it/s] 18%|        | 2329/12630 [38:45<2:32:38,  1.12it/s] 18%|        | 2330/12630 [38:46<2:32:34,  1.13it/s] 18%|        | 2331/12630 [38:47<2:31:17,  1.13it/s] 18%|        | 2332/12630 [38:47<2:31:45,  1.13it/s] 18%|        | 2333/12630 [38:48<2:31:23,  1.13it/s] 18%|        | 2334/12630 [38:49<2:31:06,  1.14it/s] 18%|        | 2335/12630 [38:50<2:32:02,  1.13it/s] 18%|        | 2336/12630 [38:51<2:33:21,  1.12it/s] 19%|        | 2337/12630 [38:52<2:33:38,  1.12it/s] 19%|        | 2338/12630 [38:53<2:31:54,  1.13it/s] 19%|        | 2339/12630 [38:54<2:32:40,  1.12it/s] 19%|        | 2340/12630 [38:55<2:31:37,  1.13it/s] 19%|        | 2341/12630 [38:55<2:31:29,  1.13it/s] 19%|        | 2342/12630 [38:56<2:30:30,  1.14it/s] 19%|        | 2343/12630 [38:57<2:32:04,  1.13it/s] 19%|        | 2344/12630 [38:58<2:30:57,  1.14it/s] 19%|        | 2345/12630 [38:59<2:32:49,  1.12it/s] 19%|        | 2346/12630 [39:00<2:31:50,  1.13it/s] 19%|        | 2347/12630 [39:01<2:31:14,  1.13it/s] 19%|        | 2348/12630 [39:02<2:30:34,  1.14it/s] 19%|        | 2349/12630 [39:03<2:31:55,  1.13it/s] 19%|        | 2350/12630 [39:03<2:31:34,  1.13it/s] 19%|        | 2351/12630 [39:04<2:32:23,  1.12it/s] 19%|        | 2352/12630 [39:05<2:32:02,  1.13it/s] 19%|        | 2353/12630 [39:06<2:31:09,  1.13it/s] 19%|        | 2354/12630 [39:07<2:31:26,  1.13it/s] 19%|        | 2355/12630 [39:08<2:31:37,  1.13it/s] 19%|        | 2356/12630 [39:09<2:31:22,  1.13it/s] 19%|        | 2357/12630 [39:10<2:31:55,  1.13it/s] 19%|        | 2358/12630 [39:10<2:31:07,  1.13it/s] 19%|        | 2359/12630 [39:11<2:31:13,  1.13it/s] 19%|        | 2360/12630 [39:12<2:31:32,  1.13it/s] 19%|        | 2361/12630 [39:13<2:31:42,  1.13it/s] 19%|        | 2362/12630 [39:14<2:30:58,  1.13it/s] 19%|        | 2363/12630 [39:15<2:30:15,  1.14it/s] 19%|        | 2364/12630 [39:16<2:30:08,  1.14it/s] 19%|        | 2365/12630 [39:17<2:30:34,  1.14it/s] 19%|        | 2366/12630 [39:18<2:31:29,  1.13it/s] 19%|        | 2367/12630 [39:18<2:31:59,  1.13it/s] 19%|        | 2368/12630 [39:19<2:31:42,  1.13it/s] 19%|        | 2369/12630 [39:20<2:31:16,  1.13it/s] 19%|        | 2370/12630 [39:21<2:30:40,  1.13it/s] 19%|        | 2371/12630 [39:22<2:31:07,  1.13it/s] 19%|        | 2372/12630 [39:23<2:30:38,  1.13it/s] 19%|        | 2373/12630 [39:24<2:30:58,  1.13it/s] 19%|        | 2374/12630 [39:25<2:32:33,  1.12it/s] 19%|        | 2375/12630 [39:26<2:33:16,  1.12it/s] 19%|        | 2376/12630 [39:26<2:31:33,  1.13it/s] 19%|        | 2377/12630 [39:27<2:32:11,  1.12it/s] 19%|        | 2378/12630 [39:28<2:31:34,  1.13it/s] 19%|        | 2379/12630 [39:29<2:31:46,  1.13it/s] 19%|        | 2380/12630 [39:30<2:31:54,  1.12it/s] 19%|        | 2381/12630 [39:31<2:30:35,  1.13it/s] 19%|        | 2382/12630 [39:32<2:32:08,  1.12it/s] 19%|        | 2383/12630 [39:33<2:31:30,  1.13it/s] 19%|        | 2384/12630 [39:33<2:30:30,  1.13it/s] 19%|        | 2385/12630 [39:34<2:31:11,  1.13it/s] 19%|        | 2386/12630 [39:35<2:31:24,  1.13it/s] 19%|        | 2387/12630 [39:36<2:31:02,  1.13it/s] 19%|        | 2388/12630 [39:37<2:31:33,  1.13it/s] 19%|        | 2389/12630 [39:38<2:31:48,  1.12it/s] 19%|        | 2390/12630 [39:39<2:31:18,  1.13it/s] 19%|        | 2391/12630 [39:40<2:31:15,  1.13it/s] 19%|        | 2392/12630 [39:41<2:30:58,  1.13it/s] 19%|        | 2393/12630 [39:41<2:31:00,  1.13it/s] 19%|        | 2394/12630 [39:42<2:30:54,  1.13it/s] 19%|        | 2395/12630 [39:43<2:30:52,  1.13it/s] 19%|        | 2396/12630 [39:44<2:31:11,  1.13it/s] 19%|        | 2397/12630 [39:45<2:30:37,  1.13it/s] 19%|        | 2398/12630 [39:46<2:30:11,  1.14it/s] 19%|        | 2399/12630 [39:47<2:30:45,  1.13it/s] 19%|        | 2400/12630 [39:48<2:30:19,  1.13it/s] 19%|        | 2401/12630 [39:49<2:31:54,  1.12it/s] 19%|        | 2402/12630 [39:49<2:31:22,  1.13it/s] 19%|        | 2403/12630 [39:50<2:31:16,  1.13it/s] 19%|        | 2404/12630 [39:51<2:31:53,  1.12it/s] 19%|        | 2405/12630 [39:52<2:31:40,  1.12it/s] 19%|        | 2406/12630 [39:53<2:31:41,  1.12it/s] 19%|        | 2407/12630 [39:54<2:31:09,  1.13it/s] 19%|        | 2408/12630 [39:55<2:30:38,  1.13it/s] 19%|        | 2409/12630 [39:56<2:30:25,  1.13it/s] 19%|        | 2410/12630 [39:57<2:31:42,  1.12it/s] 19%|        | 2411/12630 [39:57<2:31:08,  1.13it/s] 19%|        | 2412/12630 [39:58<2:30:29,  1.13it/s] 19%|        | 2413/12630 [39:59<2:31:08,  1.13it/s] 19%|        | 2414/12630 [40:00<2:30:43,  1.13it/s] 19%|        | 2415/12630 [40:01<2:30:54,  1.13it/s] 19%|        | 2416/12630 [40:02<2:31:34,  1.12it/s] 19%|        | 2417/12630 [40:03<2:31:39,  1.12it/s] 19%|        | 2418/12630 [40:04<2:31:27,  1.12it/s] 19%|        | 2419/12630 [40:05<2:30:25,  1.13it/s] 19%|        | 2420/12630 [40:05<2:30:38,  1.13it/s] 19%|        | 2421/12630 [40:06<2:29:47,  1.14it/s] 19%|        | 2422/12630 [40:07<2:30:23,  1.13it/s] 19%|        | 2423/12630 [40:08<2:30:21,  1.13it/s] 19%|        | 2424/12630 [40:09<2:29:51,  1.14it/s] 19%|        | 2425/12630 [40:10<2:29:54,  1.13it/s] 19%|        | 2426/12630 [40:11<2:30:10,  1.13it/s] 19%|        | 2427/12630 [40:12<2:30:04,  1.13it/s] 19%|        | 2428/12630 [40:12<2:28:57,  1.14it/s] 19%|        | 2429/12630 [40:13<2:28:53,  1.14it/s] 19%|        | 2430/12630 [40:14<2:29:43,  1.14it/s] 19%|        | 2431/12630 [40:15<2:29:52,  1.13it/s] 19%|        | 2432/12630 [40:16<2:30:03,  1.13it/s] 19%|        | 2433/12630 [40:17<2:29:28,  1.14it/s] 19%|        | 2434/12630 [40:18<2:30:27,  1.13it/s] 19%|        | 2435/12630 [40:19<2:30:25,  1.13it/s] 19%|        | 2436/12630 [40:20<2:30:01,  1.13it/s] 19%|        | 2437/12630 [40:20<2:29:16,  1.14it/s] 19%|        | 2438/12630 [40:21<2:29:46,  1.13it/s] 19%|        | 2439/12630 [40:22<2:30:47,  1.13it/s] 19%|        | 2440/12630 [40:23<2:30:27,  1.13it/s] 19%|        | 2441/12630 [40:24<2:30:01,  1.13it/s] 19%|        | 2442/12630 [40:25<2:29:25,  1.14it/s] 19%|        | 2443/12630 [40:26<2:29:17,  1.14it/s] 19%|        | 2444/12630 [40:27<2:30:14,  1.13it/s] 19%|        | 2445/12630 [40:27<2:29:05,  1.14it/s] 19%|        | 2446/12630 [40:28<2:29:53,  1.13it/s] 19%|        | 2447/12630 [40:29<2:29:09,  1.14it/s] 19%|        | 2448/12630 [40:30<2:30:07,  1.13it/s] 19%|        | 2449/12630 [40:31<2:30:01,  1.13it/s] 19%|        | 2450/12630 [40:32<2:30:12,  1.13it/s] 19%|        | 2451/12630 [40:33<2:29:45,  1.13it/s] 19%|        | 2452/12630 [40:34<2:30:05,  1.13it/s] 19%|        | 2453/12630 [40:35<2:31:03,  1.12it/s] 19%|        | 2454/12630 [40:35<2:30:08,  1.13it/s] 19%|        | 2455/12630 [40:36<2:29:33,  1.13it/s] 19%|        | 2456/12630 [40:37<2:29:35,  1.13it/s] 19%|        | 2457/12630 [40:38<2:29:38,  1.13it/s] 19%|        | 2458/12630 [40:39<2:29:43,  1.13it/s] 19%|        | 2459/12630 [40:40<2:30:55,  1.12it/s] 19%|        | 2460/12630 [40:41<2:31:32,  1.12it/s] 19%|        | 2461/12630 [40:42<2:29:56,  1.13it/s] 19%|        | 2462/12630 [40:43<2:31:34,  1.12it/s] 20%|        | 2463/12630 [40:43<2:31:06,  1.12it/s] 20%|        | 2464/12630 [40:44<2:30:30,  1.13it/s] 20%|        | 2465/12630 [40:45<2:30:44,  1.12it/s] 20%|        | 2466/12630 [40:46<2:30:28,  1.13it/s] 20%|        | 2467/12630 [40:47<2:29:50,  1.13it/s] 20%|        | 2468/12630 [40:48<2:30:00,  1.13it/s] 20%|        | 2469/12630 [40:49<2:29:35,  1.13it/s] 20%|        | 2470/12630 [40:50<2:30:03,  1.13it/s] 20%|        | 2471/12630 [40:50<2:30:06,  1.13it/s] 20%|        | 2472/12630 [40:51<2:31:16,  1.12it/s] 20%|        | 2473/12630 [40:52<2:30:58,  1.12it/s] 20%|        | 2474/12630 [40:53<2:31:04,  1.12it/s] 20%|        | 2475/12630 [40:54<2:30:58,  1.12it/s] 20%|        | 2476/12630 [40:55<2:30:46,  1.12it/s] 20%|        | 2477/12630 [40:56<2:30:26,  1.12it/s] 20%|        | 2478/12630 [40:57<2:29:45,  1.13it/s] 20%|        | 2479/12630 [40:58<2:29:11,  1.13it/s] 20%|        | 2480/12630 [40:58<2:29:45,  1.13it/s] 20%|        | 2481/12630 [40:59<2:30:28,  1.12it/s] 20%|        | 2482/12630 [41:00<2:31:44,  1.11it/s] 20%|        | 2483/12630 [41:01<2:30:53,  1.12it/s] 20%|        | 2484/12630 [41:02<2:29:59,  1.13it/s] 20%|        | 2485/12630 [41:03<2:29:36,  1.13it/s] 20%|        | 2486/12630 [41:04<2:29:59,  {'loss': 0.31, 'learning_rate': 4.0102929532858274e-05, 'epoch': 0.59}
1.13it/s] 20%|        | 2487/12630 [41:05<2:30:01,  1.13it/s] 20%|        | 2488/12630 [41:06<2:29:56,  1.13it/s] 20%|        | 2489/12630 [41:06<2:29:45,  1.13it/s] 20%|        | 2490/12630 [41:07<2:29:50,  1.13it/s] 20%|        | 2491/12630 [41:08<2:29:33,  1.13it/s] 20%|        | 2492/12630 [41:09<2:29:00,  1.13it/s] 20%|        | 2493/12630 [41:10<2:29:31,  1.13it/s] 20%|        | 2494/12630 [41:11<2:28:59,  1.13it/s] 20%|        | 2495/12630 [41:12<2:28:43,  1.14it/s] 20%|        | 2496/12630 [41:13<2:29:39,  1.13it/s] 20%|        | 2497/12630 [41:14<2:29:56,  1.13it/s] 20%|        | 2498/12630 [41:14<2:29:18,  1.13it/s] 20%|        | 2499/12630 [41:15<2:30:05,  1.13it/s] 20%|        | 2500/12630 [41:16<2:29:24,  1.13it/s]                                                       20%|        | 2500/12630 [41:16<2:29:24,  1.13it/s][INFO|trainer.py:2985] 2024-02-13 00:51:16,707 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2500
[INFO|configuration_utils.py:473] 2024-02-13 00:51:16,723 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 00:51:41,062 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 00:51:41,073 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 00:51:41,074 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-2500/special_tokens_map.json
 20%|        | 2501/12630 [42:28<62:24:43, 22.18s/it] 20%|        | 2502/12630 [42:29<44:37:52, 15.86s/it] 20%|        | 2503/12630 [42:30<32:16:05, 11.47s/it] 20%|        | 2504/12630 [42:31<23:26:03,  8.33s/it] 20%|        | 2505/12630 [42:32<17:14:08,  6.13s/it] 20%|        | 2506/12630 [42:33<12:51:19,  4.57s/it] 20%|        | 2507/12630 [42:34<9:46:59,  3.48s/it]  20%|        | 2508/12630 [42:35<7:41:31,  2.74s/it] 20%|        | 2509/12630 [42:37<6:40:26,  2.37s/it] 20%|        | 2510/12630 [42:38<5:30:51,  1.96s/it] 20%|        | 2511/12630 [42:39<4:44:00,  1.68s/it] 20%|        | 2512/12630 [42:40<4:08:55,  1.48s/it] 20%|        | 2513/12630 [42:41<3:47:30,  1.35s/it] 20%|        | 2514/12630 [42:42<3:24:52,  1.22s/it] 20%|        | 2515/12630 [42:43<3:13:19,  1.15s/it] 20%|        | 2516/12630 [42:44<3:02:27,  1.08s/it] 20%|        | 2517/12630 [42:45<3:10:37,  1.13s/it] 20%|        | 2518/12630 [42:46<2:59:34,  1.07s/it] 20%|        | 2519/12630 [42:47<2:53:21,  1.03s/it] 20%|        | 2520/12630 [42:48<2:51:16,  1.02s/it] 20%|        | 2521/12630 [42:49<2:54:13,  1.03s/it] 20%|        | 2522/12630 [42:50<2:51:55,  1.02s/it] 20%|        | 2523/12630 [42:51<2:46:07,  1.01it/s] 20%|        | 2524/12630 [42:52<2:50:06,  1.01s/it] 20%|        | 2525/12630 [42:53<2:50:49,  1.01s/it] 20%|        | 2526/12630 [42:54<2:46:11,  1.01it/s] 20%|        | 2527/12630 [42:55<2:43:09,  1.03it/s] 20%|        | 2528/12630 [42:56<2:40:52,  1.05it/s] 20%|        | 2529/12630 [42:57<3:16:04,  1.16s/it] 20%|        | 2530/12630 [42:58<3:05:21,  1.10s/it] 20%|        | 2531/12630 [42:59<2:58:18,  1.06s/it] 20%|        | 2532/12630 [43:00<2:51:45,  1.02s/it] 20%|        | 2533/12630 [43:01<2:46:53,  1.01it/s] 20%|        | 2534/12630 [43:02<2:43:18,  1.03it/s] 20%|        | 2535/12630 [43:03<2:42:45,  1.03it/s] 20%|        | 2536/12630 [43:04<2:41:06,  1.04it/s] 20%|        | 2537/12630 [43:05<2:41:53,  1.04it/s] 20%|        | 2538/12630 [43:06<2:41:10,  1.04it/s] 20%|        | 2539/12630 [43:07<2:37:28,  1.07it/s] 20%|        | 2540/12630 [43:08<2:36:11,  1.08it/s] 20%|        | 2541/12630 [43:09<2:35:58,  1.08it/s] 20%|        | 2542/12630 [43:09<2:35:15,  1.08it/s] 20%|        | 2543/12630 [43:10<2:34:50,  1.09it/s] 20%|        | 2544/12630 [43:11<2:34:23,  1.09it/s] 20%|        | 2545/12630 [43:12<2:34:44,  1.09it/s] 20%|        | 2546/12630 [43:13<2:33:58,  1.09it/s] 20%|        | 2547/12630 [43:14<2:36:36,  1.07it/s] 20%|        | 2548/12630 [43:15<2:36:05,  1.08it/s] 20%|        | 2549/12630 [43:16<2:35:46,  1.08it/s] 20%|        | 2550/12630 [43:17<2:36:39,  1.07it/s] 20%|        | 2551/12630 [43:18<2:35:21,  1.08it/s] 20%|        | 2552/12630 [43:19<2:35:36,  1.08it/s] 20%|        | 2553/12630 [43:20<2:35:03,  1.08it/s] 20%|        | 2554/12630 [43:21<2:35:41,  1.08it/s] 20%|        | 2555/12630 [43:22<2:37:13,  1.07it/s] 20%|        | 2556/12630 [43:22<2:36:24,  1.07it/s] 20%|        | 2557/12630 [43:23<2:36:02,  1.08it/s] 20%|        | 2558/12630 [43:24<2:33:56,  1.09it/s] 20%|        | 2559/12630 [43:25<2:34:08,  1.09it/s] 20%|        | 2560/12630 [43:26<2:34:11,  1.09it/s] 20%|        | 2561/12630 [43:27<2:34:12,  1.09it/s] 20%|        | 2562/12630 [43:28<2:33:36,  1.09it/s] 20%|        | 2563/12630 [43:29<2:36:56,  1.07it/s] 20%|        | 2564/12630 [43:30<2:36:39,  1.07it/s] 20%|        | 2565/12630 [43:31<2:37:45,  1.06it/s] 20%|        | 2566/12630 [43:32<2:35:07,  1.08it/s] 20%|        | 2567/12630 [43:33<2:33:48,  1.09it/s] 20%|        | 2568/12630 [43:33<2:33:45,  1.09it/s] 20%|        | 2569/12630 [43:34<2:33:23,  1.09it/s] 20%|        | 2570/12630 [43:35<2:36:14,  1.07it/s] 20%|        | 2571/12630 [43:36<2:36:06,  1.07it/s] 20%|        | 2572/12630 [43:37<2:36:06,  1.07it/s] 20%|        | 2573/12630 [43:38<2:34:36,  1.08it/s] 20%|        | 2574/12630 [43:39<2:33:28,  1.09it/s] 20%|        | 2575/12630 [43:40<2:31:58,  1.10it/s] 20%|        | 2576/12630 [43:41<2:37:50,  1.06it/s] 20%|        | 2577/12630 [43:42<2:38:43,  1.06it/s] 20%|        | 2578/12630 [43:43<2:37:15,  1.07it/s] 20%|        | 2579/12630 [43:44<2:38:41,  1.06it/s] 20%|        | 2580/12630 [43:45<2:46:53,  1.00it/s] 20%|        | 2581/12630 [43:46<2:44:31,  1.02it/s] 20%|        | 2582/12630 [43:47<2:41:34,  1.04it/s] 20%|        | 2583/12630 [43:48<2:39:52,  1.05it/s] 20%|        | 2584/12630 [43:49<2:53:01,  1.03s/it] 20%|        | 2585/12630 [43:50<2:47:53,  1.00s/it] 20%|        | 2586/12630 [43:51<2:42:54,  1.03it/s] 20%|        | 2587/12630 [43:52<2:40:31,  1.04it/s] 20%|        | 2588/12630 [43:53<2:37:14,  1.06it/s] 20%|        | 2589/12630 [43:53<2:35:17,  1.08it/s] 21%|        | 2590/12630 [43:54<2:35:46,  1.07it/s] 21%|        | 2591/12630 [43:55<2:36:52,  1.07it/s] 21%|        | 2592/12630 [43:56<2:35:56,  1.07it/s] 21%|        | 2593/12630 [43:57<2:36:17,  1.07it/s] 21%|        | 2594/12630 [43:58<2:37:08,  1.06it/s] 21%|        | 2595/12630 [43:59<2:38:28,  1.06it/s] 21%|        | 2596/12630 [44:00<2:36:00,  1.07it/s] 21%|        | 2597/12630 [44:01<2:34:23,  1.08it/s] 21%|        | 2598/12630 [44:02<2:36:12,  1.07it/s] 21%|        | 2599/12630 [44:03<2:35:48,  1.07it/s] 21%|        | 2600/12630 [44:04<2:36:20,  1.07it/s] 21%|        | 2601/12630 [44:05<2:48:19,  1.01s/it] 21%|        | 2602/12630 [44:06<2:44:09,  1.02it/s] 21%|        | 2603/12630 [44:07<2:42:40,  1.03it/s] 21%|        | 2604/12630 [44:08<2:41:08,  1.04it/s] 21%|        | 2605/12630 [44:09<2:51:46,  1.03s/it] 21%|        | 2606/12630 [44:10<2:48:11,  1.01s/it] 21%|        | 2607/12630 [44:11<2:43:02,  1.02it/s] 21%|        | 2608/12630 [44:12<2:39:23,  1.05it/s] 21%|        | 2609/12630 [44:13<2:51:05,  1.02s/it] 21%|        | 2610/12630 [44:14<2:48:05,  1.01s/it] 21%|        | 2611/12630 [44:15<2:45:07,  1.01it/s] 21%|        | 2612/12630 [44:16<2:43:22,  1.02it/s] 21%|        | 2613/12630 [44:17<2:52:25,  1.03s/it] 21%|        | 2614/12630 [44:18<2:51:09,  1.03s/it] 21%|        | 2615/12630 [44:19<2:50:29,  1.02s/it] 21%|        | 2616/12630 [44:20<2:49:27,  1.02s/it] 21%|        | 2617/12630 [44:21<2:52:09,  1.03s/it] 21%|        | 2618/12630 [44:22<2:48:27,  1.01s/it] 21%|        | 2619/12630 [44:23<2:45:16,  1.01it/s] 21%|        | 2620/12630 [44:24<2:43:51,  1.02it/s] 21%|        | 2621/12630 [44:25<2:45:57,  1.01it/s] 21%|        | 2622/12630 [44:26<2:42:24,  1.03it/s] 21%|        | 2623/12630 [44:27<2:42:06,  1.03it/s] 21%|        | 2624/12630 [44:28<2:42:14,  1.03it/s] 21%|        | 2625/12630 [44:29<2:52:53,  1.04s/it] 21%|        | 2626/12630 [44:30<2:49:26,  1.02s/it] 21%|        | 2627/12630 [44:31<2:45:45,  1.01it/s] 21%|        | 2628/12630 [44:32<2:43:12,  1.02it/s] 21%|        | 2629/12630 [44:33<2:50:57,  1.03s/it] 21%|        | 2630/12630 [44:34<2:47:55,  1.01s/it] 21%|        | 2631/12630 [44:35<2:45:42,  1.01it/s] 21%|        | 2632/12630 [44:36<2:46:51,  1.00s/it] 21%|        | 2633/12630 [44:37<2:50:59,  1.03s/it] 21%|        | 2634/12630 [44:38<2:45:49,  1.00it/s] 21%|        | 2635/12630 [44:39<2:44:56,  1.01it/s] 21%|        | 2636/12630 [44:40<2:42:32,  1.02it/s] 21%|        | 2637/12630 [44:41<2:47:39,  1.01s/it] 21%|        | 2638/12630 [44:42<2:44:18,  1.01it/s] 21%|        | 2639/12630 [44:43<2:42:45,  1.02it/s] 21%|        | 2640/12630 [44:44<2:40:34,  1.04it/s] 21%|        | 2641/12630 [44:45<2:38:43,  1.05it/s] 21%|        | 2642/12630 [44:46<2:37:48,  1.05it/s] 21%|        | 2643/12630 [44:47<2:37:17,  1.06it/s] 21%|        | 2644/12630 [44:47<2:34:37,  1.08it/s] 21%|        | 2645/12630 [44:48<2:33:27,  1.08it/s] 21%|        | 2646/12630 [44:49<2:34:59,  1.07it/s] 21%|        | 2647/12630 [44:50<2:35:12,  1.07it/s] 21%|        | 2648/12630 [44:51<2:35:21,  1.07it/s] 21%|        | 2649/12630 [44:52<2:36:57,  1.06it/s] 21%|        | 2650/12630 [44:53<2:36:28,  1.06it/s] 21%|        | 2651/12630 [44:54<2:35:23,  1.07it/s] 21%|        | 2652/12630 [44:55<2:34:37,  1.08it/s] 21%|        | 2653/12630 [44:56<2:35:24,  1.07it/s] 21%|        | 2654/12630 [44:57<2:45:31,  1.00it/s] 21%|        | 2655/12630 [44:58<2:42:00,  1.03it/s] 21%|        | 2656/12630 [44:59<2:40:15,  1.04it/s] 21%|        | 2657/12630 [45:00<2:38:44,  1.05it/s] 21%|        | 2658/12630 [45:01<2:51:02,  1.03s/it] 21%|        | 2659/12630 [45:02<2:44:34,  1.01it/s] 21%|        | 2660/12630 [45:03<2:40:33,  1.03it/s] 21%|        | 2661/12630 [45:04<2:41:05,  1.03it/s] 21%|        | 2662/12630 [45:05<2:49:44,  1.02s/it] 21%|        | 2663/12630 [45:06<2:45:14,  1.01it/s] 21%|        | 2664/12630 [45:07<2:40:32,  1.03it/s] 21%|        | 2665/12630 [45:08<2:43:53,  1.01it/s] 21%|        | 2666/12630 [45:09<2:53:26,  1.04s/it] 21%|        | 2667/12630 [45:10<2:48:25,  1.01s/it] 21%|        | 2668/12630 [45:11<2:45:03,  1.01it/s] 21%|        | 2669/12630 [45:12<2:42:20,  1.02it/s] 21%|        | 2670/12630 [45:13<2:52:33,  1.04s/it] 21%|        | 2671/12630 [45:14<2:48:22,  1.01s/it] 21%|        | 2672/12630 [45:15<2:42:30,  1.02it/s] 21%|        | 2673/12630 [45:16<2:37:05,  1.06it/s] 21%|        | 2674/12630 [45:17<2:35:00,  1.07it/s] 21%|        | 2675/12630 [45:18<2:34:44,  1.07it/s] 21%|        | 2676/12630 [45:18<2:34:56,  1.07it/s] 21%|        | 2677/12630 [45:19<2:32:01,  1.09it/s] 21%|        | 2678/12630 [45:20<2:32:09,  1.09it/s] 21%|        | 2679/12630 [45:21<2:35:29,  1.07it/s] 21%|        | 2680/12630 [45:22<2:33:57,  1.08it/s] 21%|        | 2681/12630 [45:23<2:32:14,  1.09it/s] 21%|        | 2682/12630 [45:24<2:31:53,  1.09it/s] 21%|        | 2683/12630 [45:25<2:35:44,  1.06it/s] 21%|       | 2684/12630 [45:26<2:32:57,  1.08it/s] 21%|       | 2685/12630 [45:27<2:31:51,  1.09it/s] 21%|       | 2686/12630 [45:28<2:32:20,  1.09it/s] 21%|       | 2687/12630 [45:29<2:33:24,  1.08it/s] 21%|       | 2688/12630 [45:30<2:33:21,  1.08it/s] 21%|       | 2689/12630 [45:30<2:31:34,  1.09it/s] 21%|       | 2690/12630 [45:31<2:32:11,  1.09it/s] 21%|       | 2691/12630 [45:32<2:30:46,  1.10it/s] 21%|       | 2692/12630 [45:33<2:31:08,  1.10it/s] 21%|       | 2693/12630 [45:34<2:31:48,  1.09it/s] 21%|       | 2694/12630 [45:35<2:31:42,  1.09it/s] 21%|       | 2695/12630 [45:36<2:31:12,  1.10it/s] 21%|       | 2696/12630 [45:37<2:41:42,  1.02it/s] 21%|       | 2697/12630 [45:38<2:39:09,  1.04it/s] 21%|       | 2698/12630 [45:39<2:38:24,  1.04it/s] 21%|       | 2699/12630 [45:40<2:37:56,  1.05it/s] 21%|       | 2700/12630 [45:41<2:48:37,  1.02s/it] 21%|       | 2701/12630 [45:42<2:42:54,  1.02it/s] 21%|       | 2702/12630 [45:43<2:46:35,  1.01s/it] 21%|       | 2703/12630 [45:44<2:42:46,  1.02it/s] 21%|       | 2704/12630 [45:45<2:46:36,  1.01s/it] 21%|       | 2705/12630 [45:46<2:41:57,  1.02it/s] 21%|       | 2706/12630 [45:47<2:39:31,  1.04it/s] 21%|       | 2707/12630 [45:48<2:39:34,  1.04it/s] 21%|       | 2708/12630 [45:49<2:49:44,  1.03s/it] 21%|       | 2709/12630 [45:50<2:43:39,  1.01it/s] 21%|       | 2710/12630 [45:51<2:41:28,  1.02it/s] 21%|       | 2711/12630 [45:52<2:38:05,  1.05it/s] 21%|       | 2712/12630 [45:53<2:36:44,  1.05it/s] 21%|       | 2713/12630 [45:54<2:34:18,  1.07it/s] 21%|       | 2714/12630 [45:54<2:33:25,  1.08it/s] 21%|       | 2715/12630 [45:55<2:32:08,  1.09it/s] 22%|       | 2716/12630 [45:56<2:32:26,  1.08it/s] 22%|       | 2717/12630 [45:57<2:31:50,  1.09it/s] 22%|       | 2718/12630 [45:58<2:30:27,  1.10it/s] 22%|       | 2719/12630 [45:59<2:30:44,  1.10it/s] 22%|       | 2720/12630 [46:00<2:30:59,  1.09it/s] 22%|       | 2721/12630 [46:01<2:36:33,  1.05it/s] 22%|       | 2722/12630 [46:02<2:36:30,  1.06it/s] 22%|       | 2723/12630 [46:03<2:35:28,  1.06it/s] 22%|       | 2724/12630 [46:04<2:34:29,  1.07it/s] 22%|       | 2725/12630 [46:05<2:33:16,  1.08it/s] 22%|       | 2726/12630 [46:06<2:33:47,  1.07it/s] 22%|       | 2727/12630 [46:07<2:32:30,  1.08it/s] 22%|       | 2728/12630 [46:07<2:31:39,  1.09it/s] 22%|       | 2729/12630 [46:08<2:30:50,  1.09it/s] 22%|       | 2730/12630 [46:09<2:33:11,  1.08it/s] 22%|       | 2731/12630 [46:10<2:31:22,  1.09it/s] 22%|       | 2732/12630 [46:11<2:31:05,  1.09it/s] 22%|       | 2733/12630 [46:12<2:31:55,  1.09it/s] 22%|       | 2734/12630 [46:13<2:36:30,  1.05it/s] 22%|       | 2735/12630 [46:14<2:34:12,  1.07it/s] 22%|       | 2736/12630 [46:15<2:34:40,  1.07it/s] 22%|       | 2737/12630 [46:16<2:32:48,  1.08it/s] 22%|       | 2738/12630 [46:17<2:44:28,  1.00it/s] 22%|       | 2739/12630 [46:18<2:43:15,  1.01it/s] 22%|       | 2740/12630 [46:19<2:39:32,  1.03it/s] 22%|       | 2741/12630 [46:20<2:36:20,  1.05it/s] 22%|       | 2742/12630 [46:21<2:32:36,  1.08it/s] 22%|       | 2743/12630 [46:22<2:31:57,  1.08it/s] 22%|       | 2744/12630 [46:22<2:31:55,  1.08it/s] 22%|       | 2745/12630 [46:23<2:32:24,  1.08it/s] 22%|       | 2746/12630 [46:24<2:31:27,  1.09it/s] 22%|       | 2747/12630 [46:25<2:30:55,  1.09it/s] 22%|       | 2748/12630 [46:26<2:30:25,  1.09it/s] 22%|       | 2749/12630 [46:27<2:31:32,  1.09it/s] 22%|       | 2750/12630 [46:28<2:31:13,  1.09it/s] 22%|       | 2751/12630 [46:29<2:34:25,  1.07it/s] 22%|       | 2752/12630 [46:30<2:33:19,  1.07it/s] 22%|       | 2753/12630 [46:31<2:31:51,  1.08it/s] 22%|       | 2754/12630 [46:32<2:31:16,  1.09it/s] 22%|       | 2755/12630 [46:33<2:31:27,  1.09it/s] 22%|       | 2756/12630 [46:33<2:31:26,  1.09it/s] 22%|       | 2757/12630 [46:34<2:33:03,  1.08it/s] 22%|       | 2758/12630 [46:35<2:32:15,  1.08it/s] 22%|       | 2759/12630 [46:36<2:34:23,  1.07it/s] 22%|       | 2760/12630 [46:37<2:32:01,  1.08it/s] 22%|       | 2761/12630 [46:38<2:31:19,  1.09it/s] 22%|       | 2762/12630 [46:39<2:31:48,  1.08it/s] 22%|       | 2763/12630 [46:40<2:30:56,  1.09it/s] 22%|       | 2764/12630 [46:41<2:38:31,  1.04it/s] 22%|       | 2765/12630 [46:42<2:39:28,  1.03it/s] 22%|       | 2766/12630 [46:43<2:36:14,  1.05it/s] 22%|       | 2767/12630 [46:44<2:35:03,  1.06it/s] 22%|       | 2768/12630 [46:45<2:42:41,  1.01it/s] 22%|       | 2769/12630 [46:46<2:38:07,  1.04it/s] 22%|       | 2770/12630 [46:47<2:35:06,  1.06it/s] 22%|       | 2771/12630 [46:48<2:34:21,  1.06it/s] 22%|       | 2772/12630 [46:49<2:35:53,  1.05it/s] 22%|       | 2773/12630 [46:50<2:33:17,  1.07it/s] 22%|       | 2774/12630 [46:50<2:31:54,  1.08it/s] 22%|       | 2775/12630 [46:51<2:31:17,  1.09it/s] 22%|       | 2776/12630 [46:52<2:32:39,  1.08it/s] 22%|       | 2777/12630 [46:53<2:36:50,  1.05it/s] 22%|       | 2778/12630 [46:54<2:35:19,  1.06it/s] 22%|       | 2779/12630 [46:55<2:33:31,  1.07it/s] 22%|       | 2780/12630 [46:56<2:32:10,  1.08it/s] 22%|       | 2781/12630 [46:57<2:36:11,  1.05it/s] 22%|       | 2782/12630 [46:58<2:35:17,  1.06it/s] 22%|       | 2783/12630 [46:59<2:32:47,  1.07it/s] 22%|       | 2784/12630 [47:00<2:33:12,  1.07it/s] 22%|       | 2785/12630 [47:01<2:42:48,  1.01it/s] 22%|       | 2786/12630 [47:02<2:40:05,  1.02it/s] 22%|       | 2787/12630 [47:03<2:38:08,  1.04it/s] 22%|       | 2788/12630 [47:04<2:36:35,  1.05it/s] 22%|       | 2789/12630 [47:05<2:49:06,  1.03s/it] 22%|       | 2790/12630 [47:06<2:44:55,  1.01s/it] 22%|       | 2791/12630 [47:07<2:41:23,  1.02it/s] 22%|       | 2792/12630 [47:08<2:38:20,  1.04it/s] 22%|       | 2793/12630 [47:09<2:46:59,  1.02s/it] 22%|       | 2794/12630 [47:10<2:41:13,  1.02it/s] 22%|       | 2795/12630 [47:11<2:36:23,  1.05it/s] 22%|       | 2796/12630 [47:12<2:34:29,  1.06it/s] 22%|       | 2797/12630 [47:13<2:32:08,  1.08it/s] 22%|       | 2798/12630 [47:13<2:33:02,  1.07it/s] 22%|       | 2799/12630 [47:14<2:32:58,  1.07it/s] 22%|       | 2800/12630 [47:15<2:34:13,  1.06it/s] 22%|       | 2801/12630 [47:16<2:32:18,  1.08it/s] 22%|       | 2802/12630 [47:17<2:32:12,  1.08it/s] 22%|       | 2803/12630 [47:18<2:32:09,  1.08it/s] 22%|       | 2804/12630 [47:19<2:30:32,  1.09it/s] 22%|       | 2805/12630 [47:20<2:32:14,  1.08it/s] 22%|       | 2806/12630 [47:21<2:38:20,  1.03it/s] 22%|       | 2807/12630 [47:22<2:33:51,  1.06it/s] 22%|       | 2808/12630 [47:23<2:32:38,  1.07it/s] 22%|       | 2809/12630 [47:24<2:33:11,  1.07it/s] 22%|       | 2810/12630 [47:25<2:45:10,  1.01s/it] 22%|       | 2811/12630 [47:26<2:39:50,  1.02it/s] 22%|       | 2812/12630 [47:27<2:38:39,  1.03it/s] 22%|       | 2813/12630 [47:28<2:36:32,  1.05it/s] 22%|       | 2814/12630 [47:29<2:50:15,  1.04s/it] 22%|       | 2815/12630 [47:30<2:44:47,  1.01s/it] 22%|       | 2816/12630 [47:31<2:40:14,  1.02it/s] 22%|       | 2817/12630 [47:32<2:38:39,  1.03it/s] 22%|       | 2818/12630 [47:33<2:48:00,  1.03s/it] 22%|       | 2819/12630 [47:34<2:45:55,  1.01s/it] 22%|       | 2820/12630 [47:35<2:42:08,  1.01it/s] 22%|       | 2821/12630 [47:36<2:38:00,  1.03it/s] 22%|       | 2822/12630 [47:37<2:49:32,  1.04s/it] 22%|       | 2823/12630 [47:38<2:43:29,  1.00s/it] 22%|       | 2824/12630 [47:39<2:41:05,  1.01it/s] 22%|       | 2825/12630 [47:40<2:41:06,  1.01it/s] 22%|       | 2826/12630 [47:41<2:49:24,  1.04s/it] 22%|       | 2827/12630 [47:42<2:43:24,  1.00s/it] 22%|       | 2828/12630 [47:43<2:39:27,  1.02it/s] 22%|       | 2829/12630 [47:44<2:36:05,  1.05it/s] 22%|       | 2830/12630 [47:45<2:33:24,  1.06it/s] 22%|       | 2831/12630 [47:46<2:32:09,  1.07it/s] 22%|       | 2832/12630 [47:46<2:30:19,  1.09it/s] 22%|       | 2833/12630 [47:47<2:29:40,  1.09it/s] 22%|       | 2834/12630 [47:48<2:30:24,  1.09it/s] 22%|       | 2835/12630 [47:49<2:31:19,  1.08it/s] 22%|       | 2836/12630 [47:50<2:29:34,  1.09it/s]slurmstepd: error: *** STEP 22207793.8 ON v005 CANCELLED AT 2024-02-13T00:57:51 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 302 seconds for job step to finish.
srun: error: v005: task 0: Terminated
