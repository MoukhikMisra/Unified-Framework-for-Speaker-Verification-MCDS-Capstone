WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 18:44:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 18:44:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/runs/Feb12_18-44-08_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 18:44:09 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 18:44:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:44:10 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 18:44:10 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:44:10 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 18:44:11,133 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--TinyLlama--TinyLlama-1.1B-intermediate-step-715k-1.5T/snapshots/19a81efa07bf28ec81dc4de327776fa00e34cf3f/config.json
[INFO|configuration_utils.py:792] 2024-02-12 18:44:11,138 >> Model config LlamaConfig {
  "_name_or_path": "TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32000
}

tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 541kB/s]
tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 21.8MB/s]
tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 45.2MB/s]
special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 306kB/s]
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:44:12,424 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:44:12,424 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:44:12,424 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:44:12,424 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:44:12,425 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer_config.json
model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]model.safetensors:   0%|          | 10.5M/4.40G [00:00<00:44, 99.1MB/s]model.safetensors:   1%|          | 31.5M/4.40G [00:00<00:33, 129MB/s] model.safetensors:   1%|▏         | 62.9M/4.40G [00:00<00:24, 174MB/s]model.safetensors:   2%|▏         | 94.4M/4.40G [00:00<00:22, 192MB/s]model.safetensors:   3%|▎         | 126M/4.40G [00:00<00:20, 208MB/s] model.safetensors:   4%|▎         | 157M/4.40G [00:00<00:19, 221MB/s]model.safetensors:   4%|▍         | 189M/4.40G [00:00<00:19, 220MB/s]model.safetensors:   5%|▌         | 220M/4.40G [00:01<00:18, 220MB/s]model.safetensors:   6%|▌         | 252M/4.40G [00:01<00:21, 190MB/s]model.safetensors:   6%|▌         | 273M/4.40G [00:01<00:22, 183MB/s]model.safetensors:   7%|▋         | 294M/4.40G [00:01<00:21, 188MB/s]model.safetensors:   7%|▋         | 325M/4.40G [00:01<00:20, 197MB/s]model.safetensors:   8%|▊         | 357M/4.40G [00:01<00:19, 207MB/s]model.safetensors:   9%|▉         | 388M/4.40G [00:01<00:19, 207MB/s]model.safetensors:  10%|▉         | 419M/4.40G [00:02<00:18, 214MB/s]model.safetensors:  10%|█         | 451M/4.40G [00:02<00:17, 220MB/s]model.safetensors:  11%|█         | 482M/4.40G [00:02<00:17, 227MB/s]model.safetensors:  12%|█▏        | 514M/4.40G [00:02<00:16, 230MB/s]model.safetensors:  12%|█▏        | 545M/4.40G [00:02<00:16, 230MB/s]model.safetensors:  13%|█▎        | 577M/4.40G [00:02<00:16, 234MB/s]model.safetensors:  14%|█▍        | 608M/4.40G [00:02<00:15, 238MB/s]model.safetensors:  15%|█▍        | 640M/4.40G [00:03<00:16, 232MB/s]model.safetensors:  15%|█▌        | 671M/4.40G [00:03<00:16, 224MB/s]model.safetensors:  16%|█▌        | 703M/4.40G [00:03<00:16, 223MB/s]model.safetensors:  17%|█▋        | 734M/4.40G [00:03<00:16, 222MB/s]model.safetensors:  17%|█▋        | 765M/4.40G [00:03<00:16, 221MB/s]model.safetensors:  18%|█▊        | 797M/4.40G [00:03<00:16, 217MB/s]model.safetensors:  19%|█▉        | 828M/4.40G [00:03<00:15, 227MB/s]model.safetensors:  20%|█▉        | 860M/4.40G [00:04<00:15, 226MB/s]model.safetensors:  20%|██        | 891M/4.40G [00:04<00:15, 233MB/s]model.safetensors:  21%|██        | 923M/4.40G [00:04<00:14, 238MB/s]model.safetensors:  22%|██▏       | 954M/4.40G [00:04<00:14, 239MB/s]model.safetensors:  22%|██▏       | 986M/4.40G [00:04<00:13, 247MB/s]model.safetensors:  23%|██▎       | 1.02G/4.40G [00:04<00:13, 245MB/s]model.safetensors:  24%|██▍       | 1.05G/4.40G [00:04<00:13, 242MB/s]model.safetensors:  25%|██▍       | 1.08G/4.40G [00:04<00:13, 246MB/s]model.safetensors:  25%|██▌       | 1.11G/4.40G [00:05<00:13, 244MB/s]model.safetensors:  26%|██▌       | 1.14G/4.40G [00:05<00:13, 243MB/s]model.safetensors:  27%|██▋       | 1.17G/4.40G [00:05<00:13, 240MB/s]model.safetensors:  27%|██▋       | 1.21G/4.40G [00:05<00:13, 237MB/s]model.safetensors:  28%|██▊       | 1.24G/4.40G [00:05<00:13, 237MB/s]model.safetensors:  29%|██▉       | 1.27G/4.40G [00:05<00:13, 231MB/s]model.safetensors:  30%|██▉       | 1.30G/4.40G [00:05<00:13, 232MB/s]model.safetensors:  30%|███       | 1.33G/4.40G [00:05<00:13, 230MB/s]model.safetensors:  31%|███       | 1.36G/4.40G [00:06<00:13, 233MB/s]model.safetensors:  32%|███▏      | 1.39G/4.40G [00:06<00:12, 237MB/s]model.safetensors:  32%|███▏      | 1.43G/4.40G [00:06<00:12, 239MB/s]model.safetensors:  33%|███▎      | 1.46G/4.40G [00:06<00:12, 233MB/s]model.safetensors:  34%|███▍      | 1.49G/4.40G [00:06<00:12, 233MB/s]model.safetensors:  35%|███▍      | 1.52G/4.40G [00:06<00:12, 228MB/s]model.safetensors:  35%|███▌      | 1.55G/4.40G [00:06<00:12, 225MB/s]model.safetensors:  36%|███▌      | 1.58G/4.40G [00:07<00:12, 220MB/s]model.safetensors:  37%|███▋      | 1.61G/4.40G [00:07<00:12, 226MB/s]model.safetensors:  37%|███▋      | 1.65G/4.40G [00:07<00:12, 219MB/s]model.safetensors:  38%|███▊      | 1.68G/4.40G [00:07<00:12, 215MB/s]model.safetensors:  39%|███▉      | 1.71G/4.40G [00:07<00:12, 222MB/s]model.safetensors:  40%|███▉      | 1.74G/4.40G [00:07<00:11, 226MB/s]model.safetensors:  40%|████      | 1.77G/4.40G [00:07<00:11, 234MB/s]model.safetensors:  41%|████      | 1.80G/4.40G [00:08<00:10, 236MB/s]model.safetensors:  42%|████▏     | 1.84G/4.40G [00:08<00:10, 238MB/s]model.safetensors:  42%|████▏     | 1.87G/4.40G [00:08<00:10, 241MB/s]model.safetensors:  43%|████▎     | 1.90G/4.40G [00:08<00:10, 243MB/s]model.safetensors:  44%|████▍     | 1.93G/4.40G [00:08<00:10, 241MB/s]model.safetensors:  45%|████▍     | 1.96G/4.40G [00:08<00:09, 246MB/s]model.safetensors:  45%|████▌     | 1.99G/4.40G [00:08<00:10, 230MB/s]model.safetensors:  46%|████▌     | 2.02G/4.40G [00:08<00:10, 233MB/s]model.safetensors:  47%|████▋     | 2.06G/4.40G [00:09<00:09, 238MB/s]model.safetensors:  47%|████▋     | 2.09G/4.40G [00:09<00:09, 242MB/s]model.safetensors:  48%|████▊     | 2.12G/4.40G [00:09<00:09, 250MB/s]model.safetensors:  49%|████▉     | 2.15G/4.40G [00:09<00:09, 235MB/s]model.safetensors:  50%|████▉     | 2.18G/4.40G [00:09<00:09, 238MB/s]model.safetensors:  50%|█████     | 2.21G/4.40G [00:09<00:08, 245MB/s]model.safetensors:  51%|█████     | 2.24G/4.40G [00:09<00:08, 249MB/s]model.safetensors:  52%|█████▏    | 2.28G/4.40G [00:09<00:08, 249MB/s]model.safetensors:  52%|█████▏    | 2.31G/4.40G [00:10<00:08, 237MB/s]model.safetensors:  53%|█████▎    | 2.34G/4.40G [00:10<00:11, 181MB/s]model.safetensors:  54%|█████▎    | 2.36G/4.40G [00:10<00:11, 180MB/s]model.safetensors:  54%|█████▍    | 2.38G/4.40G [00:10<00:13, 148MB/s]model.safetensors:  55%|█████▍    | 2.40G/4.40G [00:10<00:13, 148MB/s]model.safetensors:  55%|█████▌    | 2.42G/4.40G [00:11<00:15, 129MB/s]model.safetensors:  56%|█████▌    | 2.44G/4.40G [00:11<00:15, 127MB/s]model.safetensors:  56%|█████▌    | 2.47G/4.40G [00:11<00:12, 150MB/s]model.safetensors:  57%|█████▋    | 2.50G/4.40G [00:11<00:13, 141MB/s]model.safetensors:  57%|█████▋    | 2.52G/4.40G [00:11<00:15, 121MB/s]model.safetensors:  58%|█████▊    | 2.54G/4.40G [00:12<00:15, 122MB/s]model.safetensors:  58%|█████▊    | 2.56G/4.40G [00:12<00:17, 107MB/s]model.safetensors:  59%|█████▊    | 2.58G/4.40G [00:12<00:17, 105MB/s]model.safetensors:  59%|█████▉    | 2.60G/4.40G [00:12<00:14, 122MB/s]model.safetensors:  60%|█████▉    | 2.62G/4.40G [00:12<00:15, 116MB/s]model.safetensors:  60%|██████    | 2.64G/4.40G [00:12<00:15, 116MB/s]model.safetensors:  61%|██████    | 2.66G/4.40G [00:13<00:15, 116MB/s]model.safetensors:  61%|██████    | 2.68G/4.40G [00:13<00:14, 118MB/s]model.safetensors:  62%|██████▏   | 2.72G/4.40G [00:13<00:11, 141MB/s]model.safetensors:  62%|██████▏   | 2.74G/4.40G [00:13<00:10, 153MB/s]model.safetensors:  63%|██████▎   | 2.76G/4.40G [00:13<00:13, 120MB/s]model.safetensors:  63%|██████▎   | 2.78G/4.40G [00:14<00:14, 113MB/s]model.safetensors:  64%|██████▎   | 2.80G/4.40G [00:14<00:15, 104MB/s]model.safetensors:  64%|██████▍   | 2.82G/4.40G [00:14<00:15, 101MB/s]model.safetensors:  65%|██████▍   | 2.84G/4.40G [00:14<00:15, 101MB/s]model.safetensors:  65%|██████▌   | 2.86G/4.40G [00:14<00:15, 98.8MB/s]model.safetensors:  66%|██████▌   | 2.88G/4.40G [00:15<00:15, 98.0MB/s]model.safetensors:  66%|██████▌   | 2.92G/4.40G [00:15<00:12, 121MB/s] model.safetensors:  67%|██████▋   | 2.94G/4.40G [00:15<00:14, 101MB/s]model.safetensors:  67%|██████▋   | 2.96G/4.40G [00:15<00:13, 104MB/s]model.safetensors:  68%|██████▊   | 2.98G/4.40G [00:15<00:11, 121MB/s]model.safetensors:  68%|██████▊   | 3.00G/4.40G [00:16<00:11, 118MB/s]model.safetensors:  69%|██████▊   | 3.02G/4.40G [00:16<00:10, 133MB/s]model.safetensors:  69%|██████▉   | 3.04G/4.40G [00:16<00:09, 139MB/s]model.safetensors:  70%|██████▉   | 3.06G/4.40G [00:16<00:12, 104MB/s]model.safetensors:  70%|███████   | 3.08G/4.40G [00:17<00:15, 87.3MB/s]model.safetensors:  71%|███████   | 3.10G/4.40G [00:17<00:15, 86.1MB/s]model.safetensors:  71%|███████   | 3.12G/4.40G [00:17<00:12, 101MB/s] model.safetensors:  72%|███████▏  | 3.16G/4.40G [00:17<00:09, 127MB/s]model.safetensors:  72%|███████▏  | 3.19G/4.40G [00:17<00:08, 151MB/s]model.safetensors:  73%|███████▎  | 3.22G/4.40G [00:17<00:06, 169MB/s]model.safetensors:  74%|███████▍  | 3.25G/4.40G [00:17<00:06, 187MB/s]model.safetensors:  75%|███████▍  | 3.28G/4.40G [00:18<00:05, 200MB/s]model.safetensors:  75%|███████▌  | 3.31G/4.40G [00:18<00:05, 206MB/s]model.safetensors:  76%|███████▌  | 3.34G/4.40G [00:18<00:04, 213MB/s]model.safetensors:  77%|███████▋  | 3.38G/4.40G [00:18<00:04, 221MB/s]model.safetensors:  77%|███████▋  | 3.41G/4.40G [00:18<00:04, 228MB/s]model.safetensors:  78%|███████▊  | 3.44G/4.40G [00:18<00:04, 233MB/s]model.safetensors:  79%|███████▉  | 3.47G/4.40G [00:18<00:04, 232MB/s]model.safetensors:  80%|███████▉  | 3.50G/4.40G [00:19<00:03, 231MB/s]model.safetensors:  80%|████████  | 3.53G/4.40G [00:19<00:03, 233MB/s]model.safetensors:  81%|████████  | 3.57G/4.40G [00:19<00:03, 236MB/s]model.safetensors:  82%|████████▏ | 3.60G/4.40G [00:19<00:03, 237MB/s]model.safetensors:  82%|████████▏ | 3.63G/4.40G [00:19<00:03, 238MB/s]model.safetensors:  83%|████████▎ | 3.66G/4.40G [00:19<00:03, 222MB/s]model.safetensors:  84%|████████▍ | 3.69G/4.40G [00:19<00:03, 224MB/s]model.safetensors:  85%|████████▍ | 3.72G/4.40G [00:19<00:03, 226MB/s]model.safetensors:  85%|████████▌ | 3.75G/4.40G [00:20<00:02, 225MB/s]model.safetensors:  86%|████████▌ | 3.79G/4.40G [00:20<00:02, 233MB/s]model.safetensors:  87%|████████▋ | 3.82G/4.40G [00:20<00:02, 241MB/s]model.safetensors:  87%|████████▋ | 3.85G/4.40G [00:20<00:02, 244MB/s]model.safetensors:  88%|████████▊ | 3.88G/4.40G [00:20<00:02, 241MB/s]model.safetensors:  89%|████████▉ | 3.91G/4.40G [00:20<00:02, 238MB/s]model.safetensors:  90%|████████▉ | 3.94G/4.40G [00:20<00:01, 242MB/s]model.safetensors:  90%|█████████ | 3.97G/4.40G [00:21<00:01, 230MB/s]model.safetensors:  91%|█████████ | 4.01G/4.40G [00:21<00:01, 223MB/s]model.safetensors:  92%|█████████▏| 4.04G/4.40G [00:21<00:01, 225MB/s]model.safetensors:  92%|█████████▏| 4.07G/4.40G [00:21<00:01, 225MB/s]model.safetensors:  93%|█████████▎| 4.10G/4.40G [00:21<00:01, 224MB/s]model.safetensors:  94%|█████████▍| 4.13G/4.40G [00:21<00:01, 223MB/s]model.safetensors:  95%|█████████▍| 4.16G/4.40G [00:21<00:01, 228MB/s]model.safetensors:  95%|█████████▌| 4.19G/4.40G [00:22<00:00, 234MB/s]model.safetensors:  96%|█████████▌| 4.23G/4.40G [00:22<00:00, 243MB/s]model.safetensors:  97%|█████████▋| 4.26G/4.40G [00:22<00:00, 243MB/s]model.safetensors:  97%|█████████▋| 4.29G/4.40G [00:22<00:00, 245MB/s]model.safetensors:  98%|█████████▊| 4.32G/4.40G [00:22<00:00, 238MB/s]model.safetensors:  99%|█████████▉| 4.35G/4.40G [00:22<00:00, 236MB/s]model.safetensors: 100%|█████████▉| 4.38G/4.40G [00:22<00:00, 226MB/s]model.safetensors: 100%|██████████| 4.40G/4.40G [00:22<00:00, 192MB/s]
[INFO|modeling_utils.py:3259] 2024-02-12 18:44:36,413 >> loading weights file model.safetensors from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--TinyLlama--TinyLlama-1.1B-intermediate-step-715k-1.5T/snapshots/19a81efa07bf28ec81dc4de327776fa00e34cf3f/model.safetensors
[INFO|modeling_utils.py:3984] 2024-02-12 18:44:41,070 >> Some weights of the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 18:44:41,082 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-12 18:44:41,082 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
[2024-02-12 18:44:45,449] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 89539) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_18:44:45
  host      : v021.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 89540)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_18:44:45
  host      : v021.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 89539)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v021: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 18:48:48 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 18:48:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/runs/Feb12_18-48-48_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 18:48:49 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 18:48:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:48:50 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 18:48:50 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:48:50 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]config.json: 100%|██████████| 664/664 [00:00<00:00, 218kB/s]
[INFO|configuration_utils.py:729] 2024-02-12 18:48:50,906 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-12 18:48:51,347 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:48:51,385 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:48:51,390 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:48:51,390 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:48:51,390 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:48:51,391 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/tokenizer_config.json
pytorch_model.bin:   0%|          | 0.00/5.38G [00:00<?, ?B/s]pytorch_model.bin:   0%|          | 10.5M/5.38G [00:00<00:52, 102MB/s]pytorch_model.bin:   0%|          | 21.0M/5.38G [00:00<01:04, 83.4MB/s]pytorch_model.bin:   1%|          | 41.9M/5.38G [00:00<01:08, 77.8MB/s]pytorch_model.bin:   1%|          | 52.4M/5.38G [00:00<02:01, 44.0MB/s]pytorch_model.bin:   1%|▏         | 73.4M/5.38G [00:01<01:27, 60.8MB/s]pytorch_model.bin:   2%|▏         | 83.9M/5.38G [00:01<01:26, 61.2MB/s]pytorch_model.bin:   2%|▏         | 105M/5.38G [00:01<01:19, 66.3MB/s] pytorch_model.bin:   2%|▏         | 115M/5.38G [00:02<01:50, 47.9MB/s]pytorch_model.bin:   3%|▎         | 136M/5.38G [00:02<01:40, 52.0MB/s]pytorch_model.bin:   3%|▎         | 147M/5.38G [00:02<01:37, 53.7MB/s]pytorch_model.bin:   3%|▎         | 157M/5.38G [00:02<01:35, 54.7MB/s]pytorch_model.bin:   3%|▎         | 168M/5.38G [00:03<01:46, 48.9MB/s]pytorch_model.bin:   3%|▎         | 178M/5.38G [00:03<01:42, 50.9MB/s]pytorch_model.bin:   4%|▎         | 189M/5.38G [00:03<01:41, 51.1MB/s]pytorch_model.bin:   4%|▎         | 199M/5.38G [00:03<01:32, 55.8MB/s]pytorch_model.bin:   4%|▍         | 210M/5.38G [00:03<01:33, 55.5MB/s]pytorch_model.bin:   4%|▍         | 231M/5.38G [00:04<01:20, 63.7MB/s]pytorch_model.bin:   4%|▍         | 241M/5.38G [00:04<01:34, 54.2MB/s]pytorch_model.bin:   5%|▍         | 252M/5.38G [00:04<01:43, 49.5MB/s]pytorch_model.bin:   5%|▍         | 262M/5.38G [00:04<02:03, 41.6MB/s]pytorch_model.bin:   5%|▌         | 273M/5.38G [00:05<01:48, 47.2MB/s]pytorch_model.bin:   5%|▌         | 294M/5.38G [00:05<01:27, 58.0MB/s]pytorch_model.bin:   6%|▌         | 304M/5.38G [00:05<01:45, 48.1MB/s]pytorch_model.bin:   6%|▌         | 325M/5.38G [00:05<01:30, 55.7MB/s]pytorch_model.bin:   6%|▋         | 346M/5.38G [00:06<01:56, 43.2MB/s]pytorch_model.bin:   7%|▋         | 357M/5.38G [00:07<02:41, 31.1MB/s]pytorch_model.bin:   7%|▋         | 377M/5.38G [00:07<02:07, 39.1MB/s]pytorch_model.bin:   7%|▋         | 388M/5.38G [00:07<02:00, 41.5MB/s]pytorch_model.bin:   8%|▊         | 409M/5.38G [00:08<01:33, 53.3MB/s]pytorch_model.bin:   8%|▊         | 419M/5.38G [00:08<01:38, 50.5MB/s]pytorch_model.bin:   8%|▊         | 440M/5.38G [00:08<01:27, 56.8MB/s]pytorch_model.bin:   8%|▊         | 451M/5.38G [00:08<01:44, 47.3MB/s]pytorch_model.bin:   9%|▉         | 472M/5.38G [00:09<01:27, 55.9MB/s]pytorch_model.bin:   9%|▉         | 482M/5.38G [00:09<01:33, 52.5MB/s]pytorch_model.bin:   9%|▉         | 493M/5.38G [00:09<01:36, 50.5MB/s]pytorch_model.bin:   9%|▉         | 503M/5.38G [00:10<02:01, 40.1MB/s]pytorch_model.bin:  10%|▉         | 514M/5.38G [00:10<02:24, 33.7MB/s]pytorch_model.bin:  10%|▉         | 524M/5.38G [00:10<02:08, 37.7MB/s]pytorch_model.bin:  10%|▉         | 535M/5.38G [00:10<02:00, 40.1MB/s]pytorch_model.bin:  10%|█         | 545M/5.38G [00:11<01:47, 44.9MB/s]pytorch_model.bin:  11%|█         | 566M/5.38G [00:11<01:20, 60.1MB/s]pytorch_model.bin:  11%|█         | 577M/5.38G [00:11<01:24, 56.9MB/s]pytorch_model.bin:  11%|█         | 598M/5.38G [00:11<01:10, 68.1MB/s]pytorch_model.bin:  11%|█▏        | 608M/5.38G [00:12<01:29, 53.2MB/s]pytorch_model.bin:  12%|█▏        | 629M/5.38G [00:12<01:37, 48.8MB/s]pytorch_model.bin:  12%|█▏        | 640M/5.38G [00:12<01:31, 52.0MB/s]pytorch_model.bin:  12%|█▏        | 650M/5.38G [00:12<01:28, 53.4MB/s]pytorch_model.bin:  12%|█▏        | 661M/5.38G [00:13<01:45, 44.9MB/s]pytorch_model.bin:  13%|█▎        | 682M/5.38G [00:13<01:49, 42.8MB/s]pytorch_model.bin:  13%|█▎        | 692M/5.38G [00:13<01:42, 45.9MB/s]pytorch_model.bin:  13%|█▎        | 713M/5.38G [00:14<02:01, 38.4MB/s]pytorch_model.bin:  13%|█▎        | 724M/5.38G [00:14<01:57, 39.5MB/s]pytorch_model.bin:  14%|█▍        | 744M/5.38G [00:15<01:30, 51.2MB/s]pytorch_model.bin:  14%|█▍        | 755M/5.38G [00:15<01:25, 54.1MB/s]pytorch_model.bin:  14%|█▍        | 776M/5.38G [00:15<01:22, 55.9MB/s]pytorch_model.bin:  15%|█▍        | 786M/5.38G [00:15<01:33, 49.2MB/s]pytorch_model.bin:  15%|█▍        | 807M/5.38G [00:16<01:30, 50.6MB/s]pytorch_model.bin:  15%|█▌        | 818M/5.38G [00:16<01:43, 44.1MB/s]pytorch_model.bin:  16%|█▌        | 839M/5.38G [00:16<01:29, 50.7MB/s]pytorch_model.bin:  16%|█▌        | 849M/5.38G [00:17<01:25, 52.9MB/s]pytorch_model.bin:  16%|█▌        | 870M/5.38G [00:17<01:14, 60.7MB/s]pytorch_model.bin:  16%|█▋        | 881M/5.38G [00:17<01:22, 54.4MB/s]pytorch_model.bin:  17%|█▋        | 902M/5.38G [00:18<01:30, 49.6MB/s]pytorch_model.bin:  17%|█▋        | 912M/5.38G [00:18<01:37, 46.0MB/s]pytorch_model.bin:  17%|█▋        | 933M/5.38G [00:18<01:20, 55.1MB/s]pytorch_model.bin:  18%|█▊        | 944M/5.38G [00:18<01:26, 51.6MB/s]pytorch_model.bin:  18%|█▊        | 954M/5.38G [00:19<01:21, 54.3MB/s]pytorch_model.bin:  18%|█▊        | 965M/5.38G [00:19<01:24, 52.2MB/s]pytorch_model.bin:  18%|█▊        | 986M/5.38G [00:19<01:23, 52.7MB/s]pytorch_model.bin:  19%|█▊        | 996M/5.38G [00:20<01:30, 48.5MB/s]pytorch_model.bin:  19%|█▉        | 1.02G/5.38G [00:20<01:25, 51.3MB/s]pytorch_model.bin:  19%|█▉        | 1.03G/5.38G [00:20<01:34, 46.0MB/s]pytorch_model.bin:  19%|█▉        | 1.05G/5.38G [00:20<01:19, 54.9MB/s]pytorch_model.bin:  20%|█▉        | 1.06G/5.38G [00:21<01:21, 53.4MB/s]pytorch_model.bin:  20%|█▉        | 1.07G/5.38G [00:21<01:29, 48.4MB/s]pytorch_model.bin:  20%|██        | 1.09G/5.38G [00:21<01:12, 59.5MB/s]pytorch_model.bin:  21%|██        | 1.11G/5.38G [00:21<01:07, 63.2MB/s]pytorch_model.bin:  21%|██        | 1.12G/5.38G [00:22<01:14, 57.1MB/s]pytorch_model.bin:  21%|██        | 1.14G/5.38G [00:22<01:18, 53.9MB/s]pytorch_model.bin:  21%|██▏       | 1.15G/5.38G [00:22<01:22, 51.3MB/s]pytorch_model.bin:  22%|██▏       | 1.17G/5.38G [00:23<01:11, 58.9MB/s]pytorch_model.bin:  22%|██▏       | 1.18G/5.38G [00:23<01:19, 52.7MB/s]pytorch_model.bin:  22%|██▏       | 1.21G/5.38G [00:23<01:11, 58.6MB/s]pytorch_model.bin:  23%|██▎       | 1.22G/5.38G [00:24<01:22, 50.3MB/s]pytorch_model.bin:  23%|██▎       | 1.24G/5.38G [00:24<01:16, 54.0MB/s]pytorch_model.bin:  23%|██▎       | 1.25G/5.38G [00:24<01:22, 50.3MB/s]pytorch_model.bin:  23%|██▎       | 1.26G/5.38G [00:24<01:23, 49.3MB/s]pytorch_model.bin:  24%|██▎       | 1.27G/5.38G [00:25<01:31, 45.1MB/s]pytorch_model.bin:  24%|██▍       | 1.29G/5.38G [00:25<01:17, 52.8MB/s]pytorch_model.bin:  24%|██▍       | 1.30G/5.38G [00:25<01:28, 45.9MB/s]pytorch_model.bin:  24%|██▍       | 1.31G/5.38G [00:25<01:23, 48.5MB/s]pytorch_model.bin:  25%|██▍       | 1.32G/5.38G [00:26<01:47, 37.9MB/s]pytorch_model.bin:  25%|██▍       | 1.33G/5.38G [00:26<01:37, 41.6MB/s]pytorch_model.bin:  25%|██▍       | 1.34G/5.38G [00:26<01:22, 49.3MB/s]pytorch_model.bin:  25%|██▌       | 1.35G/5.38G [00:26<01:21, 49.4MB/s]pytorch_model.bin:  25%|██▌       | 1.36G/5.38G [00:27<01:35, 41.9MB/s]pytorch_model.bin:  26%|██▌       | 1.37G/5.38G [00:27<01:24, 47.6MB/s]pytorch_model.bin:  26%|██▌       | 1.38G/5.38G [00:27<01:25, 46.8MB/s]pytorch_model.bin:  26%|██▌       | 1.39G/5.38G [00:27<01:24, 47.1MB/s]pytorch_model.bin:  26%|██▋       | 1.42G/5.38G [00:28<01:44, 38.1MB/s]pytorch_model.bin:  26%|██▋       | 1.43G/5.38G [00:28<01:46, 37.2MB/s]pytorch_model.bin:  27%|██▋       | 1.45G/5.38G [00:29<01:27, 45.0MB/s]pytorch_model.bin:  27%|██▋       | 1.46G/5.38G [00:29<01:32, 42.4MB/s]pytorch_model.bin:  27%|██▋       | 1.48G/5.38G [00:29<01:16, 50.9MB/s]pytorch_model.bin:  28%|██▊       | 1.49G/5.38G [00:29<01:11, 54.1MB/s]pytorch_model.bin:  28%|██▊       | 1.51G/5.38G [00:30<01:26, 44.7MB/s]pytorch_model.bin:  28%|██▊       | 1.52G/5.38G [00:30<01:27, 43.9MB/s]pytorch_model.bin:  28%|██▊       | 1.53G/5.38G [00:30<01:16, 50.2MB/s]pytorch_model.bin:  29%|██▊       | 1.54G/5.38G [00:31<01:26, 44.2MB/s]pytorch_model.bin:  29%|██▉       | 1.55G/5.38G [00:31<01:28, 43.2MB/s]pytorch_model.bin:  29%|██▉       | 1.56G/5.38G [00:31<01:18, 48.6MB/s]pytorch_model.bin:  29%|██▉       | 1.57G/5.38G [00:31<01:21, 46.8MB/s]pytorch_model.bin:  29%|██▉       | 1.58G/5.38G [00:32<01:13, 51.4MB/s]pytorch_model.bin:  30%|██▉       | 1.59G/5.38G [00:32<01:21, 46.3MB/s]pytorch_model.bin:  30%|██▉       | 1.60G/5.38G [00:32<01:30, 41.8MB/s]pytorch_model.bin:  30%|███       | 1.63G/5.38G [00:32<01:05, 57.3MB/s]pytorch_model.bin:  30%|███       | 1.64G/5.38G [00:32<01:04, 58.1MB/s]pytorch_model.bin:  31%|███       | 1.66G/5.38G [00:33<00:55, 67.4MB/s]pytorch_model.bin:  31%|███       | 1.67G/5.38G [00:33<01:05, 57.1MB/s]pytorch_model.bin:  31%|███▏      | 1.69G/5.38G [00:33<00:59, 62.5MB/s]pytorch_model.bin:  32%|███▏      | 1.70G/5.38G [00:34<01:06, 55.7MB/s]pytorch_model.bin:  32%|███▏      | 1.71G/5.38G [00:34<00:58, 62.4MB/s]pytorch_model.bin:  32%|███▏      | 1.72G/5.38G [00:34<00:57, 63.7MB/s]pytorch_model.bin:  32%|███▏      | 1.73G/5.38G [00:34<00:57, 63.2MB/s]pytorch_model.bin:  33%|███▎      | 1.75G/5.38G [00:34<01:07, 54.1MB/s]pytorch_model.bin:  33%|███▎      | 1.76G/5.38G [00:35<01:10, 51.2MB/s]pytorch_model.bin:  33%|███▎      | 1.78G/5.38G [00:35<01:03, 56.7MB/s]pytorch_model.bin:  33%|███▎      | 1.79G/5.38G [00:35<00:58, 61.2MB/s]pytorch_model.bin:  34%|███▎      | 1.81G/5.38G [00:35<00:51, 69.7MB/s]pytorch_model.bin:  34%|███▍      | 1.82G/5.38G [00:36<00:57, 62.2MB/s]pytorch_model.bin:  34%|███▍      | 1.85G/5.38G [00:36<01:08, 52.0MB/s]pytorch_model.bin:  34%|███▍      | 1.86G/5.38G [00:36<01:04, 54.8MB/s]pytorch_model.bin:  35%|███▍      | 1.87G/5.38G [00:36<01:03, 55.3MB/s]pytorch_model.bin:  35%|███▍      | 1.88G/5.38G [00:37<01:03, 54.9MB/s]pytorch_model.bin:  35%|███▌      | 1.90G/5.38G [00:37<01:02, 55.8MB/s]pytorch_model.bin:  35%|███▌      | 1.91G/5.38G [00:37<01:16, 45.3MB/s]pytorch_model.bin:  36%|███▌      | 1.93G/5.38G [00:38<01:19, 43.2MB/s]pytorch_model.bin:  36%|███▌      | 1.94G/5.38G [00:38<01:24, 40.8MB/s]pytorch_model.bin:  36%|███▌      | 1.95G/5.38G [00:38<01:13, 47.0MB/s]pytorch_model.bin:  36%|███▋      | 1.96G/5.38G [00:39<01:21, 42.1MB/s]pytorch_model.bin:  37%|███▋      | 1.97G/5.38G [00:39<01:22, 41.6MB/s]pytorch_model.bin:  37%|███▋      | 1.99G/5.38G [00:39<01:13, 46.1MB/s]pytorch_model.bin:  37%|███▋      | 2.00G/5.38G [00:40<01:16, 44.2MB/s]pytorch_model.bin:  38%|███▊      | 2.02G/5.38G [00:40<01:06, 50.8MB/s]pytorch_model.bin:  38%|███▊      | 2.03G/5.38G [00:40<01:06, 50.2MB/s]pytorch_model.bin:  38%|███▊      | 2.06G/5.38G [00:40<00:59, 56.2MB/s]pytorch_model.bin:  38%|███▊      | 2.07G/5.38G [00:41<00:58, 56.4MB/s]pytorch_model.bin:  39%|███▊      | 2.08G/5.38G [00:41<00:55, 59.6MB/s]pytorch_model.bin:  39%|███▉      | 2.09G/5.38G [00:41<00:54, 60.5MB/s]pytorch_model.bin:  39%|███▉      | 2.10G/5.38G [00:41<01:09, 47.6MB/s]pytorch_model.bin:  39%|███▉      | 2.12G/5.38G [00:42<01:03, 51.2MB/s]pytorch_model.bin:  40%|███▉      | 2.13G/5.38G [00:42<01:12, 44.9MB/s]pytorch_model.bin:  40%|███▉      | 2.15G/5.38G [00:42<01:04, 50.3MB/s]pytorch_model.bin:  40%|████      | 2.16G/5.38G [00:43<01:14, 43.0MB/s]pytorch_model.bin:  41%|████      | 2.18G/5.38G [00:43<01:04, 49.8MB/s]pytorch_model.bin:  41%|████      | 2.20G/5.38G [00:43<00:52, 61.1MB/s]pytorch_model.bin:  41%|████      | 2.21G/5.38G [00:44<01:02, 50.6MB/s]pytorch_model.bin:  41%|████▏     | 2.23G/5.38G [00:44<00:58, 54.2MB/s]pytorch_model.bin:  42%|████▏     | 2.24G/5.38G [00:44<00:54, 57.3MB/s]pytorch_model.bin:  42%|████▏     | 2.26G/5.38G [00:44<01:01, 51.0MB/s]pytorch_model.bin:  42%|████▏     | 2.28G/5.38G [00:45<01:00, 51.4MB/s]pytorch_model.bin:  43%|████▎     | 2.30G/5.38G [00:45<00:49, 62.2MB/s]pytorch_model.bin:  43%|████▎     | 2.31G/5.38G [00:45<00:47, 64.4MB/s]pytorch_model.bin:  43%|████▎     | 2.33G/5.38G [00:45<00:49, 61.8MB/s]pytorch_model.bin:  43%|████▎     | 2.34G/5.38G [00:46<00:59, 50.9MB/s]pytorch_model.bin:  44%|████▍     | 2.36G/5.38G [00:46<00:55, 54.2MB/s]pytorch_model.bin:  44%|████▍     | 2.37G/5.38G [00:46<00:57, 52.8MB/s]pytorch_model.bin:  44%|████▍     | 2.39G/5.38G [00:47<00:53, 56.2MB/s]pytorch_model.bin:  45%|████▍     | 2.40G/5.38G [00:47<01:09, 42.7MB/s]pytorch_model.bin:  45%|████▍     | 2.41G/5.38G [00:47<01:01, 48.1MB/s]pytorch_model.bin:  45%|████▍     | 2.42G/5.38G [00:48<01:09, 42.6MB/s]pytorch_model.bin:  45%|████▌     | 2.43G/5.38G [00:48<01:15, 38.9MB/s]pytorch_model.bin:  46%|████▌     | 2.45G/5.38G [00:48<01:01, 47.7MB/s]pytorch_model.bin:  46%|████▌     | 2.46G/5.38G [00:48<01:00, 48.6MB/s]pytorch_model.bin:  46%|████▌     | 2.49G/5.38G [00:49<00:49, 58.1MB/s]pytorch_model.bin:  47%|████▋     | 2.51G/5.38G [00:49<00:44, 64.2MB/s]pytorch_model.bin:  47%|████▋     | 2.52G/5.38G [00:49<01:01, 46.5MB/s]pytorch_model.bin:  47%|████▋     | 2.54G/5.38G [00:50<00:50, 56.8MB/s]pytorch_model.bin:  47%|████▋     | 2.55G/5.38G [00:50<00:50, 55.8MB/s]pytorch_model.bin:  48%|████▊     | 2.56G/5.38G [00:50<00:51, 55.1MB/s]pytorch_model.bin:  48%|████▊     | 2.57G/5.38G [00:50<00:47, 58.9MB/s]pytorch_model.bin:  48%|████▊     | 2.58G/5.38G [00:50<00:53, 52.1MB/s]pytorch_model.bin:  48%|████▊     | 2.60G/5.38G [00:51<00:47, 58.2MB/s]pytorch_model.bin:  48%|████▊     | 2.61G/5.38G [00:51<00:49, 56.1MB/s]pytorch_model.bin:  49%|████▉     | 2.63G/5.38G [00:51<00:42, 64.8MB/s]pytorch_model.bin:  49%|████▉     | 2.64G/5.38G [00:51<00:41, 65.8MB/s]pytorch_model.bin:  49%|████▉     | 2.66G/5.38G [00:52<00:45, 59.3MB/s]pytorch_model.bin:  50%|████▉     | 2.67G/5.38G [00:52<00:46, 58.7MB/s]pytorch_model.bin:  50%|█████     | 2.69G/5.38G [00:52<00:44, 61.0MB/s]pytorch_model.bin:  50%|█████     | 2.71G/5.38G [00:53<00:52, 51.4MB/s]pytorch_model.bin:  51%|█████     | 2.73G/5.38G [00:53<00:48, 54.5MB/s]pytorch_model.bin:  51%|█████     | 2.74G/5.38G [00:53<00:55, 48.0MB/s]pytorch_model.bin:  51%|█████     | 2.76G/5.38G [00:54<00:52, 50.2MB/s]pytorch_model.bin:  51%|█████▏    | 2.77G/5.38G [00:54<01:07, 38.6MB/s]pytorch_model.bin:  52%|█████▏    | 2.79G/5.38G [00:54<00:58, 44.2MB/s]pytorch_model.bin:  52%|█████▏    | 2.81G/5.38G [00:55<00:44, 58.0MB/s]pytorch_model.bin:  52%|█████▏    | 2.82G/5.38G [00:55<00:51, 49.8MB/s]pytorch_model.bin:  53%|█████▎    | 2.84G/5.38G [00:55<00:49, 51.4MB/s]pytorch_model.bin:  53%|█████▎    | 2.85G/5.38G [00:56<00:55, 45.8MB/s]pytorch_model.bin:  53%|█████▎    | 2.87G/5.38G [00:56<00:42, 59.2MB/s]pytorch_model.bin:  54%|█████▎    | 2.88G/5.38G [00:56<00:42, 59.3MB/s]pytorch_model.bin:  54%|█████▍    | 2.90G/5.38G [00:56<00:36, 67.9MB/s]pytorch_model.bin:  54%|█████▍    | 2.92G/5.38G [00:57<00:42, 58.1MB/s]pytorch_model.bin:  55%|█████▍    | 2.94G/5.38G [00:57<00:39, 62.1MB/s]pytorch_model.bin:  55%|█████▍    | 2.95G/5.38G [00:57<00:38, 63.3MB/s]pytorch_model.bin:  55%|█████▌    | 2.97G/5.38G [00:57<00:43, 56.1MB/s]pytorch_model.bin:  55%|█████▌    | 2.98G/5.38G [00:58<00:40, 58.9MB/s]pytorch_model.bin:  56%|█████▌    | 2.99G/5.38G [00:58<00:36, 65.4MB/s]pytorch_model.bin:  56%|█████▌    | 3.00G/5.38G [00:58<00:45, 52.0MB/s]pytorch_model.bin:  56%|█████▌    | 3.01G/5.38G [00:58<00:45, 51.8MB/s]pytorch_model.bin:  56%|█████▋    | 3.03G/5.38G [00:59<00:39, 59.6MB/s]pytorch_model.bin:  56%|█████▋    | 3.04G/5.38G [00:59<00:48, 48.6MB/s]pytorch_model.bin:  57%|█████▋    | 3.06G/5.38G [00:59<00:46, 50.0MB/s]pytorch_model.bin:  57%|█████▋    | 3.07G/5.38G [01:00<00:50, 46.1MB/s]pytorch_model.bin:  57%|█████▋    | 3.09G/5.38G [01:00<00:39, 57.3MB/s]pytorch_model.bin:  58%|█████▊    | 3.11G/5.38G [01:00<00:32, 68.9MB/s]pytorch_model.bin:  58%|█████▊    | 3.12G/5.38G [01:00<00:32, 68.6MB/s]pytorch_model.bin:  58%|█████▊    | 3.15G/5.38G [01:00<00:34, 65.4MB/s]pytorch_model.bin:  59%|█████▊    | 3.16G/5.38G [01:01<00:35, 62.2MB/s]pytorch_model.bin:  59%|█████▉    | 3.17G/5.38G [01:01<00:38, 57.8MB/s]pytorch_model.bin:  59%|█████▉    | 3.18G/5.38G [01:01<00:39, 55.7MB/s]pytorch_model.bin:  59%|█████▉    | 3.19G/5.38G [01:01<00:42, 52.0MB/s]pytorch_model.bin:  59%|█████▉    | 3.20G/5.38G [01:02<00:40, 53.7MB/s]pytorch_model.bin:  60%|█████▉    | 3.21G/5.38G [01:02<00:40, 54.1MB/s]pytorch_model.bin:  60%|█████▉    | 3.22G/5.38G [01:02<00:37, 57.2MB/s]pytorch_model.bin:  60%|██████    | 3.24G/5.38G [01:02<00:41, 51.6MB/s]pytorch_model.bin:  60%|██████    | 3.25G/5.38G [01:02<00:39, 53.6MB/s]pytorch_model.bin:  61%|██████    | 3.27G/5.38G [01:03<00:31, 66.4MB/s]pytorch_model.bin:  61%|██████    | 3.28G/5.38G [01:03<00:33, 62.3MB/s]pytorch_model.bin:  61%|██████    | 3.29G/5.38G [01:03<00:32, 65.3MB/s]pytorch_model.bin:  61%|██████▏   | 3.30G/5.38G [01:03<00:34, 60.9MB/s]pytorch_model.bin:  62%|██████▏   | 3.31G/5.38G [01:04<00:40, 51.7MB/s]pytorch_model.bin:  62%|██████▏   | 3.33G/5.38G [01:04<00:30, 67.7MB/s]pytorch_model.bin:  62%|██████▏   | 3.34G/5.38G [01:04<00:32, 62.3MB/s]pytorch_model.bin:  62%|██████▏   | 3.36G/5.38G [01:04<00:32, 62.9MB/s]pytorch_model.bin:  63%|██████▎   | 3.37G/5.38G [01:04<00:41, 48.9MB/s]pytorch_model.bin:  63%|██████▎   | 3.38G/5.38G [01:05<00:37, 52.9MB/s]pytorch_model.bin:  63%|██████▎   | 3.40G/5.38G [01:05<00:39, 50.6MB/s]pytorch_model.bin:  63%|██████▎   | 3.41G/5.38G [01:05<00:35, 55.3MB/s]pytorch_model.bin:  63%|██████▎   | 3.42G/5.38G [01:05<00:37, 51.8MB/s]pytorch_model.bin:  64%|██████▎   | 3.43G/5.38G [01:06<00:37, 51.7MB/s]pytorch_model.bin:  64%|██████▍   | 3.45G/5.38G [01:06<00:33, 57.6MB/s]pytorch_model.bin:  64%|██████▍   | 3.46G/5.38G [01:06<00:37, 50.6MB/s]pytorch_model.bin:  65%|██████▍   | 3.48G/5.38G [01:06<00:32, 58.1MB/s]pytorch_model.bin:  65%|██████▍   | 3.49G/5.38G [01:07<00:35, 52.6MB/s]pytorch_model.bin:  65%|██████▌   | 3.51G/5.38G [01:07<00:40, 46.5MB/s]pytorch_model.bin:  65%|██████▌   | 3.52G/5.38G [01:08<00:41, 45.3MB/s]pytorch_model.bin:  66%|██████▌   | 3.54G/5.38G [01:08<00:37, 49.7MB/s]pytorch_model.bin:  66%|██████▌   | 3.55G/5.38G [01:08<00:41, 44.5MB/s]pytorch_model.bin:  66%|██████▋   | 3.58G/5.38G [01:09<00:37, 47.7MB/s]pytorch_model.bin:  67%|██████▋   | 3.59G/5.38G [01:09<00:36, 49.8MB/s]pytorch_model.bin:  67%|██████▋   | 3.61G/5.38G [01:09<00:30, 59.0MB/s]pytorch_model.bin:  67%|██████▋   | 3.62G/5.38G [01:09<00:35, 49.5MB/s]pytorch_model.bin:  68%|██████▊   | 3.64G/5.38G [01:10<00:40, 43.1MB/s]pytorch_model.bin:  68%|██████▊   | 3.65G/5.38G [01:10<00:39, 43.7MB/s]pytorch_model.bin:  68%|██████▊   | 3.67G/5.38G [01:10<00:31, 53.9MB/s]pytorch_model.bin:  68%|██████▊   | 3.68G/5.38G [01:11<00:32, 52.8MB/s]pytorch_model.bin:  69%|██████▉   | 3.70G/5.38G [01:11<00:32, 51.0MB/s]pytorch_model.bin:  69%|██████▉   | 3.72G/5.38G [01:11<00:25, 64.3MB/s]pytorch_model.bin:  69%|██████▉   | 3.73G/5.38G [01:11<00:26, 63.0MB/s]pytorch_model.bin:  70%|██████▉   | 3.75G/5.38G [01:12<00:23, 67.9MB/s]pytorch_model.bin:  70%|██████▉   | 3.76G/5.38G [01:12<00:27, 58.3MB/s]pytorch_model.bin:  70%|███████   | 3.79G/5.38G [01:12<00:25, 62.6MB/s]pytorch_model.bin:  71%|███████   | 3.80G/5.38G [01:12<00:25, 61.6MB/s]pytorch_model.bin:  71%|███████   | 3.82G/5.38G [01:13<00:24, 64.5MB/s]pytorch_model.bin:  71%|███████   | 3.83G/5.38G [01:13<00:28, 54.5MB/s]pytorch_model.bin:  71%|███████▏  | 3.85G/5.38G [01:13<00:26, 58.6MB/s]pytorch_model.bin:  72%|███████▏  | 3.86G/5.38G [01:14<00:26, 56.8MB/s]pytorch_model.bin:  72%|███████▏  | 3.88G/5.38G [01:14<00:28, 53.5MB/s]pytorch_model.bin:  72%|███████▏  | 3.89G/5.38G [01:14<00:32, 46.5MB/s]pytorch_model.bin:  72%|███████▏  | 3.90G/5.38G [01:15<00:31, 47.3MB/s]pytorch_model.bin:  73%|███████▎  | 3.91G/5.38G [01:15<00:28, 52.4MB/s]pytorch_model.bin:  73%|███████▎  | 3.92G/5.38G [01:15<00:26, 55.4MB/s]pytorch_model.bin:  73%|███████▎  | 3.93G/5.38G [01:15<00:30, 48.3MB/s]pytorch_model.bin:  73%|███████▎  | 3.94G/5.38G [01:16<00:36, 39.8MB/s]pytorch_model.bin:  73%|███████▎  | 3.95G/5.38G [01:16<00:36, 38.7MB/s]pytorch_model.bin:  74%|███████▍  | 3.97G/5.38G [01:16<00:32, 42.8MB/s]pytorch_model.bin:  74%|███████▍  | 3.98G/5.38G [01:17<00:36, 38.0MB/s]pytorch_model.bin:  74%|███████▍  | 4.01G/5.38G [01:17<00:32, 42.4MB/s]pytorch_model.bin:  75%|███████▍  | 4.02G/5.38G [01:17<00:30, 44.4MB/s]pytorch_model.bin:  75%|███████▍  | 4.04G/5.38G [01:18<00:26, 51.7MB/s]pytorch_model.bin:  75%|███████▌  | 4.05G/5.38G [01:18<00:23, 57.9MB/s]pytorch_model.bin:  75%|███████▌  | 4.06G/5.38G [01:18<00:22, 57.7MB/s]pytorch_model.bin:  76%|███████▌  | 4.07G/5.38G [01:18<00:27, 47.8MB/s]pytorch_model.bin:  76%|███████▌  | 4.09G/5.38G [01:18<00:22, 56.8MB/s]pytorch_model.bin:  76%|███████▌  | 4.10G/5.38G [01:19<00:28, 44.4MB/s]pytorch_model.bin:  76%|███████▋  | 4.11G/5.38G [01:19<00:26, 48.6MB/s]pytorch_model.bin:  77%|███████▋  | 4.12G/5.38G [01:19<00:26, 47.2MB/s]pytorch_model.bin:  77%|███████▋  | 4.13G/5.38G [01:19<00:26, 48.0MB/s]pytorch_model.bin:  77%|███████▋  | 4.15G/5.38G [01:20<00:22, 54.5MB/s]pytorch_model.bin:  77%|███████▋  | 4.16G/5.38G [01:20<00:24, 49.6MB/s]pytorch_model.bin:  78%|███████▊  | 4.18G/5.38G [01:20<00:21, 54.7MB/s]pytorch_model.bin:  78%|███████▊  | 4.19G/5.38G [01:21<00:25, 46.7MB/s]pytorch_model.bin:  78%|███████▊  | 4.22G/5.38G [01:21<00:20, 56.2MB/s]pytorch_model.bin:  78%|███████▊  | 4.23G/5.38G [01:21<00:20, 55.6MB/s]pytorch_model.bin:  79%|███████▉  | 4.25G/5.38G [01:21<00:16, 67.4MB/s]pytorch_model.bin:  79%|███████▉  | 4.26G/5.38G [01:22<00:20, 54.5MB/s]pytorch_model.bin:  79%|███████▉  | 4.28G/5.38G [01:22<00:18, 58.6MB/s]pytorch_model.bin:  80%|███████▉  | 4.29G/5.38G [01:22<00:20, 53.1MB/s]pytorch_model.bin:  80%|████████  | 4.31G/5.38G [01:22<00:17, 61.5MB/s]pytorch_model.bin:  80%|████████  | 4.32G/5.38G [01:23<00:19, 54.1MB/s]pytorch_model.bin:  81%|████████  | 4.34G/5.38G [01:23<00:19, 54.5MB/s]pytorch_model.bin:  81%|████████  | 4.36G/5.38G [01:23<00:15, 65.2MB/s]pytorch_model.bin:  81%|████████  | 4.37G/5.38G [01:24<00:17, 58.1MB/s]pytorch_model.bin:  82%|████████▏ | 4.39G/5.38G [01:24<00:22, 44.5MB/s]pytorch_model.bin:  82%|████████▏ | 4.40G/5.38G [01:24<00:21, 46.1MB/s]pytorch_model.bin:  82%|████████▏ | 4.42G/5.38G [01:25<00:19, 50.4MB/s]pytorch_model.bin:  82%|████████▏ | 4.44G/5.38G [01:25<00:25, 37.3MB/s]pytorch_model.bin:  83%|████████▎ | 4.45G/5.38G [01:25<00:21, 43.2MB/s]pytorch_model.bin:  83%|████████▎ | 4.46G/5.38G [01:26<00:26, 35.4MB/s]pytorch_model.bin:  83%|████████▎ | 4.47G/5.38G [01:26<00:23, 39.2MB/s]pytorch_model.bin:  83%|████████▎ | 4.49G/5.38G [01:26<00:17, 51.6MB/s]pytorch_model.bin:  84%|████████▎ | 4.50G/5.38G [01:27<00:17, 50.2MB/s]pytorch_model.bin:  84%|████████▍ | 4.52G/5.38G [01:27<00:17, 50.6MB/s]pytorch_model.bin:  84%|████████▍ | 4.53G/5.38G [01:27<00:15, 54.1MB/s]pytorch_model.bin:  85%|████████▍ | 4.55G/5.38G [01:27<00:13, 62.1MB/s]pytorch_model.bin:  85%|████████▍ | 4.56G/5.38G [01:28<00:16, 49.5MB/s]pytorch_model.bin:  85%|████████▌ | 4.58G/5.38G [01:28<00:13, 58.6MB/s]pytorch_model.bin:  85%|████████▌ | 4.59G/5.38G [01:28<00:16, 49.2MB/s]pytorch_model.bin:  86%|████████▌ | 4.61G/5.38G [01:29<00:13, 57.8MB/s]pytorch_model.bin:  86%|████████▌ | 4.62G/5.38G [01:29<00:13, 54.8MB/s]pytorch_model.bin:  86%|████████▋ | 4.65G/5.38G [01:29<00:11, 62.5MB/s]pytorch_model.bin:  87%|████████▋ | 4.67G/5.38G [01:29<00:11, 62.0MB/s]pytorch_model.bin:  87%|████████▋ | 4.68G/5.38G [01:30<00:11, 60.3MB/s]pytorch_model.bin:  87%|████████▋ | 4.70G/5.38G [01:30<00:11, 60.6MB/s]pytorch_model.bin:  87%|████████▋ | 4.71G/5.38G [01:30<00:14, 46.6MB/s]pytorch_model.bin:  88%|████████▊ | 4.73G/5.38G [01:31<00:11, 55.6MB/s]pytorch_model.bin:  88%|████████▊ | 4.74G/5.38G [01:31<00:12, 53.4MB/s]pytorch_model.bin:  88%|████████▊ | 4.76G/5.38G [01:31<00:12, 51.8MB/s]pytorch_model.bin:  89%|████████▊ | 4.77G/5.38G [01:32<00:12, 50.2MB/s]pytorch_model.bin:  89%|████████▉ | 4.79G/5.38G [01:32<00:09, 60.7MB/s]pytorch_model.bin:  89%|████████▉ | 4.80G/5.38G [01:32<00:09, 58.7MB/s]pytorch_model.bin:  90%|████████▉ | 4.82G/5.38G [01:32<00:08, 69.4MB/s]pytorch_model.bin:  90%|████████▉ | 4.83G/5.38G [01:32<00:07, 69.0MB/s]pytorch_model.bin:  90%|█████████ | 4.85G/5.38G [01:33<00:08, 59.4MB/s]pytorch_model.bin:  90%|█████████ | 4.87G/5.38G [01:33<00:12, 42.0MB/s]pytorch_model.bin:  91%|█████████ | 4.89G/5.38G [01:34<00:09, 50.3MB/s]pytorch_model.bin:  91%|█████████ | 4.90G/5.38G [01:34<00:10, 46.8MB/s]pytorch_model.bin:  91%|█████████▏| 4.92G/5.38G [01:34<00:07, 60.0MB/s]pytorch_model.bin:  92%|█████████▏| 4.93G/5.38G [01:34<00:07, 61.5MB/s]pytorch_model.bin:  92%|█████████▏| 4.95G/5.38G [01:34<00:06, 65.3MB/s]pytorch_model.bin:  92%|█████████▏| 4.96G/5.38G [01:35<00:07, 57.6MB/s]pytorch_model.bin:  92%|█████████▏| 4.97G/5.38G [01:35<00:08, 48.2MB/s]pytorch_model.bin:  93%|█████████▎| 4.98G/5.38G [01:35<00:08, 46.0MB/s]pytorch_model.bin:  93%|█████████▎| 5.00G/5.38G [01:36<00:08, 47.5MB/s]pytorch_model.bin:  93%|█████████▎| 5.01G/5.38G [01:36<00:07, 47.9MB/s]pytorch_model.bin:  93%|█████████▎| 5.03G/5.38G [01:37<00:09, 36.3MB/s]pytorch_model.bin:  94%|█████████▎| 5.04G/5.38G [01:37<00:11, 28.7MB/s]pytorch_model.bin:  94%|█████████▍| 5.06G/5.38G [01:38<00:08, 37.8MB/s]pytorch_model.bin:  94%|█████████▍| 5.08G/5.38G [01:38<00:07, 42.2MB/s]pytorch_model.bin:  95%|█████████▍| 5.10G/5.38G [01:38<00:05, 50.4MB/s]pytorch_model.bin:  95%|█████████▍| 5.11G/5.38G [01:38<00:05, 53.1MB/s]pytorch_model.bin:  95%|█████████▌| 5.13G/5.38G [01:39<00:04, 57.7MB/s]pytorch_model.bin:  95%|█████████▌| 5.14G/5.38G [01:39<00:04, 53.4MB/s]pytorch_model.bin:  96%|█████████▌| 5.16G/5.38G [01:39<00:04, 52.6MB/s]pytorch_model.bin:  96%|█████████▌| 5.17G/5.38G [01:39<00:04, 52.6MB/s]pytorch_model.bin:  96%|█████████▋| 5.19G/5.38G [01:40<00:03, 57.5MB/s]pytorch_model.bin:  97%|█████████▋| 5.20G/5.38G [01:40<00:02, 62.4MB/s]pytorch_model.bin:  97%|█████████▋| 5.22G/5.38G [01:40<00:02, 73.1MB/s]pytorch_model.bin:  97%|█████████▋| 5.23G/5.38G [01:40<00:02, 68.8MB/s]pytorch_model.bin:  98%|█████████▊| 5.25G/5.38G [01:40<00:01, 76.0MB/s]pytorch_model.bin:  98%|█████████▊| 5.26G/5.38G [01:41<00:01, 76.8MB/s]pytorch_model.bin:  98%|█████████▊| 5.27G/5.38G [01:41<00:01, 70.0MB/s]pytorch_model.bin:  98%|█████████▊| 5.28G/5.38G [01:41<00:01, 59.9MB/s]pytorch_model.bin:  99%|█████████▊| 5.31G/5.38G [01:41<00:01, 60.2MB/s]pytorch_model.bin:  99%|█████████▊| 5.32G/5.38G [01:42<00:01, 42.9MB/s]pytorch_model.bin:  99%|█████████▉| 5.34G/5.38G [01:42<00:00, 50.6MB/s]pytorch_model.bin:  99%|█████████▉| 5.35G/5.38G [01:42<00:00, 46.5MB/s]pytorch_model.bin: 100%|█████████▉| 5.37G/5.38G [01:43<00:00, 49.8MB/s]pytorch_model.bin: 100%|█████████▉| 5.38G/5.38G [01:43<00:00, 49.3MB/s]pytorch_model.bin: 100%|██████████| 5.38G/5.38G [01:43<00:00, 51.9MB/s]
[INFO|modeling_utils.py:3259] 2024-02-12 18:50:36,152 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-12 18:50:42,147 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 18:50:42,160 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-12 18:50:42,172 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 480, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 472, in preprocess_function
    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2805, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2891, in _call_one
    return self.batch_encode_plus(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3073, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2710, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
[2024-02-12 18:50:49,088] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 101682) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_18:50:49
  host      : v021.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 101683)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_18:50:49
  host      : v021.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 101682)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v021: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 18:52:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 18:52:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/runs/Feb12_18-52-00_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 18:52:00 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 18:52:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:52:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 18:52:02 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:52:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 18:52:02,245 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-12 18:52:02,247 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

tokenizer_config.json:   0%|          | 0.00/1.89k [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 1.89k/1.89k [00:00<00:00, 620kB/s]
tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 19.7MB/s]
tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 49.0MB/s]
added_tokens.json:   0%|          | 0.00/94.0 [00:00<?, ?B/s]added_tokens.json: 100%|██████████| 94.0/94.0 [00:00<00:00, 37.7kB/s]
special_tokens_map.json:   0%|          | 0.00/675 [00:00<?, ?B/s]special_tokens_map.json: 100%|██████████| 675/675 [00:00<00:00, 234kB/s]
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:52:03,135 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:52:03,135 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:52:03,135 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:52:03,136 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:52:03,136 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-12 18:52:03,227 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-12 18:52:03,291 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-12 18:52:03,292 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-12 18:52:09,438 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 18:52:09,445 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-12 18:52:09,459 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
02/12/2024 18:52:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:01, 2079.49 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 3214.99 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:00<00:00, 3147.02 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 3629.62 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 3259.94 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f2cee68a76931ee7.arrow
02/12/2024 18:52:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f2cee68a76931ee7.arrow
Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 4660.48 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-cc3740de5157e924.arrow
02/12/2024 18:52:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-cc3740de5157e924.arrow
Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 5157.50 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 4678.88 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 3344.44 examples/s]
02/12/2024 18:52:12 - INFO - __main__ - Sample 81 of the training set: {'sentence1': 'But Mitsubishi Tokyo Financial ( JP : 8306 : news , chart , profile ) declined 3,000 yen , or 0.65 percent , to 456,000 yen .', 'sentence2': 'Sumitomo Mitsui Financial ( JP : 8316 : news , chart , profile ) was down 2.5 percent at 198,000 yen .', 'label': 0, 'idx': 94, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1205, 341, 1169, 431, 29603, 20377, 4231, 273, 1455, 313, 435, 29925, 584, 29871, 29947, 29941, 29900, 29953, 584, 9763, 1919, 8727, 1919, 8722, 1723, 4845, 1312, 29871, 29941, 29892, 29900, 29900, 29900, 343, 264, 1919, 470, 29871, 29900, 29889, 29953, 29945, 10151, 1919, 304, 29871, 29946, 29945, 29953, 29892, 29900, 29900, 29900, 343, 264, 869, 1, 6991, 277, 10730, 341, 1169, 1481, 4231, 273, 1455, 313, 435, 29925, 584, 29871, 29947, 29941, 29896, 29953, 584, 9763, 1919, 8727, 1919, 8722, 1723, 471, 1623, 29871, 29906, 29889, 29945, 10151, 472, 29871, 29896, 29929, 29947, 29892, 29900, 29900, 29900, 343, 264, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 18:52:12 - INFO - __main__ - Sample 14 of the training set: {'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .', 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .', 'label': 0, 'idx': 15, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 12842, 990, 29891, 940, 466, 1358, 1919, 2343, 310, 278, 1887, 766, 1901, 5190, 1919, 1497, 278, 11182, 471, 19436, 29871, 29941, 29947, 28134, 869, 1, 450, 2343, 310, 278, 1887, 766, 1901, 5190, 1919, 12842, 990, 29891, 940, 466, 1358, 1919, 1497, 278, 11182, 7156, 750, 5229, 304, 540, 287, 2654, 5040, 26068, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 18:52:12 - INFO - __main__ - Sample 3 of the training set: {'sentence1': 'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .', 'sentence2': 'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 826, 618, 29871, 29900, 29941, 29941, 29945, 402, 11490, 1919, 11090, 29358, 892, 701, 29871, 29896, 29929, 274, 1237, 1919, 470, 29871, 29946, 29889, 29946, 1273, 1919, 472, 319, 395, 29871, 29946, 29889, 29945, 29953, 1919, 2534, 8859, 731, 263, 2407, 1880, 310, 319, 395, 29871, 29946, 29889, 29945, 29955, 869, 1, 11090, 29358, 12500, 287, 29871, 29906, 29900, 274, 1237, 1919, 470, 29871, 29946, 29889, 29953, 1273, 1919, 304, 731, 263, 2407, 14382, 1880, 472, 319, 395, 29871, 29946, 29889, 29945, 29955, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 4926.97 examples/s]
Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 26.1MB/s]
02/12/2024 18:52:12 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 24.1MB/s]
[INFO|trainer.py:737] 2024-02-12 18:52:15,339 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-12 18:52:16,292 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-12 18:52:16,294 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-12 18:52:16,294 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-12 18:52:16,295 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-12 18:52:16,295 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-12 18:52:16,295 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-12 18:52:16,295 >>   Total optimization steps = 21
[INFO|trainer.py:1756] 2024-02-12 18:52:16,297 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/21 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  5%|▍         | 1/21 [00:02<00:59,  2.97s/it] 10%|▉         | 2/21 [00:03<00:31,  1.67s/it] 14%|█▍        | 3/21 [00:04<00:23,  1.29s/it] 19%|█▉        | 4/21 [00:05<00:18,  1.11s/it] 24%|██▍       | 5/21 [00:06<00:16,  1.03s/it] 29%|██▊       | 6/21 [00:07<00:14,  1.03it/s] 33%|███▎      | 7/21 [00:07<00:12,  1.08it/s] 38%|███▊      | 8/21 [00:08<00:11,  1.10it/s] 43%|████▎     | 9/21 [00:09<00:10,  1.12it/s] 48%|████▊     | 10/21 [00:10<00:09,  1.14it/s] 52%|█████▏    | 11/21 [00:11<00:08,  1.15it/s] 57%|█████▋    | 12/21 [00:12<00:07,  1.16it/s] 62%|██████▏   | 13/21 [00:13<00:06,  1.16it/s] 67%|██████▋   | 14/21 [00:13<00:06,  1.16it/s] 71%|███████▏  | 15/21 [00:14<00:05,  1.17it/s] 76%|███████▌  | 16/21 [00:15<00:04,  1.17it/s] 81%|████████  | 17/21 [00:16<00:03,  1.18it/s] 86%|████████▌ | 18/21 [00:17<00:02,  1.18it/s] 90%|█████████ | 19/21 [00:18<00:01,  1.17it/s] 95%|█████████▌| 20/21 [00:19<00:00,  1.17it/s]100%|██████████| 21/21 [00:19<00:00,  1.18it/s][INFO|trainer.py:1988] 2024-02-12 18:52:36,239 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 20.0621, 'train_samples_per_second': 14.954, 'train_steps_per_second': 1.047, 'train_loss': 1.017921629406157, 'epoch': 3.0}
                                               100%|██████████| 21/21 [00:20<00:00,  1.18it/s]100%|██████████| 21/21 [00:20<00:00,  1.05it/s]
[INFO|trainer.py:2985] 2024-02-12 18:52:36,379 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments
[INFO|configuration_utils.py:473] 2024-02-12 18:52:36,385 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/config.json
[INFO|modeling_utils.py:2462] 2024-02-12 18:53:03,334 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-12 18:53:03,336 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-12 18:53:03,337 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.0179
  train_runtime            = 0:00:20.06
  train_samples            =        100
  train_samples_per_second =     14.954
  train_steps_per_second   =      1.047
02/12/2024 18:53:03 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-12 18:53:03,393 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-12 18:53:03,395 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-12 18:53:03,395 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-12 18:53:03,395 >>   Batch size = 8
  0%|          | 0/7 [00:00<?, ?it/s] 29%|██▊       | 2/7 [00:00<00:00,  7.23it/s] 43%|████▎     | 3/7 [00:00<00:00,  5.60it/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.67it/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.31it/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.22it/s]100%|██████████| 7/7 [00:01<00:00,  4.22it/s]100%|██████████| 7/7 [00:01<00:00,  4.23it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.74
  eval_combined_score     =     0.7888
  eval_f1                 =     0.8375
  eval_loss               =     0.7217
  eval_runtime            = 0:00:01.90
  eval_samples            =        100
  eval_samples_per_second =     52.511
  eval_steps_per_second   =      3.676
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 18:55:26 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 18:55:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/runs/Feb12_18-55-25_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 279, in main
    raise ValueError(
ValueError: Output directory (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments) already exists and is not empty. Use --overwrite_output_dir to overcome.
02/12/2024 18:55:26 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Traceback (most recent call last):
  File "run_glue.py", line 652, in <module>
    main()
  File "run_glue.py", line 279, in main
    raise ValueError(
ValueError: Output directory (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments) already exists and is not empty. Use --overwrite_output_dir to overcome.
[2024-02-12 18:55:31,433] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 15706) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-12_18:55:31
  host      : v021.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 15707)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-12_18:55:31
  host      : v021.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 15706)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v021: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 18:57:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 18:57:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/runs/Feb12_18-57-21_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 18:57:22 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/12/2024 18:57:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:57:23 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 18:57:23 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 18:57:23 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 18:57:23,719 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-12 18:57:23,721 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:57:23,754 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:57:23,754 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:57:23,754 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:57:23,754 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 18:57:23,754 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-12 18:57:23,811 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-12 18:57:23,846 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|logging.py:314] 2024-02-12 18:57:24,828 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3984] 2024-02-12 18:57:29,130 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 18:57:29,131 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
02/12/2024 18:57:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d53d7a09499f5bfb.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
02/12/2024 18:57:29 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-12e393c323965c42.arrow
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
02/12/2024 18:57:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-65fa5a7617106458.arrow
Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 2940.19 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 3041.96 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 2928.25 examples/s]
[WARNING|modeling_utils.py:3996] 2024-02-12 18:57:30,877 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
02/12/2024 18:57:31 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 18:57:31 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 18:57:31 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 18:57:31 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-12 18:57:34,815 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-12 18:57:34,933 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-12 18:57:34,946 >>   Num examples = 3,668
[INFO|trainer.py:1749] 2024-02-12 18:57:34,946 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-12 18:57:34,946 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-12 18:57:34,946 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-12 18:57:34,946 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-12 18:57:34,947 >>   Total optimization steps = 690
[INFO|trainer.py:1756] 2024-02-12 18:57:34,947 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/690 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/690 [00:02<30:01,  2.62s/it]  0%|          | 2/690 [00:03<17:46,  1.55s/it]  0%|          | 3/690 [00:04<13:58,  1.22s/it]  1%|          | 4/690 [00:05<12:16,  1.07s/it]  1%|          | 5/690 [00:05<11:23,  1.00it/s]  1%|          | 6/690 [00:06<10:48,  1.05it/s]  1%|          | 7/690 [00:07<10:26,  1.09it/s]  1%|          | 8/690 [00:08<10:13,  1.11it/s]  1%|▏         | 9/690 [00:09<10:00,  1.13it/s]  1%|▏         | 10/690 [00:10<09:54,  1.14it/s]  2%|▏         | 11/690 [00:11<09:51,  1.15it/s]  2%|▏         | 12/690 [00:11<09:44,  1.16it/s]  2%|▏         | 13/690 [00:12<09:42,  1.16it/s]  2%|▏         | 14/690 [00:13<09:39,  1.17it/s]  2%|▏         | 15/690 [00:14<09:39,  1.16it/s]  2%|▏         | 16/690 [00:15<09:35,  1.17it/s]  2%|▏         | 17/690 [00:16<09:31,  1.18it/s]  3%|▎         | 18/690 [00:17<09:30,  1.18it/s]  3%|▎         | 19/690 [00:17<09:32,  1.17it/s]  3%|▎         | 20/690 [00:18<09:30,  1.17it/s]  3%|▎         | 21/690 [00:19<09:26,  1.18it/s]  3%|▎         | 22/690 [00:20<09:27,  1.18it/s]  3%|▎         | 23/690 [00:21<09:23,  1.18it/s]  3%|▎         | 24/690 [00:22<09:25,  1.18it/s]  4%|▎         | 25/690 [00:22<09:23,  1.18it/s]  4%|▍         | 26/690 [00:23<09:21,  1.18it/s]  4%|▍         | 27/690 [00:24<09:22,  1.18it/s]  4%|▍         | 28/690 [00:25<09:25,  1.17it/s]  4%|▍         | 29/690 [00:26<09:20,  1.18it/s]  4%|▍         | 30/690 [00:27<09:22,  1.17it/s]  4%|▍         | 31/690 [00:28<09:21,  1.17it/s]  5%|▍         | 32/690 [00:28<09:21,  1.17it/s]  5%|▍         | 33/690 [00:29<09:19,  1.17it/s]  5%|▍         | 34/690 [00:30<09:18,  1.18it/s]  5%|▌         | 35/690 [00:31<09:18,  1.17it/s]  5%|▌         | 36/690 [00:32<09:17,  1.17it/s]  5%|▌         | 37/690 [00:33<09:14,  1.18it/s]  6%|▌         | 38/690 [00:34<09:16,  1.17it/s]  6%|▌         | 39/690 [00:34<09:16,  1.17it/s]  6%|▌         | 40/690 [00:35<09:15,  1.17it/s]  6%|▌         | 41/690 [00:36<09:10,  1.18it/s]  6%|▌         | 42/690 [00:37<09:11,  1.18it/s]  6%|▌         | 43/690 [00:38<09:06,  1.18it/s]  6%|▋         | 44/690 [00:39<09:04,  1.19it/s]  7%|▋         | 45/690 [00:39<09:03,  1.19it/s]  7%|▋         | 46/690 [00:40<09:03,  1.19it/s]  7%|▋         | 47/690 [00:41<09:03,  1.18it/s]  7%|▋         | 48/690 [00:42<09:05,  1.18it/s]  7%|▋         | 49/690 [00:43<09:06,  1.17it/s]  7%|▋         | 50/690 [00:44<09:02,  1.18it/s]  7%|▋         | 51/690 [00:45<09:02,  1.18it/s]  8%|▊         | 52/690 [00:45<09:02,  1.18it/s]  8%|▊         | 53/690 [00:46<08:59,  1.18it/s]  8%|▊         | 54/690 [00:47<09:00,  1.18it/s]  8%|▊         | 55/690 [00:48<09:02,  1.17it/s]  8%|▊         | 56/690 [00:49<08:59,  1.18it/s]  8%|▊         | 57/690 [00:50<08:59,  1.17it/s]  8%|▊         | 58/690 [00:51<08:55,  1.18it/s]  9%|▊         | 59/690 [00:51<08:52,  1.18it/s]  9%|▊         | 60/690 [00:52<08:51,  1.18it/s]  9%|▉         | 61/690 [00:53<08:54,  1.18it/s]  9%|▉         | 62/690 [00:54<08:52,  1.18it/s]  9%|▉         | 63/690 [00:55<08:53,  1.17it/s]  9%|▉         | 64/690 [00:56<08:52,  1.18it/s]  9%|▉         | 65/690 [00:56<08:51,  1.18it/s] 10%|▉         | 66/690 [00:57<08:51,  1.18it/s] 10%|▉         | 67/690 [00:58<08:49,  1.18it/s] 10%|▉         | 68/690 [00:59<08:49,  1.18it/s] 10%|█         | 69/690 [01:00<08:48,  1.17it/s] 10%|█         | 70/690 [01:01<08:48,  1.17it/s] 10%|█         | 71/690 [01:02<08:47,  1.17it/s] 10%|█         | 72/690 [01:02<08:45,  1.18it/s] 11%|█         | 73/690 [01:03<08:45,  1.18it/s] 11%|█         | 74/690 [01:04<08:41,  1.18it/s] 11%|█         | 75/690 [01:05<08:39,  1.18it/s] 11%|█         | 76/690 [01:06<08:39,  1.18it/s] 11%|█         | 77/690 [01:07<08:38,  1.18it/s] 11%|█▏        | 78/690 [01:08<08:43,  1.17it/s] 11%|█▏        | 79/690 [01:08<08:40,  1.17it/s] 12%|█▏        | 80/690 [01:09<08:38,  1.18it/s] 12%|█▏        | 81/690 [01:10<08:37,  1.18it/s] 12%|█▏        | 82/690 [01:11<08:37,  1.17it/s] 12%|█▏        | 83/690 [01:12<08:36,  1.18it/s] 12%|█▏        | 84/690 [01:13<08:35,  1.18it/s] 12%|█▏        | 85/690 [01:13<08:33,  1.18it/s] 12%|█▏        | 86/690 [01:14<08:37,  1.17it/s] 13%|█▎        | 87/690 [01:15<08:35,  1.17it/s] 13%|█▎        | 88/690 [01:16<08:35,  1.17it/s] 13%|█▎        | 89/690 [01:17<08:33,  1.17it/s] 13%|█▎        | 90/690 [01:18<08:34,  1.17it/s] 13%|█▎        | 91/690 [01:19<08:30,  1.17it/s] 13%|█▎        | 92/690 [01:19<08:30,  1.17it/s] 13%|█▎        | 93/690 [01:20<08:29,  1.17it/s] 14%|█▎        | 94/690 [01:21<08:29,  1.17it/s] 14%|█▍        | 95/690 [01:22<08:27,  1.17it/s] 14%|█▍        | 96/690 [01:23<08:26,  1.17it/s] 14%|█▍        | 97/690 [01:24<08:25,  1.17it/s] 14%|█▍        | 98/690 [01:25<08:24,  1.17it/s] 14%|█▍        | 99/690 [01:25<08:25,  1.17it/s] 14%|█▍        | 100/690 [01:26<08:22,  1.17it/s] 15%|█▍        | 101/690 [01:27<08:21,  1.18it/s] 15%|█▍        | 102/690 [01:28<08:20,  1.18it/s] 15%|█▍        | 103/690 [01:29<08:21,  1.17it/s] 15%|█▌        | 104/690 [01:30<08:18,  1.18it/s] 15%|█▌        | 105/690 [01:31<08:19,  1.17it/s] 15%|█▌        | 106/690 [01:31<08:19,  1.17it/s] 16%|█▌        | 107/690 [01:32<08:17,  1.17it/s] 16%|█▌        | 108/690 [01:33<08:15,  1.17it/s] 16%|█▌        | 109/690 [01:34<08:12,  1.18it/s] 16%|█▌        | 110/690 [01:35<08:12,  1.18it/s] 16%|█▌        | 111/690 [01:36<08:15,  1.17it/s] 16%|█▌        | 112/690 [01:36<08:10,  1.18it/s] 16%|█▋        | 113/690 [01:37<08:12,  1.17it/s] 17%|█▋        | 114/690 [01:38<08:09,  1.18it/s] 17%|█▋        | 115/690 [01:39<08:10,  1.17it/s] 17%|█▋        | 116/690 [01:40<08:09,  1.17it/s] 17%|█▋        | 117/690 [01:41<08:10,  1.17it/s] 17%|█▋        | 118/690 [01:42<08:10,  1.17it/s] 17%|█▋        | 119/690 [01:42<08:08,  1.17it/s] 17%|█▋        | 120/690 [01:43<08:09,  1.16it/s] 18%|█▊        | 121/690 [01:44<08:07,  1.17it/s] 18%|█▊        | 122/690 [01:45<08:05,  1.17it/s] 18%|█▊        | 123/690 [01:46<08:00,  1.18it/s] 18%|█▊        | 124/690 [01:47<08:00,  1.18it/s] 18%|█▊        | 125/690 [01:48<08:03,  1.17it/s] 18%|█▊        | 126/690 [01:48<08:03,  1.17it/s] 18%|█▊        | 127/690 [01:49<08:00,  1.17it/s] 19%|█▊        | 128/690 [01:50<08:00,  1.17it/s] 19%|█▊        | 129/690 [01:51<07:55,  1.18it/s] 19%|█▉        | 130/690 [01:52<07:57,  1.17it/s] 19%|█▉        | 131/690 [01:53<07:56,  1.17it/s] 19%|█▉        | 132/690 [01:54<07:56,  1.17it/s] 19%|█▉        | 133/690 [01:54<07:56,  1.17it/s] 19%|█▉        | 134/690 [01:55<07:56,  1.17it/s] 20%|█▉        | 135/690 [01:56<07:55,  1.17it/s] 20%|█▉        | 136/690 [01:57<07:50,  1.18it/s] 20%|█▉        | 137/690 [01:58<07:51,  1.17it/s] 20%|██        | 138/690 [01:59<07:48,  1.18it/s] 20%|██        | 139/690 [02:00<07:50,  1.17it/s] 20%|██        | 140/690 [02:00<07:49,  1.17it/s] 20%|██        | 141/690 [02:01<07:48,  1.17it/s] 21%|██        | 142/690 [02:02<07:45,  1.18it/s] 21%|██        | 143/690 [02:03<07:47,  1.17it/s] 21%|██        | 144/690 [02:04<07:44,  1.18it/s] 21%|██        | 145/690 [02:05<07:43,  1.18it/s] 21%|██        | 146/690 [02:05<07:40,  1.18it/s] 21%|██▏       | 147/690 [02:06<07:42,  1.17it/s] 21%|██▏       | 148/690 [02:07<07:42,  1.17it/s] 22%|██▏       | 149/690 [02:08<07:39,  1.18it/s] 22%|██▏       | 150/690 [02:09<07:41,  1.17it/s] 22%|██▏       | 151/690 [02:10<07:38,  1.18it/s] 22%|██▏       | 152/690 [02:11<07:38,  1.17it/s] 22%|██▏       | 153/690 [02:11<07:38,  1.17it/s] 22%|██▏       | 154/690 [02:12<07:38,  1.17it/s] 22%|██▏       | 155/690 [02:13<07:38,  1.17it/s] 23%|██▎       | 156/690 [02:14<07:37,  1.17it/s] 23%|██▎       | 157/690 [02:15<07:34,  1.17it/s] 23%|██▎       | 158/690 [02:16<07:35,  1.17it/s] 23%|██▎       | 159/690 [02:17<07:34,  1.17it/s] 23%|██▎       | 160/690 [02:17<07:33,  1.17it/s] 23%|██▎       | 161/690 [02:18<07:32,  1.17it/s] 23%|██▎       | 162/690 [02:19<07:32,  1.17it/s] 24%|██▎       | 163/690 [02:20<07:31,  1.17it/s] 24%|██▍       | 164/690 [02:21<07:29,  1.17it/s] 24%|██▍       | 165/690 [02:22<07:29,  1.17it/s] 24%|██▍       | 166/690 [02:23<07:25,  1.18it/s] 24%|██▍       | 167/690 [02:23<07:26,  1.17it/s] 24%|██▍       | 168/690 [02:24<07:23,  1.18it/s] 24%|██▍       | 169/690 [02:25<07:23,  1.17it/s] 25%|██▍       | 170/690 [02:26<07:23,  1.17it/s] 25%|██▍       | 171/690 [02:27<07:22,  1.17it/s] 25%|██▍       | 172/690 [02:28<07:19,  1.18it/s] 25%|██▌       | 173/690 [02:29<07:18,  1.18it/s] 25%|██▌       | 174/690 [02:29<07:17,  1.18it/s] 25%|██▌       | 175/690 [02:30<07:16,  1.18it/s] 26%|██▌       | 176/690 [02:31<07:16,  1.18it/s] 26%|██▌       | 177/690 [02:32<07:19,  1.17it/s] 26%|██▌       | 178/690 [02:33<07:19,  1.16it/s] 26%|██▌       | 179/690 [02:34<07:15,  1.17it/s] 26%|██▌       | 180/690 [02:34<07:14,  1.17it/s] 26%|██▌       | 181/690 [02:35<07:15,  1.17it/s] 26%|██▋       | 182/690 [02:36<07:13,  1.17it/s] 27%|██▋       | 183/690 [02:37<07:11,  1.17it/s] 27%|██▋       | 184/690 [02:38<07:12,  1.17it/s] 27%|██▋       | 185/690 [02:39<07:10,  1.17it/s] 27%|██▋       | 186/690 [02:40<07:07,  1.18it/s] 27%|██▋       | 187/690 [02:40<07:08,  1.17it/s] 27%|██▋       | 188/690 [02:41<07:08,  1.17it/s] 27%|██▋       | 189/690 [02:42<07:08,  1.17it/s] 28%|██▊       | 190/690 [02:43<07:09,  1.17it/s] 28%|██▊       | 191/690 [02:44<07:05,  1.17it/s] 28%|██▊       | 192/690 [02:45<07:04,  1.17it/s] 28%|██▊       | 193/690 [02:46<07:04,  1.17it/s] 28%|██▊       | 194/690 [02:46<07:02,  1.17it/s] 28%|██▊       | 195/690 [02:47<07:02,  1.17it/s] 28%|██▊       | 196/690 [02:48<07:02,  1.17it/s] 29%|██▊       | 197/690 [02:49<06:58,  1.18it/s] 29%|██▊       | 198/690 [02:50<06:59,  1.17it/s] 29%|██▉       | 199/690 [02:51<06:58,  1.17it/s] 29%|██▉       | 200/690 [02:52<06:57,  1.17it/s] 29%|██▉       | 201/690 [02:52<06:58,  1.17it/s] 29%|██▉       | 202/690 [02:53<06:58,  1.17it/s] 29%|██▉       | 203/690 [02:54<06:56,  1.17it/s] 30%|██▉       | 204/690 [02:55<06:56,  1.17it/s] 30%|██▉       | 205/690 [02:56<06:55,  1.17it/s] 30%|██▉       | 206/690 [02:57<06:54,  1.17it/s] 30%|███       | 207/690 [02:58<06:53,  1.17it/s] 30%|███       | 208/690 [02:58<06:54,  1.16it/s] 30%|███       | 209/690 [02:59<06:53,  1.16it/s] 30%|███       | 210/690 [03:00<06:49,  1.17it/s] 31%|███       | 211/690 [03:01<06:51,  1.16it/s] 31%|███       | 212/690 [03:02<06:49,  1.17it/s] 31%|███       | 213/690 [03:03<06:47,  1.17it/s] 31%|███       | 214/690 [03:04<06:46,  1.17it/s] 31%|███       | 215/690 [03:04<06:46,  1.17it/s] 31%|███▏      | 216/690 [03:05<06:46,  1.17it/s] 31%|███▏      | 217/690 [03:06<06:46,  1.16it/s] 32%|███▏      | 218/690 [03:07<06:41,  1.17it/s] 32%|███▏      | 219/690 [03:08<06:43,  1.17it/s] 32%|███▏      | 220/690 [03:09<06:43,  1.16it/s] 32%|███▏      | 221/690 [03:10<06:40,  1.17it/s] 32%|███▏      | 222/690 [03:10<06:42,  1.16it/s] 32%|███▏      | 223/690 [03:11<06:39,  1.17it/s] 32%|███▏      | 224/690 [03:12<06:38,  1.17it/s] 33%|███▎      | 225/690 [03:13<06:35,  1.18it/s] 33%|███▎      | 226/690 [03:14<06:34,  1.18it/s] 33%|███▎      | 227/690 [03:15<06:32,  1.18it/s] 33%|███▎      | 228/690 [03:16<06:33,  1.17it/s] 33%|███▎      | 229/690 [03:16<06:33,  1.17it/s] 33%|███▎      | 230/690 [03:17<06:34,  1.17it/s] 33%|███▎      | 231/690 [03:18<06:30,  1.18it/s] 34%|███▎      | 232/690 [03:19<06:30,  1.17it/s] 34%|███▍      | 233/690 [03:20<06:29,  1.17it/s] 34%|███▍      | 234/690 [03:21<06:29,  1.17it/s] 34%|███▍      | 235/690 [03:22<06:29,  1.17it/s] 34%|███▍      | 236/690 [03:22<06:27,  1.17it/s] 34%|███▍      | 237/690 [03:23<06:26,  1.17it/s] 34%|███▍      | 238/690 [03:24<06:26,  1.17it/s] 35%|███▍      | 239/690 [03:25<06:23,  1.18it/s] 35%|███▍      | 240/690 [03:26<06:26,  1.17it/s] 35%|███▍      | 241/690 [03:27<06:23,  1.17it/s] 35%|███▌      | 242/690 [03:27<06:21,  1.17it/s] 35%|███▌      | 243/690 [03:28<06:21,  1.17it/s] 35%|███▌      | 244/690 [03:29<06:17,  1.18it/s] 36%|███▌      | 245/690 [03:30<06:20,  1.17it/s] 36%|███▌      | 246/690 [03:31<06:17,  1.18it/s] 36%|███▌      | 247/690 [03:32<06:18,  1.17it/s] 36%|███▌      | 248/690 [03:33<06:17,  1.17it/s] 36%|███▌      | 249/690 [03:33<06:17,  1.17it/s] 36%|███▌      | 250/690 [03:34<06:18,  1.16it/s] 36%|███▋      | 251/690 [03:35<06:14,  1.17it/s] 37%|███▋      | 252/690 [03:36<06:15,  1.17it/s] 37%|███▋      | 253/690 [03:37<06:15,  1.16it/s] 37%|███▋      | 254/690 [03:38<06:15,  1.16it/s] 37%|███▋      | 255/690 [03:39<06:12,  1.17it/s] 37%|███▋      | 256/690 [03:39<06:10,  1.17it/s] 37%|███▋      | 257/690 [03:40<06:09,  1.17it/s] 37%|███▋      | 258/690 [03:41<06:08,  1.17it/s] 38%|███▊      | 259/690 [03:42<06:07,  1.17it/s] 38%|███▊      | 260/690 [03:43<06:05,  1.18it/s] 38%|███▊      | 261/690 [03:44<06:06,  1.17it/s] 38%|███▊      | 262/690 [03:45<06:05,  1.17it/s] 38%|███▊      | 263/690 [03:45<06:03,  1.17it/s] 38%|███▊      | 264/690 [03:46<06:02,  1.17it/s] 38%|███▊      | 265/690 [03:47<06:04,  1.17it/s] 39%|███▊      | 266/690 [03:48<06:03,  1.17it/s] 39%|███▊      | 267/690 [03:49<06:01,  1.17it/s] 39%|███▉      | 268/690 [03:50<05:59,  1.17it/s] 39%|███▉      | 269/690 [03:51<06:00,  1.17it/s] 39%|███▉      | 270/690 [03:51<05:58,  1.17it/s] 39%|███▉      | 271/690 [03:52<05:59,  1.17it/s] 39%|███▉      | 272/690 [03:53<05:58,  1.17it/s] 40%|███▉      | 273/690 [03:54<05:55,  1.17it/s] 40%|███▉      | 274/690 [03:55<05:54,  1.17it/s] 40%|███▉      | 275/690 [03:56<05:54,  1.17it/s] 40%|████      | 276/690 [03:57<05:53,  1.17it/s] 40%|████      | 277/690 [03:57<05:52,  1.17it/s] 40%|████      | 278/690 [03:58<05:50,  1.18it/s] 40%|████      | 279/690 [03:59<05:50,  1.17it/s] 41%|████      | 280/690 [04:00<05:48,  1.17it/s] 41%|████      | 281/690 [04:01<05:48,  1.17it/s] 41%|████      | 282/690 [04:02<05:46,  1.18it/s] 41%|████      | 283/690 [04:02<05:46,  1.17it/s] 41%|████      | 284/690 [04:03<05:44,  1.18it/s] 41%|████▏     | 285/690 [04:04<05:43,  1.18it/s] 41%|████▏     | 286/690 [04:05<05:43,  1.18it/s] 42%|████▏     | 287/690 [04:06<05:43,  1.17it/s] 42%|████▏     | 288/690 [04:07<05:41,  1.18it/s] 42%|████▏     | 289/690 [04:08<05:38,  1.18it/s] 42%|████▏     | 290/690 [04:08<05:39,  1.18it/s] 42%|████▏     | 291/690 [04:09<05:37,  1.18it/s] 42%|████▏     | 292/690 [04:10<05:38,  1.17it/s] 42%|████▏     | 293/690 [04:11<05:38,  1.17it/s] 43%|████▎     | 294/690 [04:12<05:39,  1.17it/s] 43%|████▎     | 295/690 [04:13<05:36,  1.17it/s] 43%|████▎     | 296/690 [04:14<05:37,  1.17it/s] 43%|████▎     | 297/690 [04:14<05:36,  1.17it/s] 43%|████▎     | 298/690 [04:15<05:36,  1.17it/s] 43%|████▎     | 299/690 [04:16<05:34,  1.17it/s] 43%|████▎     | 300/690 [04:17<05:34,  1.17it/s] 44%|████▎     | 301/690 [04:18<05:33,  1.17it/s] 44%|████▍     | 302/690 [04:19<05:33,  1.16it/s] 44%|████▍     | 303/690 [04:20<05:31,  1.17it/s] 44%|████▍     | 304/690 [04:20<05:30,  1.17it/s] 44%|████▍     | 305/690 [04:21<05:30,  1.17it/s] 44%|████▍     | 306/690 [04:22<05:28,  1.17it/s] 44%|████▍     | 307/690 [04:23<05:28,  1.17it/s] 45%|████▍     | 308/690 [04:24<05:29,  1.16it/s] 45%|████▍     | 309/690 [04:25<05:28,  1.16it/s] 45%|████▍     | 310/690 [04:26<05:24,  1.17it/s] 45%|████▌     | 311/690 [04:26<05:23,  1.17it/s] 45%|████▌     | 312/690 [04:27<05:23,  1.17it/s] 45%|████▌     | 313/690 [04:28<05:20,  1.17it/s] 46%|████▌     | 314/690 [04:29<05:22,  1.16it/s] 46%|████▌     | 315/690 [04:30<05:21,  1.17it/s] 46%|████▌     | 316/690 [04:31<05:19,  1.17it/s] 46%|████▌     | 317/690 [04:32<05:19,  1.17it/s] 46%|████▌     | 318/690 [04:32<05:18,  1.17it/s] 46%|████▌     | 319/690 [04:33<05:16,  1.17it/s] 46%|████▋     | 320/690 [04:34<05:15,  1.17it/s] 47%|████▋     | 321/690 [04:35<05:14,  1.17it/s] 47%|████▋     | 322/690 [04:36<05:13,  1.17it/s] 47%|████▋     | 323/690 [04:37<05:12,  1.17it/s] 47%|████▋     | 324/690 [04:38<05:12,  1.17it/s] 47%|████▋     | 325/690 [04:38<05:14,  1.16it/s] 47%|████▋     | 326/690 [04:39<05:13,  1.16it/s] 47%|████▋     | 327/690 [04:40<05:13,  1.16it/s] 48%|████▊     | 328/690 [04:41<05:12,  1.16it/s] 48%|████▊     | 329/690 [04:42<05:11,  1.16it/s] 48%|████▊     | 330/690 [04:43<05:09,  1.16it/s] 48%|████▊     | 331/690 [04:44<05:09,  1.16it/s] 48%|████▊     | 332/690 [04:44<05:06,  1.17it/s] 48%|████▊     | 333/690 [04:45<05:07,  1.16it/s] 48%|████▊     | 334/690 [04:46<05:04,  1.17it/s] 49%|████▊     | 335/690 [04:47<05:04,  1.17it/s] 49%|████▊     | 336/690 [04:48<05:00,  1.18it/s] 49%|████▉     | 337/690 [04:49<05:01,  1.17it/s] 49%|████▉     | 338/690 [04:50<04:59,  1.17it/s] 49%|████▉     | 339/690 [04:50<04:58,  1.18it/s] 49%|████▉     | 340/690 [04:51<04:58,  1.17it/s] 49%|████▉     | 341/690 [04:52<04:59,  1.16it/s] 50%|████▉     | 342/690 [04:53<04:55,  1.18it/s] 50%|████▉     | 343/690 [04:54<04:56,  1.17it/s] 50%|████▉     | 344/690 [04:55<04:56,  1.17it/s] 50%|█████     | 345/690 [04:55<04:54,  1.17it/s] 50%|█████     | 346/690 [04:56<04:54,  1.17it/s] 50%|█████     | 347/690 [04:57<04:53,  1.17it/s] 50%|█████     | 348/690 [04:58<04:53,  1.16it/s] 51%|█████     | 349/690 [04:59<04:52,  1.17it/s] 51%|█████     | 350/690 [05:00<04:51,  1.17it/s] 51%|█████     | 351/690 [05:01<04:49,  1.17it/s] 51%|█████     | 352/690 [05:01<04:48,  1.17it/s] 51%|█████     | 353/690 [05:02<04:46,  1.18it/s] 51%|█████▏    | 354/690 [05:03<04:45,  1.18it/s] 51%|█████▏    | 355/690 [05:04<04:46,  1.17it/s] 52%|█████▏    | 356/690 [05:05<04:46,  1.17it/s] 52%|█████▏    | 357/690 [05:06<04:46,  1.16it/s] 52%|█████▏    | 358/690 [05:07<04:43,  1.17it/s] 52%|█████▏    | 359/690 [05:07<04:42,  1.17it/s] 52%|█████▏    | 360/690 [05:08<04:40,  1.18it/s] 52%|█████▏    | 361/690 [05:09<04:40,  1.17it/s] 52%|█████▏    | 362/690 [05:10<04:40,  1.17it/s] 53%|█████▎    | 363/690 [05:11<04:38,  1.17it/s] 53%|█████▎    | 364/690 [05:12<04:38,  1.17it/s] 53%|█████▎    | 365/690 [05:13<04:38,  1.17it/s] 53%|█████▎    | 366/690 [05:13<04:36,  1.17it/s] 53%|█████▎    | 367/690 [05:14<04:36,  1.17it/s] 53%|█████▎    | 368/690 [05:15<04:35,  1.17it/s] 53%|█████▎    | 369/690 [05:16<04:36,  1.16it/s] 54%|█████▎    | 370/690 [05:17<04:34,  1.16it/s] 54%|█████▍    | 371/690 [05:18<04:31,  1.17it/s] 54%|█████▍    | 372/690 [05:19<04:31,  1.17it/s] 54%|█████▍    | 373/690 [05:19<04:33,  1.16it/s] 54%|█████▍    | 374/690 [05:20<04:31,  1.16it/s] 54%|█████▍    | 375/690 [05:21<04:29,  1.17it/s] 54%|█████▍    | 376/690 [05:22<04:27,  1.17it/s] 55%|█████▍    | 377/690 [05:23<04:27,  1.17it/s] 55%|█████▍    | 378/690 [05:24<04:25,  1.17it/s] 55%|█████▍    | 379/690 [05:25<04:26,  1.17it/s] 55%|█████▌    | 380/690 [05:25<04:23,  1.17it/s] 55%|█████▌    | 381/690 [05:26<04:24,  1.17it/s] 55%|█████▌    | 382/690 [05:27<04:22,  1.17it/s] 56%|█████▌    | 383/690 [05:28<04:22,  1.17it/s] 56%|█████▌    | 384/690 [05:29<04:19,  1.18it/s] 56%|█████▌    | 385/690 [05:30<04:20,  1.17it/s] 56%|█████▌    | 386/690 [05:31<04:20,  1.17it/s] 56%|█████▌    | 387/690 [05:31<04:18,  1.17it/s] 56%|█████▌    | 388/690 [05:32<04:17,  1.17it/s] 56%|█████▋    | 389/690 [05:33<04:17,  1.17it/s] 57%|█████▋    | 390/690 [05:34<04:16,  1.17it/s] 57%|█████▋    | 391/690 [05:35<04:15,  1.17it/s] 57%|█████▋    | 392/690 [05:36<04:13,  1.17it/s] 57%|█████▋    | 393/690 [05:37<04:14,  1.17it/s] 57%|█████▋    | 394/690 [05:37<04:11,  1.18it/s] 57%|█████▋    | 395/690 [05:38<04:11,  1.17it/s] 57%|█████▋    | 396/690 [05:39<04:09,  1.18it/s] 58%|█████▊    | 397/690 [05:40<04:10,  1.17it/s] 58%|█████▊    | 398/690 [05:41<04:07,  1.18it/s] 58%|█████▊    | 399/690 [05:42<04:09,  1.17it/s] 58%|█████▊    | 400/690 [05:42<04:08,  1.17it/s] 58%|█████▊    | 401/690 [05:43<04:06,  1.17it/s] 58%|█████▊    | 402/690 [05:44<04:06,  1.17it/s] 58%|█████▊    | 403/690 [05:45<04:05,  1.17it/s] 59%|█████▊    | 404/690 [05:46<04:03,  1.17it/s] 59%|█████▊    | 405/690 [05:47<04:03,  1.17it/s] 59%|█████▉    | 406/690 [05:48<04:03,  1.17it/s] 59%|█████▉    | 407/690 [05:48<04:02,  1.16it/s] 59%|█████▉    | 408/690 [05:49<04:01,  1.17it/s] 59%|█████▉    | 409/690 [05:50<04:02,  1.16it/s] 59%|█████▉    | 410/690 [05:51<03:58,  1.17it/s] 60%|█████▉    | 411/690 [05:52<03:58,  1.17it/s] 60%|█████▉    | 412/690 [05:53<03:57,  1.17it/s] 60%|█████▉    | 413/690 [05:54<03:55,  1.18it/s] 60%|██████    | 414/690 [05:54<03:56,  1.17it/s] 60%|██████    | 415/690 [05:55<03:53,  1.18it/s] 60%|██████    | 416/690 [05:56<03:53,  1.17it/s] 60%|██████    | 417/690 [05:57<03:52,  1.17it/s] 61%|██████    | 418/690 [05:58<03:51,  1.17it/s] 61%|██████    | 419/690 [05:59<03:49,  1.18it/s] 61%|██████    | 420/690 [06:00<03:51,  1.17it/s] 61%|██████    | 421/690 [06:00<03:48,  1.18it/s] 61%|██████    | 422/690 [06:01<03:49,  1.17it/s] 61%|██████▏   | 423/690 [06:02<03:47,  1.17it/s] 61%|██████▏   | 424/690 [06:03<03:48,  1.17it/s] 62%|██████▏   | 425/690 [06:04<03:47,  1.17it/s] 62%|██████▏   | 426/690 [06:05<03:45,  1.17it/s] 62%|██████▏   | 427/690 [06:06<03:44,  1.17it/s] 62%|██████▏   | 428/690 [06:06<03:42,  1.18it/s] 62%|██████▏   | 429/690 [06:07<03:41,  1.18it/s] 62%|██████▏   | 430/690 [06:08<03:41,  1.17it/s] 62%|██████▏   | 431/690 [06:09<03:40,  1.17it/s] 63%|██████▎   | 432/690 [06:10<03:39,  1.18it/s] 63%|██████▎   | 433/690 [06:11<03:40,  1.17it/s] 63%|██████▎   | 434/690 [06:12<03:38,  1.17it/s] 63%|██████▎   | 435/690 [06:12<03:38,  1.17it/s] 63%|██████▎   | 436/690 [06:13<03:35,  1.18it/s] 63%|██████▎   | 437/690 [06:14<03:35,  1.17it/s] 63%|██████▎   | 438/690 [06:15<03:34,  1.18it/s] 64%|██████▎   | 439/690 [06:16<03:32,  1.18it/s] 64%|██████▍   | 440/690 [06:17<03:31,  1.18it/s] 64%|██████▍   | 441/690 [06:17<03:31,  1.18it/s] 64%|██████▍   | 442/690 [06:18<03:32,  1.17it/s] 64%|██████▍   | 443/690 [06:19<03:31,  1.17it/s] 64%|██████▍   | 444/690 [06:20<03:30,  1.17it/s] 64%|██████▍   | 445/690 [06:21<03:30,  1.17it/s] 65%|██████▍   | 446/690 [06:22<03:28,  1.17it/s] 65%|██████▍   | 447/690 [06:23<03:27,  1.17it/s] 65%|██████▍   | 448/690 [06:23<03:26,  1.17it/s] 65%|██████▌   | 449/690 [06:24<03:25,  1.17it/s] 65%|██████▌   | 450/690 [06:25<03:23,  1.18it/s] 65%|██████▌   | 451/690 [06:26<03:23,  1.17it/s] 66%|██████▌   | 452/690 [06:27<03:21,  1.18it/s] 66%|██████▌   | 453/690 [06:28<03:21,  1.17it/s] 66%|██████▌   | 454/690 [06:29<03:20,  1.18it/s] 66%|██████▌   | 455/690 [06:29<03:19,  1.18it/s] 66%|██████▌   | 456/690 [06:30<03:18,  1.18it/s] 66%|██████▌   | 457/690 [06:31<03:18,  1.18it/s] 66%|██████▋   | 458/690 [06:32<03:17,  1.18it/s] 67%|██████▋   | 459/690 [06:33<03:16,  1.17it/s] 67%|██████▋   | 460/690 [06:34<03:15,  1.18it/s] 67%|██████▋   | 461/690 [06:34<03:14,  1.18it/s] 67%|██████▋   | 462/690 [06:35<03:13,  1.18it/s] 67%|██████▋   | 463/690 [06:36<03:13,  1.17it/s] 67%|██████▋   | 464/690 [06:37<03:11,  1.18it/s] 67%|██████▋   | 465/690 [06:38<03:12,  1.17it/s] 68%|██████▊   | 466/690 [06:39<03:11,  1.17it/s] 68%|██████▊   | 467/690 [06:40<03:10,  1.17it/s] 68%|██████▊   | 468/690 [06:40<03:08,  1.18it/s] 68%|██████▊   | 469/690 [06:41<03:08,  1.17it/s] 68%|██████▊   | 470/690 [06:42<03:07,  1.17it/s] 68%|██████▊   | 471/690 [06:43<03:07,  1.17it/s] 68%|██████▊   | 472/690 [06:44<03:05,  1.17it/s] 69%|██████▊   | 473/690 [06:45<03:04,  1.17it/s] 69%|██████▊   | 474/690 [06:46<03:03,  1.18it/s] 69%|██████▉   | 475/690 [06:46<03:03,  1.17it/s] 69%|██████▉   | 476/690 [06:47<03:02,  1.17it/s] 69%|██████▉   | 477/690 [06:48<03:01,  1.17it/s] 69%|██████▉   | 478/690 [06:49<03:01,  1.17it/s] 69%|██████▉   | 479/690 [06:50<02:59,  1.18it/s] 70%|██████▉   | 480/690 [06:51<02:58,  1.17it/s] 70%|██████▉   | 481/690 [06:52<02:58,  1.17it/s] 70%|██████▉   | 482/690 [06:52<02:56,  1.18it/s] 70%|███████   | 483/690 [06:53<02:56,  1.17it/s] 70%|███████   | 484/690 [06:54<02:56,  1.17it/s] 70%|███████   | 485/690 [06:55<02:56,  1.16it/s] 70%|███████   | 486/690 [06:56<02:54,  1.17it/s] 71%|███████   | 487/690 [06:57<02:53,  1.17it/s] 71%|███████   | 488/690 [06:58<02:51,  1.18it/s] 71%|███████   | 489/690 [06:58<02:51,  1.18it/s] 71%|███████   | 490/690 [06:59<02:50,  1.17it/s] 71%|███████   | 491/690 [07:00<02:48,  1.18it/s] 71%|███████▏  | 492/690 [07:01<02:49,  1.17it/s] 71%|███████▏  | 493/690 [07:02<02:48,  1.17it/s] 72%|███████▏  | 494/690 [07:03<02:47,  1.17it/s] 72%|███████▏  | 495/690 [07:03<02:46,  1.17it/s] 72%|███████▏  | 496/690 [07:04<02:45,  1.17it/s] 72%|███████▏  | 49{'loss': 0.6291, 'learning_rate': 1.3768115942028985e-05, 'epoch': 2.17}
7/690 [07:05<02:43,  1.18it/s] 72%|███████▏  | 498/690 [07:06<02:42,  1.18it/s] 72%|███████▏  | 499/690 [07:07<02:42,  1.17it/s] 72%|███████▏  | 500/690 [07:08<02:41,  1.18it/s]                                                  72%|███████▏  | 500/690 [07:08<02:41,  1.18it/s][INFO|trainer.py:2985] 2024-02-12 19:04:43,411 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-12 19:04:43,433 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-12 19:05:07,176 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-12 19:05:07,178 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-12 19:05:07,179 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tmp-checkpoint-500/special_tokens_map.json
 73%|███████▎  | 501/690 [08:17<1:07:38, 21.47s/it] 73%|███████▎  | 502/690 [08:18<47:53, 15.28s/it]   73%|███████▎  | 503/690 [08:19<34:09, 10.96s/it] 73%|███████▎  | 504/690 [08:20<24:33,  7.92s/it] 73%|███████▎  | 505/690 [08:21<17:54,  5.81s/it] 73%|███████▎  | 506/690 [08:22<13:14,  4.32s/it] 73%|███████▎  | 507/690 [08:22<09:58,  3.27s/it] 74%|███████▎  | 508/690 [08:23<07:43,  2.55s/it] 74%|███████▍  | 509/690 [08:24<06:10,  2.05s/it] 74%|███████▍  | 510/690 [08:25<05:03,  1.69s/it] 74%|███████▍  | 511/690 [08:26<04:16,  1.43s/it] 74%|███████▍  | 512/690 [08:27<03:43,  1.26s/it] 74%|███████▍  | 513/690 [08:28<03:21,  1.14s/it] 74%|███████▍  | 514/690 [08:28<03:04,  1.05s/it] 75%|███████▍  | 515/690 [08:29<02:54,  1.00it/s] 75%|███████▍  | 516/690 [08:30<02:45,  1.05it/s] 75%|███████▍  | 517/690 [08:31<02:38,  1.09it/s] 75%|███████▌  | 518/690 [08:32<02:34,  1.11it/s] 75%|███████▌  | 519/690 [08:33<02:30,  1.13it/s] 75%|███████▌  | 520/690 [08:34<02:28,  1.14it/s] 76%|███████▌  | 521/690 [08:34<02:26,  1.15it/s] 76%|███████▌  | 522/690 [08:35<02:24,  1.17it/s] 76%|███████▌  | 523/690 [08:36<02:22,  1.17it/s] 76%|███████▌  | 524/690 [08:37<02:21,  1.17it/s] 76%|███████▌  | 525/690 [08:38<02:20,  1.18it/s] 76%|███████▌  | 526/690 [08:39<02:19,  1.17it/s] 76%|███████▋  | 527/690 [08:39<02:18,  1.18it/s] 77%|███████▋  | 528/690 [08:40<02:17,  1.18it/s] 77%|███████▋  | 529/690 [08:41<02:16,  1.18it/s] 77%|███████▋  | 530/690 [08:42<02:16,  1.17it/s] 77%|███████▋  | 531/690 [08:43<02:16,  1.16it/s] 77%|███████▋  | 532/690 [08:44<02:15,  1.17it/s] 77%|███████▋  | 533/690 [08:45<02:14,  1.17it/s] 77%|███████▋  | 534/690 [08:45<02:13,  1.17it/s] 78%|███████▊  | 535/690 [08:46<02:11,  1.18it/s] 78%|███████▊  | 536/690 [08:47<02:10,  1.18it/s] 78%|███████▊  | 537/690 [08:48<02:10,  1.17it/s] 78%|███████▊  | 538/690 [08:49<02:09,  1.17it/s] 78%|███████▊  | 539/690 [08:50<02:08,  1.17it/s] 78%|███████▊  | 540/690 [08:51<02:07,  1.18it/s] 78%|███████▊  | 541/690 [08:51<02:06,  1.17it/s] 79%|███████▊  | 542/690 [08:52<02:05,  1.18it/s] 79%|███████▊  | 543/690 [08:53<02:03,  1.19it/s] 79%|███████▉  | 544/690 [08:54<02:04,  1.17it/s] 79%|███████▉  | 545/690 [08:55<02:03,  1.18it/s] 79%|███████▉  | 546/690 [08:56<02:02,  1.18it/s] 79%|███████▉  | 547/690 [08:56<02:01,  1.17it/s] 79%|███████▉  | 548/690 [08:57<02:00,  1.17it/s] 80%|███████▉  | 549/690 [08:58<01:59,  1.18it/s] 80%|███████▉  | 550/690 [08:59<01:58,  1.18it/s] 80%|███████▉  | 551/690 [09:00<01:57,  1.18it/s] 80%|████████  | 552/690 [09:01<01:58,  1.17it/s] 80%|████████  | 553/690 [09:02<01:56,  1.17it/s] 80%|████████  | 554/690 [09:02<01:55,  1.17it/s] 80%|████████  | 555/690 [09:03<01:54,  1.18it/s] 81%|████████  | 556/690 [09:04<01:54,  1.17it/s] 81%|████████  | 557/690 [09:05<01:53,  1.17it/s] 81%|████████  | 558/690 [09:06<01:52,  1.17it/s] 81%|████████  | 559/690 [09:07<01:53,  1.16it/s] 81%|████████  | 560/690 [09:08<01:51,  1.16it/s] 81%|████████▏ | 561/690 [09:08<01:50,  1.16it/s] 81%|████████▏ | 562/690 [09:09<01:48,  1.17it/s] 82%|████████▏ | 563/690 [09:10<01:48,  1.17it/s] 82%|████████▏ | 564/690 [09:11<01:48,  1.17it/s] 82%|████████▏ | 565/690 [09:12<01:47,  1.17it/s] 82%|████████▏ | 566/690 [09:13<01:45,  1.17it/s] 82%|████████▏ | 567/690 [09:14<01:45,  1.17it/s] 82%|████████▏ | 568/690 [09:14<01:44,  1.17it/s] 82%|████████▏ | 569/690 [09:15<01:43,  1.17it/s] 83%|████████▎ | 570/690 [09:16<01:42,  1.17it/s] 83%|████████▎ | 571/690 [09:17<01:41,  1.17it/s] 83%|████████▎ | 572/690 [09:18<01:40,  1.17it/s] 83%|████████▎ | 573/690 [09:19<01:39,  1.17it/s] 83%|████████▎ | 574/690 [09:19<01:38,  1.18it/s] 83%|████████▎ | 575/690 [09:20<01:38,  1.17it/s] 83%|████████▎ | 576/690 [09:21<01:36,  1.18it/s] 84%|████████▎ | 577/690 [09:22<01:35,  1.18it/s] 84%|████████▍ | 578/690 [09:23<01:35,  1.18it/s] 84%|████████▍ | 579/690 [09:24<01:34,  1.17it/s] 84%|████████▍ | 580/690 [09:25<01:33,  1.18it/s] 84%|████████▍ | 581/690 [09:25<01:32,  1.18it/s] 84%|████████▍ | 582/690 [09:26<01:31,  1.18it/s] 84%|████████▍ | 583/690 [09:27<01:30,  1.18it/s] 85%|████████▍ | 584/690 [09:28<01:30,  1.17it/s] 85%|████████▍ | 585/690 [09:29<01:29,  1.17it/s] 85%|████████▍ | 586/690 [09:30<01:28,  1.17it/s] 85%|████████▌ | 587/690 [09:31<01:27,  1.18it/s] 85%|████████▌ | 588/690 [09:31<01:27,  1.17it/s] 85%|████████▌ | 589/690 [09:32<01:26,  1.17it/s] 86%|████████▌ | 590/690 [09:33<01:25,  1.17it/s] 86%|████████▌ | 591/690 [09:34<01:24,  1.17it/s] 86%|████████▌ | 592/690 [09:35<01:24,  1.16it/s] 86%|████████▌ | 593/690 [09:36<01:22,  1.17it/s] 86%|████████▌ | 594/690 [09:37<01:22,  1.17it/s] 86%|████████▌ | 595/690 [09:37<01:21,  1.17it/s] 86%|████████▋ | 596/690 [09:38<01:20,  1.17it/s] 87%|████████▋ | 597/690 [09:39<01:19,  1.17it/s] 87%|████████▋ | 598/690 [09:40<01:18,  1.17it/s] 87%|████████▋ | 599/690 [09:41<01:17,  1.17it/s] 87%|████████▋ | 600/690 [09:42<01:16,  1.18it/s] 87%|████████▋ | 601/690 [09:43<01:15,  1.17it/s] 87%|████████▋ | 602/690 [09:43<01:15,  1.17it/s] 87%|████████▋ | 603/690 [09:44<01:14,  1.17it/s] 88%|████████▊ | 604/690 [09:45<01:13,  1.16it/s] 88%|████████▊ | 605/690 [09:46<01:13,  1.16it/s] 88%|████████▊ | 606/690 [09:47<01:12,  1.16it/s] 88%|████████▊ | 607/690 [09:48<01:11,  1.16it/s] 88%|████████▊ | 608/690 [09:49<01:10,  1.17it/s] 88%|████████▊ | 609/690 [09:49<01:09,  1.17it/s] 88%|████████▊ | 610/690 [09:50<01:08,  1.16it/s] 89%|████████▊ | 611/690 [09:51<01:07,  1.17it/s] 89%|████████▊ | 612/690 [09:52<01:07,  1.16it/s] 89%|████████▉ | 613/690 [09:53<01:06,  1.16it/s] 89%|████████▉ | 614/690 [09:54<01:05,  1.17it/s] 89%|████████▉ | 615/690 [09:55<01:04,  1.17it/s] 89%|████████▉ | 616/690 [09:55<01:03,  1.17it/s] 89%|████████▉ | 617/690 [09:56<01:02,  1.17it/s] 90%|████████▉ | 618/690 [09:57<01:01,  1.17it/s] 90%|████████▉ | 619/690 [09:58<01:00,  1.17it/s] 90%|████████▉ | 620/690 [09:59<00:59,  1.17it/s] 90%|█████████ | 621/690 [10:00<00:58,  1.18it/s] 90%|█████████ | 622/690 [10:01<00:57,  1.17it/s] 90%|█████████ | 623/690 [10:01<00:56,  1.18it/s] 90%|█████████ | 624/690 [10:02<00:56,  1.18it/s] 91%|█████████ | 625/690 [10:03<00:55,  1.17it/s] 91%|█████████ | 626/690 [10:04<00:54,  1.17it/s] 91%|█████████ | 627/690 [10:05<00:53,  1.17it/s] 91%|█████████ | 628/690 [10:06<00:53,  1.16it/s] 91%|█████████ | 629/690 [10:06<00:52,  1.17it/s] 91%|█████████▏| 630/690 [10:07<00:51,  1.17it/s] 91%|█████████▏| 631/690 [10:08<00:50,  1.17it/s] 92%|█████████▏| 632/690 [10:09<00:49,  1.17it/s] 92%|█████████▏| 633/690 [10:10<00:48,  1.18it/s] 92%|█████████▏| 634/690 [10:11<00:47,  1.18it/s] 92%|█████████▏| 635/690 [10:12<00:46,  1.18it/s] 92%|█████████▏| 636/690 [10:12<00:45,  1.17it/s] 92%|█████████▏| 637/690 [10:13<00:44,  1.18it/s] 92%|█████████▏| 638/690 [10:14<00:44,  1.17it/s] 93%|█████████▎| 639/690 [10:15<00:43,  1.18it/s] 93%|█████████▎| 640/690 [10:16<00:42,  1.17it/s] 93%|█████████▎| 641/690 [10:17<00:41,  1.17it/s] 93%|█████████▎| 642/690 [10:18<00:41,  1.17it/s] 93%|█████████▎| 643/690 [10:18<00:40,  1.17it/s] 93%|█████████▎| 644/690 [10:19<00:39,  1.17it/s] 93%|█████████▎| 645/690 [10:20<00:38,  1.17it/s] 94%|█████████▎| 646/690 [10:21<00:37,  1.18it/s] 94%|█████████▍| 647/690 [10:22<00:36,  1.17it/s] 94%|█████████▍| 648/690 [10:23<00:35,  1.17it/s] 94%|█████████▍| 649/690 [10:24<00:35,  1.17it/s] 94%|█████████▍| 650/690 [10:24<00:34,  1.17it/s] 94%|█████████▍| 651/690 [10:25<00:33,  1.17it/s] 94%|█████████▍| 652/690 [10:26<00:32,  1.18it/s] 95%|█████████▍| 653/690 [10:27<00:31,  1.17it/s] 95%|█████████▍| 654/690 [10:28<00:30,  1.17it/s] 95%|█████████▍| 655/690 [10:29<00:29,  1.17it/s] 95%|█████████▌| 656/690 [10:30<00:29,  1.17it/s] 95%|█████████▌| 657/690 [10:30<00:28,  1.17it/s] 95%|█████████▌| 658/690 [10:31<00:27,  1.16it/s] 96%|█████████▌| 659/690 [10:32<00:26,  1.17it/s] 96%|█████████▌| 660/690 [10:33<00:25,  1.17it/s] 96%|█████████▌| 661/690 [10:34<00:24,  1.17it/s] 96%|█████████▌| 662/690 [10:35<00:24,  1.17it/s] 96%|█████████▌| 663/690 [10:36<00:23,  1.17it/s] 96%|█████████▌| 664/690 [10:36<00:22,  1.17it/s] 96%|█████████▋| 665/690 [10:37<00:21,  1.18it/s] 97%|█████████▋| 666/690 [10:38<00:20,  1.18it/s] 97%|█████████▋| 667/690 [10:39<00:19,  1.17it/s] 97%|█████████▋| 668/690 [10:40<00:18,  1.18it/s] 97%|█████████▋| 669/690 [10:41<00:17,  1.17it/s] 97%|█████████▋| 670/690 [10:41<00:16,  1.18it/s] 97%|█████████▋| 671/690 [10:42<00:16,  1.17it/s] 97%|█████████▋| 672/690 [10:43<00:15,  1.17it/s] 98%|█████████▊| 673/690 [10:44<00:14,  1.18it/s] 98%|█████████▊| 674/690 [10:45<00:13,  1.17it/s] 98%|█████████▊| 675/690 [10:46<00:12,  1.17it/s] 98%|█████████▊| 676/690 [10:47<00:11,  1.17it/s] 98%|█████████▊| 677/690 [10:47<00:11,  1.17it/s] 98%|█████████▊| 678/690 [10:48<00:10,  1.16it/s] 98%|█████████▊| 679/690 [10:49<00:09,  1.17it/s] 99%|█████████▊| 680/690 [10:50<00:08,  1.18it/s] 99%|█████████▊| 681/690 [10:51<00:07,  1.18it/s] 99%|█████████▉| 682/690 [10:52<00:06,  1.17it/s] 99%|█████████▉| 683/690 [10:53<00:05,  1.17it/s] 99%|█████████▉| 684/690 [10:53<00:05,  1.18it/s] 99%|█████████▉| 685/690 [10:54<00:04,  1.18it/s] 99%|█████████▉| 686/690 [10:55<00:03,  1.18it/s]100%|█████████▉| 687/690 [10:56<00:02,  1.18it/s]100%|█████████▉| 688/690 [10:57<00:01,  1.18it/s]100%|█████████▉| 689/690 [10:58<00:00,  1.17it/s]100%|██████████| 690/690 [10:59<00:00,  1.18it/s][INFO|trainer.py:1988] 2024-02-12 19:08:34,003 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 659.1883, 'train_samples_per_second': 16.693, 'train_steps_per_second': 1.047, 'train_loss': 0.5416578209918478, 'epoch': 3.0}
                                                 100%|██████████| 690/690 [10:59<00:00,  1.18it/s]100%|██████████| 690/690 [10:59<00:00,  1.05it/s]
[INFO|trainer.py:2985] 2024-02-12 19:08:34,151 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588
[INFO|configuration_utils.py:473] 2024-02-12 19:08:34,197 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/config.json
[INFO|modeling_utils.py:2462] 2024-02-12 19:08:56,212 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-12 19:08:56,214 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-12 19:08:56,215 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.5417
  train_runtime            = 0:10:59.18
  train_samples            =       3668
  train_samples_per_second =     16.693
  train_steps_per_second   =      1.047
02/12/2024 19:08:56 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-12 19:08:56,269 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-12 19:08:56,271 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-12 19:08:56,271 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-12 19:08:56,271 >>   Batch size = 8
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:02,  8.04it/s] 12%|█▏        | 3/26 [00:00<00:04,  5.36it/s] 15%|█▌        | 4/26 [00:00<00:04,  4.73it/s] 19%|█▉        | 5/26 [00:01<00:04,  4.55it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.34it/s] 27%|██▋       | 7/26 [00:01<00:04,  4.16it/s] 31%|███       | 8/26 [00:01<00:04,  4.10it/s] 35%|███▍      | 9/26 [00:02<00:04,  4.01it/s] 38%|███▊      | 10/26 [00:02<00:03,  4.06it/s] 42%|████▏     | 11/26 [00:02<00:03,  4.05it/s] 46%|████▌     | 12/26 [00:02<00:03,  4.02it/s] 50%|█████     | 13/26 [00:03<00:03,  4.03it/s] 54%|█████▍    | 14/26 [00:03<00:03,  3.95it/s] 58%|█████▊    | 15/26 [00:03<00:02,  3.97it/s] 62%|██████▏   | 16/26 [00:03<00:02,  3.97it/s] 65%|██████▌   | 17/26 [00:04<00:02,  4.05it/s] 69%|██████▉   | 18/26 [00:04<00:02,  3.94it/s] 73%|███████▎  | 19/26 [00:04<00:01,  3.99it/s] 77%|███████▋  | 20/26 [00:04<00:01,  3.98it/s] 81%|████████  | 21/26 [00:05<00:01,  3.99it/s] 85%|████████▍ | 22/26 [00:05<00:01,  3.99it/s] 88%|████████▊ | 23/26 [00:05<00:00,  4.06it/s] 92%|█████████▏| 24/26 [00:05<00:00,  4.03it/s] 96%|█████████▌| 25/26 [00:06<00:00,  3.97it/s]100%|██████████| 26/26 [00:06<00:00,  4.03it/s]100%|██████████| 26/26 [00:06<00:00,  4.08it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.7132
  eval_combined_score     =     0.7576
  eval_f1                 =      0.802
  eval_loss               =     0.8239
  eval_runtime            = 0:00:06.61
  eval_samples            =        408
  eval_samples_per_second =     61.633
  eval_steps_per_second   =      3.928
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/12/2024 19:12:33 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/12/2024 19:12:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588/runs/Feb12_19-12-33_v021.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/22189588,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/12/2024 19:12:33 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Downloading data:   0%|          | 0.00/33.6M [00:00<?, ?B/s]Downloading data:  12%|█▏        | 4.19M/33.6M [00:00<00:01, 16.2MB/s]Downloading data:  37%|███▋      | 12.6M/33.6M [00:00<00:00, 40.2MB/s]Downloading data:  62%|██████▏   | 21.0M/33.6M [00:00<00:00, 53.4MB/s]Downloading data: 100%|██████████| 33.6M/33.6M [00:00<00:00, 76.4MB/s]Downloading data: 100%|██████████| 33.6M/33.6M [00:00<00:00, 59.0MB/s]
Downloading data:   0%|          | 0.00/3.73M [00:00<?, ?B/s]Downloading data: 100%|██████████| 3.73M/3.73M [00:00<00:00, 55.5MB/s]
Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]Downloading data:  11%|█▏        | 4.19M/36.7M [00:00<00:01, 31.4MB/s]Downloading data:  46%|████▌     | 16.8M/36.7M [00:00<00:00, 79.1MB/s]Downloading data:  80%|████████  | 29.4M/36.7M [00:00<00:00, 67.3MB/s]Downloading data: 100%|██████████| 36.7M/36.7M [00:00<00:00, 70.6MB/s]
Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]Generating train split:  44%|████▍     | 160000/363846 [00:00<00:00, 1454042.59 examples/s]Generating train split:  96%|█████████▌| 350000/363846 [00:00<00:00, 1631747.18 examples/s]Generating train split: 100%|██████████| 363846/363846 [00:00<00:00, 1584935.74 examples/s]
Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 40430/40430 [00:00<00:00, 1632686.43 examples/s]
Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]Generating test split:  46%|████▌     | 180000/390965 [00:00<00:00, 1692616.63 examples/s]Generating test split:  92%|█████████▏| 360000/390965 [00:00<00:00, 1729926.95 examples/s]Generating test split: 100%|██████████| 390965/390965 [00:00<00:00, 1708785.82 examples/s]
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/12/2024 19:12:39 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/12/2024 19:12:39 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-12 19:12:39,402 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-12 19:12:39,403 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "qqp",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-12 19:12:39,446 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-12 19:12:39,446 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 19:12:39,446 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 19:12:39,446 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-12 19:12:39,446 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-12 19:12:39,545 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-12 19:12:39,572 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-12 19:12:39,632 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-12 19:12:45,555 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-12 19:12:45,557 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-12 19:12:45,570 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/363846 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24389be96ed7fa13.arrow
02/12/2024 19:12:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24389be96ed7fa13.arrow
Running tokenizer on dataset:   0%|          | 1000/363846 [00:00<01:59, 3045.25 examples/s]Running tokenizer on dataset:   1%|          | 2000/363846 [00:00<01:12, 5019.56 examples/s]Running tokenizer on dataset:   1%|          | 3000/363846 [00:00<01:17, 4658.45 examples/s]Running tokenizer on dataset:   1%|          | 4000/363846 [00:00<01:02, 5765.82 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/363846 [00:00<00:53, 6682.36 examples/s]Running tokenizer on dataset:   2%|▏         | 6000/363846 [00:00<00:48, 7360.97 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/363846 [00:01<00:44, 7956.44 examples/s]Running tokenizer on dataset:   2%|▏         | 8000/363846 [00:01<00:42, 8369.37 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/363846 [00:01<00:41, 8609.29 examples/s]Running tokenizer on dataset:   3%|▎         | 10000/363846 [00:01<00:39, 8869.11 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/363846 [00:01<00:39, 8981.21 examples/s]Running tokenizer on dataset:   3%|▎         | 12000/363846 [00:01<00:38, 9049.27 examples/s]Running tokenizer on dataset:   4%|▎         | 13000/363846 [00:01<00:37, 9235.82 examples/s]Running tokenizer on dataset:   4%|▍         | 14000/363846 [00:01<00:37, 9299.23 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/363846 [00:01<00:37, 9262.12 examples/s]Running tokenizer on dataset:   4%|▍         | 16000/363846 [00:02<00:37, 9264.05 examples/s]Running tokenizer on dataset:   5%|▍         | 17000/363846 [00:02<00:37, 9280.49 examples/s]Running tokenizer on dataset:   5%|▍         | 18000/363846 [00:02<00:37, 9337.62 examples/s]Running tokenizer on dataset:   5%|▌         | 19000/363846 [00:02<00:36, 9371.59 examples/s]Running tokenizer on dataset:   5%|▌         | 20000/363846 [00:02<00:36, 9416.43 examples/s]Running tokenizer on dataset:   6%|▌         | 21000/363846 [00:02<00:36, 9304.52 examples/s]Running tokenizer on dataset:   6%|▌         | 22000/363846 [00:02<00:36, 9257.52 examples/s]Running tokenizer on dataset:   6%|▋         | 23000/363846 [00:02<00:36, 9250.93 examples/s]Running tokenizer on dataset:   7%|▋         | 24000/363846 [00:02<00:36, 9281.47 examples/s]Running tokenizer on dataset:   7%|▋         | 25000/363846 [00:03<00:36, 9332.27 examples/s]Running tokenizer on dataset:   7%|▋         | 26000/363846 [00:03<00:36, 9303.45 examples/s]Running tokenizer on dataset:   7%|▋         | 27000/363846 [00:03<00:36, 9317.01 examples/s]Running tokenizer on dataset:   8%|▊         | 28000/363846 [00:03<00:36, 9258.80 examples/s]Running tokenizer on dataset:   8%|▊         | 29000/363846 [00:03<00:35, 9321.37 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/363846 [00:03<00:36, 9262.06 examples/s]Running tokenizer on dataset:   9%|▊         | 31000/363846 [00:03<00:36, 9235.94 examples/s]Running tokenizer on dataset:   9%|▉         | 32000/363846 [00:03<00:35, 9301.24 examples/s]Running tokenizer on dataset:   9%|▉         | 33000/363846 [00:03<00:35, 9306.90 examples/s]Running tokenizer on dataset:   9%|▉         | 34000/363846 [00:03<00:35, 9398.72 examples/s]Running tokenizer on dataset:  10%|▉         | 35000/363846 [00:04<00:34, 9417.56 examples/s]Running tokenizer on dataset:  10%|▉         | 36000/363846 [00:04<00:35, 9292.89 examples/s]Running tokenizer on dataset:  10%|█         | 37000/363846 [00:04<00:35, 9330.98 examples/s]Running tokenizer on dataset:  10%|█         | 38000/363846 [00:04<00:34, 9372.62 examples/s]Running tokenizer on dataset:  11%|█         | 39000/363846 [00:04<00:44, 7279.56 examples/s]Running tokenizer on dataset:  11%|█         | 40000/363846 [00:04<00:41, 7744.33 examples/s]Running tokenizer on dataset:  11%|█▏        | 41000/363846 [00:04<00:39, 8133.07 examples/s]Running tokenizer on dataset:  12%|█▏        | 42000/363846 [00:04<00:38, 8388.50 examples/s]Running tokenizer on dataset:  12%|█▏        | 43000/363846 [00:05<00:36, 8672.70 examples/s]Running tokenizer on dataset:  12%|█▏        | 44000/363846 [00:05<00:36, 8863.10 examples/s]Running tokenizer on dataset:  12%|█▏        | 45000/363846 [00:05<00:35, 9073.37 examples/s]Running tokenizer on dataset:  13%|█▎        | 46000/363846 [00:05<00:35, 9033.53 examples/s]Running tokenizer on dataset:  13%|█▎        | 47000/363846 [00:05<00:34, 9195.61 examples/s]Running tokenizer on dataset:  13%|█▎        | 48000/363846 [00:05<00:34, 9168.38 examples/s]Running tokenizer on dataset:  13%|█▎        | 49000/363846 [00:05<00:34, 9221.54 examples/s]Running tokenizer on dataset:  14%|█▎        | 50000/363846 [00:05<00:33, 9295.51 examples/s]Running tokenizer on dataset:  14%|█▍        | 51000/363846 [00:05<00:33, 9235.63 examples/s]Running tokenizer on dataset:  14%|█▍        | 52000/363846 [00:06<00:33, 9252.83 examples/s]Running tokenizer on dataset:  15%|█▍        | 53000/363846 [00:06<00:33, 9196.75 examples/s]Running tokenizer on dataset:  15%|█▍        | 54000/363846 [00:06<00:33, 9156.12 examples/s]Running tokenizer on dataset:  15%|█▌        | 55000/363846 [00:06<00:33, 9248.66 examples/s]Running tokenizer on dataset:  15%|█▌        | 56000/363846 [00:06<00:33, 9182.16 examples/s]Running tokenizer on dataset:  16%|█▌        | 57000/363846 [00:06<00:33, 9137.98 examples/s]Running tokenizer on dataset:  16%|█▌        | 58000/363846 [00:06<00:32, 9286.44 examples/s]Running tokenizer on dataset:  16%|█▌        | 59000/363846 [00:06<00:32, 9337.50 examples/s]Running tokenizer on dataset:  16%|█▋        | 60000/363846 [00:06<00:32, 9341.19 examples/s]Running tokenizer on dataset:  17%|█▋        | 61000/363846 [00:07<00:32, 9313.48 examples/s]Running tokenizer on dataset:  17%|█▋        | 62000/363846 [00:07<00:32, 9253.47 examples/s]Running tokenizer on dataset:  17%|█▋        | 63000/363846 [00:07<00:32, 9310.47 examples/s]Running tokenizer on dataset:  18%|█▊        | 64000/363846 [00:07<00:32, 9359.58 examples/s]Running tokenizer on dataset:  18%|█▊        | 65000/363846 [00:07<00:32, 9277.16 examples/s]Running tokenizer on dataset:  18%|█▊        | 66000/363846 [00:07<00:32, 9272.46 examples/s]Running tokenizer on dataset:  18%|█▊        | 67000/363846 [00:07<00:32, 9232.64 examples/s]Running tokenizer on dataset:  19%|█▊        | 68000/363846 [00:07<00:31, 9314.41 examples/s]Running tokenizer on dataset:  19%|█▉        | 69000/363846 [00:07<00:41, 7172.44 examples/s]Running tokenizer on dataset:  19%|█▉        | 70000/363846 [00:08<00:38, 7644.76 examples/s]Running tokenizer on dataset:  20%|█▉        | 71000/363846 [00:08<00:36, 8095.04 examples/s]Running tokenizer on dataset:  20%|█▉        | 72000/363846 [00:08<00:34, 8420.65 examples/s]Running tokenizer on dataset:  20%|██        | 73000/363846 [00:08<00:33, 8689.33 examples/s]Running tokenizer on dataset:  20%|██        | 74000/363846 [00:08<00:32, 8861.12 examples/s]Running tokenizer on dataset:  21%|██        | 75000/363846 [00:08<00:32, 8995.25 examples/s]Running tokenizer on dataset:  21%|██        | 76000/363846 [00:08<00:31, 9038.68 examples/s]Running tokenizer on dataset:  21%|██        | 77000/363846 [00:08<00:31, 9197.94 examples/s]Running tokenizer on dataset:  21%|██▏       | 78000/363846 [00:08<00:30, 9244.42 examples/s]Running tokenizer on dataset:  22%|██▏       | 79000/363846 [00:09<00:30, 9323.91 examples/s]Running tokenizer on dataset:  22%|██▏       | 80000/363846 [00:09<00:30, 9301.49 examples/s]Running tokenizer on dataset:  22%|██▏       | 81000/363846 [00:09<00:30, 9231.14 examples/s]Running tokenizer on dataset:  23%|██▎       | 82000/363846 [00:09<00:30, 9239.55 examples/s]Running tokenizer on dataset:  23%|██▎       | 83000/363846 [00:09<00:30, 9344.51 examples/s]Running tokenizer on dataset:  23%|██▎       | 84000/363846 [00:09<00:29, 9332.44 examples/s]Running tokenizer on dataset:  23%|██▎       | 85000/363846 [00:09<00:29, 9472.75 examples/s]Running tokenizer on dataset:  24%|██▎       | 86000/363846 [00:09<00:29, 9350.62 examples/s]Running tokenizer on dataset:  24%|██▍       | 87000/363846 [00:09<00:29, 9392.04 examples/s]Running tokenizer on dataset:  24%|██▍       | 88000/363846 [00:10<00:29, 9378.21 examples/s]Running tokenizer on dataset:  24%|██▍       | 89000/363846 [00:10<00:29, 9413.15 examples/s]Running tokenizer on dataset:  25%|██▍       | 90000/363846 [00:10<00:29, 9404.32 examples/s]Running tokenizer on dataset:  25%|██▌       | 91000/363846 [00:10<00:29, 9390.83 examples/s]Running tokenizer on dataset:  25%|██▌       | 92000/363846 [00:10<00:28, 9394.71 examples/s]Running tokenizer on dataset:  26%|██▌       | 93000/363846 [00:10<00:29, 9300.40 examples/s]Running tokenizer on dataset:  26%|██▌       | 94000/363846 [00:10<00:37, 7258.51 examples/s]Running tokenizer on dataset:  26%|██▌       | 95000/363846 [00:10<00:34, 7725.51 examples/s]Running tokenizer on dataset:  26%|██▋       | 96000/363846 [00:10<00:33, 8089.37 examples/s]Running tokenizer on dataset:  27%|██▋       | 97000/363846 [00:11<00:31, 8438.59 examples/s]Running tokenizer on dataset:  27%|██▋       | 98000/363846 [00:11<00:30, 8736.97 examples/s]Running tokenizer on dataset:  27%|██▋       | 99000/363846 [00:11<00:29, 8926.14 examples/s]Running tokenizer on dataset:  27%|██▋       | 100000/363846 [00:11<00:29, 9007.97 examples/s]Running tokenizer on dataset:  28%|██▊       | 101000/363846 [00:11<00:28, 9098.24 examples/s]Running tokenizer on dataset:  28%|██▊       | 102000/363846 [00:11<00:28, 9208.21 examples/s]Running tokenizer on dataset:  28%|██▊       | 103000/363846 [00:11<00:28, 9284.29 examples/s]Running tokenizer on dataset:  29%|██▊       | 104000/363846 [00:11<00:27, 9353.51 examples/s]Running tokenizer on dataset:  29%|██▉       | 105000/363846 [00:11<00:27, 9310.33 examples/s]Running tokenizer on dataset:  29%|██▉       | 106000/363846 [00:12<00:27, 9270.55 examples/s]Running tokenizer on dataset:  29%|██▉       | 107000/363846 [00:12<00:27, 9291.33 examples/s]Running tokenizer on dataset:  30%|██▉       | 108000/363846 [00:12<00:27, 9301.55 examples/s]Running tokenizer on dataset:  30%|██▉       | 109000/363846 [00:12<00:27, 9356.53 examples/s]Running tokenizer on dataset:  30%|███       | 110000/363846 [00:12<00:27, 9371.02 examples/s]Running tokenizer on dataset:  31%|███       | 111000/363846 [00:12<00:27, 9280.08 examples/s]Running tokenizer on dataset:  31%|███       | 112000/363846 [00:12<00:26, 9350.70 examples/s]Running tokenizer on dataset:  31%|███       | 113000/363846 [00:12<00:26, 9338.27 examples/s]Running tokenizer on dataset:  31%|███▏      | 114000/363846 [00:12<00:26, 9369.15 examples/s]Running tokenizer on dataset:  32%|███▏      | 115000/363846 [00:12<00:26, 9450.45 examples/s]Running tokenizer on dataset:  32%|███▏      | 116000/363846 [00:13<00:26, 9367.39 examples/s]Running tokenizer on dataset:  32%|███▏      | 117000/363846 [00:13<00:26, 9297.56 examples/s]Running tokenizer on dataset:  32%|███▏      | 118000/363846 [00:13<00:33, 7254.62 examples/s]Running tokenizer on dataset:  33%|███▎      | 119000/363846 [00:13<00:31, 7752.10 examples/s]Running tokenizer on dataset:  33%|███▎      | 120000/363846 [00:13<00:29, 8140.30 examples/s]Running tokenizer on dataset:  33%|███▎      | 121000/363846 [00:13<00:28, 8455.80 examples/s]Running tokenizer on dataset:  34%|███▎      | 122000/363846 [00:13<00:27, 8708.31 examples/s]Running tokenizer on dataset:  34%|███▍      | 123000/363846 [00:13<00:26, 8956.46 examples/s]Running tokenizer on dataset:  34%|███▍      | 124000/363846 [00:14<00:26, 9098.02 examples/s]Running tokenizer on dataset:  34%|███▍      | 125000/363846 [00:14<00:26, 9123.88 examples/s]Running tokenizer on dataset:  35%|███▍      | 126000/363846 [00:14<00:26, 9108.05 examples/s]Running tokenizer on dataset:  35%|███▍      | 127000/363846 [00:14<00:25, 9176.11 examples/s]Running tokenizer on dataset:  35%|███▌      | 128000/363846 [00:14<00:25, 9254.71 examples/s]Running tokenizer on dataset:  35%|███▌      | 129000/363846 [00:14<00:25, 9280.93 examples/s]Running tokenizer on dataset:  36%|███▌      | 130000/363846 [00:14<00:25, 9329.34 examples/s]Running tokenizer on dataset:  36%|███▌      | 131000/363846 [00:14<00:25, 9259.35 examples/s]Running tokenizer on dataset:  36%|███▋      | 132000/363846 [00:14<00:25, 9175.25 examples/s]Running tokenizer on dataset:  37%|███▋      | 133000/363846 [00:15<00:24, 9244.75 examples/s]Running tokenizer on dataset:  37%|███▋      | 134000/363846 [00:15<00:24, 9292.73 examples/s]Running tokenizer on dataset:  37%|███▋      | 135000/363846 [00:15<00:24, 9352.97 examples/s]Running tokenizer on dataset:  37%|███▋      | 136000/363846 [00:15<00:24, 9270.27 examples/s]Running tokenizer on dataset:  38%|███▊      | 137000/363846 [00:15<00:24, 9350.12 examples/s]Running tokenizer on dataset:  38%|███▊      | 138000/363846 [00:15<00:24, 9329.78 examples/s]Running tokenizer on dataset:  38%|███▊      | 139000/363846 [00:15<00:23, 9411.50 examples/s]Running tokenizer on dataset:  38%|███▊      | 140000/363846 [00:15<00:23, 9342.91 examples/s]Running tokenizer on dataset:  39%|███▉      | 141000/363846 [00:15<00:24, 9279.12 examples/s]Running tokenizer on dataset:  39%|███▉      | 142000/363846 [00:16<00:30, 7269.99 examples/s]Running tokenizer on dataset:  39%|███▉      | 143000/363846 [00:16<00:28, 7785.98 examples/s]Running tokenizer on dataset:  40%|███▉      | 144000/363846 [00:16<00:27, 8105.72 examples/s]Running tokenizer on dataset:  40%|███▉      | 145000/363846 [00:16<00:25, 8526.80 examples/s]Running tokenizer on dataset:  40%|████      | 146000/363846 [00:16<00:25, 8623.58 examples/s]Running tokenizer on dataset:  40%|████      | 147000/363846 [00:16<00:24, 8788.82 examples/s]Running tokenizer on dataset:  41%|████      | 148000/363846 [00:16<00:24, 8970.98 examples/s]Running tokenizer on dataset:  41%|████      | 149000/363846 [00:16<00:23, 9161.71 examples/s]Running tokenizer on dataset:  41%|████      | 150000/363846 [00:16<00:23, 9194.26 examples/s]Running tokenizer on dataset:  42%|████▏     | 151000/363846 [00:17<00:23, 9221.23 examples/s]Running tokenizer on dataset:  42%|████▏     | 152000/363846 [00:17<00:23, 9184.85 examples/s]Running tokenizer on dataset:  42%|████▏     | 153000/363846 [00:17<00:22, 9184.30 examples/s]Running tokenizer on dataset:  42%|████▏     | 154000/363846 [00:17<00:22, 9234.64 examples/s]Running tokenizer on dataset:  43%|████▎     | 155000/363846 [00:17<00:22, 9361.85 examples/s]Running tokenizer on dataset:  43%|████▎     | 156000/363846 [00:17<00:22, 9297.85 examples/s]Running tokenizer on dataset:  43%|████▎     | 157000/363846 [00:17<00:22, 9277.81 examples/s]Running tokenizer on dataset:  43%|████▎     | 158000/363846 [00:17<00:22, 9247.34 examples/s]Running tokenizer on dataset:  44%|████▎     | 159000/363846 [00:17<00:21, 9341.73 examples/s]Running tokenizer on dataset:  44%|████▍     | 160000/363846 [00:18<00:21, 9405.00 examples/s]Running tokenizer on dataset:  44%|████▍     | 161000/363846 [00:18<00:21, 9293.73 examples/s]Running tokenizer on dataset:  45%|████▍     | 162000/363846 [00:18<00:21, 9228.15 examples/s]Running tokenizer on dataset:  45%|████▍     | 163000/363846 [00:18<00:21, 9349.27 examples/s]Running tokenizer on dataset:  45%|████▌     | 164000/363846 [00:18<00:21, 9372.76 examples/s]Running tokenizer on dataset:  45%|████▌     | 165000/363846 [00:18<00:21, 9401.42 examples/s]Running tokenizer on dataset:  46%|████▌     | 166000/363846 [00:18<00:27, 7185.82 examples/s]Running tokenizer on dataset:  46%|████▌     | 167000/363846 [00:18<00:25, 7670.17 examples/s]Running tokenizer on dataset:  46%|████▌     | 168000/363846 [00:19<00:24, 8140.74 examples/s]Running tokenizer on dataset:  46%|████▋     | 169000/363846 [00:19<00:22, 8519.63 examples/s]Running tokenizer on dataset:  47%|████▋     | 170000/363846 [00:19<00:22, 8750.32 examples/s]Running tokenizer on dataset:  47%|████▋     | 171000/363846 [00:19<00:21, 8887.68 examples/s]Running tokenizer on dataset:  47%|████▋     | 172000/363846 [00:19<00:21, 9009.97 examples/s]Running tokenizer on dataset:  48%|████▊     | 173000/363846 [00:19<00:20, 9111.21 examples/s]Running tokenizer on dataset:  48%|████▊     | 174000/363846 [00:19<00:20, 9195.92 examples/s]Running tokenizer on dataset:  48%|████▊     | 175000/363846 [00:19<00:20, 9303.61 examples/s]Running tokenizer on dataset:  48%|████▊     | 176000/363846 [00:19<00:20, 9258.82 examples/s]Running tokenizer on dataset:  49%|████▊     | 177000/363846 [00:19<00:20, 9200.72 examples/s]Running tokenizer on dataset:  49%|████▉     | 178000/363846 [00:20<00:19, 9324.13 examples/s]Running tokenizer on dataset:  49%|████▉     | 179000/363846 [00:20<00:19, 9343.89 examples/s]Running tokenizer on dataset:  49%|████▉     | 180000/363846 [00:20<00:19, 9275.35 examples/s]Running tokenizer on dataset:  50%|████▉     | 181000/363846 [00:20<00:19, 9334.70 examples/s]Running tokenizer on dataset:  50%|█████     | 182000/363846 [00:20<00:19, 9314.71 examples/s]Running tokenizer on dataset:  50%|█████     | 183000/363846 [00:20<00:19, 9276.30 examples/s]Running tokenizer on dataset:  51%|█████     | 184000/363846 [00:20<00:19, 9335.68 examples/s]Running tokenizer on dataset:  51%|█████     | 185000/363846 [00:20<00:19, 9340.91 examples/s]Running tokenizer on dataset:  51%|█████     | 186000/363846 [00:20<00:19, 9279.15 examples/s]Running tokenizer on dataset:  51%|█████▏    | 187000/363846 [00:21<00:18, 9339.28 examples/s]Running tokenizer on dataset:  52%|█████▏    | 188000/363846 [00:21<00:18, 9429.27 examples/s]Running tokenizer on dataset:  52%|█████▏    | 189000/363846 [00:21<00:18, 9473.40 examples/s]Running tokenizer on dataset:  52%|█████▏    | 190000/363846 [00:21<00:23, 7318.87 examples/s]Running tokenizer on dataset:  52%|█████▏    | 191000/363846 [00:21<00:22, 7739.43 examples/s]Running tokenizer on dataset:  53%|█████▎    | 192000/363846 [00:21<00:21, 8147.89 examples/s]Running tokenizer on dataset:  53%|█████▎    | 193000/363846 [00:21<00:20, 8520.25 examples/s]Running tokenizer on dataset:  53%|█████▎    | 194000/363846 [00:21<00:19, 8785.49 examples/s]Running tokenizer on dataset:  54%|█████▎    | 195000/363846 [00:21<00:18, 8942.86 examples/s]Running tokenizer on dataset:  54%|█████▍    | 196000/363846 [00:22<00:18, 8989.06 examples/s]Running tokenizer on dataset:  54%|█████▍    | 197000/363846 [00:22<00:18, 9018.40 examples/s]Running tokenizer on dataset:  54%|█████▍    | 198000/363846 [00:22<00:18, 9005.07 examples/s]Running tokenizer on dataset:  55%|█████▍    | 199000/363846 [00:22<00:17, 9171.96 examples/s]Running tokenizer on dataset:  55%|█████▍    | 200000/363846 [00:22<00:17, 9253.02 examples/s]Running tokenizer on dataset:  55%|█████▌    | 201000/363846 [00:22<00:17, 9173.80 examples/s]Running tokenizer on dataset:  56%|█████▌    | 202000/363846 [00:22<00:17, 9319.40 examples/s]Running tokenizer on dataset:  56%|█████▌    | 203000/363846 [00:22<00:17, 9353.25 examples/s]Running tokenizer on dataset:  56%|█████▌    | 204000/363846 [00:22<00:17, 9332.89 examples/s]Running tokenizer on dataset:  56%|█████▋    | 205000/363846 [00:23<00:16, 9349.00 examples/s]Running tokenizer on dataset:  57%|█████▋    | 206000/363846 [00:23<00:16, 9338.11 examples/s]Running tokenizer on dataset:  57%|█████▋    | 207000/363846 [00:23<00:16, 9330.55 examples/s]Running tokenizer on dataset:  57%|█████▋    | 208000/363846 [00:23<00:16, 9274.72 examples/s]Running tokenizer on dataset:  57%|█████▋    | 209000/363846 [00:23<00:16, 9271.59 examples/s]Running tokenizer on dataset:  58%|█████▊    | 210000/363846 [00:23<00:16, 9329.24 examples/s]Running tokenizer on dataset:  58%|█████▊    | 211000/363846 [00:23<00:16, 9259.84 examples/s]Running tokenizer on dataset:  58%|█████▊    | 212000/363846 [00:23<00:16, 9220.83 examples/s]Running tokenizer on dataset:  59%|█████▊    | 213000/363846 [00:23<00:16, 9323.43 examples/s]Running tokenizer on dataset:  59%|█████▉    | 214000/363846 [00:24<00:20, 7308.80 examples/s]Running tokenizer on dataset:  59%|█████▉    | 215000/363846 [00:24<00:19, 7814.33 examples/s]Running tokenizer on dataset:  59%|█████▉    | 216000/363846 [00:24<00:18, 8163.56 examples/s]Running tokenizer on dataset:  60%|█████▉    | 217000/363846 [00:24<00:17, 8426.25 examples/s]Running tokenizer on dataset:  60%|█████▉    | 218000/363846 [00:24<00:16, 8662.11 examples/s]Running tokenizer on dataset:  60%|██████    | 219000/363846 [00:24<00:16, 8898.60 examples/s]Running tokenizer on dataset:  60%|██████    | 220000/363846 [00:24<00:15, 9077.01 examples/s]Running tokenizer on dataset:  61%|██████    | 221000/363846 [00:24<00:15, 9139.17 examples/s]Running tokenizer on dataset:  61%|██████    | 222000/363846 [00:24<00:15, 9157.50 examples/s]Running tokenizer on dataset:  61%|██████▏   | 223000/363846 [00:25<00:15, 9199.15 examples/s]Running tokenizer on dataset:  62%|██████▏   | 224000/363846 [00:25<00:15, 9305.76 examples/s]Running tokenizer on dataset:  62%|██████▏   | 225000/363846 [00:25<00:14, 9287.13 examples/s]Running tokenizer on dataset:  62%|██████▏   | 226000/363846 [00:25<00:15, 9189.69 examples/s]Running tokenizer on dataset:  62%|██████▏   | 227000/363846 [00:25<00:14, 9324.81 examples/s]Running tokenizer on dataset:  63%|██████▎   | 228000/363846 [00:25<00:14, 9352.54 examples/s]Running tokenizer on dataset:  63%|██████▎   | 229000/363846 [00:25<00:14, 9398.88 examples/s]Running tokenizer on dataset:  63%|██████▎   | 230000/363846 [00:25<00:14, 9290.89 examples/s]Running tokenizer on dataset:  63%|██████▎   | 231000/363846 [00:25<00:14, 9327.44 examples/s]Running tokenizer on dataset:  64%|██████▍   | 232000/363846 [00:26<00:14, 9355.06 examples/s]Running tokenizer on dataset:  64%|██████▍   | 233000/363846 [00:26<00:13, 9366.80 examples/s]Running tokenizer on dataset:  64%|██████▍   | 234000/363846 [00:26<00:13, 9445.35 examples/s]Running tokenizer on dataset:  65%|██████▍   | 235000/363846 [00:26<00:13, 9370.56 examples/s]Running tokenizer on dataset:  65%|██████▍   | 236000/363846 [00:26<00:13, 9373.72 examples/s]Running tokenizer on dataset:  65%|██████▌   | 237000/363846 [00:26<00:13, 9393.00 examples/s]Running tokenizer on dataset:  65%|██████▌   | 238000/363846 [00:26<00:17, 7325.44 examples/s]Running tokenizer on dataset:  66%|██████▌   | 239000/363846 [00:26<00:16, 7791.58 examples/s]Running tokenizer on dataset:  66%|██████▌   | 240000/363846 [00:27<00:15, 8170.63 examples/s]Running tokenizer on dataset:  66%|██████▌   | 241000/363846 [00:27<00:14, 8454.06 examples/s]Running tokenizer on dataset:  67%|██████▋   | 242000/363846 [00:27<00:13, 8705.54 examples/s]Running tokenizer on dataset:  67%|██████▋   | 243000/363846 [00:27<00:13, 8874.85 examples/s]Running tokenizer on dataset:  67%|██████▋   | 244000/363846 [00:27<00:13, 8989.27 examples/s]Running tokenizer on dataset:  67%|██████▋   | 245000/363846 [00:27<00:13, 9116.76 examples/s]Running tokenizer on dataset:  68%|██████▊   | 246000/363846 [00:27<00:12, 9116.72 examples/s]Running tokenizer on dataset:  68%|██████▊   | 247000/363846 [00:27<00:12, 9243.90 examples/s]Running tokenizer on dataset:  68%|██████▊   | 248000/363846 [00:27<00:12, 9269.78 examples/s]Running tokenizer on dataset:  68%|██████▊   | 249000/363846 [00:27<00:12, 9322.33 examples/s]Running tokenizer on dataset:  69%|██████▊   | 250000/363846 [00:28<00:12, 9339.97 examples/s]Running tokenizer on dataset:  69%|██████▉   | 251000/363846 [00:28<00:12, 9292.10 examples/s]Running tokenizer on dataset:  69%|██████▉   | 252000/363846 [00:28<00:12, 9266.53 examples/s]Running tokenizer on dataset:  70%|██████▉   | 253000/363846 [00:28<00:11, 9372.95 examples/s]Running tokenizer on dataset:  70%|██████▉   | 254000/363846 [00:28<00:11, 9337.74 examples/s]Running tokenizer on dataset:  70%|███████   | 255000/363846 [00:28<00:11, 9403.77 examples/s]Running tokenizer on dataset:  70%|███████   | 256000/363846 [00:28<00:11, 9301.68 examples/s]Running tokenizer on dataset:  71%|███████   | 257000/363846 [00:28<00:11, 9282.35 examples/s]Running tokenizer on dataset:  71%|███████   | 258000/363846 [00:28<00:11, 9226.31 examples/s]Running tokenizer on dataset:  71%|███████   | 259000/363846 [00:29<00:11, 9386.51 examples/s]Running tokenizer on dataset:  71%|███████▏  | 260000/363846 [00:29<00:11, 9414.56 examples/s]Running tokenizer on dataset:  72%|███████▏  | 261000/363846 [00:29<00:11, 9250.34 examples/s]Running tokenizer on dataset:  72%|███████▏  | 262000/363846 [00:29<00:14, 7230.24 examples/s]Running tokenizer on dataset:  72%|███████▏  | 263000/363846 [00:29<00:13, 7706.93 examples/s]Running tokenizer on dataset:  73%|███████▎  | 264000/363846 [00:29<00:12, 8137.70 examples/s]Running tokenizer on dataset:  73%|███████▎  | 265000/363846 [00:29<00:11, 8541.59 examples/s]Running tokenizer on dataset:  73%|███████▎  | 266000/363846 [00:29<00:11, 8702.01 examples/s]Running tokenizer on dataset:  73%|███████▎  | 267000/363846 [00:30<00:10, 8918.76 examples/s]Running tokenizer on dataset:  74%|███████▎  | 268000/363846 [00:30<00:10, 9129.34 examples/s]Running tokenizer on dataset:  74%|███████▍  | 269000/363846 [00:30<00:10, 9131.77 examples/s]Running tokenizer on dataset:  74%|███████▍  | 270000/363846 [00:30<00:10, 9125.29 examples/s]Running tokenizer on dataset:  74%|███████▍  | 271000/363846 [00:30<00:10, 9212.36 examples/s]Running tokenizer on dataset:  75%|███████▍  | 272000/363846 [00:30<00:09, 9222.56 examples/s]Running tokenizer on dataset:  75%|███████▌  | 273000/363846 [00:30<00:09, 9203.15 examples/s]Running tokenizer on dataset:  75%|███████▌  | 274000/363846 [00:30<00:09, 9253.66 examples/s]Running tokenizer on dataset:  76%|███████▌  | 275000/363846 [00:30<00:09, 9265.06 examples/s]Running tokenizer on dataset:  76%|███████▌  | 276000/363846 [00:30<00:09, 9136.34 examples/s]Running tokenizer on dataset:  76%|███████▌  | 277000/363846 [00:31<00:09, 9168.74 examples/s]Running tokenizer on dataset:  76%|███████▋  | 278000/363846 [00:31<00:09, 9263.28 examples/s]Running tokenizer on dataset:  77%|███████▋  | 279000/363846 [00:31<00:09, 9310.03 examples/s]Running tokenizer on dataset:  77%|███████▋  | 280000/363846 [00:31<00:08, 9336.58 examples/s]Running tokenizer on dataset:  77%|███████▋  | 281000/363846 [00:31<00:08, 9217.71 examples/s]Running tokenizer on dataset:  78%|███████▊  | 282000/363846 [00:31<00:08, 9182.76 examples/s]Running tokenizer on dataset:  78%|███████▊  | 283000/363846 [00:31<00:08, 9293.30 examples/s]Running tokenizer on dataset:  78%|███████▊  | 284000/363846 [00:31<00:08, 9354.69 examples/s]Running tokenizer on dataset:  78%|███████▊  | 285000/363846 [00:31<00:08, 9303.87 examples/s]Running tokenizer on dataset:  79%|███████▊  | 286000/363846 [00:32<00:10, 7256.14 examples/s]Running tokenizer on dataset:  79%|███████▉  | 287000/363846 [00:32<00:09, 7736.36 examples/s]Running tokenizer on dataset:  79%|███████▉  | 288000/363846 [00:32<00:09, 8160.81 examples/s]Running tokenizer on dataset:  79%|███████▉  | 289000/363846 [00:32<00:08, 8484.88 examples/s]Running tokenizer on dataset:  80%|███████▉  | 290000/363846 [00:32<00:08, 8747.27 examples/s]Running tokenizer on dataset:  80%|███████▉  | 291000/363846 [00:32<00:08, 8846.14 examples/s]Running tokenizer on dataset:  80%|████████  | 292000/363846 [00:32<00:07, 8999.73 examples/s]Running tokenizer on dataset:  81%|████████  | 293000/363846 [00:32<00:07, 9118.49 examples/s]Running tokenizer on dataset:  81%|████████  | 294000/363846 [00:33<00:07, 9230.58 examples/s]Running tokenizer on dataset:  81%|████████  | 295000/363846 [00:33<00:07, 9226.56 examples/s]Running tokenizer on dataset:  81%|████████▏ | 296000/363846 [00:33<00:07, 9268.89 examples/s]Running tokenizer on dataset:  82%|████████▏ | 297000/363846 [00:33<00:07, 9273.86 examples/s]Running tokenizer on dataset:  82%|████████▏ | 298000/363846 [00:33<00:07, 9297.66 examples/s]Running tokenizer on dataset:  82%|████████▏ | 299000/363846 [00:33<00:06, 9315.82 examples/s]Running tokenizer on dataset:  82%|████████▏ | 300000/363846 [00:33<00:06, 9239.16 examples/s]Running tokenizer on dataset:  83%|████████▎ | 301000/363846 [00:33<00:06, 9217.13 examples/s]Running tokenizer on dataset:  83%|████████▎ | 302000/363846 [00:33<00:06, 9240.53 examples/s]Running tokenizer on dataset:  83%|████████▎ | 303000/363846 [00:33<00:06, 9311.56 examples/s]Running tokenizer on dataset:  84%|████████▎ | 304000/363846 [00:34<00:06, 9409.54 examples/s]Running tokenizer on dataset:  84%|████████▍ | 305000/363846 [00:34<00:06, 9376.05 examples/s]Running tokenizer on dataset:  84%|████████▍ | 306000/363846 [00:34<00:06, 9289.94 examples/s]Running tokenizer on dataset:  84%|████████▍ | 307000/363846 [00:34<00:06, 9299.21 examples/s]Running tokenizer on dataset:  85%|████████▍ | 308000/363846 [00:34<00:06, 9210.28 examples/s]Running tokenizer on dataset:  85%|████████▍ | 309000/363846 [00:34<00:05, 9286.74 examples/s]Running tokenizer on dataset:  85%|████████▌ | 310000/363846 [00:34<00:07, 7221.68 examples/s]Running tokenizer on dataset:  85%|████████▌ | 311000/363846 [00:34<00:06, 7746.49 examples/s]Running tokenizer on dataset:  86%|████████▌ | 312000/363846 [00:35<00:06, 8155.07 examples/s]Running tokenizer on dataset:  86%|████████▌ | 313000/363846 [00:35<00:06, 8455.97 examples/s]Running tokenizer on dataset:  86%|████████▋ | 314000/363846 [00:35<00:05, 8721.50 examples/s]Running tokenizer on dataset:  87%|████████▋ | 315000/363846 [00:35<00:05, 8890.49 examples/s]Running tokenizer on dataset:  87%|████████▋ | 316000/363846 [00:35<00:05, 8940.69 examples/s]Running tokenizer on dataset:  87%|████████▋ | 317000/363846 [00:35<00:05, 9055.19 examples/s]Running tokenizer on dataset:  87%|████████▋ | 318000/363846 [00:35<00:05, 9141.69 examples/s]Running tokenizer on dataset:  88%|████████▊ | 319000/363846 [00:35<00:04, 9236.19 examples/s]Running tokenizer on dataset:  88%|████████▊ | 320000/363846 [00:35<00:04, 9306.70 examples/s]Running tokenizer on dataset:  88%|████████▊ | 321000/363846 [00:36<00:04, 9176.63 examples/s]Running tokenizer on dataset:  88%|████████▊ | 322000/363846 [00:36<00:04, 9261.20 examples/s]Running tokenizer on dataset:  89%|████████▉ | 323000/363846 [00:36<00:04, 9316.84 examples/s]Running tokenizer on dataset:  89%|████████▉ | 324000/363846 [00:36<00:04, 9348.66 examples/s]Running tokenizer on dataset:  89%|████████▉ | 325000/363846 [00:36<00:04, 9412.67 examples/s]Running tokenizer on dataset:  90%|████████▉ | 326000/363846 [00:36<00:04, 9338.85 examples/s]Running tokenizer on dataset:  90%|████████▉ | 327000/363846 [00:36<00:03, 9342.93 examples/s]Running tokenizer on dataset:  90%|█████████ | 328000/363846 [00:36<00:03, 9259.56 examples/s]Running tokenizer on dataset:  90%|█████████ | 329000/363846 [00:36<00:03, 9336.21 examples/s]Running tokenizer on dataset:  91%|█████████ | 330000/363846 [00:36<00:03, 9411.88 examples/s]Running tokenizer on dataset:  91%|█████████ | 331000/363846 [00:37<00:03, 9348.51 examples/s]Running tokenizer on dataset:  91%|█████████ | 332000/363846 [00:37<00:03, 9358.70 examples/s]Running tokenizer on dataset:  92%|█████████▏| 333000/363846 [00:37<00:03, 9304.24 examples/s]Running tokenizer on dataset:  92%|█████████▏| 334000/363846 [00:37<00:04, 7286.73 examples/s]Running tokenizer on dataset:  92%|█████████▏| 335000/363846 [00:37<00:03, 7767.11 examples/s]Running tokenizer on dataset:  92%|█████████▏| 336000/363846 [00:37<00:03, 8085.35 examples/s]Running tokenizer on dataset:  93%|█████████▎| 337000/363846 [00:37<00:03, 8470.22 examples/s]Running tokenizer on dataset:  93%|█████████▎| 338000/363846 [00:37<00:02, 8738.72 examples/s]Running tokenizer on dataset:  93%|█████████▎| 339000/363846 [00:38<00:02, 8865.89 examples/s]Running tokenizer on dataset:  93%|█████████▎| 340000/363846 [00:38<00:02, 9009.67 examples/s]Running tokenizer on dataset:  94%|█████████▎| 341000/363846 [00:38<00:02, 9056.91 examples/s]Running tokenizer on dataset:  94%|█████████▍| 342000/363846 [00:38<00:02, 9146.37 examples/s]Running tokenizer on dataset:  94%|█████████▍| 343000/363846 [00:38<00:02, 9226.41 examples/s]Running tokenizer on dataset:  95%|█████████▍| 344000/363846 [00:38<00:02, 9318.74 examples/s]Running tokenizer on dataset:  95%|█████████▍| 345000/363846 [00:38<00:02, 9288.73 examples/s]Running tokenizer on dataset:  95%|█████████▌| 346000/363846 [00:38<00:01, 9300.27 examples/s]Running tokenizer on dataset:  95%|█████████▌| 347000/363846 [00:38<00:01, 9181.49 examples/s]Running tokenizer on dataset:  96%|█████████▌| 348000/363846 [00:39<00:01, 8982.53 examples/s]Running tokenizer on dataset:  96%|█████████▌| 349000/363846 [00:39<00:01, 9151.15 examples/s]Running tokenizer on dataset:  96%|█████████▌| 350000/363846 [00:39<00:01, 9227.71 examples/s]Running tokenizer on dataset:  96%|█████████▋| 351000/363846 [00:39<00:01, 9209.29 examples/s]Running tokenizer on dataset:  97%|█████████▋| 352000/363846 [00:39<00:01, 9198.38 examples/s]Running tokenizer on dataset:  97%|█████████▋| 353000/363846 [00:39<00:01, 9257.84 examples/s]Running tokenizer on dataset:  97%|█████████▋| 354000/363846 [00:39<00:01, 9170.18 examples/s]Running tokenizer on dataset:  98%|█████████▊| 355000/363846 [00:39<00:00, 9348.80 examples/s]Running tokenizer on dataset:  98%|█████████▊| 356000/363846 [00:39<00:00, 9273.45 examples/s]Running tokenizer on dataset:  98%|█████████▊| 357000/363846 [00:40<00:00, 9259.36 examples/s]Running tokenizer on dataset:  98%|█████████▊| 358000/363846 [00:40<00:00, 6967.91 examples/s]Running tokenizer on dataset:  99%|█████████▊| 359000/363846 [00:40<00:00, 7478.97 examples/s]Running tokenizer on dataset:  99%|█████████▉| 360000/363846 [00:40<00:00, 7960.24 examples/s]Running tokenizer on dataset:  99%|█████████▉| 361000/363846 [00:40<00:00, 8288.88 examples/s]Running tokenizer on dataset:  99%|█████████▉| 362000/363846 [00:40<00:00, 8648.22 examples/s]Running tokenizer on dataset: 100%|█████████▉| 363000/363846 [00:40<00:00, 8891.86 examples/s]Running tokenizer on dataset: 100%|██████████| 363846/363846 [00:40<00:00, 8901.67 examples/s]
Running tokenizer on dataset:   0%|          | 0/40430 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8b1cfe379831e3cf.arrow
02/12/2024 19:13:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8b1cfe379831e3cf.arrow
Running tokenizer on dataset:   2%|▏         | 1000/40430 [00:00<00:04, 9002.53 examples/s]Running tokenizer on dataset:   5%|▍         | 2000/40430 [00:00<00:04, 8996.63 examples/s]Running tokenizer on dataset:   7%|▋         | 3000/40430 [00:00<00:04, 9182.38 examples/s]Running tokenizer on dataset:  10%|▉         | 4000/40430 [00:00<00:03, 9297.62 examples/s]Running tokenizer on dataset:  12%|█▏        | 5000/40430 [00:00<00:03, 9346.94 examples/s]Running tokenizer on dataset:  15%|█▍        | 6000/40430 [00:00<00:03, 9297.68 examples/s]Running tokenizer on dataset:  17%|█▋        | 7000/40430 [00:00<00:03, 9251.85 examples/s]Running tokenizer on dataset:  20%|█▉        | 8000/40430 [00:00<00:03, 9296.89 examples/s]Running tokenizer on dataset:  22%|██▏       | 9000/40430 [00:00<00:03, 9220.26 examples/s]Running tokenizer on dataset:  25%|██▍       | 10000/40430 [00:01<00:03, 9127.02 examples/s]Running tokenizer on dataset:  27%|██▋       | 11000/40430 [00:01<00:03, 8932.00 examples/s]Running tokenizer on dataset:  30%|██▉       | 12000/40430 [00:01<00:03, 8827.37 examples/s]Running tokenizer on dataset:  32%|███▏      | 13000/40430 [00:01<00:03, 8731.99 examples/s]Running tokenizer on dataset:  35%|███▍      | 14000/40430 [00:01<00:03, 8630.42 examples/s]Running tokenizer on dataset:  37%|███▋      | 15000/40430 [00:01<00:02, 8690.52 examples/s]Running tokenizer on dataset:  40%|███▉      | 16000/40430 [00:01<00:02, 8917.11 examples/s]Running tokenizer on dataset:  42%|████▏     | 17000/40430 [00:01<00:02, 9073.25 examples/s]Running tokenizer on dataset:  45%|████▍     | 18000/40430 [00:02<00:03, 6881.45 examples/s]Running tokenizer on dataset:  47%|████▋     | 19000/40430 [00:02<00:02, 7301.79 examples/s]Running tokenizer on dataset:  49%|████▉     | 20000/40430 [00:02<00:02, 7571.68 examples/s]Running tokenizer on dataset:  52%|█████▏    | 21000/40430 [00:02<00:02, 7792.20 examples/s]Running tokenizer on dataset:  54%|█████▍    | 22000/40430 [00:02<00:02, 8055.29 examples/s]Running tokenizer on dataset:  57%|█████▋    | 23000/40430 [00:02<00:02, 8178.01 examples/s]Running tokenizer on dataset:  59%|█████▉    | 24000/40430 [00:02<00:01, 8345.07 examples/s]Running tokenizer on dataset:  62%|██████▏   | 25000/40430 [00:02<00:01, 8427.38 examples/s]Running tokenizer on dataset:  64%|██████▍   | 26000/40430 [00:03<00:01, 8332.78 examples/s]Running tokenizer on dataset:  67%|██████▋   | 27000/40430 [00:03<00:01, 8366.69 examples/s]Running tokenizer on dataset:  69%|██████▉   | 28000/40430 [00:03<00:01, 8408.39 examples/s]Running tokenizer on dataset:  72%|███████▏  | 29000/40430 [00:03<00:01, 8347.26 examples/s]Running tokenizer on dataset:  74%|███████▍  | 30000/40430 [00:03<00:01, 8380.06 examples/s]Running tokenizer on dataset:  77%|███████▋  | 31000/40430 [00:03<00:01, 8317.92 examples/s]Running tokenizer on dataset:  79%|███████▉  | 32000/40430 [00:03<00:01, 8289.70 examples/s]Running tokenizer on dataset:  82%|████████▏ | 33000/40430 [00:03<00:00, 8306.99 examples/s]Running tokenizer on dataset:  84%|████████▍ | 34000/40430 [00:04<00:00, 8351.52 examples/s]Running tokenizer on dataset:  87%|████████▋ | 35000/40430 [00:04<00:00, 8293.52 examples/s]Running tokenizer on dataset:  89%|████████▉ | 36000/40430 [00:04<00:00, 8349.71 examples/s]Running tokenizer on dataset:  92%|█████████▏| 37000/40430 [00:04<00:00, 8453.70 examples/s]Running tokenizer on dataset:  94%|█████████▍| 38000/40430 [00:04<00:00, 8341.66 examples/s]Running tokenizer on dataset:  96%|█████████▋| 39000/40430 [00:04<00:00, 8393.75 examples/s]Running tokenizer on dataset:  99%|█████████▉| 40000/40430 [00:04<00:00, 8448.26 examples/s]Running tokenizer on dataset: 100%|██████████| 40430/40430 [00:04<00:00, 8450.43 examples/s]
Running tokenizer on dataset:   0%|          | 0/390965 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-87ed31448fb56bbc.arrow
02/12/2024 19:13:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-87ed31448fb56bbc.arrow
Running tokenizer on dataset:   0%|          | 1000/390965 [00:00<00:44, 8860.89 examples/s]Running tokenizer on dataset:   1%|          | 2000/390965 [00:00<00:44, 8654.89 examples/s]Running tokenizer on dataset:   1%|          | 3000/390965 [00:00<01:03, 6149.33 examples/s]Running tokenizer on dataset:   1%|          | 4000/390965 [00:00<00:56, 6888.93 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/390965 [00:00<00:52, 7384.82 examples/s]Running tokenizer on dataset:   2%|▏         | 6000/390965 [00:00<00:50, 7644.87 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/390965 [00:00<00:48, 7868.96 examples/s]Running tokenizer on dataset:   2%|▏         | 8000/390965 [00:01<00:47, 8024.12 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/390965 [00:01<00:46, 8251.38 examples/s]Running tokenizer on dataset:   3%|▎         | 10000/390965 [00:01<00:44, 8501.45 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/390965 [00:01<00:44, 8564.62 examples/s]Running tokenizer on dataset:   3%|▎         | 12000/390965 [00:01<00:44, 8598.79 examples/s]Running tokenizer on dataset:   3%|▎         | 13000/390965 [00:01<00:44, 8484.17 examples/s]Running tokenizer on dataset:   4%|▎         | 14000/390965 [00:01<00:44, 8551.40 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/390965 [00:01<00:44, 8471.90 examples/s]Running tokenizer on dataset:   4%|▍         | 16000/390965 [00:01<00:43, 8545.51 examples/s]Running tokenizer on dataset:   4%|▍         | 17000/390965 [00:02<00:43, 8559.90 examples/s]Running tokenizer on dataset:   5%|▍         | 18000/390965 [00:02<00:43, 8567.82 examples/s]Running tokenizer on dataset:   5%|▍         | 19000/390965 [00:02<00:43, 8489.68 examples/s]Running tokenizer on dataset:   5%|▌         | 20000/390965 [00:02<00:43, 8491.59 examples/s]Running tokenizer on dataset:   5%|▌         | 21000/390965 [00:02<00:44, 8364.99 examples/s]Running tokenizer on dataset:   6%|▌         | 22000/390965 [00:02<00:44, 8374.47 examples/s]Running tokenizer on dataset:   6%|▌         | 23000/390965 [00:02<00:43, 8368.37 examples/s]Running tokenizer on dataset:   6%|▌         | 24000/390965 [00:02<00:43, 8415.63 examples/s]Running tokenizer on dataset:   6%|▋         | 25000/390965 [00:03<00:42, 8639.19 examples/s]Running tokenizer on dataset:   7%|▋         | 26000/390965 [00:03<00:42, 8623.79 examples/s]Running tokenizer on dataset:   7%|▋         | 27000/390965 [00:03<00:42, 8553.35 examples/s]Running tokenizer on dataset:   7%|▋         | 28000/390965 [00:03<00:42, 8508.47 examples/s]Running tokenizer on dataset:   7%|▋         | 29000/390965 [00:03<00:42, 8607.10 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/390965 [00:03<00:42, 8536.33 examples/s]Running tokenizer on dataset:   8%|▊         | 31000/390965 [00:03<00:43, 8244.57 examples/s]Running tokenizer on dataset:   8%|▊         | 32000/390965 [00:03<00:43, 8274.09 examples/s]Running tokenizer on dataset:   8%|▊         | 33000/390965 [00:03<00:42, 8343.18 examples/s]Running tokenizer on dataset:   9%|▊         | 34000/390965 [00:04<00:42, 8437.80 examples/s]Running tokenizer on dataset:   9%|▉         | 35000/390965 [00:04<00:41, 8499.23 examples/s]Running tokenizer on dataset:   9%|▉         | 36000/390965 [00:04<00:42, 8405.69 examples/s]Running tokenizer on dataset:   9%|▉         | 37000/390965 [00:04<00:41, 8449.26 examples/s]Running tokenizer on dataset:  10%|▉         | 38000/390965 [00:04<00:40, 8653.96 examples/s]Running tokenizer on dataset:  10%|▉         | 39000/390965 [00:04<00:39, 8898.46 examples/s]Running tokenizer on dataset:  10%|█         | 40000/390965 [00:04<00:39, 8974.86 examples/s]Running tokenizer on dataset:  10%|█         | 41000/390965 [00:04<00:39, 8755.65 examples/s]Running tokenizer on dataset:  11%|█         | 42000/390965 [00:05<00:40, 8579.50 examples/s]Running tokenizer on dataset:  11%|█         | 43000/390965 [00:05<00:40, 8579.39 examples/s]Running tokenizer on dataset:  11%|█▏        | 44000/390965 [00:05<00:40, 8528.05 examples/s]Running tokenizer on dataset:  12%|█▏        | 45000/390965 [00:05<00:41, 8429.91 examples/s]Running tokenizer on dataset:  12%|█▏        | 46000/390965 [00:05<00:41, 8409.57 examples/s]Running tokenizer on dataset:  12%|█▏        | 47000/390965 [00:05<00:40, 8409.02 examples/s]Running tokenizer on dataset:  12%|█▏        | 48000/390965 [00:05<00:41, 8321.45 examples/s]Running tokenizer on dataset:  13%|█▎        | 49000/390965 [00:05<00:54, 6282.28 examples/s]Running tokenizer on dataset:  13%|█▎        | 50000/390965 [00:06<00:50, 6751.19 examples/s]Running tokenizer on dataset:  13%|█▎        | 51000/390965 [00:06<00:48, 7053.20 examples/s]Running tokenizer on dataset:  13%|█▎        | 52000/390965 [00:06<00:46, 7355.64 examples/s]Running tokenizer on dataset:  14%|█▎        | 53000/390965 [00:06<00:44, 7659.01 examples/s]Running tokenizer on dataset:  14%|█▍        | 54000/390965 [00:06<00:43, 7825.72 examples/s]Running tokenizer on dataset:  14%|█▍        | 55000/390965 [00:06<00:42, 7905.02 examples/s]Running tokenizer on dataset:  14%|█▍        | 56000/390965 [00:06<00:41, 8042.79 examples/s]Running tokenizer on dataset:  15%|█▍        | 57000/390965 [00:06<00:40, 8220.63 examples/s]Running tokenizer on dataset:  15%|█▍        | 58000/390965 [00:07<00:40, 8301.81 examples/s]Running tokenizer on dataset:  15%|█▌        | 59000/390965 [00:07<00:40, 8290.41 examples/s]Running tokenizer on dataset:  15%|█▌        | 60000/390965 [00:07<00:39, 8304.36 examples/s]Running tokenizer on dataset:  16%|█▌        | 61000/390965 [00:07<00:40, 8227.92 examples/s]Running tokenizer on dataset:  16%|█▌        | 62000/390965 [00:07<00:39, 8266.09 examples/s]Running tokenizer on dataset:  16%|█▌        | 63000/390965 [00:07<00:39, 8325.34 examples/s]Running tokenizer on dataset:  16%|█▋        | 64000/390965 [00:07<00:38, 8429.60 examples/s]Running tokenizer on dataset:  17%|█▋        | 65000/390965 [00:07<00:38, 8560.65 examples/s]Running tokenizer on dataset:  17%|█▋        | 66000/390965 [00:08<00:37, 8768.01 examples/s]Running tokenizer on dataset:  17%|█▋        | 67000/390965 [00:08<00:36, 8961.73 examples/s]Running tokenizer on dataset:  17%|█▋        | 68000/390965 [00:08<00:35, 8989.70 examples/s]Running tokenizer on dataset:  18%|█▊        | 69000/390965 [00:08<00:36, 8863.82 examples/s]Running tokenizer on dataset:  18%|█▊        | 70000/390965 [00:08<00:36, 8732.96 examples/s]Running tokenizer on dataset:  18%|█▊        | 71000/390965 [00:08<00:37, 8623.66 examples/s]Running tokenizer on dataset:  18%|█▊        | 72000/390965 [00:08<00:37, 8540.62 examples/s]Running tokenizer on dataset:  19%|█▊        | 73000/390965 [00:08<00:37, 8426.25 examples/s]Running tokenizer on dataset:  19%|█▉        | 74000/390965 [00:08<00:37, 8374.34 examples/s]Running tokenizer on dataset:  19%|█▉        | 75000/390965 [00:09<00:37, 8389.21 examples/s]Running tokenizer on dataset:  19%|█▉        | 76000/390965 [00:09<00:37, 8325.58 examples/s]Running tokenizer on dataset:  20%|█▉        | 77000/390965 [00:09<00:37, 8468.45 examples/s]Running tokenizer on dataset:  20%|█▉        | 78000/390965 [00:09<00:35, 8729.97 examples/s]Running tokenizer on dataset:  20%|██        | 79000/390965 [00:09<00:34, 8921.25 examples/s]Running tokenizer on dataset:  20%|██        | 80000/390965 [00:09<00:34, 8909.50 examples/s]Running tokenizer on dataset:  21%|██        | 81000/390965 [00:09<00:35, 8684.99 examples/s]Running tokenizer on dataset:  21%|██        | 82000/390965 [00:09<00:36, 8529.14 examples/s]Running tokenizer on dataset:  21%|██        | 83000/390965 [00:09<00:36, 8470.29 examples/s]Running tokenizer on dataset:  21%|██▏       | 84000/390965 [00:10<00:36, 8397.03 examples/s]Running tokenizer on dataset:  22%|██▏       | 85000/390965 [00:10<00:47, 6463.30 examples/s]Running tokenizer on dataset:  22%|██▏       | 86000/390965 [00:10<00:44, 6877.44 examples/s]Running tokenizer on dataset:  22%|██▏       | 87000/390965 [00:10<00:41, 7358.26 examples/s]Running tokenizer on dataset:  23%|██▎       | 88000/390965 [00:10<00:39, 7624.81 examples/s]Running tokenizer on dataset:  23%|██▎       | 89000/390965 [00:10<00:37, 8040.78 examples/s]Running tokenizer on dataset:  23%|██▎       | 90000/390965 [00:10<00:35, 8404.43 examples/s]Running tokenizer on dataset:  23%|██▎       | 91000/390965 [00:11<00:34, 8616.84 examples/s]Running tokenizer on dataset:  24%|██▎       | 92000/390965 [00:11<00:34, 8740.08 examples/s]Running tokenizer on dataset:  24%|██▍       | 93000/390965 [00:11<00:34, 8675.92 examples/s]Running tokenizer on dataset:  24%|██▍       | 94000/390965 [00:11<00:34, 8563.87 examples/s]Running tokenizer on dataset:  24%|██▍       | 95000/390965 [00:11<00:34, 8497.53 examples/s]Running tokenizer on dataset:  25%|██▍       | 96000/390965 [00:11<00:34, 8467.14 examples/s]Running tokenizer on dataset:  25%|██▍       | 97000/390965 [00:11<00:34, 8401.96 examples/s]Running tokenizer on dataset:  25%|██▌       | 98000/390965 [00:11<00:33, 8650.03 examples/s]Running tokenizer on dataset:  25%|██▌       | 99000/390965 [00:11<00:33, 8727.64 examples/s]Running tokenizer on dataset:  26%|██▌       | 100000/390965 [00:12<00:33, 8673.86 examples/s]Running tokenizer on dataset:  26%|██▌       | 101000/390965 [00:12<00:33, 8553.79 examples/s]Running tokenizer on dataset:  26%|██▌       | 102000/390965 [00:12<00:34, 8421.13 examples/s]Running tokenizer on dataset:  26%|██▋       | 103000/390965 [00:12<00:34, 8352.85 examples/s]Running tokenizer on dataset:  27%|██▋       | 104000/390965 [00:12<00:34, 8402.02 examples/s]Running tokenizer on dataset:  27%|██▋       | 105000/390965 [00:12<00:34, 8403.18 examples/s]Running tokenizer on dataset:  27%|██▋       | 106000/390965 [00:12<00:34, 8314.34 examples/s]Running tokenizer on dataset:  27%|██▋       | 107000/390965 [00:12<00:34, 8284.42 examples/s]Running tokenizer on dataset:  28%|██▊       | 108000/390965 [00:13<00:33, 8326.48 examples/s]Running tokenizer on dataset:  28%|██▊       | 109000/390965 [00:13<00:33, 8305.15 examples/s]Running tokenizer on dataset:  28%|██▊       | 110000/390965 [00:13<00:33, 8368.90 examples/s]Running tokenizer on dataset:  28%|██▊       | 111000/390965 [00:13<00:33, 8397.15 examples/s]Running tokenizer on dataset:  29%|██▊       | 112000/390965 [00:13<00:33, 8434.81 examples/s]Running tokenizer on dataset:  29%|██▉       | 113000/390965 [00:13<00:32, 8544.57 examples/s]Running tokenizer on dataset:  29%|██▉       | 114000/390965 [00:13<00:32, 8607.92 examples/s]Running tokenizer on dataset:  29%|██▉       | 115000/390965 [00:13<00:41, 6588.57 examples/s]Running tokenizer on dataset:  30%|██▉       | 116000/390965 [00:14<00:39, 6974.95 examples/s]Running tokenizer on dataset:  30%|██▉       | 117000/390965 [00:14<00:37, 7290.58 examples/s]Running tokenizer on dataset:  30%|███       | 118000/390965 [00:14<00:37, 7346.16 examples/s]Running tokenizer on dataset:  30%|███       | 119000/390965 [00:14<00:35, 7675.12 examples/s]Running tokenizer on dataset:  31%|███       | 120000/390965 [00:14<00:34, 7896.17 examples/s]Running tokenizer on dataset:  31%|███       | 121000/390965 [00:14<00:33, 7964.92 examples/s]Running tokenizer on dataset:  31%|███       | 122000/390965 [00:14<00:33, 8035.08 examples/s]Running tokenizer on dataset:  31%|███▏      | 123000/390965 [00:14<00:33, 8083.78 examples/s]Running tokenizer on dataset:  32%|███▏      | 124000/390965 [00:15<00:33, 8079.38 examples/s]Running tokenizer on dataset:  32%|███▏      | 125000/390965 [00:15<00:32, 8117.98 examples/s]Running tokenizer on dataset:  32%|███▏      | 126000/390965 [00:15<00:32, 8066.12 examples/s]Running tokenizer on dataset:  32%|███▏      | 127000/390965 [00:15<00:32, 8005.00 examples/s]Running tokenizer on dataset:  33%|███▎      | 128000/390965 [00:15<00:32, 8058.97 examples/s]Running tokenizer on dataset:  33%|███▎      | 129000/390965 [00:15<00:31, 8212.52 examples/s]Running tokenizer on dataset:  33%|███▎      | 130000/390965 [00:15<00:31, 8198.74 examples/s]Running tokenizer on dataset:  34%|███▎      | 131000/390965 [00:15<00:31, 8170.44 examples/s]Running tokenizer on dataset:  34%|███▍      | 132000/390965 [00:16<00:31, 8176.25 examples/s]Running tokenizer on dataset:  34%|███▍      | 133000/390965 [00:16<00:31, 8134.32 examples/s]Running tokenizer on dataset:  34%|███▍      | 134000/390965 [00:16<00:31, 8064.03 examples/s]Running tokenizer on dataset:  35%|███▍      | 135000/390965 [00:16<00:32, 7972.03 examples/s]Running tokenizer on dataset:  35%|███▍      | 136000/390965 [00:16<00:31, 8046.24 examples/s]Running tokenizer on dataset:  35%|███▌      | 137000/390965 [00:16<00:31, 8128.14 examples/s]Running tokenizer on dataset:  35%|███▌      | 138000/390965 [00:16<00:30, 8211.73 examples/s]Running tokenizer on dataset:  36%|███▌      | 139000/390965 [00:16<00:30, 8379.12 examples/s]Running tokenizer on dataset:  36%|███▌      | 140000/390965 [00:17<00:39, 6391.61 examples/s]Running tokenizer on dataset:  36%|███▌      | 141000/390965 [00:17<00:35, 6996.42 examples/s]Running tokenizer on dataset:  36%|███▋      | 142000/390965 [00:17<00:33, 7482.81 examples/s]Running tokenizer on dataset:  37%|███▋      | 143000/390965 [00:17<00:32, 7712.35 examples/s]Running tokenizer on dataset:  37%|███▋      | 144000/390965 [00:17<00:31, 7718.35 examples/s]Running tokenizer on dataset:  37%|███▋      | 145000/390965 [00:17<00:30, 8005.84 examples/s]Running tokenizer on dataset:  37%|███▋      | 146000/390965 [00:17<00:29, 8237.42 examples/s]Running tokenizer on dataset:  38%|███▊      | 147000/390965 [00:17<00:29, 8372.12 examples/s]Running tokenizer on dataset:  38%|███▊      | 148000/390965 [00:18<00:28, 8562.30 examples/s]Running tokenizer on dataset:  38%|███▊      | 149000/390965 [00:18<00:27, 8844.64 examples/s]Running tokenizer on dataset:  38%|███▊      | 150000/390965 [00:18<00:27, 8740.86 examples/s]Running tokenizer on dataset:  39%|███▊      | 151000/390965 [00:18<00:27, 8709.16 examples/s]Running tokenizer on dataset:  39%|███▉      | 152000/390965 [00:18<00:27, 8556.36 examples/s]Running tokenizer on dataset:  39%|███▉      | 153000/390965 [00:18<00:28, 8476.12 examples/s]Running tokenizer on dataset:  39%|███▉      | 154000/390965 [00:18<00:27, 8728.56 examples/s]Running tokenizer on dataset:  40%|███▉      | 155000/390965 [00:18<00:26, 8924.41 examples/s]Running tokenizer on dataset:  40%|███▉      | 156000/390965 [00:18<00:26, 8986.82 examples/s]Running tokenizer on dataset:  40%|████      | 157000/390965 [00:19<00:26, 8813.45 examples/s]Running tokenizer on dataset:  40%|████      | 158000/390965 [00:19<00:27, 8583.09 examples/s]Running tokenizer on dataset:  41%|████      | 159000/390965 [00:19<00:27, 8439.37 examples/s]Running tokenizer on dataset:  41%|████      | 160000/390965 [00:19<00:27, 8392.91 examples/s]Running tokenizer on dataset:  41%|████      | 161000/390965 [00:19<00:27, 8347.28 examples/s]Running tokenizer on dataset:  41%|████▏     | 162000/390965 [00:19<00:27, 8237.12 examples/s]Running tokenizer on dataset:  42%|████▏     | 163000/390965 [00:19<00:28, 8123.00 examples/s]Running tokenizer on dataset:  42%|████▏     | 164000/390965 [00:20<00:33, 6685.16 examples/s]Running tokenizer on dataset:  42%|████▏     | 165000/390965 [00:20<00:31, 7235.22 examples/s]Running tokenizer on dataset:  42%|████▏     | 166000/390965 [00:20<00:30, 7412.72 examples/s]Running tokenizer on dataset:  43%|████▎     | 167000/390965 [00:20<00:28, 7823.18 examples/s]Running tokenizer on dataset:  43%|████▎     | 168000/390965 [00:20<00:27, 8166.20 examples/s]Running tokenizer on dataset:  43%|████▎     | 169000/390965 [00:20<00:27, 8217.51 examples/s]Running tokenizer on dataset:  43%|████▎     | 170000/390965 [00:20<00:26, 8374.43 examples/s]Running tokenizer on dataset:  44%|████▎     | 171000/390965 [00:20<00:26, 8420.03 examples/s]Running tokenizer on dataset:  44%|████▍     | 172000/390965 [00:20<00:25, 8590.02 examples/s]Running tokenizer on dataset:  44%|████▍     | 173000/390965 [00:21<00:25, 8621.34 examples/s]Running tokenizer on dataset:  45%|████▍     | 174000/390965 [00:21<00:25, 8466.05 examples/s]Running tokenizer on dataset:  45%|████▍     | 175000/390965 [00:21<00:25, 8405.82 examples/s]Running tokenizer on dataset:  45%|████▌     | 176000/390965 [00:21<00:25, 8360.22 examples/s]Running tokenizer on dataset:  45%|████▌     | 177000/390965 [00:21<00:25, 8515.69 examples/s]Running tokenizer on dataset:  46%|████▌     | 178000/390965 [00:21<00:24, 8776.29 examples/s]Running tokenizer on dataset:  46%|████▌     | 179000/390965 [00:21<00:23, 8976.15 examples/s]Running tokenizer on dataset:  46%|████▌     | 180000/390965 [00:21<00:23, 8889.43 examples/s]Running tokenizer on dataset:  46%|████▋     | 181000/390965 [00:22<00:24, 8497.82 examples/s]Running tokenizer on dataset:  47%|████▋     | 182000/390965 [00:22<00:24, 8405.04 examples/s]Running tokenizer on dataset:  47%|████▋     | 183000/390965 [00:22<00:24, 8570.26 examples/s]Running tokenizer on dataset:  47%|████▋     | 184000/390965 [00:22<00:23, 8667.56 examples/s]Running tokenizer on dataset:  47%|████▋     | 185000/390965 [00:22<00:23, 8795.77 examples/s]Running tokenizer on dataset:  48%|████▊     | 186000/390965 [00:22<00:23, 8866.49 examples/s]Running tokenizer on dataset:  48%|████▊     | 187000/390965 [00:22<00:23, 8862.81 examples/s]Running tokenizer on dataset:  48%|████▊     | 188000/390965 [00:22<00:29, 6798.79 examples/s]Running tokenizer on dataset:  48%|████▊     | 189000/390965 [00:23<00:28, 7068.66 examples/s]Running tokenizer on dataset:  49%|████▊     | 190000/390965 [00:23<00:26, 7449.29 examples/s]Running tokenizer on dataset:  49%|████▉     | 191000/390965 [00:23<00:25, 7776.66 examples/s]Running tokenizer on dataset:  49%|████▉     | 192000/390965 [00:23<00:24, 8152.77 examples/s]Running tokenizer on dataset:  49%|████▉     | 193000/390965 [00:23<00:23, 8357.78 examples/s]Running tokenizer on dataset:  50%|████▉     | 194000/390965 [00:23<00:23, 8239.82 examples/s]Running tokenizer on dataset:  50%|████▉     | 195000/390965 [00:23<00:24, 8140.10 examples/s]Running tokenizer on dataset:  50%|█████     | 196000/390965 [00:23<00:24, 8012.32 examples/s]Running tokenizer on dataset:  50%|█████     | 197000/390965 [00:24<00:24, 8065.39 examples/s]Running tokenizer on dataset:  51%|█████     | 198000/390965 [00:24<00:23, 8202.06 examples/s]Running tokenizer on dataset:  51%|█████     | 199000/390965 [00:24<00:22, 8396.51 examples/s]Running tokenizer on dataset:  51%|█████     | 200000/390965 [00:24<00:22, 8565.13 examples/s]Running tokenizer on dataset:  51%|█████▏    | 201000/390965 [00:24<00:22, 8416.33 examples/s]Running tokenizer on dataset:  52%|█████▏    | 202000/390965 [00:24<00:22, 8342.93 examples/s]Running tokenizer on dataset:  52%|█████▏    | 203000/390965 [00:24<00:21, 8566.95 examples/s]Running tokenizer on dataset:  52%|█████▏    | 204000/390965 [00:24<00:21, 8504.85 examples/s]Running tokenizer on dataset:  52%|█████▏    | 205000/390965 [00:24<00:21, 8603.34 examples/s]Running tokenizer on dataset:  53%|█████▎    | 206000/390965 [00:25<00:21, 8631.75 examples/s]Running tokenizer on dataset:  53%|█████▎    | 207000/390965 [00:25<00:20, 8782.73 examples/s]Running tokenizer on dataset:  53%|█████▎    | 208000/390965 [00:25<00:21, 8635.23 examples/s]Running tokenizer on dataset:  53%|█████▎    | 209000/390965 [00:25<00:22, 8209.30 examples/s]Running tokenizer on dataset:  54%|█████▎    | 210000/390965 [00:25<00:22, 8161.73 examples/s]Running tokenizer on dataset:  54%|█████▍    | 211000/390965 [00:25<00:22, 8055.02 examples/s]Running tokenizer on dataset:  54%|█████▍    | 212000/390965 [00:25<00:28, 6384.03 examples/s]Running tokenizer on dataset:  54%|█████▍    | 213000/390965 [00:26<00:25, 6967.97 examples/s]Running tokenizer on dataset:  55%|█████▍    | 214000/390965 [00:26<00:24, 7296.71 examples/s]Running tokenizer on dataset:  55%|█████▍    | 215000/390965 [00:26<00:22, 7677.91 examples/s]Running tokenizer on dataset:  55%|█████▌    | 216000/390965 [00:26<00:21, 8065.26 examples/s]Running tokenizer on dataset:  56%|█████▌    | 217000/390965 [00:26<00:20, 8327.71 examples/s]Running tokenizer on dataset:  56%|█████▌    | 218000/390965 [00:26<00:20, 8493.49 examples/s]Running tokenizer on dataset:  56%|█████▌    | 219000/390965 [00:26<00:20, 8580.30 examples/s]Running tokenizer on dataset:  56%|█████▋    | 220000/390965 [00:26<00:19, 8692.71 examples/s]Running tokenizer on dataset:  57%|█████▋    | 221000/390965 [00:26<00:19, 8644.83 examples/s]Running tokenizer on dataset:  57%|█████▋    | 222000/390965 [00:27<00:19, 8482.41 examples/s]Running tokenizer on dataset:  57%|█████▋    | 223000/390965 [00:27<00:20, 8300.23 examples/s]Running tokenizer on dataset:  57%|█████▋    | 224000/390965 [00:27<00:20, 8261.59 examples/s]Running tokenizer on dataset:  58%|█████▊    | 225000/390965 [00:27<00:20, 8188.96 examples/s]Running tokenizer on dataset:  58%|█████▊    | 226000/390965 [00:27<00:20, 7978.04 examples/s]Running tokenizer on dataset:  58%|█████▊    | 227000/390965 [00:27<00:20, 8017.97 examples/s]Running tokenizer on dataset:  58%|█████▊    | 228000/390965 [00:27<00:20, 8018.57 examples/s]Running tokenizer on dataset:  59%|█████▊    | 229000/390965 [00:27<00:19, 8213.47 examples/s]Running tokenizer on dataset:  59%|█████▉    | 230000/390965 [00:28<00:18, 8495.79 examples/s]Running tokenizer on dataset:  59%|█████▉    | 231000/390965 [00:28<00:18, 8612.03 examples/s]Running tokenizer on dataset:  59%|█████▉    | 232000/390965 [00:28<00:18, 8715.14 examples/s]Running tokenizer on dataset:  60%|█████▉    | 233000/390965 [00:28<00:17, 8889.24 examples/s]Running tokenizer on dataset:  60%|█████▉    | 234000/390965 [00:28<00:18, 8705.29 examples/s]Running tokenizer on dataset:  60%|██████    | 235000/390965 [00:28<00:18, 8377.75 examples/s]Running tokenizer on dataset:  60%|██████    | 236000/390965 [00:28<00:23, 6529.24 examples/s]Running tokenizer on dataset:  61%|██████    | 237000/390965 [00:28<00:21, 7064.85 examples/s]Running tokenizer on dataset:  61%|██████    | 238000/390965 [00:29<00:20, 7580.42 examples/s]Running tokenizer on dataset:  61%|██████    | 239000/390965 [00:29<00:18, 8024.46 examples/s]Running tokenizer on dataset:  61%|██████▏   | 240000/390965 [00:29<00:18, 8259.06 examples/s]Running tokenizer on dataset:  62%|██████▏   | 241000/390965 [00:29<00:18, 8144.00 examples/s]Running tokenizer on dataset:  62%|██████▏   | 242000/390965 [00:29<00:18, 8252.77 examples/s]Running tokenizer on dataset:  62%|██████▏   | 243000/390965 [00:29<00:17, 8306.35 examples/s]Running tokenizer on dataset:  62%|██████▏   | 244000/390965 [00:29<00:17, 8472.34 examples/s]Running tokenizer on dataset:  63%|██████▎   | 245000/390965 [00:29<00:16, 8634.12 examples/s]Running tokenizer on dataset:  63%|██████▎   | 246000/390965 [00:30<00:16, 8702.51 examples/s]Running tokenizer on dataset:  63%|██████▎   | 247000/390965 [00:30<00:16, 8591.86 examples/s]Running tokenizer on dataset:  63%|██████▎   | 248000/390965 [00:30<00:16, 8589.02 examples/s]Running tokenizer on dataset:  64%|██████▎   | 249000/390965 [00:30<00:16, 8763.76 examples/s]Running tokenizer on dataset:  64%|██████▍   | 250000/390965 [00:30<00:15, 8837.23 examples/s]Running tokenizer on dataset:  64%|██████▍   | 251000/390965 [00:30<00:15, 8895.12 examples/s]Running tokenizer on dataset:  64%|██████▍   | 252000/390965 [00:30<00:15, 8949.75 examples/s]Running tokenizer on dataset:  65%|██████▍   | 253000/390965 [00:30<00:15, 9056.18 examples/s]Running tokenizer on dataset:  65%|██████▍   | 254000/390965 [00:30<00:15, 9004.97 examples/s]Running tokenizer on dataset:  65%|██████▌   | 255000/390965 [00:31<00:15, 8655.16 examples/s]Running tokenizer on dataset:  65%|██████▌   | 256000/390965 [00:31<00:17, 7771.03 examples/s]Running tokenizer on dataset:  66%|██████▌   | 257000/390965 [00:31<00:17, 7869.38 examples/s]Running tokenizer on dataset:  66%|██████▌   | 258000/390965 [00:31<00:16, 8099.28 examples/s]Running tokenizer on dataset:  66%|██████▌   | 259000/390965 [00:31<00:15, 8436.43 examples/s]Running tokenizer on dataset:  67%|██████▋   | 260000/390965 [00:31<00:19, 6748.97 examples/s]Running tokenizer on dataset:  67%|██████▋   | 261000/390965 [00:31<00:18, 7127.80 examples/s]Running tokenizer on dataset:  67%|██████▋   | 262000/390965 [00:31<00:17, 7539.34 examples/s]Running tokenizer on dataset:  67%|██████▋   | 263000/390965 [00:32<00:16, 7889.94 examples/s]Running tokenizer on dataset:  68%|██████▊   | 264000/390965 [00:32<00:15, 8071.30 examples/s]Running tokenizer on dataset:  68%|██████▊   | 265000/390965 [00:32<00:15, 8392.79 examples/s]Running tokenizer on dataset:  68%|██████▊   | 266000/390965 [00:32<00:14, 8581.24 examples/s]Running tokenizer on dataset:  68%|██████▊   | 267000/390965 [00:32<00:14, 8802.53 examples/s]Running tokenizer on dataset:  69%|██████▊   | 268000/390965 [00:32<00:14, 8613.79 examples/s]Running tokenizer on dataset:  69%|██████▉   | 269000/390965 [00:32<00:14, 8486.94 examples/s]Running tokenizer on dataset:  69%|██████▉   | 270000/390965 [00:32<00:14, 8386.11 examples/s]Running tokenizer on dataset:  69%|██████▉   | 271000/390965 [00:33<00:14, 8382.55 examples/s]Running tokenizer on dataset:  70%|██████▉   | 272000/390965 [00:33<00:13, 8549.17 examples/s]Running tokenizer on dataset:  70%|██████▉   | 273000/390965 [00:33<00:13, 8743.24 examples/s]Running tokenizer on dataset:  70%|███████   | 274000/390965 [00:33<00:13, 8910.12 examples/s]Running tokenizer on dataset:  70%|███████   | 275000/390965 [00:33<00:13, 8780.22 examples/s]Running tokenizer on dataset:  71%|███████   | 276000/390965 [00:33<00:13, 8401.62 examples/s]Running tokenizer on dataset:  71%|███████   | 277000/390965 [00:33<00:13, 8246.73 examples/s]Running tokenizer on dataset:  71%|███████   | 278000/390965 [00:33<00:13, 8233.99 examples/s]Running tokenizer on dataset:  71%|███████▏  | 279000/390965 [00:33<00:13, 8447.26 examples/s]Running tokenizer on dataset:  72%|███████▏  | 280000/390965 [00:34<00:12, 8634.94 examples/s]Running tokenizer on dataset:  72%|███████▏  | 281000/390965 [00:34<00:12, 8781.53 examples/s]Running tokenizer on dataset:  72%|███████▏  | 282000/390965 [00:34<00:12, 8786.49 examples/s]Running tokenizer on dataset:  72%|███████▏  | 283000/390965 [00:34<00:12, 8656.71 examples/s]Running tokenizer on dataset:  73%|███████▎  | 284000/390965 [00:34<00:16, 6506.18 examples/s]Running tokenizer on dataset:  73%|███████▎  | 285000/390965 [00:34<00:14, 7085.43 examples/s]Running tokenizer on dataset:  73%|███████▎  | 286000/390965 [00:34<00:13, 7547.26 examples/s]Running tokenizer on dataset:  73%|███████▎  | 287000/390965 [00:34<00:13, 7967.14 examples/s]Running tokenizer on dataset:  74%|███████▎  | 288000/390965 [00:35<00:12, 8323.64 examples/s]Running tokenizer on dataset:  74%|███████▍  | 289000/390965 [00:35<00:12, 8174.39 examples/s]Running tokenizer on dataset:  74%|███████▍  | 290000/390965 [00:35<00:12, 8123.21 examples/s]Running tokenizer on dataset:  74%|███████▍  | 291000/390965 [00:35<00:12, 7984.34 examples/s]Running tokenizer on dataset:  75%|███████▍  | 292000/390965 [00:35<00:12, 8109.12 examples/s]Running tokenizer on dataset:  75%|███████▍  | 293000/390965 [00:35<00:11, 8505.25 examples/s]Running tokenizer on dataset:  75%|███████▌  | 294000/390965 [00:35<00:11, 8694.62 examples/s]Running tokenizer on dataset:  75%|███████▌  | 295000/390965 [00:35<00:10, 8814.43 examples/s]Running tokenizer on dataset:  76%|███████▌  | 296000/390965 [00:36<00:11, 8504.48 examples/s]Running tokenizer on dataset:  76%|███████▌  | 297000/390965 [00:36<00:10, 8621.38 examples/s]Running tokenizer on dataset:  76%|███████▌  | 298000/390965 [00:36<00:10, 8454.55 examples/s]Running tokenizer on dataset:  76%|███████▋  | 299000/390965 [00:36<00:10, 8446.66 examples/s]Running tokenizer on dataset:  77%|███████▋  | 300000/390965 [00:36<00:10, 8546.43 examples/s]Running tokenizer on dataset:  77%|███████▋  | 301000/390965 [00:36<00:10, 8675.02 examples/s]Running tokenizer on dataset:  77%|███████▋  | 302000/390965 [00:36<00:10, 8862.38 examples/s]Running tokenizer on dataset:  78%|███████▊  | 303000/390965 [00:36<00:10, 8585.45 examples/s]Running tokenizer on dataset:  78%|███████▊  | 304000/390965 [00:36<00:10, 8352.36 examples/s]Running tokenizer on dataset:  78%|███████▊  | 305000/390965 [00:37<00:10, 8230.92 examples/s]Running tokenizer on dataset:  78%|███████▊  | 306000/390965 [00:37<00:10, 8269.62 examples/s]Running tokenizer on dataset:  79%|███████▊  | 307000/390965 [00:37<00:09, 8431.81 examples/s]Running tokenizer on dataset:  79%|███████▉  | 308000/390965 [00:37<00:12, 6855.92 examples/s]Running tokenizer on dataset:  79%|███████▉  | 309000/390965 [00:37<00:11, 7158.88 examples/s]Running tokenizer on dataset:  79%|███████▉  | 310000/390965 [00:37<00:10, 7362.71 examples/s]Running tokenizer on dataset:  80%|███████▉  | 311000/390965 [00:37<00:10, 7693.45 examples/s]Running tokenizer on dataset:  80%|███████▉  | 312000/390965 [00:38<00:09, 8090.05 examples/s]Running tokenizer on dataset:  80%|████████  | 313000/390965 [00:38<00:09, 8444.25 examples/s]Running tokenizer on dataset:  80%|████████  | 314000/390965 [00:38<00:08, 8706.89 examples/s]Running tokenizer on dataset:  81%|████████  | 315000/390965 [00:38<00:08, 8738.84 examples/s]Running tokenizer on dataset:  81%|████████  | 316000/390965 [00:38<00:08, 8420.21 examples/s]Running tokenizer on dataset:  81%|████████  | 317000/390965 [00:38<00:08, 8453.70 examples/s]Running tokenizer on dataset:  81%|████████▏ | 318000/390965 [00:38<00:08, 8438.04 examples/s]Running tokenizer on dataset:  82%|████████▏ | 319000/390965 [00:38<00:08, 8235.52 examples/s]Running tokenizer on dataset:  82%|████████▏ | 320000/390965 [00:38<00:08, 8452.64 examples/s]Running tokenizer on dataset:  82%|████████▏ | 321000/390965 [00:39<00:08, 8635.87 examples/s]Running tokenizer on dataset:  82%|████████▏ | 322000/390965 [00:39<00:07, 8742.49 examples/s]Running tokenizer on dataset:  83%|████████▎ | 323000/390965 [00:39<00:07, 8733.80 examples/s]Running tokenizer on dataset:  83%|████████▎ | 324000/390965 [00:39<00:07, 8546.99 examples/s]Running tokenizer on dataset:  83%|████████▎ | 325000/390965 [00:39<00:07, 8438.45 examples/s]Running tokenizer on dataset:  83%|████████▎ | 326000/390965 [00:39<00:07, 8564.64 examples/s]Running tokenizer on dataset:  84%|████████▎ | 327000/390965 [00:39<00:07, 8618.52 examples/s]Running tokenizer on dataset:  84%|████████▍ | 328000/390965 [00:39<00:07, 8732.53 examples/s]Running tokenizer on dataset:  84%|████████▍ | 329000/390965 [00:39<00:06, 8906.85 examples/s]Running tokenizer on dataset:  84%|████████▍ | 330000/390965 [00:40<00:06, 8836.94 examples/s]Running tokenizer on dataset:  85%|████████▍ | 331000/390965 [00:40<00:07, 8481.16 examples/s]Running tokenizer on dataset:  85%|████████▍ | 332000/390965 [00:40<00:08, 6725.64 examples/s]Running tokenizer on dataset:  85%|████████▌ | 333000/390965 [00:40<00:07, 7295.45 examples/s]Running tokenizer on dataset:  85%|████████▌ | 334000/390965 [00:40<00:07, 7480.37 examples/s]Running tokenizer on dataset:  86%|████████▌ | 335000/390965 [00:40<00:07, 7721.74 examples/s]Running tokenizer on dataset:  86%|████████▌ | 336000/390965 [00:40<00:07, 7545.16 examples/s]Running tokenizer on dataset:  86%|████████▌ | 337000/390965 [00:41<00:07, 7674.42 examples/s]Running tokenizer on dataset:  86%|████████▋ | 338000/390965 [00:41<00:06, 8001.09 examples/s]Running tokenizer on dataset:  87%|████████▋ | 339000/390965 [00:41<00:06, 8341.67 examples/s]Running tokenizer on dataset:  87%|████████▋ | 340000/390965 [00:41<00:06, 8328.62 examples/s]Running tokenizer on dataset:  87%|████████▋ | 341000/390965 [00:41<00:05, 8353.74 examples/s]Running tokenizer on dataset:  87%|████████▋ | 342000/390965 [00:41<00:05, 8286.23 examples/s]Running tokenizer on dataset:  88%|████████▊ | 343000/390965 [00:41<00:05, 8169.98 examples/s]Running tokenizer on dataset:  88%|████████▊ | 344000/390965 [00:41<00:05, 8132.04 examples/s]Running tokenizer on dataset:  88%|████████▊ | 345000/390965 [00:42<00:05, 8144.07 examples/s]Running tokenizer on dataset:  88%|████████▊ | 346000/390965 [00:42<00:05, 8357.72 examples/s]Running tokenizer on dataset:  89%|████████▉ | 347000/390965 [00:42<00:05, 8587.39 examples/s]Running tokenizer on dataset:  89%|████████▉ | 348000/390965 [00:42<00:04, 8888.55 examples/s]Running tokenizer on dataset:  89%|████████▉ | 349000/390965 [00:42<00:04, 8887.42 examples/s]Running tokenizer on dataset:  90%|████████▉ | 350000/390965 [00:42<00:04, 8863.13 examples/s]Running tokenizer on dataset:  90%|████████▉ | 351000/390965 [00:42<00:04, 8813.19 examples/s]Running tokenizer on dataset:  90%|█████████ | 352000/390965 [00:42<00:04, 8788.17 examples/s]Running tokenizer on dataset:  90%|█████████ | 353000/390965 [00:42<00:04, 8807.98 examples/s]Running tokenizer on dataset:  91%|█████████ | 354000/390965 [00:43<00:04, 8652.64 examples/s]Running tokenizer on dataset:  91%|█████████ | 355000/390965 [00:43<00:04, 8531.23 examples/s]Running tokenizer on dataset:  91%|█████████ | 356000/390965 [00:43<00:05, 6426.46 examples/s]Running tokenizer on dataset:  91%|█████████▏| 357000/390965 [00:43<00:04, 6985.32 examples/s]Running tokenizer on dataset:  92%|█████████▏| 358000/390965 [00:43<00:04, 7510.72 examples/s]Running tokenizer on dataset:  92%|█████████▏| 359000/390965 [00:43<00:04, 7842.75 examples/s]Running tokenizer on dataset:  92%|█████████▏| 360000/390965 [00:43<00:03, 8101.46 examples/s]Running tokenizer on dataset:  92%|█████████▏| 361000/390965 [00:43<00:03, 8296.41 examples/s]Running tokenizer on dataset:  93%|█████████▎| 362000/390965 [00:44<00:03, 8411.92 examples/s]Running tokenizer on dataset:  93%|█████████▎| 363000/390965 [00:44<00:03, 8402.91 examples/s]Running tokenizer on dataset:  93%|█████████▎| 364000/390965 [00:44<00:03, 8574.24 examples/s]Running tokenizer on dataset:  93%|█████████▎| 365000/390965 [00:44<00:02, 8771.13 examples/s]Running tokenizer on dataset:  94%|█████████▎| 366000/390965 [00:44<00:02, 8727.22 examples/s]Running tokenizer on dataset:  94%|█████████▍| 367000/390965 [00:44<00:02, 8649.40 examples/s]Running tokenizer on dataset:  94%|█████████▍| 368000/390965 [00:44<00:02, 8570.42 examples/s]Running tokenizer on dataset:  94%|█████████▍| 369000/390965 [00:44<00:02, 8265.28 examples/s]Running tokenizer on dataset:  95%|█████████▍| 370000/390965 [00:45<00:02, 8123.96 examples/s]Running tokenizer on dataset:  95%|█████████▍| 371000/390965 [00:45<00:02, 8279.94 examples/s]Running tokenizer on dataset:  95%|█████████▌| 372000/390965 [00:45<00:02, 8509.05 examples/s]Running tokenizer on dataset:  95%|█████████▌| 373000/390965 [00:45<00:02, 8697.08 examples/s]Running tokenizer on dataset:  96%|█████████▌| 374000/390965 [00:45<00:01, 8839.62 examples/s]Running tokenizer on dataset:  96%|█████████▌| 375000/390965 [00:45<00:01, 8438.94 examples/s]Running tokenizer on dataset:  96%|█████████▌| 376000/390965 [00:45<00:01, 8080.11 examples/s]Running tokenizer on dataset:  96%|█████████▋| 377000/390965 [00:45<00:01, 8162.65 examples/s]Running tokenizer on dataset:  97%|█████████▋| 378000/390965 [00:45<00:01, 8391.23 examples/s]Running tokenizer on dataset:  97%|█████████▋| 379000/390965 [00:46<00:01, 8593.45 examples/s]Running tokenizer on dataset:  97%|█████████▋| 380000/390965 [00:46<00:01, 6864.90 examples/s]Running tokenizer on dataset:  97%|█████████▋| 381000/390965 [00:46<00:01, 7113.24 examples/s]Running tokenizer on dataset:  98%|█████████▊| 382000/390965 [00:46<00:01, 7305.72 examples/s]Running tokenizer on dataset:  98%|█████████▊| 383000/390965 [00:46<00:01, 7469.70 examples/s]Running tokenizer on dataset:  98%|█████████▊| 384000/390965 [00:46<00:00, 7625.50 examples/s]Running tokenizer on dataset:  98%|█████████▊| 385000/390965 [00:46<00:00, 7748.65 examples/s]Running tokenizer on dataset:  99%|█████████▊| 386000/390965 [00:47<00:00, 7966.77 examples/s]Running tokenizer on dataset:  99%|█████████▉| 387000/390965 [00:47<00:00, 8224.56 examples/s]Running tokenizer on dataset:  99%|█████████▉| 388000/390965 [00:47<00:00, 8455.81 examples/s]Running tokenizer on dataset:  99%|█████████▉| 389000/390965 [00:47<00:00, 8396.00 examples/s]Running tokenizer on dataset: 100%|█████████▉| 390000/390965 [00:47<00:00, 8109.09 examples/s]Running tokenizer on dataset: 100%|██████████| 390965/390965 [00:47<00:00, 7837.03 examples/s]Running tokenizer on dataset: 100%|██████████| 390965/390965 [00:47<00:00, 8170.17 examples/s]
02/12/2024 19:14:20 - INFO - __main__ - Sample 335243 of the training set: {'question1': 'How do I crack JEE in a month?', 'question2': 'How do I crack JEE in 4-5 months?', 'label': 0, 'idx': 335243, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1128, 437, 306, 26755, 435, 17896, 297, 263, 4098, 29973, 1, 1128, 437, 306, 26755, 435, 17896, 297, 29871, 29946, 29899, 29945, 7378, 29973], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 19:14:20 - INFO - __main__ - Sample 58369 of the training set: {'question1': 'Who are the greatest people in the world?', 'question2': 'Can you name some people who have really saved the world?', 'label': 0, 'idx': 58369, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 11644, 526, 278, 14176, 2305, 297, 278, 3186, 29973, 1, 1815, 366, 1024, 777, 2305, 1058, 505, 2289, 7160, 278, 3186, 29973], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 19:14:20 - INFO - __main__ - Sample 13112 of the training set: {'question1': 'What is inside a Camel Crush cigarette?', 'question2': 'Are Camel Crush cigarettes designed to attract teen smokers?', 'label': 0, 'idx': 13112, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1724, 338, 2768, 263, 5500, 295, 6781, 1878, 29507, 10474, 371, 29973, 1, 4683, 5500, 295, 6781, 1878, 274, 25667, 698, 267, 8688, 304, 13978, 734, 264, 1560, 554, 414, 29973], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/12/2024 19:14:20 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Running tokenizer on dataset:   0%|          | 0/40430 [00:00<?, ? examples/s]Running tokenizer on dataset:   2%|▏         | 1000/40430 [00:00<00:16, 2334.68 examples/s]Running tokenizer on dataset:   5%|▍         | 2000/40430 [00:00<00:12, 3164.36 examples/s]Running tokenizer on dataset:   7%|▋         | 3000/40430 [00:00<00:09, 3799.66 examples/s]Running tokenizer on dataset:  10%|▉         | 4000/40430 [00:01<00:08, 4110.05 examples/s]Running tokenizer on dataset:  12%|█▏        | 5000/40430 [00:01<00:08, 4235.77 examples/s]Running tokenizer on dataset:  15%|█▍        | 6000/40430 [00:01<00:08, 4260.33 examples/s]Running tokenizer on dataset:  17%|█▋        | 7000/40430 [00:01<00:07, 4350.13 examples/s]Running tokenizer on dataset:  20%|█▉        | 8000/40430 [00:01<00:07, 4500.57 examples/s]Running tokenizer on dataset:  22%|██▏       | 9000/40430 [00:02<00:06, 4603.54 examples/s]Running tokenizer on dataset:  25%|██▍       | 10000/40430 [00:02<00:06, 4381.04 examples/s]Running tokenizer on dataset:  27%|██▋       | 11000/40430 [00:02<00:06, 4429.46 examples/s]Running tokenizer on dataset:  30%|██▉       | 12000/40430 [00:02<00:06, 4515.56 examples/s]Running tokenizer on dataset:  32%|███▏      | 13000/40430 [00:03<00:06, 4545.56 examples/s]Running tokenizer on dataset:  35%|███▍      | 14000/40430 [00:03<00:05, 4520.73 examples/s]Running tokenizer on dataset:  37%|███▋      | 15000/40430 [00:03<00:05, 4615.66 examples/s][INFO|trainer.py:737] 2024-02-12 19:14:23,628 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, question1, question2. If idx, question1, question2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
Running tokenizer on dataset:  40%|███▉      | 16000/40430 [00:03<00:05, 4686.75 examples/s]Running tokenizer on dataset:  42%|████▏     | 17000/40430 [00:03<00:05, 4563.28 examples/s]Running tokenizer on dataset:  45%|████▍     | 18000/40430 [00:04<00:04, 4653.62 examples/s]Running tokenizer on dataset:  47%|████▋     | 19000/40430 [00:04<00:04, 4697.05 examples/s]Running tokenizer on dataset:  49%|████▉     | 20000/40430 [00:04<00:04, 4907.69 examples/s]Running tokenizer on dataset:  52%|█████▏    | 21000/40430 [00:04<00:04, 4627.99 examples/s]Running tokenizer on dataset:  54%|█████▍    | 22000/40430 [00:04<00:04, 4602.42 examples/s]Running tokenizer on dataset:  57%|█████▋    | 23000/40430 [00:05<00:03, 4683.41 examples/s]Running tokenizer on dataset:  59%|█████▉    | 24000/40430 [00:05<00:03, 4819.92 examples/s]Running tokenizer on dataset:  62%|██████▏   | 25000/40430 [00:05<00:03, 4699.93 examples/s]Running tokenizer on dataset:  64%|██████▍   | 26000/40430 [00:05<00:03, 4717.47 examples/s]Running tokenizer on dataset:  67%|██████▋   | 27000/40430 [00:06<00:02, 4909.55 examples/s]Running tokenizer on dataset:  69%|██████▉   | 28000/40430 [00:06<00:02, 4975.62 examples/s]Running tokenizer on dataset:  72%|███████▏  | 29000/40430 [00:06<00:02, 4729.20 examples/s]Running tokenizer on dataset:  74%|███████▍  | 30000/40430 [00:06<00:02, 4687.67 examples/s]Running tokenizer on dataset:  77%|███████▋  | 31000/40430 [00:06<00:02, 4672.12 examples/s]Running tokenizer on dataset:  79%|███████▉  | 32000/40430 [00:07<00:01, 4751.81 examples/s]Running tokenizer on dataset:  82%|████████▏ | 33000/40430 [00:07<00:01, 4843.31 examples/s]Running tokenizer on dataset:  84%|████████▍ | 34000/40430 [00:07<00:01, 4698.04 examples/s]Running tokenizer on dataset:  87%|████████▋ | 35000/40430 [00:07<00:01, 4657.85 examples/s]Running tokenizer on dataset:  89%|████████▉ | 36000/40430 [00:07<00:00, 4742.88 examples/s]Running tokenizer on dataset:  92%|█████████▏| 37000/40430 [00:08<00:00, 4793.03 examples/s]Running tokenizer on dataset:  94%|█████████▍| 38000/40430 [00:08<00:00, 4699.56 examples/s]Running tokenizer on dataset:  96%|█████████▋| 39000/40430 [00:08<00:00, 4633.41 examples/s]Running tokenizer on dataset:  99%|█████████▉| 40000/40430 [00:08<00:00, 4743.81 examples/s]Running tokenizer on dataset: 100%|██████████| 40430/40430 [00:08<00:00, 4557.11 examples/s]
[INFO|trainer.py:1747] 2024-02-12 19:14:32,300 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-12 19:14:32,306 >>   Num examples = 363,846
[INFO|trainer.py:1749] 2024-02-12 19:14:32,306 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-12 19:14:32,307 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-12 19:14:32,307 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-12 19:14:32,307 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-12 19:14:32,307 >>   Total optimization steps = 68,223
[INFO|trainer.py:1756] 2024-02-12 19:14:32,308 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/68223 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/68223 [00:02<50:06:29,  2.64s/it]  0%|          | 2/68223 [00:03<29:24:48,  1.55s/it]  0%|          | 3/68223 [00:04<23:10:19,  1.22s/it]  0%|          | 4/68223 [00:05<20:22:40,  1.08s/it]  0%|          | 5/68223 [00:05<18:58:59,  1.00s/it]  0%|          | 6/68223 [00:06<17:52:32,  1.06it/s]  0%|          | 7/68223 [00:07<17:20:03,  1.09it/s]  0%|          | 8/68223 [00:08<16:52:04,  1.12it/s]  0%|          | 9/68223 [00:09<16:47:09,  1.13it/s]  0%|          | 10/68223 [00:10<16:30:12,  1.15it/s]  0%|          | 11/68223 [00:11<16:19:20,  1.16it/s]  0%|          | 12/68223 [00:11<16:14:41,  1.17it/s]  0%|          | 13/68223 [00:12<16:08:17,  1.17it/s]  0%|          | 14/68223 [00:13<16:12:00,  1.17it/s]  0%|          | 15/68223 [00:14<16:12:01,  1.17it/s]  0%|          | 16/68223 [00:15<16:04:40,  1.18it/s]  0%|          | 17/68223 [00:16<16:02:10,  1.18it/s]  0%|          | 18/68223 [00:17<16:07:23,  1.18it/s]  0%|          | 19/68223 [00:17<16:04:45,  1.18it/s]  0%|          | 20/68223 [00:18<16:02:40,  1.18it/s]  0%|          | 21/68223 [00:19<16:02:54,  1.18it/s]  0%|          | 22/68223 [00:20<16:04:37,  1.18it/s]  0%|          | 23/68223 [00:21<16:04:42,  1.18it/s]  0%|          | 24/68223 [00:22<16:07:32,  1.17it/s]  0%|          | 25/68223 [00:22<16:07:05,  1.18it/s]  0%|          | 26/68223 [00:23<16:07:25,  1.17it/s]  0%|          | 27/68223 [00:24<16:06:37,  1.18it/s]  0%|          | 28/68223 [00:25<16:01:07,  1.18it/s]  0%|          | 29/68223 [00:26<16:03:10,  1.18it/s]  0%|          | 30/68223 [00:27<15:59:43,  1.18it/s]  0%|          | 31/68223 [00:28<15:58:42,  1.19it/s]  0%|          | 32/68223 [00:28<16:03:00,  1.18it/s]  0%|          | 33/68223 [00:29<16:01:51,  1.18it/s]  0%|          | 34/68223 [00:30<16:05:48,  1.18it/s]  0%|          | 35/68223 [00:31<16:02:08,  1.18it/s]  0%|          | 36/68223 [00:32<15:59:23,  1.18it/s]  0%|          | 37/68223 [00:33<16:13:00,  1.17it/s]  0%|          | 38/68223 [00:33<16:12:18,  1.17it/s]  0%|          | 39/68223 [00:34<16:09:02,  1.17it/s]  0%|          | 40/68223 [00:35<16:06:02,  1.18it/s]  0%|          | 41/68223 [00:36<16:04:34,  1.18it/s]  0%|          | 42/68223 [00:37<16:02:58,  1.18it/s]  0%|          | 43/68223 [00:38<16:05:40,  1.18it/s]  0%|          | 44/68223 [00:39<16:04:51,  1.18it/s]  0%|          | 45/68223 [00:39<16:02:40,  1.18it/s]  0%|          | 46/68223 [00:40<16:06:47,  1.18it/s]  0%|          | 47/68223 [00:41<15:59:59,  1.18it/s]  0%|          | 48/68223 [00:42<16:02:15,  1.18it/s]  0%|          | 49/68223 [00:43<16:08:28,  1.17it/s]  0%|          | 50/68223 [00:44<16:08:40,  1.17it/s]  0%|          | 51/68223 [00:45<16:05:43,  1.18it/s]  0%|          | 52/68223 [00:45<16:06:55,  1.18it/s]  0%|          | 53/68223 [00:46<16:01:27,  1.18it/s]  0%|          | 54/68223 [00:47<16:03:49,  1.18it/s]  0%|          | 55/68223 [00:48<16:05:57,  1.18it/s]  0%|          | 56/68223 [00:49<16:04:14,  1.18it/s]  0%|          | 57/68223 [00:50<15:59:46,  1.18it/s]  0%|          | 58/68223 [00:50<16:02:04,  1.18it/s]  0%|          | 59/68223 [00:51<16:02:42,  1.18it/s]  0%|          | 60/68223 [00:52<16:02:26,  1.18it/s]  0%|          | 61/68223 [00:53<16:04:49,  1.18it/s]  0%|          | 62/68223 [00:54<16:03:04,  1.18it/s]  0%|          | 63/68223 [00:55<16:02:50,  1.18it/s]  0%|          | 64/68223 [00:56<15:57:21,  1.19it/s]  0%|          | 65/68223 [00:56<16:00:19,  1.18it/s]  0%|          | 66/68223 [00:57<16:02:03,  1.18it/s]  0%|          | 67/68223 [00:58<16:05:10,  1.18it/s]  0%|          | 68/68223 [00:59<16:00:09,  1.18it/s]  0%|          | 69/68223 [01:00<16:01:25,  1.18it/s]  0%|          | 70/68223 [01:01<16:04:27,  1.18it/s]  0%|          | 71/68223 [01:01<16:11:13,  1.17it/s]  0%|          | 72/68223 [01:02<16:07:11,  1.17it/s]  0%|          | 73/68223 [01:03<16:09:14,  1.17it/s]  0%|          | 74/68223 [01:04<16:09:44,  1.17it/s]  0%|          | 75/68223 [01:05<16:03:54,  1.18it/s]  0%|          | 76/68223 [01:06<16:10:53,  1.17it/s]  0%|          | 77/68223 [01:07<16:08:12,  1.17it/s]  0%|          | 78/68223 [01:07<16:02:06,  1.18it/s]  0%|          | 79/68223 [01:08<16:09:01,  1.17it/s]  0%|          | 80/68223 [01:09<16:06:32,  1.18it/s]  0%|          | 81/68223 [01:10<16:09:54,  1.17it/s]  0%|          | 82/68223 [01:11<16:05:05,  1.18it/s]  0%|          | 83/68223 [01:12<16:01:50,  1.18it/s]  0%|          | 84/68223 [01:13<16:03:49,  1.18it/s]  0%|          | 85/68223 [01:13<16:10:57,  1.17it/s]  0%|          | 86/68223 [01:14<16:07:35,  1.17it/s]  0%|          | 87/68223 [01:15<16:05:38,  1.18it/s]  0%|          | 88/68223 [01:16<16:06:08,  1.18it/s]  0%|          | 89/68223 [01:17<15:59:02,  1.18it/s]  0%|          | 90/68223 [01:18<16:04:49,  1.18it/s]  0%|          | 91/68223 [01:18<16:02:02,  1.18it/s]  0%|          | 92/68223 [01:19<15:58:25,  1.18it/s]  0%|          | 93/68223 [01:20<16:03:34,  1.18it/s]  0%|          | 94/68223 [01:21<16:06:00,  1.18it/s]  0%|          | 95/68223 [01:22<16:04:48,  1.18it/s]  0%|          | 96/68223 [01:23<16:07:43,  1.17it/s]  0%|          | 97/68223 [01:24<16:07:58,  1.17it/s]  0%|          | 98/68223 [01:24<16:08:22,  1.17it/s]  0%|          | 99/68223 [01:25<16:04:19,  1.18it/s]  0%|          | 100/68223 [01:26<16:09:40,  1.17it/s]  0%|          | 101/68223 [01:27<16:06:34,  1.17it/s]  0%|          | 102/68223 [01:28<16:02:43,  1.18it/s]  0%|          | 103/68223 [01:29<16:04:04,  1.18it/s]  0%|          | 104/68223 [01:30<16:03:37,  1.18it/s]  0%|          | 105/68223 [01:30<15:57:42,  1.19it/s]  0%|          | 106/68223 [01:31<16:06:36,  1.17it/s]  0%|          | 107/68223 [01:32<16:08:47,  1.17it/s]  0%|          | 108/68223 [01:33<16:02:39,  1.18it/s]  0%|          | 109/68223 [01:34<16:06:50,  1.17it/s]  0%|          | 110/68223 [01:35<16:02:03,  1.18it/s]  0%|          | 111/68223 [01:35<16:06:37,  1.17it/s]  0%|          | 112/68223 [01:36<16:10:26,  1.17it/s]  0%|          | 113/68223 [01:37<16:09:28,  1.17it/s]  0%|          | 114/68223 [01:38<16:07:01,  1.17it/s]  0%|          | 115/68223 [01:39<16:04:45,  1.18it/s]  0%|          | 116/68223 [01:40<16:02:41,  1.18it/s]  0%|          | 117/68223 [01:41<16:10:33,  1.17it/s]  0%|          | 118/68223 [01:41<16:16:45,  1.16it/s]  0%|          | 119/68223 [01:42<16:15:06,  1.16it/s]  0%|          | 120/68223 [01:43<16:07:58,  1.17it/s]  0%|          | 121/68223 [01:44<16:08:33,  1.17it/s]  0%|          | 122/68223 [01:45<16:03:21,  1.18it/s]  0%|          | 123/68223 [01:46<16:02:25,  1.18it/s]  0%|          | 124/68223 [01:47<16:10:02,  1.17it/s]  0%|          | 125/68223 [01:47<16:04:01,  1.18it/s]  0%|          | 126/68223 [01:48<15:59:53,  1.18it/s]  0%|          | 127/68223 [01:49<16:01:18,  1.18it/s]  0%|          | 128/68223 [01:50<16:00:08,  1.18it/s]  0%|          | 129/68223 [01:51<15:59:52,  1.18it/s]  0%|          | 130/68223 [01:52<15:57:07,  1.19it/s]  0%|          | 131/68223 [01:52<16:00:17,  1.18it/s]  0%|          | 132/68223 [01:53<16:03:47,  1.18it/s]  0%|          | 133/68223 [01:54<16:01:58,  1.18it/s]  0%|          | 134/68223 [01:55<15:58:49,  1.18it/s]  0%|          | 135/68223 [01:56<16:01:51,  1.18it/s]  0%|          | 136/68223 [01:57<15:58:07,  1.18it/s]  0%|          | 137/68223 [01:58<16:00:55,  1.18it/s]  0%|          | 138/68223 [01:58<15:57:15,  1.19it/s]  0%|          | 139/68223 [01:59<16:01:06,  1.18it/s]  0%|          | 140/68223 [02:00<16:05:05,  1.18it/s]  0%|          | 141/68223 [02:01<15:59:31,  1.18it/s]  0%|          | 142/68223 [02:02<15:57:25,  1.19it/s]  0%|          | 143/68223 [02:03<16:11:20,  1.17it/s]  0%|          | 144/68223 [02:04<16:11:22,  1.17it/s]  0%|          | 145/68223 [02:04<16:10:47,  1.17it/s]  0%|          | 146/68223 [02:05<16:07:59,  1.17it/s]  0%|          | 147/68223 [02:06<16:10:28,  1.17it/s]  0%|          | 148/68223 [02:07<16:08:23,  1.17it/s]  0%|          | 149/68223 [02:08<16:08:15,  1.17it/s]  0%|          | 150/68223 [02:09<16:14:18,  1.16it/s]  0%|          | 151/68223 [02:10<16:08:13,  1.17it/s]  0%|          | 152/68223 [02:10<16:14:39,  1.16it/s]  0%|          | 153/68223 [02:11<16:10:58,  1.17it/s]  0%|          | 154/68223 [02:12<16:06:42,  1.17it/s]  0%|          | 155/68223 [02:13<16:07:48,  1.17it/s]  0%|          | 156/68223 [02:14<16:06:15,  1.17it/s]  0%|          | 157/68223 [02:15<16:11:14,  1.17it/s]  0%|          | 158/68223 [02:16<16:11:08,  1.17it/s]  0%|          | 159/68223 [02:16<16:11:32,  1.17it/s]  0%|          | 160/68223 [02:17<16:06:15,  1.17it/s]  0%|          | 161/68223 [02:18<16:09:47,  1.17it/s]  0%|          | 162/68223 [02:19<16:10:42,  1.17it/s]  0%|          | 163/68223 [02:20<16:15:30,  1.16it/s]  0%|          | 164/68223 [02:21<16:14:06,  1.16it/s]  0%|          | 165/68223 [02:21<16:08:18,  1.17it/s]  0%|          | 166/68223 [02:22<16:06:30,  1.17it/s]  0%|          | 167/68223 [02:23<16:02:08,  1.18it/s]  0%|          | 168/68223 [02:24<16:06:38,  1.17it/s]  0%|          | 169/68223 [02:25<16:07:18,  1.17it/s]  0%|          | 170/68223 [02:26<16:05:11,  1.18it/s]  0%|          | 171/68223 [02:27<16:05:34,  1.17it/s]  0%|          | 172/68223 [02:27<16:03:08,  1.18it/s]  0%|          | 173/68223 [02:28<16:04:15,  1.18it/s]  0%|          | 174/68223 [02:29<16:01:14,  1.18it/s]  0%|          | 175/68223 [02:30<16:02:29,  1.18it/s]  0%|          | 176/68223 [02:31<15:58:07,  1.18it/s]  0%|          | 177/68223 [02:32<16:04:21,  1.18it/s]  0%|          | 178/68223 [02:33<16:05:59,  1.17it/s]  0%|          | 179/68223 [02:33<16:06:10,  1.17it/s]  0%|          | 180/68223 [02:34<16:04:05,  1.18it/s]  0%|          | 181/68223 [02:35<16:02:57,  1.18it/s]  0%|          | 182/68223 [02:36<16:00:46,  1.18it/s]  0%|          | 183/68223 [02:37<16:05:40,  1.17it/s]  0%|          | 184/68223 [02:38<16:01:58,  1.18it/s]  0%|          | 185/68223 [02:38<16:10:49,  1.17it/s]  0%|          | 186/68223 [02:39<16:08:28,  1.17it/s]  0%|          | 187/68223 [02:40<16:03:35,  1.18it/s]  0%|          | 188/68223 [02:41<16:05:19,  1.17it/s]  0%|          | 189/68223 [02:42<16:00:47,  1.18it/s]  0%|          | 190/68223 [02:43<16:05:21,  1.17it/s]  0%|          | 191/68223 [02:44<16:01:08,  1.18it/s]  0%|          | 192/68223 [02:44<16:06:30,  1.17it/s]  0%|          | 193/68223 [02:45<16:06:15,  1.17it/s]  0%|          | 194/68223 [02:46<16:03:47,  1.18it/s]  0%|          | 195/68223 [02:47<16:06:27,  1.17it/s]  0%|          | 196/68223 [02:48<16:07:57,  1.17it/s]  0%|          | 197/68223 [02:49<15:58:24,  1.18it/s]  0%|          | 198/68223 [02:50<16:06:15,  1.17it/s]  0%|          | 199/68223 [02:50<16:04:20,  1.18it/s]  0%|          | 200/68223 [02:51<16:01:56,  1.18it/s]  0%|          | 201/68223 [02:52<16:06:03,  1.17it/s]  0%|          | 202/68223 [02:53<16:03:56,  1.18it/s]  0%|          | 203/68223 [02:54<16:04:47,  1.18it/s]  0%|          | 204/68223 [02:55<16:06:25,  1.17it/s]  0%|          | 205/68223 [02:56<16:04:14,  1.18it/s]  0%|          | 206/68223 [02:56<16:02:00,  1.18it/s]  0%|          | 207/68223 [02:57<16:05:10,  1.17it/s]  0%|          | 208/68223 [02:58<16:11:04,  1.17it/s]  0%|          | 209/68223 [02:59<16:17:02,  1.16it/s]  0%|          | 210/68223 [03:00<16:12:13,  1.17it/s]  0%|          | 211/68223 [03:01<16:10:17,  1.17it/s]  0%|          | 212/68223 [03:02<16:11:06,  1.17it/s]  0%|          | 213/68223 [03:02<16:09:05,  1.17it/s]  0%|          | 214/68223 [03:03<16:04:37,  1.18it/s]  0%|          | 215/68223 [03:04<15:58:03,  1.18it/s]  0%|          | 216/68223 [03:05<16:05:06,  1.17it/s]  0%|          | 217/68223 [03:06<16:05:49,  1.17it/s]  0%|          | 218/68223 [03:07<16:05:55,  1.17it/s]  0%|          | 219/68223 [03:07<16:02:31,  1.18it/s]  0%|          | 220/68223 [03:08<15:59:04,  1.18it/s]  0%|          | 221/68223 [03:09<16:04:45,  1.17it/s]  0%|          | 222/68223 [03:10<16:06:02,  1.17it/s]  0%|          | 223/68223 [03:11<16:06:11,  1.17it/s]  0%|          | 224/68223 [03:12<15:56:14,  1.19it/s]  0%|          | 225/68223 [03:13<15:58:38,  1.18it/s]  0%|          | 226/68223 [03:13<16:07:15,  1.17it/s]  0%|          | 227/68223 [03:14<16:06:51,  1.17it/s]  0%|          | 228/68223 [03:15<16:01:29,  1.18it/s]  0%|          | 229/68223 [03:16<15:58:07,  1.18it/s]  0%|          | 230/68223 [03:17<15:56:14,  1.19it/s]  0%|          | 231/68223 [03:18<16:06:52,  1.17it/s]  0%|          | 232/68223 [03:19<16:09:45,  1.17it/s]  0%|          | 233/68223 [03:19<16:00:43,  1.18it/s]  0%|          | 234/68223 [03:20<16:00:24,  1.18it/s]  0%|          | 235/68223 [03:21<15:59:47,  1.18it/s]  0%|          | 236/68223 [03:22<16:02:45,  1.18it/s]  0%|          | 237/68223 [03:23<16:05:21,  1.17it/s]  0%|          | 238/68223 [03:24<16:12:22,  1.17it/s]  0%|          | 239/68223 [03:24<16:04:19,  1.17it/s]  0%|          | 240/68223 [03:25<15:57:04,  1.18it/s]  0%|          | 241/68223 [03:26<16:01:58,  1.18it/s]  0%|          | 242/68223 [03:27<15:56:16,  1.18it/s]  0%|          | 243/68223 [03:28<16:01:53,  1.18it/s]  0%|          | 244/68223 [03:29<15:57:11,  1.18it/s]  0%|          | 245/68223 [03:30<16:02:21,  1.18it/s]  0%|          | 246/68223 [03:30<16:00:17,  1.18it/s]  0%|          | 247/68223 [03:31<16:04:30,  1.17it/s]  0%|          | 248/68223 [03:32<16:06:39,  1.17it/s]  0%|          | 249/68223 [03:33<16:09:50,  1.17it/s]  0%|          | 250/68223 [03:34<16:10:39,  1.17it/s]  0%|          | 251/68223 [03:35<16:06:35,  1.17it/s]  0%|          | 252/68223 [03:36<16:11:58,  1.17it/s]  0%|          | 253/68223 [03:36<16:09:52,  1.17it/s]  0%|          | 254/68223 [03:37<16:00:32,  1.18it/s]  0%|          | 255/68223 [03:38<16:04:02,  1.18it/s]  0%|          | 256/68223 [03:39<16:00:26,  1.18it/s]  0%|          | 257/68223 [03:40<16:05:43,  1.17it/s]  0%|          | 258/68223 [03:41<16:02:28,  1.18it/s]  0%|          | 259/68223 [03:41<15:58:02,  1.18it/s]  0%|          | 260/68223 [03:42<16:03:24,  1.18it/s]  0%|          | 261/68223 [03:43<16:00:03,  1.18it/s]  0%|          | 262/68223 [03:44<16:05:02,  1.17it/s]  0%|          | 263/68223 [03:45<16:07:55,  1.17it/s]  0%|          | 264/68223 [03:46<16:07:04,  1.17it/s]  0%|          | 265/68223 [03:47<16:12:44,  1.16it/s]  0%|          | 266/68223 [03:47<16:08:50,  1.17it/s]  0%|          | 267/68223 [03:48<16:03:22,  1.18it/s]  0%|          | 268/68223 [03:49<15:58:36,  1.18it/s]  0%|          | 269/68223 [03:50<16:04:49,  1.17it/s]  0%|          | 270/68223 [03:51<16:11:25,  1.17it/s]  0%|          | 271/68223 [03:52<16:07:03,  1.17it/s]  0%|          | 272/68223 [03:53<16:04:02,  1.17it/s]  0%|          | 273/68223 [03:53<16:07:33,  1.17it/s]  0%|          | 274/68223 [03:54<16:01:16,  1.18it/s]  0%|          | 275/68223 [03:55<16:03:44,  1.18it/s]  0%|          | 276/68223 [03:56<16:04:11,  1.17it/s]  0%|          | 277/68223 [03:57<16:07:10,  1.17it/s]  0%|          | 278/68223 [03:58<16:03:20,  1.18it/s]  0%|          | 279/68223 [03:59<16:06:31,  1.17it/s]  0%|          | 280/68223 [03:59<16:08:44,  1.17it/s]  0%|          | 281/68223 [04:00<16:02:05,  1.18it/s]  0%|          | 282/68223 [04:01<16:02:56,  1.18it/s]  0%|          | 283/68223 [04:02<16:07:10,  1.17it/s]  0%|          | 284/68223 [04:03<16:07:00,  1.17it/s]  0%|          | 285/68223 [04:04<16:08:16,  1.17it/s]  0%|          | 286/68223 [04:04<16:04:03,  1.17it/s]  0%|          | 287/68223 [04:05<16:07:20,  1.17it/s]  0%|          | 288/68223 [04:06<16:10:43,  1.17it/s]  0%|          | 289/68223 [04:07<16:04:27,  1.17it/s]  0%|          | 290/68223 [04:08<16:05:18,  1.17it/s]  0%|          | 291/68223 [04:09<16:03:25,  1.18it/s]  0%|          | 292/68223 [04:10<16:08:52,  1.17it/s]  0%|          | 293/68223 [04:10<16:03:21,  1.18it/s]  0%|          | 294/68223 [04:11<16:05:44,  1.17it/s]  0%|          | 295/68223 [04:12<16:02:06,  1.18it/s]  0%|          | 296/68223 [04:13<16:01:41,  1.18it/s]  0%|          | 297/68223 [04:14<16:03:37,  1.17it/s]  0%|          | 298/68223 [04:15<16:03:02,  1.18it/s]  0%|          | 299/68223 [04:16<16:10:25,  1.17it/s]  0%|          | 300/68223 [04:16<16:15:29,  1.16it/s]  0%|          | 301/68223 [04:17<16:11:02,  1.17it/s]  0%|          | 302/68223 [04:18<16:04:31,  1.17it/s]  0%|          | 303/68223 [04:19<16:06:35,  1.17it/s]  0%|          | 304/68223 [04:20<16:01:16,  1.18it/s]  0%|          | 305/68223 [04:21<16:08:31,  1.17it/s]  0%|          | 306/68223 [04:22<16:08:01,  1.17it/s]  0%|          | 307/68223 [04:22<16:01:25,  1.18it/s]  0%|          | 308/68223 [04:23<16:04:24,  1.17it/s]  0%|          | 309/68223 [04:24<16:02:40,  1.18it/s]  0%|          | 310/68223 [04:25<15:59:46,  1.18it/s]  0%|          | 311/68223 [04:26<15:56:57,  1.18it/s]  0%|          | 312/68223 [04:27<15:58:20,  1.18it/s]  0%|          | 313/68223 [04:27<16:02:21,  1.18it/s]  0%|          | 314/68223 [04:28<15:59:22,  1.18it/s]  0%|          | 315/68223 [04:29<15:59:52,  1.18it/s]  0%|          | 316/68223 [04:30<16:03:02,  1.18it/s]  0%|          | 317/68223 [04:31<16:06:44,  1.17it/s]  0%|          | 318/68223 [04:32<16:03:17,  1.17it/s]  0%|          | 319/68223 [04:33<15:59:12,  1.18it/s]  0%|          | 320/68223 [04:33<15:57:13,  1.18it/s]  0%|          | 321/68223 [04:34<15:53:50,  1.19it/s]  0%|          | 322/68223 [04:35<16:02:41,  1.18it/s]  0%|          | 323/68223 [04:36<16:02:21,  1.18it/s]  0%|          | 324/68223 [04:37<16:04:09,  1.17it/s]  0%|          | 325/68223 [04:38<16:03:24,  1.17it/s]  0%|          | 326/68223 [04:39<16:00:08,  1.18it/s]  0%|          | 327/68223 [04:39<15:55:46,  1.18it/s]  0%|          | 328/68223 [04:40<15:58:07,  1.18it/s]  0%|          | 329/68223 [04:41<15:58:17,  1.18it/s]  0%|          | 330/68223 [04:42<15:57:16,  1.18it/s]  0%|          | 331/68223 [04:43<16:01:22,  1.18it/s]  0%|          | 332/68223 [04:44<15:57:50,  1.18it/s]  0%|          | 333/68223 [04:44<15:57:14,  1.18it/s]  0%|          | 334/68223 [04:45<16:02:33,  1.18it/s]  0%|          | 335/68223 [04:46<16:02:01,  1.18it/s]  0%|          | 336/68223 [04:47<16:00:34,  1.18it/s]  0%|          | 337/68223 [04:48<15:59:51,  1.18it/s]  0%|          | 338/68223 [04:49<16:03:40,  1.17it/s]  0%|          | 339/68223 [04:50<16:04:40,  1.17it/s]  0%|          | 340/68223 [04:50<16:02:26,  1.18it/s]  0%|          | 341/68223 [04:51<16:05:17,  1.17it/s]  1%|          | 342/68223 [04:52<16:03:19,  1.17it/s]  1%|          | 343/68223 [04:53<16:09:18,  1.17it/s]  1%|          | 344/68223 [04:54<16:15:40,  1.16it/s]  1%|          | 345/68223 [04:55<16:13:06,  1.16it/s]  1%|          | 346/68223 [04:56<16:12:52,  1.16it/s]  1%|          | 347/68223 [04:56<16:11:50,  1.16it/s]  1%|          | 348/68223 [04:57<16:08:31,  1.17it/s]  1%|          | 349/68223 [04:58<16:12:51,  1.16it/s]  1%|          | 350/68223 [04:59<16:10:43,  1.17it/s]  1%|          | 351/68223 [05:00<16:07:29,  1.17it/s]  1%|          | 352/68223 [05:01<16:05:54,  1.17it/s]  1%|          | 353/68223 [05:02<16:06:49,  1.17it/s]  1%|          | 354/68223 [05:02<15:59:01,  1.18it/s]  1%|          | 355/68223 [05:03<15:56:11,  1.18it/s]  1%|          | 356/68223 [05:04<15:53:56,  1.19it/s]  1%|          | 357/68223 [05:05<15:56:05,  1.18it/s]  1%|          | 358/68223 [05:06<16:00:35,  1.18it/s]  1%|          | 359/68223 [05:07<16:03:07,  1.17it/s]  1%|          | 360/68223 [05:07<15:57:12,  1.18it/s]  1%|          | 361/68223 [05:08<16:04:43,  1.17it/s]  1%|          | 362/68223 [05:09<16:01:18,  1.18it/s]  1%|          | 363/68223 [05:10<16:00:39,  1.18it/s]  1%|          | 364/68223 [05:11<16:02:17,  1.18it/s]  1%|          | 365/68223 [05:12<16:00:29,  1.18it/s]  1%|          | 366/68223 [05:13<15:56:47,  1.18it/s]  1%|          | 367/68223 [05:13<15:58:16,  1.18it/s]  1%|          | 368/68223 [05:14<15:51:55,  1.19it/s]  1%|          | 369/68223 [05:15<15:53:01,  1.19it/s]  1%|          | 370/68223 [05:16<15:52:46,  1.19it/s]  1%|          | 371/68223 [05:17<15:51:27,  1.19it/s]  1%|          | 372/68223 [05:18<15:54:02,  1.19it/s]  1%|          | 373/68223 [05:18<15:56:38,  1.18it/s]  1%|          | 374/68223 [05:19<15:55:06,  1.18it/s]  1%|          | 375/68223 [05:20<15:59:22,  1.18it/s]  1%|          | 376/68223 [05:21<15:56:54,  1.18it/s]  1%|          | 377/68223 [05:22<15:50:46,  1.19it/s]  1%|          | 378/68223 [05:23<15:53:29,  1.19it/s]  1%|          | 379/68223 [05:24<15:58:27,  1.18it/s]  1%|          | 380/68223 [05:24<15:54:44,  1.18it/s]  1%|          | 381/68223 [05:25<15:56:16,  1.18it/s]  1%|          | 382/68223 [05:26<15:54:33,  1.18it/s]  1%|          | 383/68223 [05:27<15:54:57,  1.18it/s]  1%|          | 384/68223 [05:28<15:58:18,  1.18it/s]  1%|          | 385/68223 [05:29<15:54:16,  1.18it/s]  1%|          | 386/68223 [05:29<16:03:46,  1.17it/s]  1%|          | 387/68223 [05:30<16:03:38,  1.17it/s]  1%|          | 388/68223 [05:31<15:56:51,  1.18it/s]  1%|          | 389/68223 [05:32<15:58:39,  1.18it/s]  1%|          | 390/68223 [05:33<15:59:15,  1.18it/s]  1%|          | 391/68223 [05:34<16:01:47,  1.18it/s]  1%|          | 392/68223 [05:35<16:00:04,  1.18it/s]  1%|          | 393/68223 [05:35<15:53:46,  1.19itsrun: Job 22189588 step creation temporarily disabled, retrying (Requested nodes are busy)
