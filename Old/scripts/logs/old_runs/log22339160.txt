WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: run_glue.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                   [--config_name CONFIG_NAME]
                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                   [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
                   [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]
                   [--token TOKEN] [--use_auth_token [USE_AUTH_TOKEN]]
                   [--trust_remote_code [TRUST_REMOTE_CODE]]
                   [--ignore_mismatched_sizes [IGNORE_MISMATCHED_SIZES]]
                   [--task_name TASK_NAME] [--dataset_name DATASET_NAME]
                   [--dataset_config_name DATASET_CONFIG_NAME]
                   [--max_seq_length MAX_SEQ_LENGTH]
                   [--overwrite_cache [OVERWRITE_CACHE]]
                   [--pad_to_max_length [PAD_TO_MAX_LENGTH]]
                   [--no_pad_to_max_length]
                   [--max_train_samples MAX_TRAIN_SAMPLES]
                   [--max_eval_samples MAX_EVAL_SAMPLES]
                   [--max_predict_samples MAX_PREDICT_SAMPLES]
                   [--train_file TRAIN_FILE]
                   [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]
                   --output_dir OUTPUT_DIR
                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                   [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                   [--do_predict [DO_PREDICT]]
                   [--evaluation_strategy {no,steps,epoch}]
                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                   [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]
                   [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
                   [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]
                   [--max_grad_norm MAX_GRAD_NORM]
                   [--num_train_epochs NUM_TRAIN_EPOCHS]
                   [--max_steps MAX_STEPS]
                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]
                   [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]
                   [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]
                   [--log_level {detail,debug,info,warning,error,critical,passive}]
                   [--log_level_replica {detail,debug,info,warning,error,critical,passive}]
                   [--log_on_each_node [LOG_ON_EACH_NODE]]
                   [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                   [--logging_strategy {no,steps,epoch}]
                   [--logging_first_step [LOGGING_FIRST_STEP]]
                   [--logging_steps LOGGING_STEPS]
                   [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]
                   [--no_logging_nan_inf_filter]
                   [--save_strategy {no,steps,epoch}]
                   [--save_steps SAVE_STEPS]
                   [--save_total_limit SAVE_TOTAL_LIMIT]
                   [--save_safetensors [SAVE_SAFETENSORS]]
                   [--no_save_safetensors]
                   [--save_on_each_node [SAVE_ON_EACH_NODE]]
                   [--save_only_model [SAVE_ONLY_MODEL]] [--no_cuda [NO_CUDA]]
                   [--use_cpu [USE_CPU]] [--use_mps_device [USE_MPS_DEVICE]]
                   [--seed SEED] [--data_seed DATA_SEED]
                   [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]]
                   [--bf16 [BF16]] [--fp16 [FP16]]
                   [--fp16_opt_level FP16_OPT_LEVEL]
                   [--half_precision_backend {auto,apex,cpu_amp}]
                   [--bf16_full_eval [BF16_FULL_EVAL]]
                   [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]
                   [--local_rank LOCAL_RANK]
                   [--ddp_backend {nccl,gloo,mpi,ccl,hccl}]
                   [--tpu_num_cores TPU_NUM_CORES]
                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]]
                   [--debug DEBUG [DEBUG ...]]
                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                   [--eval_steps EVAL_STEPS]
                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                   [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]
                   [--past_index PAST_INDEX] [--run_name RUN_NAME]
                   [--disable_tqdm DISABLE_TQDM]
                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                   [--no_remove_unused_columns]
                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                   [--greater_is_better GREATER_IS_BETTER]
                   [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]
                   [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
                   [--fsdp_config FSDP_CONFIG]
                   [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
                   [--deepspeed DEEPSPEED]
                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
                   [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop}]
                   [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]
                   [--group_by_length [GROUP_BY_LENGTH]]
                   [--length_column_name LENGTH_COLUMN_NAME]
                   [--report_to REPORT_TO [REPORT_TO ...]]
                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
                   [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
                   [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]
                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]
                   [--no_dataloader_pin_memory]
                   [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]
                   [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                   [--no_skip_memory_metrics]
                   [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
                   [--push_to_hub [PUSH_TO_HUB]]
                   [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                   [--hub_model_id HUB_MODEL_ID]
                   [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
                   [--hub_token HUB_TOKEN]
                   [--hub_private_repo [HUB_PRIVATE_REPO]]
                   [--hub_always_push [HUB_ALWAYS_PUSH]]
                   [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                   [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]
                   [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
                   [--fp16_backend {auto,apex,cpu_amp}]
                   [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
                   [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
                   [--push_to_hub_token PUSH_TO_HUB_TOKEN]
                   [--mp_parameters MP_PARAMETERS]
                   [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
                   [--full_determinism [FULL_DETERMINISM]]
                   [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]
                   [--ddp_timeout DDP_TIMEOUT]
                   [--torch_compile [TORCH_COMPILE]]
                   [--torch_compile_backend TORCH_COMPILE_BACKEND]
                   [--torch_compile_mode TORCH_COMPILE_MODE]
                   [--dispatch_batches DISPATCH_BATCHES]
                   [--split_batches [SPLIT_BATCHES]]
                   [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]
                   [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]
                   [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]
usage: run_glue.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                   [--config_name CONFIG_NAME]
                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                   [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
                   [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]
                   [--token TOKEN] [--use_auth_token [USE_AUTH_TOKEN]]
                   [--trust_remote_code [TRUST_REMOTE_CODE]]
                   [--ignore_mismatched_sizes [IGNORE_MISMATCHED_SIZES]]
                   [--task_name TASK_NAME] [--dataset_name DATASET_NAME]
                   [--dataset_config_name DATASET_CONFIG_NAME]
                   [--max_seq_length MAX_SEQ_LENGTH]
                   [--overwrite_cache [OVERWRITE_CACHE]]
                   [--pad_to_max_length [PAD_TO_MAX_LENGTH]]
                   [--no_pad_to_max_length]
                   [--max_train_samples MAX_TRAIN_SAMPLES]
                   [--max_eval_samples MAX_EVAL_SAMPLES]
                   [--max_predict_samples MAX_PREDICT_SAMPLES]
                   [--train_file TRAIN_FILE]
                   [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]
                   --output_dir OUTPUT_DIR
                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                   [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                   [--do_predict [DO_PREDICT]]
                   [--evaluation_strategy {no,steps,epoch}]
                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                   [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]
                   [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
                   [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]
                   [--max_grad_norm MAX_GRAD_NORM]
                   [--num_train_epochs NUM_TRAIN_EPOCHS]
                   [--max_steps MAX_STEPS]
                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau}]
                   [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]
                   [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]
                   [--log_level {detail,debug,info,warning,error,critical,passive}]
                   [--log_level_replica {detail,debug,info,warning,error,critical,passive}]
                   [--log_on_each_node [LOG_ON_EACH_NODE]]
                   [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                   [--logging_strategy {no,steps,epoch}]
                   [--logging_first_step [LOGGING_FIRST_STEP]]
                   [--logging_steps LOGGING_STEPS]
                   [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]
                   [--no_logging_nan_inf_filter]
                   [--save_strategy {no,steps,epoch}]
                   [--save_steps SAVE_STEPS]
                   [--save_total_limit SAVE_TOTAL_LIMIT]
                   [--save_safetensors [SAVE_SAFETENSORS]]
                   [--no_save_safetensors]
                   [--save_on_each_node [SAVE_ON_EACH_NODE]]
                   [--save_only_model [SAVE_ONLY_MODEL]] [--no_cuda [NO_CUDA]]
                   [--use_cpu [USE_CPU]] [--use_mps_device [USE_MPS_DEVICE]]
                   [--seed SEED] [--data_seed DATA_SEED]
                   [--jit_mode_eval [JIT_MODE_EVAL]] [--use_ipex [USE_IPEX]]
                   [--bf16 [BF16]] [--fp16 [FP16]]
                   [--fp16_opt_level FP16_OPT_LEVEL]
                   [--half_precision_backend {auto,apex,cpu_amp}]
                   [--bf16_full_eval [BF16_FULL_EVAL]]
                   [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]
                   [--local_rank LOCAL_RANK]
                   [--ddp_backend {nccl,gloo,mpi,ccl,hccl}]
                   [--tpu_num_cores TPU_NUM_CORES]
                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]]
                   [--debug DEBUG [DEBUG ...]]
                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                   [--eval_steps EVAL_STEPS]
                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                   [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]
                   [--past_index PAST_INDEX] [--run_name RUN_NAME]
                   [--disable_tqdm DISABLE_TQDM]
                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                   [--no_remove_unused_columns]
                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                   [--greater_is_better GREATER_IS_BETTER]
                   [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]
                   [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
                   [--fsdp_config FSDP_CONFIG]
                   [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
                   [--deepspeed DEEPSPEED]
                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
                   [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop}]
                   [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]
                   [--group_by_length [GROUP_BY_LENGTH]]
                   [--length_column_name LENGTH_COLUMN_NAME]
                   [--report_to REPORT_TO [REPORT_TO ...]]
                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
                   [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
                   [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]
                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]
                   [--no_dataloader_pin_memory]
                   [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]
                   [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                   [--no_skip_memory_metrics]
                   [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
                   [--push_to_hub [PUSH_TO_HUB]]
                   [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                   [--hub_model_id HUB_MODEL_ID]
                   [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
                   [--hub_token HUB_TOKEN]
                   [--hub_private_repo [HUB_PRIVATE_REPO]]
                   [--hub_always_push [HUB_ALWAYS_PUSH]]
                   [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                   [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]
                   [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
                   [--fp16_backend {auto,apex,cpu_amp}]
                   [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
                   [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
                   [--push_to_hub_token PUSH_TO_HUB_TOKEN]
                   [--mp_parameters MP_PARAMETERS]
                   [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
                   [--full_determinism [FULL_DETERMINISM]]
                   [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]
                   [--ddp_timeout DDP_TIMEOUT]
                   [--torch_compile [TORCH_COMPILE]]
                   [--torch_compile_backend TORCH_COMPILE_BACKEND]
                   [--torch_compile_mode TORCH_COMPILE_MODE]
                   [--dispatch_batches DISPATCH_BATCHES]
                   [--split_batches [SPLIT_BATCHES]]
                   [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]
                   [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]
                   [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]
run_glue.py: error: the following arguments are required: --output_dir
run_glue.py: error: the following arguments are required: --output_dir
[2024-02-16 14:35:33,278] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 0 (pid: 78094) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_14:35:33
  host      : v006.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 78095)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_14:35:33
  host      : v006.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 78094)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/usr/bin/bash: line 1: --do_train: command not found
srun: error: v006: task 0: Exited with exit code 127
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 14:36:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 14:36:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc2048/runs/Feb16_14-36-18_v006.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc2048,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc2048,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/16/2024 14:36:18 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/16/2024 14:36:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:36:19 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 14:36:20 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:36:20 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 14:36:20,320 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 14:36:20,323 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:36:20,381 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:36:20,381 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:36:20,382 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:36:20,382 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:36:20,382 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 14:36:20,536 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-16 14:36:20,537 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 14:36:20,678 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|modeling_utils.py:3996] 2024-02-16 14:36:38,404 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3984] 2024-02-16 14:36:38,409 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 14:36:38,422 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d1c3421c9bf5435d.arrow
02/16/2024 14:36:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d1c3421c9bf5435d.arrow
Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:02, 1096.85 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:01<00:01, 1125.99 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:02<00:00, 1060.44 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:03<00:00, 1090.02 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:03<00:00, 1014.17 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-23bc2fef30eaefb5.arrow
02/16/2024 14:36:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-23bc2fef30eaefb5.arrow
Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 1215.11 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 1168.62 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3fe78c5e178cc144.arrow
02/16/2024 14:36:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3fe78c5e178cc144.arrow
Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 1216.10 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:01<00:00, 1178.77 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:01<00:00, 1166.55 examples/s]
02/16/2024 14:36:45 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:36:45 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:36:45 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:36:45 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 692.56 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 659.39 examples/s]
[INFO|trainer.py:737] 2024-02-16 14:36:48,083 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 14:36:49,587 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 14:36:49,597 >>   Num examples = 3,668
[INFO|trainer.py:1749] 2024-02-16 14:36:49,597 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 14:36:49,597 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 14:36:49,598 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-16 14:36:49,598 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 14:36:49,598 >>   Total optimization steps = 690
[INFO|trainer.py:1756] 2024-02-16 14:36:49,599 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/690 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
    main()
  File "run_glue.py", line 564, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
    main()
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
  File "run_glue.py", line 564, in main
    tr_loss_step = self.training_step(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2821, in training_step
    loss = self.compute_loss(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2844, in compute_loss
    outputs = model(**inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1314, in forward
    tr_loss_step = self.training_step(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2821, in training_step
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 985, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 732, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 642, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 184, in apply_rotary_pos_emb
    q_embed = (q * cos) + (rotate_half(q) * sin)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 160, in rotate_half
    return torch.cat((-x2, x1), dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 59.31 MiB is free. Including non-PyTorch memory, this process has 31.68 GiB memory in use. Of the allocated memory 30.70 GiB is allocated by PyTorch, and 502.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    loss = self.compute_loss(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2844, in compute_loss
    outputs = model(**inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1314, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 985, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 732, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 642, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 184, in apply_rotary_pos_emb
    q_embed = (q * cos) + (rotate_half(q) * sin)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 160, in rotate_half
    return torch.cat((-x2, x1), dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 65.31 MiB is free. Including non-PyTorch memory, this process has 31.67 GiB memory in use. Of the allocated memory 30.70 GiB is allocated by PyTorch, and 496.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/690 [00:02<?, ?it/s]
[2024-02-16 14:36:55,296] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 78150) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_14:36:55
  host      : v006.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 78151)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_14:36:55
  host      : v006.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 78150)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v006: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 14:41:15 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 14:41:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512/runs/Feb16_14-41-14_v006.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/16/2024 14:41:15 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/16/2024 14:41:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:41:16 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 14:41:16 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:41:16 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 14:41:16,919 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 14:41:16,921 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:41:16,952 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:41:16,952 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:41:16,952 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:41:16,952 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:41:16,952 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 14:41:17,077 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-16 14:41:17,086 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 14:41:17,188 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|modeling_utils.py:3996] 2024-02-16 14:41:23,496 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3984] 2024-02-16 14:41:23,499 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 14:41:23,500 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-59334ae9b622ed60.arrow
02/16/2024 14:41:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-59334ae9b622ed60.arrow
Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:00, 2730.33 examples/s]Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 3060.17 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:01<00:00, 2610.78 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 2774.97 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 2759.03 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b078aa9eedc1d54c.arrow
02/16/2024 14:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b078aa9eedc1d54c.arrow
Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3047.29 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 2901.05 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-404cd6cda0c41490.arrow
02/16/2024 14:41:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-404cd6cda0c41490.arrow
Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 3147.92 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 3169.18 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 3108.97 examples/s]
02/16/2024 14:41:26 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:41:26 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:41:26 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 2502.05 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 2390.94 examples/s]
02/16/2024 14:41:26 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 14:41:29,741 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 14:41:30,202 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 14:41:30,216 >>   Num examples = 3,668
[INFO|trainer.py:1749] 2024-02-16 14:41:30,217 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 14:41:30,217 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 14:41:30,217 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-16 14:41:30,217 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 14:41:30,218 >>   Total optimization steps = 690
[INFO|trainer.py:1756] 2024-02-16 14:41:30,219 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/690 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/690 [00:04<54:10,  4.72s/it]Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
    main()
  File "run_glue.py", line 564, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
Traceback (most recent call last):
      File "run_glue.py", line 656, in <module>
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
    main()
  File "run_glue.py", line 564, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2821, in training_step
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
    loss = self.compute_loss(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2844, in compute_loss
    tr_loss_step = self.training_step(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2821, in training_step
    outputs = model(**inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    loss = self.compute_loss(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2844, in compute_loss
return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
        return forward_call(*args, **kwargs)
outputs = model(**inputs)  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward

      File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
else self._run_ddp_forward(*inputs, **kwargs)    
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
return self.module(*inputs, **kwargs)  # type: ignore[index]    
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1314, in forward
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1314, in forward
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 985, in forward
    layer_outputs = decoder_layer(
return self._call_impl(*args, **kwargs)  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl

      File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
return forward_call(*args, **kwargs)
      File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 985, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 729, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 747, in forward
    hidden_states = residual + hidden_states
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 7.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 88, in forward
    return self.weight * hidden_states.to(input_dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 19.31 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 30.12 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/690 [00:05<1:04:33,  5.62s/it]
[2024-02-16 14:41:40,404] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 85947) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_14:41:40
  host      : v006.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 85948)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_14:41:40
  host      : v006.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 85947)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v006: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 14:42:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 14:42:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/runs/Feb16_14-42-32_v006.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/16/2024 14:42:33 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/16/2024 14:42:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:42:34 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 14:42:34 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 14:42:34 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 14:42:34,321 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 14:42:34,323 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:42:34,358 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:42:34,359 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:42:34,359 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:42:34,359 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 14:42:34,359 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 14:42:34,438 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 14:42:34,503 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|logging.py:314] 2024-02-16 14:42:35,141 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3984] 2024-02-16 14:42:40,573 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 14:42:40,573 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
02/16/2024 14:42:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
Running tokenizer on dataset:  27%|██▋       | 1000/3668 [00:00<00:01, 2145.65 examples/s][WARNING|modeling_utils.py:3996] 2024-02-16 14:42:41,542 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:  55%|█████▍    | 2000/3668 [00:00<00:00, 2162.86 examples/s]Running tokenizer on dataset:  82%|████████▏ | 3000/3668 [00:01<00:00, 2368.33 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 2735.10 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:01<00:00, 2503.30 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-168eebad5e27bff2.arrow
02/16/2024 14:42:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-168eebad5e27bff2.arrow
Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 4062.89 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3883.76 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8d21548c38ba04fd.arrow
02/16/2024 14:42:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8d21548c38ba04fd.arrow
Running tokenizer on dataset:  58%|█████▊    | 1000/1725 [00:00<00:00, 4839.45 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 4633.79 examples/s]Running tokenizer on dataset: 100%|██████████| 1725/1725 [00:00<00:00, 4574.09 examples/s]
02/16/2024 14:42:43 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8469, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 29871, 29941, 29941, 6515, 310, 10701, 714, 1915, 292, 16831, 800, 2750, 1075, 869, 1, 1019, 3947, 886, 892, 4586, 701, 411, 410, 3471, 29560, 714, 1915, 292, 1009, 1206, 2750, 1913, 307, 2526, 1919, 5183, 263, 29871, 29941, 29941, 29899, 3488, 26142, 362, 5497, 304, 278, 8973, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:42:43 - INFO - __main__ - Sample 456 of the training set: {'sentence1': "Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .", 'sentence2': "Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .", 'label': 1, 'idx': 509, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6561, 2724, 24921, 1985, 363, 278, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 2446, 16340, 525, 29879, 6673, 616, 8271, 297, 1370, 29899, 29873, 1398, 6561, 305, 1460, 29874, 869, 1, 10564, 29879, 297, 6561, 305, 1460, 29874, 525, 29879, 25820, 29899, 1627, 287, 5874, 526, 263, 17091, 3646, 363, 15121, 1379, 1919, 322, 260, 2673, 338, 2734, 1880, 14432, 310, 16340, 525, 29879, 6673, 616, 8271, 297, 278, 1370, 29899, 5705, 4063, 5120, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 14:42:43 - INFO - __main__ - Sample 102 of the training set: {'sentence1': "Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .", 'sentence2': "The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .", 'label': 0, 'idx': 116, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 10961, 2380, 3105, 1973, 4845, 1312, 29871, 29946, 29889, 29946, 29900, 3291, 304, 29871, 29929, 29947, 29941, 29889, 29945, 29900, 1919, 1550, 22318, 1388, 29939, 3105, 1973, 8379, 29871, 29953, 29889, 29945, 3291, 304, 29871, 29896, 29892, 29906, 29900, 29953, 29889, 29945, 29900, 869, 1, 450, 10117, 669, 3929, 272, 525, 29879, 29871, 29945, 29900, 29900, 11374, 471, 701, 29871, 29896, 29889, 29955, 29945, 3291, 1919, 470, 29871, 29900, 29889, 29896, 29947, 10151, 1919, 304, 29871, 29929, 29955, 29955, 29889, 29953, 29947, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3445.25 examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 3293.45 examples/s]
02/16/2024 14:42:43 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 14:42:46,439 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 14:42:47,557 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 14:42:47,573 >>   Num examples = 3,668
[INFO|trainer.py:1749] 2024-02-16 14:42:47,573 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 14:42:47,573 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 14:42:47,574 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-16 14:42:47,574 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 14:42:47,574 >>   Total optimization steps = 690
[INFO|trainer.py:1756] 2024-02-16 14:42:47,575 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/690 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/690 [00:04<46:06,  4.02s/it]  0%|          | 2/690 [00:05<31:27,  2.74s/it]  0%|          | 3/690 [00:07<27:00,  2.36s/it]  1%|          | 4/690 [00:09<24:41,  2.16s/it]  1%|          | 5/690 [00:11<23:38,  2.07s/it]  1%|          | 6/690 [00:13<23:09,  2.03s/it]  1%|          | 7/690 [00:15<22:39,  1.99s/it]  1%|          | 8/690 [00:17<22:18,  1.96s/it]  1%|▏         | 9/690 [00:19<22:05,  1.95s/it]  1%|▏         | 10/690 [00:21<22:01,  1.94s/it]  2%|▏         | 11/690 [00:23<22:06,  1.95s/it]  2%|▏         | 12/690 [00:25<21:58,  1.94s/it]  2%|▏         | 13/690 [00:26<21:48,  1.93s/it]  2%|▏         | 14/690 [00:28<21:43,  1.93s/it]  2%|▏         | 15/690 [00:30<21:44,  1.93s/it]  2%|▏         | 16/690 [00:32<21:34,  1.92s/it]  2%|▏         | 17/690 [00:34<21:33,  1.92s/it]  3%|▎         | 18/690 [00:36<21:28,  1.92s/it]  3%|▎         | 19/690 [00:38<21:28,  1.92s/it]  3%|▎         | 20/690 [00:40<21:35,  1.93s/it]  3%|▎         | 21/690 [00:42<21:29,  1.93s/it]  3%|▎         | 22/690 [00:44<21:30,  1.93s/it]  3%|▎         | 23/690 [00:46<21:30,  1.93s/it]  3%|▎         | 24/690 [00:48<21:24,  1.93s/it]  4%|▎         | 25/690 [00:50<21:21,  1.93s/it]  4%|▍         | 26/690 [00:51<21:15,  1.92s/it]  4%|▍         | 27/690 [00:53<21:21,  1.93s/it]  4%|▍         | 28/690 [00:55<21:19,  1.93s/it]  4%|▍         | 29/690 [00:57<21:19,  1.94s/it]  4%|▍         | 30/690 [00:59<21:15,  1.93s/it]  4%|▍         | 31/690 [01:01<21:05,  1.92s/it]  5%|▍         | 32/690 [01:03<21:02,  1.92s/it]  5%|▍         | 33/690 [01:05<21:00,  1.92s/it]  5%|▍         | 34/690 [01:07<20:59,  1.92s/it]  5%|▌         | 35/690 [01:09<20:56,  1.92s/it]  5%|▌         | 36/690 [01:11<20:58,  1.92s/it]  5%|▌         | 37/690 [01:13<20:52,  1.92s/it]  6%|▌         | 38/690 [01:15<20:53,  1.92s/it]  6%|▌         | 39/690 [01:16<20:51,  1.92s/it]  6%|▌         | 40/690 [01:18<20:46,  1.92s/it]  6%|▌         | 41/690 [01:20<20:38,  1.91s/it]  6%|▌         | 42/690 [01:22<20:33,  1.90s/it]  6%|▌         | 43/690 [01:24<20:41,  1.92s/it]  6%|▋         | 44/690 [01:26<20:43,  1.93s/it]  7%|▋         | 45/690 [01:28<20:39,  1.92s/it]  7%|▋         | 46/690 [01:30<20:37,  1.92s/it]  7%|▋         | 47/690 [01:32<20:32,  1.92s/it]  7%|▋         | 48/690 [01:34<20:36,  1.93s/it]  7%|▋         | 49/690 [01:36<20:33,  1.92s/it]  7%|▋         | 50/690 [01:38<20:33,  1.93s/it]  7%|▋         | 51/690 [01:40<20:32,  1.93s/it]  8%|▊         | 52/690 [01:41<20:34,  1.94s/it]  8%|▊         | 53/690 [01:43<20:33,  1.94s/it]  8%|▊         | 54/690 [01:45<20:25,  1.93s/it]  8%|▊         | 55/690 [01:47<20:19,  1.92s/it]  8%|▊         | 56/690 [01:49<20:14,  1.92s/it]  8%|▊         | 57/690 [01:51<20:22,  1.93s/it]  8%|▊         | 58/690 [01:53<20:21,  1.93s/it]  9%|▊         | 59/690 [01:55<20:17,  1.93s/it]  9%|▊         | 60/690 [01:57<20:11,  1.92s/it]  9%|▉         | 61/690 [01:59<20:09,  1.92s/it]  9%|▉         | 62/690 [02:01<20:03,  1.92s/it]  9%|▉         | 63/690 [02:03<20:06,  1.92s/it]  9%|▉         | 64/690 [02:05<20:03,  1.92s/it]  9%|▉         | 65/690 [02:06<19:57,  1.92s/it] 10%|▉         | 66/690 [02:08<20:06,  1.93s/it] 10%|▉         | 67/690 [02:10<20:02,  1.93s/it] 10%|▉         | 68/690 [02:12<20:02,  1.93s/it] 10%|█         | 69/690 [02:14<20:08,  1.95s/it] 10%|█         | 70/690 [02:16<20:05,  1.95s/it] 10%|█         | 71/690 [02:18<20:08,  1.95s/it] 10%|█         | 72/690 [02:20<19:51,  1.93s/it] 11%|█         | 73/690 [02:22<19:42,  1.92s/it] 11%|█         | 74/690 [02:24<19:45,  1.93s/it] 11%|█         | 75/690 [02:26<19:42,  1.92s/it] 11%|█         | 76/690 [02:28<19:35,  1.91s/it] 11%|█         | 77/690 [02:30<19:32,  1.91s/it] 11%|█▏        | 78/690 [02:32<19:33,  1.92s/it] 11%|█▏        | 79/690 [02:33<19:32,  1.92s/it] 12%|█▏        | 80/690 [02:35<19:28,  1.92s/it] 12%|█▏        | 81/690 [02:37<19:32,  1.92s/it] 12%|█▏        | 82/690 [02:39<19:27,  1.92s/it] 12%|█▏        | 83/690 [02:41<19:29,  1.93s/it] 12%|█▏        | 84/690 [02:43<19:28,  1.93s/it] 12%|█▏        | 85/690 [02:45<19:21,  1.92s/it] 12%|█▏        | 86/690 [02:47<19:15,  1.91s/it] 13%|█▎        | 87/690 [02:49<19:21,  1.93s/it] 13%|█▎        | 88/690 [02:51<19:30,  1.94s/it] 13%|█▎        | 89/690 [02:53<19:31,  1.95s/it] 13%|█▎        | 90/690 [02:55<19:27,  1.95s/it] 13%|█▎        | 91/690 [02:57<19:26,  1.95s/it] 13%|█▎        | 92/690 [02:59<19:19,  1.94s/it] 13%|█▎        | 93/690 [03:01<19:19,  1.94s/it] 14%|█▎        | 94/690 [03:03<19:16,  1.94s/it] 14%|█▍        | 95/690 [03:04<19:15,  1.94s/it] 14%|█▍        | 96/690 [03:06<19:10,  1.94s/it] 14%|█▍        | 97/690 [03:08<19:03,  1.93s/it] 14%|█▍        | 98/690 [03:10<19:02,  1.93s/it] 14%|█▍        | 99/690 [03:12<19:03,  1.93s/it] 14%|█▍        | 100/690 [03:14<18:57,  1.93s/it] 15%|█▍        | 101/690 [03:16<18:53,  1.92s/it] 15%|█▍        | 102/690 [03:18<19:02,  1.94s/it] 15%|█▍        | 103/690 [03:20<18:58,  1.94s/it] 15%|█▌        | 104/690 [03:22<18:55,  1.94s/it] 15%|█▌        | 105/690 [03:24<18:46,  1.93s/it] 15%|█▌        | 106/690 [03:26<18:41,  1.92s/it] 16%|█▌        | 107/690 [03:28<18:34,  1.91s/it] 16%|█▌        | 108/690 [03:30<18:43,  1.93s/it] 16%|█▌        | 109/690 [03:31<18:33,  1.92s/it] 16%|█▌        | 110/690 [03:33<18:32,  1.92s/it] 16%|█▌        | 111/690 [03:35<18:28,  1.92s/it] 16%|█▌        | 112/690 [03:37<18:32,  1.92s/it] 16%|█▋        | 113/690 [03:39<18:28,  1.92s/it] 17%|█▋        | 114/690 [03:41<18:25,  1.92s/it] 17%|█▋        | 115/690 [03:43<18:28,  1.93s/it] 17%|█▋        | 116/690 [03:45<18:24,  1.92s/it] 17%|█▋        | 117/690 [03:47<18:22,  1.92s/it] 17%|█▋        | 118/690 [03:49<18:22,  1.93s/it] 17%|█▋        | 119/690 [03:51<18:19,  1.93s/it] 17%|█▋        | 120/690 [03:53<18:16,  1.92s/it] 18%|█▊        | 121/690 [03:55<18:20,  1.93s/it] 18%|█▊        | 122/690 [03:56<18:18,  1.93s/it] 18%|█▊        | 123/690 [03:58<18:14,  1.93s/it] 18%|█▊        | 124/690 [04:00<18:12,  1.93s/it] 18%|█▊        | 125/690 [04:02<18:06,  1.92s/it] 18%|█▊        | 126/690 [04:04<18:06,  1.93s/it] 18%|█▊        | 127/690 [04:06<18:07,  1.93s/it] 19%|█▊        | 128/690 [04:08<18:03,  1.93s/it] 19%|█▊        | 129/690 [04:10<18:02,  1.93s/it] 19%|█▉        | 130/690 [04:12<18:02,  1.93s/it] 19%|█▉        | 131/690 [04:14<17:58,  1.93s/it] 19%|█▉        | 132/690 [04:16<18:00,  1.94s/it] 19%|█▉        | 133/690 [04:18<18:01,  1.94s/it] 19%|█▉        | 134/690 [04:20<18:01,  1.95s/it] 20%|█▉        | 135/690 [04:22<17:56,  1.94s/it] 20%|█▉        | 136/690 [04:24<17:53,  1.94s/it] 20%|█▉        | 137/690 [04:25<17:47,  1.93s/it] 20%|██        | 138/690 [04:27<17:40,  1.92s/it] 20%|██        | 139/690 [04:29<17:37,  1.92s/it] 20%|██        | 140/690 [04:31<17:39,  1.93s/it] 20%|██        | 141/690 [04:33<17:37,  1.93s/it] 21%|██        | 142/690 [04:35<17:34,  1.92s/it] 21%|██        | 143/690 [04:37<17:36,  1.93s/it] 21%|██        | 144/690 [04:39<17:34,  1.93s/it] 21%|██        | 145/690 [04:41<17:34,  1.93s/it] 21%|██        | 146/690 [04:43<17:38,  1.95s/it] 21%|██▏       | 147/690 [04:45<17:29,  1.93s/it] 21%|██▏       | 148/690 [04:47<17:30,  1.94s/it] 22%|██▏       | 149/690 [04:49<17:28,  1.94s/it] 22%|██▏       | 150/690 [04:51<17:33,  1.95s/it] 22%|██▏       | 151/690 [04:53<17:26,  1.94s/it] 22%|██▏       | 152/690 [04:54<17:22,  1.94s/it] 22%|██▏       | 153/690 [04:56<17:20,  1.94s/it] 22%|██▏       | 154/690 [04:58<17:17,  1.94s/it] 22%|██▏       | 155/690 [05:00<17:13,  1.93s/it] 23%|██▎       | 156/690 [05:02<17:13,  1.94s/it] 23%|██▎       | 157/690 [05:04<17:17,  1.95s/it] 23%|██▎       | 158/690 [05:06<17:14,  1.94s/it] 23%|██▎       | 159/690 [05:08<17:06,  1.93s/it] 23%|██▎       | 160/690 [05:10<17:05,  1.93s/it] 23%|██▎       | 161/690 [05:12<17:00,  1.93s/it] 23%|██▎       | 162/690 [05:14<17:00,  1.93s/it] 24%|██▎       | 163/690 [05:16<16:58,  1.93s/it] 24%|██▍       | 164/690 [05:18<16:45,  1.91s/it] 24%|██▍       | 165/690 [05:20<16:44,  1.91s/it] 24%|██▍       | 166/690 [05:21<16:47,  1.92s/it] 24%|██▍       | 167/690 [05:23<16:53,  1.94s/it] 24%|██▍       | 168/690 [05:25<16:52,  1.94s/it] 24%|██▍       | 169/690 [05:27<16:46,  1.93s/it] 25%|██▍       | 170/690 [05:29<16:46,  1.93s/it] 25%|██▍       | 171/690 [05:31<16:45,  1.94s/it] 25%|██▍       | 172/690 [05:33<16:40,  1.93s/it] 25%|██▌       | 173/690 [05:35<16:41,  1.94s/it] 25%|██▌       | 174/690 [05:37<16:36,  1.93s/it] 25%|██▌       | 175/690 [05:39<16:33,  1.93s/it] 26%|██▌       | 176/690 [05:41<16:31,  1.93s/it] 26%|██▌       | 177/690 [05:43<16:30,  1.93s/it] 26%|██▌       | 178/690 [05:45<16:28,  1.93s/it] 26%|██▌       | 179/690 [05:47<16:25,  1.93s/it] 26%|██▌       | 180/690 [05:49<16:18,  1.92s/it] 26%|██▌       | 181/690 [05:50<16:16,  1.92s/it] 26%|██▋       | 182/690 [05:52<16:15,  1.92s/it] 27%|██▋       | 183/690 [05:54<16:15,  1.92s/it] 27%|██▋       | 184/690 [05:56<16:13,  1.92s/it] 27%|██▋       | 185/690 [05:58<16:14,  1.93s/it] 27%|██▋       | 186/690 [06:00<16:06,  1.92s/it] 27%|██▋       | 187/690 [06:02<16:12,  1.93s/it] 27%|██▋       | 188/690 [06:04<16:04,  1.92s/it] 27%|██▋       | 189/690 [06:06<16:05,  1.93s/it] 28%|██▊       | 190/690 [06:08<16:05,  1.93s/it] 28%|██▊       | 191/690 [06:10<15:59,  1.92s/it] 28%|██▊       | 192/690 [06:12<16:04,  1.94s/it] 28%|██▊       | 193/690 [06:14<16:00,  1.93s/it] 28%|██▊       | 194/690 [06:16<15:57,  1.93s/it] 28%|██▊       | 195/690 [06:17<15:55,  1.93s/it] 28%|██▊       | 196/690 [06:19<15:44,  1.91s/it] 29%|██▊       | 197/690 [06:21<15:46,  1.92s/it] 29%|██▊       | 198/690 [06:23<15:46,  1.92s/it] 29%|██▉       | 199/690 [06:25<15:39,  1.91s/it] 29%|██▉       | 200/690 [06:27<15:40,  1.92s/it] 29%|██▉       | 201/690 [06:29<15:36,  1.92s/it] 29%|██▉       | 202/690 [06:31<15:38,  1.92s/it] 29%|██▉       | 203/690 [06:33<15:33,  1.92s/it] 30%|██▉       | 204/690 [06:35<15:42,  1.94s/it] 30%|██▉       | 205/690 [06:37<15:32,  1.92s/it] 30%|██▉       | 206/690 [06:39<15:33,  1.93s/it] 30%|███       | 207/690 [06:40<15:29,  1.92s/it] 30%|███       | 208/690 [06:42<15:25,  1.92s/it] 30%|███       | 209/690 [06:44<15:24,  1.92s/it] 30%|███       | 210/690 [06:46<15:19,  1.92s/it] 31%|███       | 211/690 [06:48<15:17,  1.92s/it] 31%|███       | 212/690 [06:50<15:18,  1.92s/it] 31%|███       | 213/690 [06:52<15:17,  1.92s/it] 31%|███       | 214/690 [06:54<15:17,  1.93s/it] 31%|███       | 215/690 [06:56<15:18,  1.93s/it] 31%|███▏      | 216/690 [06:58<15:11,  1.92s/it] 31%|███▏      | 217/690 [07:00<15:06,  1.92s/it] 32%|███▏      | 218/690 [07:02<15:08,  1.93s/it] 32%|███▏      | 219/690 [07:04<15:05,  1.92s/it] 32%|███▏      | 220/690 [07:05<15:03,  1.92s/it] 32%|███▏      | 221/690 [07:07<14:59,  1.92s/it] 32%|███▏      | 222/690 [07:09<15:01,  1.93s/it] 32%|███▏      | 223/690 [07:11<15:04,  1.94s/it] 32%|███▏      | 224/690 [07:13<15:05,  1.94s/it] 33%|███▎      | 225/690 [07:15<15:01,  1.94s/it] 33%|███▎      | 226/690 [07:17<14:58,  1.94s/it] 33%|███▎      | 227/690 [07:19<14:52,  1.93s/it] 33%|███▎      | 228/690 [07:21<14:47,  1.92s/it] 33%|███▎      | 229/690 [07:23<14:49,  1.93s/it] 33%|███▎      | 230/690 [07:25<14:44,  1.92s/it] 33%|███▎      | 231/690 [07:27<14:42,  1.92s/it] 34%|███▎      | 232/690 [07:29<14:39,  1.92s/it] 34%|███▍      | 233/690 [07:31<14:35,  1.92s/it] 34%|███▍      | 234/690 [07:32<14:37,  1.92s/it] 34%|███▍      | 235/690 [07:34<14:38,  1.93s/it] 34%|███▍      | 236/690 [07:36<14:36,  1.93s/it] 34%|███▍      | 237/690 [07:38<14:38,  1.94s/it] 34%|███▍      | 238/690 [07:40<14:37,  1.94s/it] 35%|███▍      | 239/690 [07:42<14:31,  1.93s/it] 35%|███▍      | 240/690 [07:44<14:29,  1.93s/it] 35%|███▍      | 241/690 [07:46<14:26,  1.93s/it] 35%|███▌      | 242/690 [07:48<14:25,  1.93s/it] 35%|███▌      | 243/690 [07:50<14:23,  1.93s/it] 35%|███▌      | 244/690 [07:52<14:19,  1.93s/it] 36%|███▌      | 245/690 [07:54<14:20,  1.93s/it] 36%|███▌      | 246/690 [07:56<14:17,  1.93s/it] 36%|███▌      | 247/690 [07:58<14:12,  1.92s/it] 36%|███▌      | 248/690 [08:00<14:14,  1.93s/it] 36%|███▌      | 249/690 [08:01<14:11,  1.93s/it] 36%|███▌      | 250/690 [08:03<14:06,  1.92s/it] 36%|███▋      | 251/690 [08:05<14:03,  1.92s/it] 37%|███▋      | 252/690 [08:07<14:03,  1.92s/it] 37%|███▋      | 253/690 [08:09<14:01,  1.93s/it] 37%|███▋      | 254/690 [08:11<14:00,  1.93s/it] 37%|███▋      | 255/690 [08:13<14:00,  1.93s/it] 37%|███▋      | 256/690 [08:15<13:53,  1.92s/it] 37%|███▋      | 257/690 [08:17<13:53,  1.92s/it] 37%|███▋      | 258/690 [08:19<13:51,  1.93s/it] 38%|███▊      | 259/690 [08:21<13:51,  1.93s/it] 38%|███▊      | 260/690 [08:23<13:53,  1.94s/it] 38%|███▊      | 261/690 [08:25<13:45,  1.92s/it] 38%|███▊      | 262/690 [08:26<13:45,  1.93s/it] 38%|███▊      | 263/690 [08:28<13:41,  1.92s/it] 38%|███▊      | 264/690 [08:30<13:39,  1.92s/it] 38%|███▊      | 265/690 [08:32<13:36,  1.92s/it] 39%|███▊      | 266/690 [08:34<13:34,  1.92s/it] 39%|███▊      | 267/690 [08:36<13:30,  1.92s/it] 39%|███▉      | 268/690 [08:38<13:29,  1.92s/it] 39%|███▉      | 269/690 [08:40<13:32,  1.93s/it] 39%|███▉      | 270/690 [08:42<13:29,  1.93s/it] 39%|███▉      | 271/690 [08:44<13:29,  1.93s/it] 39%|███▉      | 272/690 [08:46<13:28,  1.93s/it] 40%|███▉      | 273/690 [08:48<13:29,  1.94s/it] 40%|███▉      | 274/690 [08:50<13:22,  1.93s/it] 40%|███▉      | 275/690 [08:52<13:22,  1.93s/it] 40%|████      | 276/690 [08:53<13:17,  1.93s/it] 40%|████      | 277/690 [08:55<13:13,  1.92s/it] 40%|████      | 278/690 [08:57<13:14,  1.93s/it] 40%|████      | 279/690 [08:59<13:14,  1.93s/it] 41%|████      | 280/690 [09:01<13:10,  1.93s/it] 41%|████      | 281/690 [09:03<13:05,  1.92s/it] 41%|████      | 282/690 [09:05<13:04,  1.92s/it] 41%|████      | 283/690 [09:07<13:04,  1.93s/it] 41%|████      | 284/690 [09:09<13:01,  1.93s/it] 41%|████▏     | 285/690 [09:11<13:05,  1.94s/it] 41%|████▏     | 286/690 [09:13<13:01,  1.93s/it] 42%|████▏     | 287/690 [09:15<12:57,  1.93s/it] 42%|████▏     | 288/690 [09:17<12:55,  1.93s/it] 42%|████▏     | 289/690 [09:19<12:53,  1.93s/it] 42%|████▏     | 290/690 [09:20<12:48,  1.92s/it] 42%|████▏     | 291/690 [09:22<12:49,  1.93s/it] 42%|████▏     | 292/690 [09:24<12:48,  1.93s/it] 42%|████▏     | 293/690 [09:26<12:44,  1.92s/it] 43%|████▎     | 294/690 [09:28<12:45,  1.93s/it] 43%|████▎     | 295/690 [09:30<12:43,  1.93s/it] 43%|████▎     | 296/690 [09:32<12:41,  1.93s/it] 43%|████▎     | 297/690 [09:34<12:39,  1.93s/it] 43%|████▎     | 298/690 [09:36<12:40,  1.94s/it] 43%|████▎     | 299/690 [09:38<12:39,  1.94s/it] 43%|████▎     | 300/690 [09:40<12:37,  1.94s/it] 44%|████▎     | 301/690 [09:42<12:31,  1.93s/it] 44%|████▍     | 302/690 [09:44<12:33,  1.94s/it] 44%|████▍     | 303/690 [09:46<12:35,  1.95s/it] 44%|████▍     | 304/690 [09:48<12:30,  1.95s/it] 44%|████▍     | 305/690 [09:50<12:28,  1.94s/it] 44%|████▍     | 306/690 [09:52<12:26,  1.94s/it] 44%|████▍     | 307/690 [09:53<12:22,  1.94s/it] 45%|████▍     | 308/690 [09:55<12:19,  1.94s/it] 45%|████▍     | 309/690 [09:57<12:14,  1.93s/it] 45%|████▍     | 310/690 [09:59<12:10,  1.92s/it] 45%|████▌     | 311/690 [10:01<12:13,  1.94s/it] 45%|████▌     | 312/690 [10:03<12:05,  1.92s/it] 45%|████▌     | 313/690 [10:05<12:06,  1.93s/it] 46%|████▌     | 314/690 [10:07<12:00,  1.92s/it] 46%|████▌     | 315/690 [10:09<11:57,  1.91s/it] 46%|████▌     | 316/690 [10:11<12:01,  1.93s/it] 46%|████▌     | 317/690 [10:13<12:02,  1.94s/it] 46%|████▌     | 318/690 [10:15<11:55,  1.92s/it] 46%|████▌     | 319/690 [10:17<11:54,  1.93s/it] 46%|████▋     | 320/690 [10:18<11:54,  1.93s/it] 47%|████▋     | 321/690 [10:20<11:47,  1.92s/it] 47%|████▋     | 322/690 [10:22<11:48,  1.93s/it] 47%|████▋     | 323/690 [10:24<11:47,  1.93s/it] 47%|████▋     | 324/690 [10:26<11:49,  1.94s/it] 47%|████▋     | 325/690 [10:28<11:46,  1.94s/it] 47%|████▋     | 326/690 [10:30<11:45,  1.94s/it] 47%|████▋     | 327/690 [10:32<11:41,  1.93s/it] 48%|████▊     | 328/690 [10:34<11:38,  1.93s/it] 48%|████▊     | 329/690 [10:36<11:37,  1.93s/it] 48%|████▊     | 330/690 [10:38<11:32,  1.92s/it] 48%|████▊     | 331/690 [10:40<11:30,  1.92s/it] 48%|████▊     | 332/690 [10:42<11:30,  1.93s/it] 48%|████▊     | 333/690 [10:44<11:27,  1.92s/it] 48%|████▊     | 334/690 [10:45<11:22,  1.92s/it] 49%|████▊     | 335/690 [10:47<11:27,  1.94s/it] 49%|████▊     | 336/690 [10:49<11:22,  1.93s/it] 49%|████▉     | 337/690 [10:51<11:22,  1.93s/it] 49%|████▉     | 338/690 [10:53<11:21,  1.94s/it] 49%|████▉     | 339/690 [10:55<11:17,  1.93s/it] 49%|████▉     | 340/690 [10:57<11:18,  1.94s/it] 49%|████▉     | 341/690 [10:59<11:16,  1.94s/it] 50%|████▉     | 342/690 [11:01<11:12,  1.93s/it] 50%|████▉     | 343/690 [11:03<11:10,  1.93s/it] 50%|████▉     | 344/690 [11:05<11:07,  1.93s/it] 50%|█████     | 345/690 [11:07<11:01,  1.92s/it] 50%|█████     | 346/690 [11:09<10:55,  1.91s/it] 50%|█████     | 347/690 [11:11<10:58,  1.92s/it] 50%|█████     | 348/690 [11:12<10:54,  1.91s/it] 51%|█████     | 349/690 [11:14<10:51,  1.91s/it] 51%|█████     | 350/690 [11:16<10:48,  1.91s/it] 51%|█████     | 351/690 [11:18<10:47,  1.91s/it] 51%|█████     | 352/690 [11:20<10:48,  1.92s/it] 51%|█████     | 353/690 [11:22<10:47,  1.92s/it] 51%|█████▏    | 354/690 [11:24<10:44,  1.92s/it] 51%|█████▏    | 355/690 [11:26<10:45,  1.93s/it] 52%|█████▏    | 356/690 [11:28<10:45,  1.93s/it] 52%|█████▏    | 357/690 [11:30<10:41,  1.93s/it] 52%|█████▏    | 358/690 [11:32<10:40,  1.93s/it] 52%|█████▏    | 359/690 [11:34<10:38,  1.93s/it] 52%|█████▏    | 360/690 [11:36<10:38,  1.93s/it] 52%|█████▏    | 361/690 [11:37<10:39,  1.94s/it] 52%|█████▏    | 362/690 [11:39<10:36,  1.94s/it] 53%|█████▎    | 363/690 [11:41<10:32,  1.93s/it] 53%|█████▎    | 364/690 [11:43<10:32,  1.94s/it] 53%|█████▎    | 365/690 [11:45<10:29,  1.94s/it] 53%|█████▎    | 366/690 [11:47<10:29,  1.94s/it] 53%|█████▎    | 367/690 [11:49<10:28,  1.95s/it] 53%|█████▎    | 368/690 [11:51<10:24,  1.94s/it] 53%|█████▎    | 369/690 [11:53<10:22,  1.94s/it] 54%|█████▎    | 370/690 [11:55<10:15,  1.92s/it] 54%|█████▍    | 371/690 [11:57<10:12,  1.92s/it] 54%|█████▍    | 372/690 [11:59<10:13,  1.93s/it] 54%|█████▍    | 373/690 [12:01<10:09,  1.92s/it] 54%|█████▍    | 374/690 [12:03<10:12,  1.94s/it] 54%|█████▍    | 375/690 [12:05<10:13,  1.95s/it] 54%|█████▍    | 376/690 [12:07<10:11,  1.95s/it] 55%|█████▍    | 377/690 [12:08<10:07,  1.94s/it] 55%|█████▍    | 378/690 [12:10<10:06,  1.94s/it] 55%|█████▍    | 379/690 [12:12<10:04,  1.94s/it] 55%|█████▌    | 380/690 [12:14<10:01,  1.94s/it] 55%|█████▌    | 381/690 [12:16<09:55,  1.93s/it] 55%|█████▌    | 382/690 [12:18<09:50,  1.92s/it] 56%|█████▌    | 383/690 [12:20<09:50,  1.92s/it] 56%|█████▌    | 384/690 [12:22<09:47,  1.92s/it] 56%|█████▌    | 385/690 [12:24<09:46,  1.92s/it] 56%|█████▌    | 386/690 [12:26<09:44,  1.92s/it] 56%|█████▌    | 387/690 [12:28<09:43,  1.92s/it] 56%|█████▌    | 388/690 [12:30<09:40,  1.92s/it] 56%|█████▋    | 389/690 [12:32<09:37,  1.92s/it] 57%|█████▋    | 390/690 [12:33<09:37,  1.93s/it] 57%|█████▋    | 391/690 [12:35<09:36,  1.93s/it] 57%|█████▋    | 392/690 [12:37<09:35,  1.93s/it] 57%|█████▋    | 393/690 [12:39<09:36,  1.94s/it] 57%|█████▋    | 394/690 [12:41<09:36,  1.95s/it] 57%|█████▋    | 395/690 [12:43<09:32,  1.94s/it] 57%|█████▋    | 396/690 [12:45<09:30,  1.94s/it] 58%|█████▊    | 397/690 [12:47<09:28,  1.94s/it] 58%|█████▊    | 398/690 [12:49<09:25,  1.94s/it] 58%|█████▊    | 399/690 [12:51<09:25,  1.94s/it] 58%|█████▊    | 400/690 [12:53<09:19,  1.93s/it] 58%|█████▊    | 401/690 [12:55<09:18,  1.93s/it] 58%|█████▊    | 402/690 [12:57<09:16,  1.93s/it] 58%|█████▊    | 403/690 [12:59<09:14,  1.93s/it] 59%|█████▊    | 404/690 [13:01<09:13,  1.93s/it] 59%|█████▊    | 405/690 [13:03<09:15,  1.95s/it] 59%|█████▉    | 406/690 [13:05<09:14,  1.95s/it] 59%|█████▉    | 407/690 [13:06<09:11,  1.95s/it] 59%|█████▉    | 408/690 [13:08<09:11,  1.95s/it] 59%|█████▉    | 409/690 [13:10<09:09,  1.96s/it] 59%|█████▉    | 410/690 [13:12<09:06,  1.95s/it] 60%|█████▉    | 411/690 [13:14<09:02,  1.94s/it] 60%|█████▉    | 412/690 [13:16<08:58,  1.94s/it] 60%|█████▉    | 413/690 [13:18<08:55,  1.93s/it] 60%|██████    | 414/690 [13:20<08:51,  1.92s/it] 60%|██████    | 415/690 [13:22<08:48,  1.92s/it] 60%|██████    | 416/690 [13:24<08:46,  1.92s/it] 60%|██████    | 417/690 [13:26<08:46,  1.93s/it] 61%|██████    | 418/690 [13:28<08:43,  1.93s/it] 61%|██████    | 419/690 [13:30<08:43,  1.93s/it] 61%|██████    | 420/690 [13:32<08:40,  1.93s/it] 61%|██████    | 421/690 [13:33<08:36,  1.92s/it] 61%|██████    | 422/690 [13:35<08:34,  1.92s/it] 61%|██████▏   | 423/690 [13:37<08:35,  1.93s/it] 61%|██████▏   | 424/690 [13:39<08:33,  1.93s/it] 62%|██████▏   | 425/690 [13:41<08:32,  1.93s/it] 62%|██████▏   | 426/690 [13:43<08:29,  1.93s/it] 62%|██████▏   | 427/690 [13:45<08:27,  1.93s/it] 62%|██████▏   | 428/690 [13:47<08:26,  1.93s/it] 62%|██████▏   | 429/690 [13:49<08:25,  1.94s/it] 62%|██████▏   | 430/690 [13:51<08:23,  1.94s/it] 62%|██████▏   | 431/690 [13:53<08:21,  1.94s/it] 63%|██████▎   | 432/690 [13:55<08:17,  1.93s/it] 63%|██████▎   | 433/690 [13:57<08:14,  1.92s/it] 63%|██████▎   | 434/690 [13:59<08:13,  1.93s/it] 63%|██████▎   | 435/690 [14:01<08:12,  1.93s/it] 63%|██████▎   | 436/690 [14:03<08:11,  1.94s/it] 63%|██████▎   | 437/690 [14:04<08:08,  1.93s/it] 63%|██████▎   | 438/690 [14:06<08:07,  1.93s/it] 64%|██████▎   | 439/690 [14:08<08:04,  1.93s/it] 64%|██████▍   | 440/690 [14:10<08:02,  1.93s/it] 64%|██████▍   | 441/690 [14:12<08:00,  1.93s/it] 64%|██████▍   | 442/690 [14:14<08:00,  1.94s/it] 64%|██████▍   | 443/690 [14:16<07:59,  1.94s/it] 64%|██████▍   | 444/690 [14:18<07:56,  1.94s/it] 64%|██████▍   | 445/690 [14:20<07:49,  1.92s/it] 65%|██████▍   | 446/690 [14:22<07:46,  1.91s/it] 65%|██████▍   | 447/690 [14:24<07:44,  1.91s/it] 65%|██████▍   | 448/690 [14:26<07:44,  1.92s/it] 65%|██████▌   | 449/690 [14:28<07:43,  1.92s/it] 65%|██████▌   | 450/690 [14:30<07:44,  1.94s/it] 65%|██████▌   | 451/690 [14:31<07:45,  1.95s/it] 66%|██████▌   | 452/690 [14:33<07:39,  1.93s/it] 66%|██████▌   | 453/690 [14:35<07:37,  1.93s/it] 66%|██████▌   | 454/690 [14:37<07:37,  1.94s/it] 66%|██████▌   | 455/690 [14:39<07:34,  1.94s/it] 66%|██████▌   | 456/690 [14:41<07:30,  1.93s/it] 66%|██████▌   | 457/690 [14:43<07:28,  1.92s/it] 66%|██████▋   | 458/690 [14:45<07:26,  1.92s/it] 67%|██████▋   | 459/690 [14:47<07:23,  1.92s/it] 67%|██████▋   | 460/690 [14:49<07:20,  1.92s/it] 67%|██████▋   | 461/690 [14:51<07:19,  1.92s/it] 67%|██████▋   | 462/690 [14:53<07:19,  1.93s/it] 67%|██████▋   | 463/690 [14:55<07:18,  1.93s/it] 67%|██████▋   | 464/690 [14:56<07:15,  1.93s/it] 67%|██████▋   | 465/690 [14:58<07:14,  1.93s/it] 68%|██████▊   | 466/690 [15:00<07:12,  1.93s/it] 68%|██████▊   | 467/690 [15:02<07:12,  1.94s/it] 68%|██████▊   | 468/690 [15:04<07:09,  1.94s/it] 68%|██████▊   | 469/690 [15:06<07:06,  1.93s/it] 68%|██████▊   | 470/690 [15:08<07:04,  1.93s/it] 68%|██████▊   | 471/690 [15:10<07:02,  1.93s/it] 68%|██████▊   | 472/690 [15:12<07:00,  1.93s/it] 69%|██████▊   | 473/690 [15:14<06:59,  1.93s/it] 69%|██████▊   | 474/690 [15:16<06:59,  1.94s/it] 69%|██████▉   | 475/690 [15:18<06:58,  1.95s/it] 69%|██████▉   | 476/690 [15:20<06:55,  1.94s/it] 69%|██████▉   | 477/690 [15:22<06:53,  1.94s/it] 69%|██████▉   | 478/690 [15:24<06:51,  1.94s/it] 69%|██████▉   | 479/690 [15:26<06:52,  1.95s/it] 70%|██████▉   | 480/690 [15:28<06:50,  1.95s/it] 70%|██████▉   | 481/690 [15:30<06:49,  1.96s/it] 70%|██████▉   | 482/690 [15:31<06:48,  1.96s/it] 70%|███████   | 483/690 [15:33<06:47,  1.97s/it] 70%|███████   | 484/690 [15:35<06:44,  1.96s/it] 70%|███████   | 485/690 [15:37<06:41,  1.96s/it] 70%|███████   | 486/690 [15:39<06:38,  1.95s/it] 71%|███████   | 487/690 [15:41<06:38,  1.96s/it] 71%|███████   | 488/690 [15:43<06:37,  1.97s/it] 71%|███████   | 489/690 [15:45<06:35,  1.97s/it] 71%|███████   | 490/690 [15:47<06:33,  1.97s/it] 71%|███████   | 491/690 [15:49<06:30,  1.96s/it] 71%|███████▏  | 492/690 [15:51<06:25,  1.95s/it] 71%|███████▏  | 493/690 [15:53<06:22,  1.94s/it] 72%|███████▏  | 494/690 [15:55<06:21,  1.94s/it] 72%|███████▏  | 495/690 [15:57<06:19,  1.95s/it] 72%|███████▏  | 496/690 [15:59<06:17,  1.95s/it] 72%|███████▏  | 49{'loss': 0.6347, 'learning_rate': 1.3768115942028985e-05, 'epoch': 2.17}
7/690 [16:01<06:15,  1.95s/it] 72%|███████▏  | 498/690 [16:03<06:14,  1.95s/it] 72%|███████▏  | 499/690 [16:05<06:11,  1.95s/it] 72%|███████▏  | 500/690 [16:07<06:09,  1.94s/it]                                                  72%|███████▏  | 500/690 [16:07<06:09,  1.94s/it][INFO|trainer.py:2985] 2024-02-16 14:58:55,374 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-16 14:58:55,378 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-16 14:59:23,581 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-16 14:59:23,588 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-16 14:59:23,590 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tmp-checkpoint-500/special_tokens_map.json
 73%|███████▎  | 501/690 [17:22<1:15:55, 24.10s/it] 73%|███████▎  | 502/690 [17:24<54:43, 17.47s/it]   73%|███████▎  | 503/690 [17:26<39:57, 12.82s/it] 73%|███████▎  | 504/690 [17:28<29:36,  9.55s/it] 73%|███████▎  | 505/690 [17:30<22:23,  7.26s/it] 73%|███████▎  | 506/690 [17:32<17:24,  5.68s/it] 73%|███████▎  | 507/690 [17:34<13:52,  4.55s/it] 74%|███████▎  | 508/690 [17:36<11:25,  3.77s/it] 74%|███████▍  | 509/690 [17:38<09:42,  3.22s/it] 74%|███████▍  | 510/690 [17:40<08:30,  2.84s/it] 74%|███████▍  | 511/690 [17:42<07:39,  2.57s/it] 74%|███████▍  | 512/690 [17:44<07:04,  2.39s/it] 74%|███████▍  | 513/690 [17:46<06:38,  2.25s/it] 74%|███████▍  | 514/690 [17:48<06:20,  2.16s/it] 75%|███████▍  | 515/690 [17:50<06:05,  2.09s/it] 75%|███████▍  | 516/690 [17:52<05:55,  2.05s/it] 75%|███████▍  | 517/690 [17:54<05:48,  2.02s/it] 75%|███████▌  | 518/690 [17:56<05:42,  1.99s/it] 75%|███████▌  | 519/690 [17:57<05:38,  1.98s/it] 75%|███████▌  | 520/690 [17:59<05:34,  1.97s/it] 76%|███████▌  | 521/690 [18:01<05:32,  1.97s/it] 76%|███████▌  | 522/690 [18:03<05:28,  1.95s/it] 76%|███████▌  | 523/690 [18:05<05:27,  1.96s/it] 76%|███████▌  | 524/690 [18:07<05:24,  1.96s/it] 76%|███████▌  | 525/690 [18:09<05:22,  1.95s/it] 76%|███████▌  | 526/690 [18:11<05:21,  1.96s/it] 76%|███████▋  | 527/690 [18:13<05:18,  1.95s/it] 77%|███████▋  | 528/690 [18:15<05:15,  1.94s/it] 77%|███████▋  | 529/690 [18:17<05:15,  1.96s/it] 77%|███████▋  | 530/690 [18:19<05:13,  1.96s/it] 77%|███████▋  | 531/690 [18:21<05:10,  1.95s/it] 77%|███████▋  | 532/690 [18:23<05:07,  1.95s/it] 77%|███████▋  | 533/690 [18:25<05:06,  1.95s/it] 77%|███████▋  | 534/690 [18:27<05:03,  1.94s/it] 78%|███████▊  | 535/690 [18:29<05:03,  1.96s/it] 78%|███████▊  | 536/690 [18:31<05:01,  1.96s/it] 78%|███████▊  | 537/690 [18:33<04:58,  1.95s/it] 78%|███████▊  | 538/690 [18:35<04:56,  1.95s/it] 78%|███████▊  | 539/690 [18:36<04:54,  1.95s/it] 78%|███████▊  | 540/690 [18:38<04:53,  1.95s/it] 78%|███████▊  | 541/690 [18:40<04:49,  1.95s/it] 79%|███████▊  | 542/690 [18:42<04:47,  1.94s/it] 79%|███████▊  | 543/690 [18:44<04:45,  1.94s/it] 79%|███████▉  | 544/690 [18:46<04:43,  1.94s/it] 79%|███████▉  | 545/690 [18:48<04:40,  1.94s/it] 79%|███████▉  | 546/690 [18:50<04:39,  1.94s/it] 79%|███████▉  | 547/690 [18:52<04:37,  1.94s/it] 79%|███████▉  | 548/690 [18:54<04:33,  1.93s/it] 80%|███████▉  | 549/690 [18:56<04:31,  1.93s/it] 80%|███████▉  | 550/690 [18:58<04:29,  1.93s/it] 80%|███████▉  | 551/690 [19:00<04:28,  1.93s/it] 80%|████████  | 552/690 [19:02<04:26,  1.93s/it] 80%|████████  | 553/690 [19:04<04:23,  1.93s/it] 80%|████████  | 554/690 [19:06<04:23,  1.94s/it] 80%|████████  | 555/690 [19:07<04:22,  1.95s/it] 81%|████████  | 556/690 [19:09<04:21,  1.95s/it] 81%|████████  | 557/690 [19:11<04:18,  1.95s/it] 81%|████████  | 558/690 [19:13<04:16,  1.94s/it] 81%|████████  | 559/690 [19:15<04:12,  1.93s/it] 81%|████████  | 560/690 [19:17<04:12,  1.94s/it] 81%|████████▏ | 561/690 [19:19<04:09,  1.93s/it] 81%|████████▏ | 562/690 [19:21<04:07,  1.93s/it] 82%|████████▏ | 563/690 [19:23<04:06,  1.94s/it] 82%|████████▏ | 564/690 [19:25<04:04,  1.94s/it] 82%|████████▏ | 565/690 [19:27<04:03,  1.95s/it] 82%|████████▏ | 566/690 [19:29<04:01,  1.95s/it] 82%|████████▏ | 567/690 [19:31<03:59,  1.95s/it] 82%|████████▏ | 568/690 [19:33<03:57,  1.95s/it] 82%|████████▏ | 569/690 [19:35<03:55,  1.95s/it] 83%|████████▎ | 570/690 [19:37<03:53,  1.94s/it] 83%|████████▎ | 571/690 [19:39<03:51,  1.95s/it] 83%|████████▎ | 572/690 [19:41<03:50,  1.95s/it] 83%|████████▎ | 573/690 [19:42<03:47,  1.94s/it] 83%|████████▎ | 574/690 [19:44<03:44,  1.93s/it] 83%|████████▎ | 575/690 [19:46<03:43,  1.94s/it] 83%|████████▎ | 576/690 [19:48<03:40,  1.94s/it] 84%|████████▎ | 577/690 [19:50<03:37,  1.92s/it] 84%|████████▍ | 578/690 [19:52<03:35,  1.93s/it] 84%|████████▍ | 579/690 [19:54<03:33,  1.93s/it] 84%|████████▍ | 580/690 [19:56<03:32,  1.93s/it] 84%|████████▍ | 581/690 [19:58<03:29,  1.93s/it] 84%|████████▍ | 582/690 [20:00<03:27,  1.92s/it] 84%|████████▍ | 583/690 [20:02<03:27,  1.94s/it] 85%|████████▍ | 584/690 [20:04<03:24,  1.93s/it] 85%|████████▍ | 585/690 [20:06<03:22,  1.93s/it] 85%|████████▍ | 586/690 [20:08<03:20,  1.93s/it] 85%|████████▌ | 587/690 [20:09<03:18,  1.93s/it] 85%|████████▌ | 588/690 [20:11<03:15,  1.92s/it] 85%|████████▌ | 589/690 [20:13<03:13,  1.91s/it] 86%|████████▌ | 590/690 [20:15<03:11,  1.91s/it] 86%|████████▌ | 591/690 [20:17<03:11,  1.93s/it] 86%|████████▌ | 592/690 [20:19<03:08,  1.93s/it] 86%|████████▌ | 593/690 [20:21<03:07,  1.93s/it] 86%|████████▌ | 594/690 [20:23<03:04,  1.92s/it] 86%|████████▌ | 595/690 [20:25<03:02,  1.92s/it] 86%|████████▋ | 596/690 [20:27<03:00,  1.93s/it] 87%|████████▋ | 597/690 [20:29<02:59,  1.93s/it] 87%|████████▋ | 598/690 [20:31<02:56,  1.92s/it] 87%|████████▋ | 599/690 [20:32<02:54,  1.91s/it] 87%|████████▋ | 600/690 [20:34<02:52,  1.92s/it] 87%|████████▋ | 601/690 [20:36<02:51,  1.93s/it] 87%|████████▋ | 602/690 [20:38<02:49,  1.92s/it] 87%|████████▋ | 603/690 [20:40<02:47,  1.92s/it] 88%|████████▊ | 604/690 [20:42<02:46,  1.94s/it] 88%|████████▊ | 605/690 [20:44<02:44,  1.94s/it] 88%|████████▊ | 606/690 [20:46<02:42,  1.94s/it] 88%|████████▊ | 607/690 [20:48<02:40,  1.93s/it] 88%|████████▊ | 608/690 [20:50<02:37,  1.92s/it] 88%|████████▊ | 609/690 [20:52<02:35,  1.92s/it] 88%|████████▊ | 610/690 [20:54<02:33,  1.92s/it] 89%|████████▊ | 611/690 [20:56<02:31,  1.92s/it] 89%|████████▊ | 612/690 [20:58<02:29,  1.92s/it] 89%|████████▉ | 613/690 [20:59<02:27,  1.92s/it] 89%|████████▉ | 614/690 [21:01<02:26,  1.93s/it] 89%|████████▉ | 615/690 [21:03<02:23,  1.92s/it] 89%|████████▉ | 616/690 [21:05<02:21,  1.92s/it] 89%|████████▉ | 617/690 [21:07<02:20,  1.92s/it] 90%|████████▉ | 618/690 [21:09<02:17,  1.92s/it] 90%|████████▉ | 619/690 [21:11<02:16,  1.92s/it] 90%|████████▉ | 620/690 [21:13<02:14,  1.92s/it] 90%|█████████ | 621/690 [21:15<02:12,  1.92s/it] 90%|█████████ | 622/690 [21:17<02:11,  1.94s/it] 90%|█████████ | 623/690 [21:19<02:09,  1.93s/it] 90%|█████████ | 624/690 [21:21<02:06,  1.92s/it] 91%|█████████ | 625/690 [21:22<02:04,  1.92s/it] 91%|█████████ | 626/690 [21:24<02:02,  1.92s/it] 91%|█████████ | 627/690 [21:26<02:01,  1.93s/it] 91%|█████████ | 628/690 [21:28<01:59,  1.92s/it] 91%|█████████ | 629/690 [21:30<01:56,  1.91s/it] 91%|█████████▏| 630/690 [21:32<01:55,  1.92s/it] 91%|█████████▏| 631/690 [21:34<01:53,  1.93s/it] 92%|█████████▏| 632/690 [21:36<01:51,  1.93s/it] 92%|█████████▏| 633/690 [21:38<01:49,  1.93s/it] 92%|█████████▏| 634/690 [21:40<01:47,  1.92s/it] 92%|█████████▏| 635/690 [21:42<01:45,  1.92s/it] 92%|█████████▏| 636/690 [21:44<01:43,  1.92s/it] 92%|█████████▏| 637/690 [21:46<01:42,  1.93s/it] 92%|█████████▏| 638/690 [21:48<01:40,  1.93s/it] 93%|█████████▎| 639/690 [21:49<01:38,  1.92s/it] 93%|█████████▎| 640/690 [21:51<01:36,  1.92s/it] 93%|█████████▎| 641/690 [21:53<01:33,  1.91s/it] 93%|█████████▎| 642/690 [21:55<01:32,  1.93s/it] 93%|█████████▎| 643/690 [21:57<01:30,  1.93s/it] 93%|█████████▎| 644/690 [21:59<01:28,  1.92s/it] 93%|█████████▎| 645/690 [22:01<01:26,  1.92s/it] 94%|█████████▎| 646/690 [22:03<01:24,  1.91s/it] 94%|█████████▍| 647/690 [22:05<01:22,  1.91s/it] 94%|█████████▍| 648/690 [22:07<01:20,  1.93s/it] 94%|█████████▍| 649/690 [22:09<01:19,  1.93s/it] 94%|█████████▍| 650/690 [22:11<01:17,  1.93s/it] 94%|█████████▍| 651/690 [22:12<01:14,  1.92s/it] 94%|█████████▍| 652/690 [22:14<01:13,  1.92s/it] 95%|█████████▍| 653/690 [22:16<01:11,  1.92s/it] 95%|█████████▍| 654/690 [22:18<01:08,  1.92s/it] 95%|█████████▍| 655/690 [22:20<01:07,  1.92s/it] 95%|█████████▌| 656/690 [22:22<01:05,  1.92s/it] 95%|█████████▌| 657/690 [22:24<01:03,  1.92s/it] 95%|█████████▌| 658/690 [22:26<01:01,  1.93s/it] 96%|█████████▌| 659/690 [22:28<00:59,  1.92s/it] 96%|█████████▌| 660/690 [22:30<00:57,  1.93s/it] 96%|█████████▌| 661/690 [22:32<00:55,  1.92s/it] 96%|█████████▌| 662/690 [22:34<00:53,  1.93s/it] 96%|█████████▌| 663/690 [22:36<00:51,  1.92s/it] 96%|█████████▌| 664/690 [22:37<00:49,  1.92s/it] 96%|█████████▋| 665/690 [22:39<00:48,  1.93s/it] 97%|█████████▋| 666/690 [22:41<00:46,  1.93s/it] 97%|█████████▋| 667/690 [22:43<00:44,  1.93s/it] 97%|█████████▋| 668/690 [22:45<00:42,  1.93s/it] 97%|█████████▋| 669/690 [22:47<00:40,  1.93s/it] 97%|█████████▋| 670/690 [22:49<00:38,  1.94s/it] 97%|█████████▋| 671/690 [22:51<00:36,  1.94s/it] 97%|█████████▋| 672/690 [22:53<00:34,  1.93s/it] 98%|█████████▊| 673/690 [22:55<00:32,  1.91s/it] 98%|█████████▊| 674/690 [22:57<00:30,  1.92s/it] 98%|█████████▊| 675/690 [22:59<00:28,  1.91s/it] 98%|█████████▊| 676/690 [23:01<00:26,  1.92s/it] 98%|█████████▊| 677/690 [23:02<00:24,  1.92s/it] 98%|█████████▊| 678/690 [23:04<00:23,  1.92s/it] 98%|█████████▊| 679/690 [23:06<00:21,  1.93s/it] 99%|█████████▊| 680/690 [23:08<00:19,  1.92s/it] 99%|█████████▊| 681/690 [23:10<00:17,  1.92s/it] 99%|█████████▉| 682/690 [23:12<00:15,  1.92s/it] 99%|█████████▉| 683/690 [23:14<00:13,  1.92s/it] 99%|█████████▉| 684/690 [23:16<00:11,  1.93s/it] 99%|█████████▉| 685/690 [23:18<00:09,  1.92s/it] 99%|█████████▉| 686/690 [23:20<00:07,  1.92s/it]100%|█████████▉| 687/690 [23:22<00:05,  1.92s/it]100%|█████████▉| 688/690 [23:24<00:03,  1.91s/it]100%|█████████▉| 689/690 [23:25<00:01,  1.90s/it]100%|██████████| 690/690 [23:27<00:00,  1.90s/it][INFO|trainer.py:1988] 2024-02-16 15:06:15,949 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1408.4852, 'train_samples_per_second': 7.813, 'train_steps_per_second': 0.49, 'train_loss': 0.5354485995527627, 'epoch': 3.0}
                                                 100%|██████████| 690/690 [23:28<00:00,  1.90s/it]100%|██████████| 690/690 [23:28<00:00,  2.04s/it]
[INFO|trainer.py:2985] 2024-02-16 15:06:16,112 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256
[INFO|configuration_utils.py:473] 2024-02-16 15:06:16,114 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/config.json
[INFO|modeling_utils.py:2462] 2024-02-16 15:06:36,744 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-16 15:06:36,751 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-16 15:06:36,752 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.5354
  train_runtime            = 0:23:28.48
  train_samples            =       3668
  train_samples_per_second =      7.813
  train_steps_per_second   =       0.49
02/16/2024 15:06:36 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-16 15:06:36,840 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-16 15:06:36,843 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-16 15:06:36,849 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-16 15:06:36,849 >>   Batch size = 8
  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:05,  4.32it/s] 12%|█▏        | 3/26 [00:00<00:07,  3.04it/s] 15%|█▌        | 4/26 [00:01<00:08,  2.65it/s] 19%|█▉        | 5/26 [00:01<00:08,  2.47it/s] 23%|██▎       | 6/26 [00:02<00:08,  2.35it/s] 27%|██▋       | 7/26 [00:02<00:08,  2.28it/s] 31%|███       | 8/26 [00:03<00:08,  2.24it/s] 35%|███▍      | 9/26 [00:03<00:07,  2.22it/s] 38%|███▊      | 10/26 [00:04<00:07,  2.20it/s] 42%|████▏     | 11/26 [00:04<00:06,  2.21it/s] 46%|████▌     | 12/26 [00:05<00:06,  2.21it/s] 50%|█████     | 13/26 [00:05<00:05,  2.23it/s] 54%|█████▍    | 14/26 [00:05<00:05,  2.20it/s] 58%|█████▊    | 15/26 [00:06<00:05,  2.19it/s] 62%|██████▏   | 16/26 [00:06<00:04,  2.17it/s] 65%|██████▌   | 17/26 [00:07<00:04,  2.16it/s] 69%|██████▉   | 18/26 [00:07<00:03,  2.19it/s] 73%|███████▎  | 19/26 [00:08<00:03,  2.18it/s] 77%|███████▋  | 20/26 [00:08<00:02,  2.18it/s] 81%|████████  | 21/26 [00:09<00:02,  2.19it/s] 85%|████████▍ | 22/26 [00:09<00:01,  2.18it/s] 88%|████████▊ | 23/26 [00:10<00:01,  2.18it/s] 92%|█████████▏| 24/26 [00:10<00:00,  2.18it/s] 96%|█████████▌| 25/26 [00:11<00:00,  2.19it/s]100%|██████████| 26/26 [00:11<00:00,  2.18it/s]100%|██████████| 26/26 [00:11<00:00,  2.26it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.6838
  eval_combined_score     =     0.7309
  eval_f1                 =      0.778
  eval_loss               =     0.8976
  eval_runtime            = 0:00:11.98
  eval_samples            =        408
  eval_samples_per_second =     34.032
  eval_steps_per_second   =      2.169
