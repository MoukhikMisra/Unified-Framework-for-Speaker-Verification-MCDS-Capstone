WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 11:54:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 11:54:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/runs/Feb13_11-54-08_v007.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 11:54:08 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 11:54:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:54:10 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 11:54:10 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:54:10 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 11:54:11,474 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 11:54:11,638 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:54:11,699 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:54:11,699 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:54:11,699 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:54:11,699 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:54:11,699 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 11:54:11,850 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-13 11:54:11,851 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 11:54:11,998 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-13 11:54:35,454 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 11:54:35,456 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-13 11:54:35,458 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 657, in <module>
    main()
  File "run_glue.py", line 485, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 481, in preprocess_function
    print(results.keys())
NameError: name 'results' is not defined
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]
Traceback (most recent call last):
  File "run_glue.py", line 657, in <module>
    main()
  File "run_glue.py", line 485, in main
    raw_datasets = raw_datasets.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 868, in map
    {
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/dataset_dict.py", line 869, in <dictcomp>
    k: dataset.map(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 593, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 558, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3105, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3482, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3361, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "run_glue.py", line 481, in preprocess_function
    print(results.keys())
NameError: name 'results' is not defined
[2024-02-13 11:54:40,131] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 12169) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-13_11:54:40
  host      : v007.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 12170)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-13_11:54:40
  host      : v007.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 12169)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v007: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 11:55:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 11:55:24 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/runs/Feb13_11-55-24_v007.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 11:55:24 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 11:55:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:55:25 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 11:55:25 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:55:25 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 11:55:25,495 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 11:55:25,496 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:55:25,527 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:55:25,527 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:55:25,527 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:55:25,527 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:55:25,527 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 11:55:25,580 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 11:55:25,639 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|logging.py:314] 2024-02-13 11:55:26,024 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3984] 2024-02-13 11:55:31,385 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 11:55:31,385 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b0c15164d9fee7a.arrow
02/13/2024 11:55:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b0c15164d9fee7a.arrow
Running tokenizer on dataset:   1%|â–         | 1000/67349 [00:00<00:12, 5499.48 examples/s][WARNING|modeling_utils.py:3996] 2024-02-13 11:55:32,003 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   3%|â–Ž         | 2000/67349 [00:00<00:20, 3264.47 examples/s]Running tokenizer on dataset:   6%|â–Œ         | 4000/67349 [00:00<00:10, 6066.15 examples/s]Running tokenizer on dataset:   9%|â–‰         | 6000/67349 [00:00<00:07, 7766.98 examples/s]Running tokenizer on dataset:  12%|â–ˆâ–        | 8000/67349 [00:01<00:06, 9385.10 examples/s]Running tokenizer on dataset:  15%|â–ˆâ–        | 10000/67349 [00:01<00:05, 9838.85 examples/s]Running tokenizer on dataset:  18%|â–ˆâ–Š        | 12000/67349 [00:01<00:05, 10829.38 examples/s]Running tokenizer on dataset:  21%|â–ˆâ–ˆ        | 14000/67349 [00:01<00:04, 11615.95 examples/s]Running tokenizer on dataset:  24%|â–ˆâ–ˆâ–       | 16000/67349 [00:01<00:04, 12208.81 examples/s]Running tokenizer on dataset:  27%|â–ˆâ–ˆâ–‹       | 18000/67349 [00:01<00:03, 12587.50 examples/s]Running tokenizer on dataset:  30%|â–ˆâ–ˆâ–‰       | 20000/67349 [00:01<00:03, 13015.05 examples/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž     dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
 | 22000/67349 [00:02<00:03, 13266.19 examples/s]Running tokenizer on dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24000/67349 [00:02<00:03, 13307.48 examples/s]Running tokenizer on dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 26000/67349 [00:02<00:03, 11175.87 examples/s]Running tokenizer on dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28000/67349 [00:02<00:03, 11716.62 examples/s]Running tokenizer on dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30000/67349 [00:02<00:03, 12280.88 examples/s]Running tokenizer on dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32000/67349 [00:02<00:02, 12691.41 examples/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34000/67349 [00:03<00:02, 12914.01 examples/s]Running tokenizer on dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36000/67349 [00:03<00:02, 13116.30 examples/s]Running tokenizer on dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38000/67349 [00:03<00:02, 13344.81 examples/s]Running tokenizer on dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40000/67349 [00:03<00:02, 13388.39 examples/s]Running tokenizer on dataset:  62%|âdict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42000/67349 [00:03<00:01, 13512.05 examples/s]Running tokenizer on dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44000/67349 [00:03<00:01, 13640.05 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46000/67349 [00:03<00:01, 13579.61 examples/s]Running tokenizer on dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48000/67349 [00:04<00:01, 13583.01 examples/s]Running tokenizer on dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50000/67349 [00:04<00:01, 11235.85 examples/s]Running tokenizer on dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52000/67349 [00:04<00:01, 11801.98 examples/s]Running tokenizer on dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54000/67349 [00:04<00:01, 12337.16 examples/s]Running tokenizer on dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56000/67349 [00:04<00:00, 12696.72 examples/s]Running tokenizer on dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58000/67349 [00:04<00:00, 12977.67 examples/s]Running tokenizer on dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60000/6dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
dict_keys(['input_ids', 'attention_mask'])
7349 [00:05<00:00, 13217.68 examples/s]Running tokenizer on dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62000/67349 [00:05<00:00, 13330.49 examples/s]Running tokenizer on dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64000/67349 [00:05<00:00, 13504.82 examples/s]Running tokenizer on dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66000/67349 [00:05<00:00, 13586.54 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67349/67349 [00:05<00:00, 11904.51 examples/s]
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d98f7ac1fa322e89.arrow
02/13/2024 11:55:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d98f7ac1fa322e89.arrow
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7669.36 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7360.00 examples/s]
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-eb34d572b93e98d6.arrow
02/13/2024 11:55:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-eb34d572b93e98d6.arrow
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1000/1821 [00:00<00:00, 7210.33 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1821/1821 [00:00<00:00, 8217.41 examples/s]
02/13/2024 11:55:38 - INFO - __main__ - Sample 81 of the training set: {'sentence': 'generates ', 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 16785, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]}.
02/13/2024 11:55:38 - INFO - __main__ - Sample 14 of the training set: {'sentence': 'lend some dignity to a dumb story ', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 301, 355, 777, 18085, 537, 304, 263, 270, 3774, 5828, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 11:55:38 - INFO - __main__ - Sample 3 of the training set: {'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 9242, 14401, 368, 15787, 304, 3933, 278, 1021, 10106, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
dict_keys(['input_ids', 'attention_mask'])
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 8624.56 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 8234.18 examples/s]
02/13/2024 11:55:38 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 11:55:41,607 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 11:55:41,779 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 11:55:41,779 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-13 11:55:41,779 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 11:55:41,779 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 11:55:41,779 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 11:55:41,780 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 11:55:41,780 >>   Total optimization steps = 21
[INFO|trainer.py:1756] 2024-02-13 11:55:41,781 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/21 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  5%|â–         | 1/21 [00:02<00:56,  2.80s/it] 10%|â–‰         | 2/21 [00:03<00:30,  1.60s/it] 14%|â–ˆâ–        | 3/21 [00:04<00:22,  1.27s/it] 19%|â–ˆâ–‰        | 4/21 [00:05<00:18,  1.10s/it] 24%|â–ˆâ–ˆâ–       | 5/21 [00:06<00:16,  1.01s/it] 29%|â–ˆâ–ˆâ–Š       | 6/21 [00:06<00:14,  1.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:07<00:12,  1.08it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:08<00:11,  1.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:09<00:10,  1.13it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:10<00:09,  1.14it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:11<00:08,  1.16it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:12<00:07,  1.16it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:12<00:06,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:13<00:05,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:14<00:05,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:15<00:04,  1.16it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:16<00:03,  1.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:17<00:02,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:18<00:01,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:18<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.17it/s][INFO|trainer.py:1988] 2024-02-13 11:56:01,589 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 19.9477, 'train_samples_per_second': 15.039, 'train_steps_per_second': 1.053, 'train_loss': 1.6787356422061013, 'epoch': 3.0}
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.05it/s]
[INFO|trainer.py:2985] 2024-02-13 11:56:01,758 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial
[INFO|configuration_utils.py:473] 2024-02-13 11:56:01,760 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 11:56:15,224 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 11:56:15,226 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 11:56:15,227 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.6787
  train_runtime            = 0:00:19.94
  train_samples            =        100
  train_samples_per_second =     15.039
  train_steps_per_second   =      1.053
02/13/2024 11:56:15 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 11:56:15,331 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 11:56:15,333 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 11:56:15,333 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-13 11:56:15,333 >>   Batch size = 8
  0%|          | 0/7 [00:00<?, ?it/s] 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00,  8.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00,  5.66it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:00<00:00,  5.00it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:01<00:00,  4.34it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:01<00:00,  4.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.57it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.48
  eval_loss               =     0.8417
  eval_runtime            = 0:00:01.78
  eval_samples            =        100
  eval_samples_per_second =      55.93
  eval_steps_per_second   =      3.915
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 11:58:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 11:58:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/runs/Feb13_11-58-23_v007.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 11:58:23 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 11:58:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:58:25 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[WARNING|logging.py:314] 2024-02-13 11:58:25,277 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 11:58:25 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 11:58:25 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 11:58:25,330 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 11:58:25,332 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:58:25,360 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:58:25,360 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:58:25,360 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:58:25,360 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 11:58:25,361 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 11:58:25,466 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 11:58:25,573 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|modeling_utils.py:3996] 2024-02-13 11:58:31,234 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3984] 2024-02-13 11:58:31,371 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 11:58:31,385 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
False
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4746344b18f34f9a.arrow
02/13/2024 11:58:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4746344b18f34f9a.arrow
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
Running tokenizer on dataset:   1%|â–         | 1000/67349 [00:00<00:29, 2226.05 examples/s]Running tokenizer on dataset:   3%|â–Ž         | 2000/67349 [00:00<00:19, 3294.53 examples/s]Running tokenizer on dataset:   6%|â–Œ         | 4000/67349 [00:00<00:10, 6250.56 examples/s]Running tokenizer on dataset:   9%|â–‰         | 6000/67349 [00:00<00:07, 8335.63 examples/s]Running tokenizer on dataset:  12%|â–ˆâ–        | 8000/67349 [00:01<00:06, 9885.92 examples/s]Running tokenizer on dataset:  15%|â–ˆâ–        | 10000/67349 [00:01<00:05, 10911.20 examples/s]Running tokenizer on dataset:  18%|â–ˆâ–Š        | 12000/67349 [00:01<00:04, 11680.54 examples/s]Running tokenizer on dataset:  21%|â–ˆâ–ˆ        | 14000/67349 [00:01<00:04, 12234.69 examples/s]Running tokenizer on dataset:  24%|â–ˆâ–ˆâ–       | 16000/67349 [00:01<00:04, 12664.00 examples/s]Running tokenizer on dataset:  27%|â–ˆâ–ˆâ–‹       | 18000/67349 [00:01<00:03, 12882.43 examples/s]Running tokenizer on dataset:  30%|â–ˆâ–ˆâ–‰       | 2000False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
0/67349 [00:01<00:03, 13192.20 examples/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22000/67349 [00:02<00:03, 13365.00 examples/s]Running tokenizer on dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24000/67349 [00:02<00:03, 13334.71 examples/s]Running tokenizer on dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 26000/67349 [00:02<00:03, 11152.38 examples/s]Running tokenizer on dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28000/67349 [00:02<00:03, 11684.60 examples/s]Running tokenizer on dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30000/67349 [00:02<00:03, 12220.05 examples/s]Running tokenizer on dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32000/67349 [00:02<00:02, 12660.97 examples/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34000/67349 [00:03<00:02, 12892.74 examples/s]Running tokenizer on dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36000/67349 [00:03<00:02, 13083.60 examples/s]Running tokenizer on dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38000/67349 [00:03<00:02, 13307.38 examples/s]Running tokenizer on dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆFalse
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
â–ˆâ–‰    | 40000/67349 [00:03<00:02, 13310.63 examples/s]Running tokenizer on dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42000/67349 [00:03<00:01, 13411.81 examples/s]Running tokenizer on dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44000/67349 [00:03<00:01, 13525.89 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46000/67349 [00:04<00:01, 13418.34 examples/s]Running tokenizer on dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48000/67349 [00:04<00:01, 13540.95 examples/s]Running tokenizer on dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50000/67349 [00:04<00:01, 11196.18 examples/s]Running tokenizer on dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52000/67349 [00:04<00:01, 11705.05 examples/s]Running tokenizer on dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54000/67349 [00:04<00:01, 12253.72 examples/s]Running tokenizer on dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56000/67349 [00:04<00:00, 12605.77 examples/s]Running tokenizer on dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58000/67349 [00:04<00:00False
False
False
False
False
False
False
False
False
False
, 12879.27 examples/s]Running tokenizer on dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60000/67349 [00:05<00:00, 13108.42 examples/s]Running tokenizer on dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62000/67349 [00:05<00:00, 13210.63 examples/s]Running tokenizer on dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64000/67349 [00:05<00:00, 13387.02 examples/s]Running tokenizer on dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66000/67349 [00:05<00:00, 13450.90 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67349/67349 [00:05<00:00, 11844.34 examples/s]
False
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f884188b497c11f3.arrow
02/13/2024 11:58:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f884188b497c11f3.arrow
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 9614.83 examples/s]
False
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2d51d3e6e87b7b14.arrow
02/13/2024 11:58:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2d51d3e6e87b7b14.arrow
False
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1821/1821 [00:00<00:00, 10494.79 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1821/1821 [00:00<00:00, 10166.00 examples/s]
02/13/2024 11:58:38 - INFO - __main__ - Sample 81 of the training set: {'sentence': 'generates ', 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 16785, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]}.
02/13/2024 11:58:38 - INFO - __main__ - Sample 14 of the training set: {'sentence': 'lend some dignity to a dumb story ', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 301, 355, 777, 18085, 537, 304, 263, 270, 3774, 5828, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 11:58:38 - INFO - __main__ - Sample 3 of the training set: {'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 9242, 14401, 368, 15787, 304, 3933, 278, 1021, 10106, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
False
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7935.57 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7605.50 examples/s]
02/13/2024 11:58:38 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 11:58:41,130 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 11:58:41,517 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 11:58:41,529 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-13 11:58:41,530 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 11:58:41,530 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 11:58:41,530 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 11:58:41,530 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 11:58:41,530 >>   Total optimization steps = 21
[INFO|trainer.py:1756] 2024-02-13 11:58:41,531 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/21 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  5%|â–         | 1/21 [00:02<00:49,  2.47s/it] 10%|â–‰         | 2/21 [00:03<00:28,  1.50s/it] 14%|â–ˆâ–        | 3/21 [00:04<00:21,  1.21s/it] 19%|â–ˆâ–‰        | 4/21 [00:04<00:18,  1.06s/it] 24%|â–ˆâ–ˆâ–       | 5/21 [00:05<00:15,  1.01it/s] 29%|â–ˆâ–ˆâ–Š       | 6/21 [00:06<00:14,  1.07it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 7/21 [00:07<00:12,  1.09it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 8/21 [00:08<00:11,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 9/21 [00:09<00:10,  1.14it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 10/21 [00:10<00:09,  1.15it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 11/21 [00:10<00:08,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 12/21 [00:11<00:07,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 13/21 [00:12<00:06,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 14/21 [00:13<00:05,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 15/21 [00:14<00:05,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 16/21 [00:15<00:04,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 17/21 [00:16<00:03,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 18/21 [00:16<00:02,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 19/21 [00:17<00:01,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 20/21 [00:18<00:00,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.18it/s][INFO|trainer.py:1988] 2024-02-13 11:59:01,002 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 19.5813, 'train_samples_per_second': 15.321, 'train_steps_per_second': 1.072, 'train_loss': 1.6787356422061013, 'epoch': 3.0}
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:19<00:00,  1.07it/s]
[INFO|trainer.py:2985] 2024-02-13 11:59:01,148 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial
[INFO|configuration_utils.py:473] 2024-02-13 11:59:01,150 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 11:59:14,465 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 11:59:14,467 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 11:59:14,469 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/trial/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.6787
  train_runtime            = 0:00:19.58
  train_samples            =        100
  train_samples_per_second =     15.321
  train_steps_per_second   =      1.072
02/13/2024 11:59:14 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 11:59:14,528 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 11:59:14,531 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 11:59:14,531 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-13 11:59:14,531 >>   Batch size = 8
  0%|          | 0/7 [00:00<?, ?it/s] 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:00,  7.58it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00,  5.72it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:00<00:00,  4.80it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:00<00:00,  4.58it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:01<00:00,  4.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.26it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.48
  eval_loss               =     0.8417
  eval_runtime            = 0:00:02.39
  eval_samples            =        100
  eval_samples_per_second =     41.681
  eval_steps_per_second   =      2.918
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 12:27:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 12:27:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/runs/Feb13_12-27-46_v007.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 12:27:47 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 12:27:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 12:27:48 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 12:27:48 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 12:27:48 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 12:27:49,273 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 12:27:49,547 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-13 12:27:49,599 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 12:27:49,599 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 12:27:49,599 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 12:27:49,599 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 12:27:49,599 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 12:27:49,722 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-02-13 12:27:49,729 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 12:27:49,840 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[WARNING|modeling_utils.py:3996] 2024-02-13 12:27:55,773 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:3984] 2024-02-13 12:27:55,777 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 12:27:55,791 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-efc6bd6f05d6bd68.arrow
02/13/2024 12:27:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-efc6bd6f05d6bd68.arrow
Running tokenizer on dataset:  23%|â–ˆâ–ˆâ–Ž       | 2000/8551 [00:00<00:01, 6426.13 examples/s]Running tokenizer on dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4000/8551 [00:00<00:00, 9411.10 examples/s]Running tokenizer on dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6000/8551 [00:00<00:00, 10793.88 examples/s]Running tokenizer on dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 8000/8551 [00:00<00:00, 11646.96 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8551/8551 [00:00<00:00, 10588.66 examples/s]
Running tokenizer on dataset:   0%|          | 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0ca378cbade7b415.arrow
02/13/2024 12:27:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0ca378cbade7b415.arrow
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 11232.22 examples/s]
Running tokenizer on dataset:   0%|          | 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1f7137d1dd09dcc6.arrow
02/13/2024 12:27:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1f7137d1dd09dcc6.arrow
Running tokenizer on dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1000/1063 [00:00<00:00, 3394.09 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1063/1063 [00:00<00:00, 3418.45 examples/s]
02/13/2024 12:27:57 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 24084, 3192, 393, 590, 4783, 29892, 540, 471, 19932, 408, 385, 8152, 29880, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 12:27:57 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1152, 1075, 304, 437, 393, 723, 367, 263, 10171, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 12:27:57 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6182, 13625, 263, 4823, 29892, 541, 9371, 2360, 1258, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/1043 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 9532.51 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 9256.91 examples/s]
02/13/2024 12:27:58 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 12:28:00,970 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 12:28:01,369 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 12:28:01,379 >>   Num examples = 8,551
[INFO|trainer.py:1749] 2024-02-13 12:28:01,380 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 12:28:01,380 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 12:28:01,380 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 12:28:01,380 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 12:28:01,380 >>   Total optimization steps = 1,605
[INFO|trainer.py:1756] 2024-02-13 12:28:01,381 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/1605 [00:00<?, ?it/s][rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1605 [00:02<1:13:33,  2.75s/it]  0%|          | 2/1605 [00:03<42:40,  1.60s/it]    0%|          | 3/1605 [00:04<33:34,  1.26s/it]  0%|          | 4/1605 [00:05<29:17,  1.10s/it]  0%|          | 5/1605 [00:06<26:51,  1.01s/it]  0%|          | 6/1605 [00:06<25:34,  1.04it/s]  0%|          | 7/1605 [00:07<24:28,  1.09it/s]  0%|          | 8/1605 [00:08<23:54,  1.11it/s]  1%|          | 9/1605 [00:09<23:28,  1.13it/s]  1%|          | 10/1605 [00:10<23:16,  1.14it/s]  1%|          | 11/1605 [00:11<23:04,  1.15it/s]  1%|          | 12/1605 [00:12<22:56,  1.16it/s]  1%|          | 13/1605 [00:12<22:51,  1.16it/s]  1%|          | 14/1605 [00:13<22:36,  1.17it/s]  1%|          | 15/1605 [00:14<22:41,  1.17it/s]  1%|          | 16/1605 [00:15<22:30,  1.18it/s]  1%|          | 17/1605 [00:16<22:44,  1.16it/s]  1%|          | 18/1605 [00:17<22:39,  1.17it/s]  1%|          | 19/1605 [00:18<22:41,  1.17it/s]  1%|          | 20/1605 [00:18<22:39,  1.17it/s]  1%|â–         | 21/1605 [00:19<22:31,  1.17it/s]  1%|â–         | 22/1605 [00:20<22:32,  1.17it/s]  1%|â–         | 23/1605 [00:21<22:27,  1.17it/s]  1%|â–         | 24/1605 [00:22<22:30,  1.17it/s]  2%|â–         | 25/1605 [00:23<22:26,  1.17it/s]  2%|â–         | 26/1605 [00:24<22:23,  1.18it/s]  2%|â–         | 27/1605 [00:24<22:22,  1.18it/s]  2%|â–         | 28/1605 [00:25<22:27,  1.17it/s]  2%|â–         | 29/1605 [00:26<22:24,  1.17it/s]  2%|â–         | 30/1605 [00:27<22:29,  1.17it/s]  2%|â–         | 31/1605 [00:28<22:13,  1.18it/s]  2%|â–         | 32/1605 [00:29<22:19,  1.17it/s]  2%|â–         | 33/1605 [00:29<22:12,  1.18it/s]  2%|â–         | 34/1605 [00:30<22:09,  1.18it/s]  2%|â–         | 35/1605 [00:31<22:13,  1.18it/s]  2%|â–         | 36/1605 [00:32<22:15,  1.17it/s]  2%|â–         | 37/1605 [00:33<22:07,  1.18it/s]  2%|â–         | 38/1605 [00:34<22:21,  1.17it/s]  2%|â–         | 39/1605 [00:35<22:22,  1.17it/s]  2%|â–         | 40/1605 [00:35<22:20,  1.17it/s]  3%|â–Ž         | 41/1605 [00:36<22:22,  1.17it/s]  3%|â–Ž         | 42/1605 [00:37<22:15,  1.17it/s]  3%|â–Ž         | 43/1605 [00:38<22:07,  1.18it/s]  3%|â–Ž         | 44/1605 [00:39<22:17,  1.17it/s]  3%|â–Ž         | 45/1605 [00:40<22:09,  1.17it/s]  3%|â–Ž         | 46/1605 [00:41<22:05,  1.18it/s]  3%|â–Ž         | 47/1605 [00:41<22:11,  1.17it/s]  3%|â–Ž         | 48/1605 [00:42<22:15,  1.17it/s]  3%|â–Ž         | 49/1605 [00:43<22:21,  1.16it/s]  3%|â–Ž         | 50/1605 [00:44<22:12,  1.17it/s]  3%|â–Ž         | 51/1605 [00:45<22:22,  1.16it/s]  3%|â–Ž         | 52/1605 [00:46<22:06,  1.17it/s]  3%|â–Ž         | 53/1605 [00:47<22:03,  1.17it/s]  3%|â–Ž         | 54/1605 [00:47<22:11,  1.17it/s]  3%|â–Ž         | 55/1605 [00:48<21:58,  1.18it/s]  3%|â–Ž         | 56/1605 [00:49<22:02,  1.17it/s]  4%|â–Ž         | 57/1605 [00:50<22:02,  1.17it/s]  4%|â–Ž         | 58/1605 [00:51<21:56,  1.18it/s]  4%|â–Ž         | 59/1605 [00:52<21:51,  1.18it/s]  4%|â–Ž         | 60/1605 [00:53<21:47,  1.18it/s]  4%|â–         | 61/1605 [00:53<21:58,  1.17it/s]  4%|â–         | 62/1605 [00:54<21:56,  1.17it/s]  4%|â–         | 63/1605 [00:55<21:57,  1.17it/s]  4%|â–         | 64/1605 [00:56<21:51,  1.17it/s]  4%|â–         | 65/1605 [00:57<21:56,  1.17it/s]  4%|â–         | 66/1605 [00:58<21:48,  1.18it/s]  4%|â–         | 67/1605 [00:58<21:42,  1.18it/s]  4%|â–         | 68/1605 [00:59<21:53,  1.17it/s]  4%|â–         | 69/1605 [01:00<21:52,  1.17it/s]  4%|â–         | 70/1605 [01:01<21:52,  1.17it/s]  4%|â–         | 71/1605 [01:02<21:53,  1.17it/s]  4%|â–         | 72/1605 [01:03<21:50,  1.17it/s]  5%|â–         | 73/1605 [01:04<21:49,  1.17it/s]  5%|â–         | 74/1605 [01:04<21:45,  1.17it/s]  5%|â–         | 75/1605 [01:05<21:53,  1.16it/s]  5%|â–         | 76/1605 [01:06<21:44,  1.17it/s]  5%|â–         | 77/1605 [01:07<21:48,  1.17it/s]  5%|â–         | 78/1605 [01:08<21:53,  1.16it/s]  5%|â–         | 79/1605 [01:09<21:59,  1.16it/s]  5%|â–         | 80/1605 [01:10<21:45,  1.17it/s]  5%|â–Œ         | 81/1605 [01:10<21:45,  1.17it/s]  5%|â–Œ         | 82/1605 [01:11<21:38,  1.17it/s]  5%|â–Œ         | 83/1605 [01:12<21:37,  1.17it/s]  5%|â–Œ         | 84/1605 [01:13<21:33,  1.18it/s]  5%|â–Œ         | 85/1605 [01:14<21:45,  1.16it/s]  5%|â–Œ         | 86/1605 [01:15<21:31,  1.18it/s]  5%|â–Œ         | 87/1605 [01:16<21:35,  1.17it/s]  5%|â–Œ         | 88/1605 [01:16<21:28,  1.18it/s]  6%|â–Œ         | 89/1605 [01:17<21:28,  1.18it/s]  6%|â–Œ         | 90/1605 [01:18<21:23,  1.18it/s]  6%|â–Œ         | 91/1605 [01:19<21:32,  1.17it/s]  6%|â–Œ         | 92/1605 [01:20<21:32,  1.17it/s]  6%|â–Œ         | 93/1605 [01:21<21:24,  1.18it/s]  6%|â–Œ         | 94/1605 [01:22<21:38,  1.16it/s]  6%|â–Œ         | 95/1605 [01:22<21:28,  1.17it/s]  6%|â–Œ         | 96/1605 [01:23<21:28,  1.17it/s]  6%|â–Œ         | 97/1605 [01:24<21:35,  1.16it/s]  6%|â–Œ         | 98/1605 [01:25<21:27,  1.17it/s]  6%|â–Œ         | 99/1605 [01:26<21:24,  1.17it/s]  6%|â–Œ         | 100/1605 [01:27<21:21,  1.17it/s]  6%|â–‹         | 101/1605 [01:28<21:17,  1.18it/s]  6%|â–‹         | 102/1605 [01:28<21:18,  1.18it/s]  6%|â–‹         | 103/1605 [01:29<21:21,  1.17it/s]  6%|â–‹         | 104/1605 [01:30<21:15,  1.18it/s]  7%|â–‹         | 105/1605 [01:31<21:19,  1.17it/s]  7%|â–‹         | 106/1605 [01:32<21:21,  1.17it/s]  7%|â–‹         | 107/1605 [01:33<21:16,  1.17it/s]  7%|â–‹         | 108/1605 [01:33<21:18,  1.17it/s]  7%|â–‹         | 109/1605 [01:34<21:12,  1.18it/s]  7%|â–‹         | 110/1605 [01:35<21:18,  1.17it/s]  7%|â–‹         | 111/1605 [01:36<21:22,  1.17it/s]  7%|â–‹         | 112/1605 [01:37<21:21,  1.16it/s]  7%|â–‹         | 113/1605 [01:38<21:21,  1.16it/s]  7%|â–‹         | 114/1605 [01:39<21:19,  1.16it/s]  7%|â–‹         | 115/1605 [01:39<21:08,  1.17it/s]  7%|â–‹         | 116/1605 [01:40<21:09,  1.17it/s]  7%|â–‹         | 117/1605 [01:41<21:18,  1.16it/s]  7%|â–‹         | 118/1605 [01:42<21:08,  1.17it/s]  7%|â–‹         | 119/1605 [01:43<21:12,  1.17it/s]  7%|â–‹         | 120/1605 [01:44<21:19,  1.16it/s]  8%|â–Š         | 121/1605 [01:45<21:16,  1.16it/s]  8%|â–Š         | 122/1605 [01:46<21:21,  1.16it/s]  8%|â–Š         | 123/1605 [01:46<21:20,  1.16it/s]  8%|â–Š         | 124/1605 [01:47<21:17,  1.16it/s]  8%|â–Š         | 125/1605 [01:48<21:07,  1.17it/s]  8%|â–Š         | 126/1605 [01:49<21:15,  1.16it/s]  8%|â–Š         | 127/1605 [01:50<21:07,  1.17it/s]  8%|â–Š         | 128/1605 [01:51<21:16,  1.16it/s]  8%|â–Š         | 129/1605 [01:52<21:10,  1.16it/s]  8%|â–Š         | 130/1605 [01:52<21:01,  1.17it/s]  8%|â–Š         | 131/1605 [01:53<20:59,  1.17it/s]  8%|â–Š         | 132/1605 [01:54<20:55,  1.17it/s]  8%|â–Š         | 133/1605 [01:55<20:56,  1.17it/s]  8%|â–Š         | 134/1605 [01:56<20:54,  1.17it/s]  8%|â–Š         | 135/1605 [01:57<20:55,  1.17it/s]  8%|â–Š         | 136/1605 [01:58<20:55,  1.17it/s]  9%|â–Š         | 137/1605 [01:58<20:56,  1.17it/s]  9%|â–Š         | 138/1605 [01:59<20:53,  1.17it/s]  9%|â–Š         | 139/1605 [02:00<21:02,  1.16it/s]  9%|â–Š         | 140/1605 [02:01<21:01,  1.16it/s]  9%|â–‰         | 141/1605 [02:02<21:03,  1.16it/s]  9%|â–‰         | 142/1605 [02:03<20:50,  1.17it/s]  9%|â–‰         | 143/1605 [02:04<20:55,  1.16it/s]  9%|â–‰         | 144/1605 [02:04<20:44,  1.17it/s]  9%|â–‰         | 145/1605 [02:05<20:54,  1.16it/s]  9%|â–‰         | 146/1605 [02:06<20:49,  1.17it/s]  9%|â–‰         | 147/1605 [02:07<20:43,  1.17it/s]  9%|â–‰         | 148/1605 [02:08<20:47,  1.17it/s]  9%|â–‰         | 149/1605 [02:09<20:49,  1.17it/s]  9%|â–‰         | 150/1605 [02:10<20:56,  1.16it/s]  9%|â–‰         | 151/1605 [02:10<20:51,  1.16it/s]  9%|â–‰         | 152/1605 [02:11<20:44,  1.17it/s] 10%|â–‰         | 153/1605 [02:12<20:46,  1.16it/s] 10%|â–‰         | 154/1605 [02:13<20:48,  1.16it/s] 10%|â–‰         | 155/1605 [02:14<20:46,  1.16it/s] 10%|â–‰         | 156/1605 [02:15<20:35,  1.17it/s] 10%|â–‰         | 157/1605 [02:16<20:43,  1.16it/s] 10%|â–‰         | 158/1605 [02:16<20:38,  1.17it/s] 10%|â–‰         | 159/1605 [02:17<20:39,  1.17it/s] 10%|â–‰         | 160/1605 [02:18<20:42,  1.16it/s] 10%|â–ˆ         | 161/1605 [02:19<20:33,  1.17it/s] 10%|â–ˆ         | 162/1605 [02:20<20:30,  1.17it/s] 10%|â–ˆ         | 163/1605 [02:21<20:29,  1.17it/s] 10%|â–ˆ         | 164/1605 [02:22<20:29,  1.17it/s] 10%|â–ˆ         | 165/1605 [02:22<20:24,  1.18it/s] 10%|â–ˆ         | 166/1605 [02:23<20:30,  1.17it/s] 10%|â–ˆ         | 167/1605 [02:24<20:39,  1.16it/s] 10%|â–ˆ         | 168/1605 [02:25<20:34,  1.16it/s] 11%|â–ˆ         | 169/1605 [02:26<20:36,  1.16it/s] 11%|â–ˆ         | 170/1605 [02:27<20:24,  1.17it/s] 11%|â–ˆ         | 171/1605 [02:27<20:23,  1.17it/s] 11%|â–ˆ         | 172/1605 [02:28<20:23,  1.17it/s] 11%|â–ˆ         | 173/1605 [02:29<20:30,  1.16it/s] 11%|â–ˆ         | 174/1605 [02:30<20:27,  1.17it/s] 11%|â–ˆ         | 175/1605 [02:31<20:27,  1.16it/s] 11%|â–ˆ         | 176/1605 [02:32<20:24,  1.17it/s] 11%|â–ˆ         | 177/1605 [02:33<20:26,  1.16it/s] 11%|â–ˆ         | 178/1605 [02:34<20:27,  1.16it/s] 11%|â–ˆ         | 179/1605 [02:34<20:29,  1.16it/s] 11%|â–ˆ         | 180/1605 [02:35<20:25,  1.16it/s] 11%|â–ˆâ–        | 181/1605 [02:36<20:21,  1.17it/s] 11%|â–ˆâ–        | 182/1605 [02:37<20:21,  1.16it/s] 11%|â–ˆâ–        | 183/1605 [02:38<20:12,  1.17it/s] 11%|â–ˆâ–        | 184/1605 [02:39<20:17,  1.17it/s] 12%|â–ˆâ–        | 185/1605 [02:40<20:13,  1.17it/s] 12%|â–ˆâ–        | 186/1605 [02:40<20:08,  1.17it/s] 12%|â–ˆâ–        | 187/1605 [02:41<20:07,  1.17it/s] 12%|â–ˆâ–        | 188/1605 [02:42<20:09,  1.17it/s] 12%|â–ˆâ–        | 189/1605 [02:43<20:12,  1.17it/s] 12%|â–ˆâ–        | 190/1605 [02:44<20:16,  1.16it/s] 12%|â–ˆâ–        | 191/1605 [02:45<20:00,  1.18it/s] 12%|â–ˆâ–        | 192/1605 [02:45<20:01,  1.18it/s] 12%|â–ˆâ–        | 193/1605 [02:46<20:09,  1.17it/s] 12%|â–ˆâ–        | 194/1605 [02:47<20:03,  1.17it/s] 12%|â–ˆâ–        | 195/1605 [02:48<20:05,  1.17it/s] 12%|â–ˆâ–        | 196/1605 [02:49<19:56,  1.18it/s] 12%|â–ˆâ–        | 197/1605 [02:50<20:07,  1.17it/s] 12%|â–ˆâ–        | 198/1605 [02:51<19:59,  1.17it/s] 12%|â–ˆâ–        | 199/1605 [02:51<19:58,  1.17it/s] 12%|â–ˆâ–        | 200/1605 [02:52<20:02,  1.17it/s] 13%|â–ˆâ–Ž        | 201/1605 [02:53<19:53,  1.18it/s] 13%|â–ˆâ–Ž        | 202/1605 [02:54<19:59,  1.17it/s] 13%|â–ˆâ–Ž        | 203/1605 [02:55<19:56,  1.17it/s] 13%|â–ˆâ–Ž        | 204/1605 [02:56<19:55,  1.17it/s] 13%|â–ˆâ–Ž        | 205/1605 [02:57<20:09,  1.16it/s] 13%|â–ˆâ–Ž        | 206/1605 [02:57<20:01,  1.16it/s] 13%|â–ˆâ–Ž        | 207/1605 [02:58<20:07,  1.16it/s] 13%|â–ˆâ–Ž        | 208/1605 [02:59<20:02,  1.16it/s] 13%|â–ˆâ–Ž        | 209/1605 [03:00<20:02,  1.16it/s] 13%|â–ˆâ–Ž        | 210/1605 [03:01<19:52,  1.17it/s] 13%|â–ˆâ–Ž        | 211/1605 [03:02<19:56,  1.16it/s] 13%|â–ˆâ–Ž        | 212/1605 [03:03<19:52,  1.17it/s] 13%|â–ˆâ–Ž        | 213/1605 [03:03<19:57,  1.16it/s] 13%|â–ˆâ–Ž        | 214/1605 [03:04<19:51,  1.17it/s] 13%|â–ˆâ–Ž        | 215/1605 [03:05<19:45,  1.17it/s] 13%|â–ˆâ–Ž        | 216/1605 [03:06<19:40,  1.18it/s] 14%|â–ˆâ–Ž        | 217/1605 [03:07<19:48,  1.17it/s] 14%|â–ˆâ–Ž        | 218/1605 [03:08<19:47,  1.17it/s] 14%|â–ˆâ–Ž        | 219/1605 [03:09<19:55,  1.16it/s] 14%|â–ˆâ–Ž        | 220/1605 [03:09<19:51,  1.16it/s] 14%|â–ˆâ–        | 221/1605 [03:10<19:56,  1.16it/s] 14%|â–ˆâ–        | 222/1605 [03:11<19:47,  1.17it/s] 14%|â–ˆâ–        | 223/1605 [03:12<19:52,  1.16it/s] 14%|â–ˆâ–        | 224/1605 [03:13<19:47,  1.16it/s] 14%|â–ˆâ–        | 225/1605 [03:14<19:39,  1.17it/s] 14%|â–ˆâ–        | 226/1605 [03:15<19:36,  1.17it/s] 14%|â–ˆâ–        | 227/1605 [03:15<19:39,  1.17it/s] 14%|â–ˆâ–        | 228/1605 [03:16<19:43,  1.16it/s] 14%|â–ˆâ–        | 229/1605 [03:17<19:41,  1.16it/s] 14%|â–ˆâ–        | 230/1605 [03:18<19:44,  1.16it/s] 14%|â–ˆâ–        | 231/1605 [03:19<19:43,  1.16it/s] 14%|â–ˆâ–        | 232/1605 [03:20<19:40,  1.16it/s] 15%|â–ˆâ–        | 233/1605 [03:21<19:30,  1.17it/s] 15%|â–ˆâ–        | 234/1605 [03:21<19:31,  1.17it/s] 15%|â–ˆâ–        | 235/1605 [03:22<19:32,  1.17it/s] 15%|â–ˆâ–        | 236/1605 [03:23<19:44,  1.16it/s] 15%|â–ˆâ–        | 237/1605 [03:24<19:47,  1.15it/s] 15%|â–ˆâ–        | 238/1605 [03:25<19:35,  1.16it/s] 15%|â–ˆâ–        | 239/1605 [03:26<19:26,  1.17it/s] 15%|â–ˆâ–        | 240/1605 [03:27<19:28,  1.17it/s] 15%|â–ˆâ–Œ        | 241/1605 [03:28<19:36,  1.16it/s] 15%|â–ˆâ–Œ        | 242/1605 [03:28<19:29,  1.17it/s] 15%|â–ˆâ–Œ        | 243/1605 [03:29<19:23,  1.17it/s] 15%|â–ˆâ–Œ        | 244/1605 [03:30<19:25,  1.17it/s] 15%|â–ˆâ–Œ        | 245/1605 [03:31<19:18,  1.17it/s] 15%|â–ˆâ–Œ        | 246/1605 [03:32<19:21,  1.17it/s] 15%|â–ˆâ–Œ        | 247/1605 [03:33<19:18,  1.17it/s] 15%|â–ˆâ–Œ        | 248/1605 [03:33<19:19,  1.17it/s] 16%|â–ˆâ–Œ        | 249/1605 [03:34<19:11,  1.18it/s] 16%|â–ˆâ–Œ        | 250/1605 [03:35<19:24,  1.16it/s] 16%|â–ˆâ–Œ        | 251/1605 [03:36<19:19,  1.17it/s] 16%|â–ˆâ–Œ        | 252/1605 [03:37<19:08,  1.18it/s] 16%|â–ˆâ–Œ        | 253/1605 [03:38<19:16,  1.17it/s] 16%|â–ˆâ–Œ        | 254/1605 [03:39<19:15,  1.17it/s] 16%|â–ˆâ–Œ        | 255/1605 [03:39<19:17,  1.17it/s] 16%|â–ˆâ–Œ        | 256/1605 [03:40<19:08,  1.17it/s] 16%|â–ˆâ–Œ        | 257/1605 [03:41<19:20,  1.16it/s] 16%|â–ˆâ–Œ        | 258/1605 [03:42<19:14,  1.17it/s] 16%|â–ˆâ–Œ        | 259/1605 [03:43<19:17,  1.16it/s] 16%|â–ˆâ–Œ        | 260/1605 [03:44<19:10,  1.17it/s] 16%|â–ˆâ–‹        | 261/1605 [03:45<19:05,  1.17it/s] 16%|â–ˆâ–‹        | 262/1605 [03:45<19:02,  1.18it/s] 16%|â–ˆâ–‹        | 263/1605 [03:46<19:01,  1.18it/s] 16%|â–ˆâ–‹        | 264/1605 [03:47<19:07,  1.17it/s] 17%|â–ˆâ–‹        | 265/1605 [03:48<19:02,  1.17it/s] 17%|â–ˆâ–‹        | 266/1605 [03:49<19:10,  1.16it/s] 17%|â–ˆâ–‹        | 267/1605 [03:50<19:06,  1.17it/s] 17%|â–ˆâ–‹        | 268/1605 [03:51<19:05,  1.17it/s] 17%|â–ˆâ–‹        | 269/1605 [03:51<18:56,  1.18it/s] 17%|â–ˆâ–‹        | 270/1605 [03:52<18:59,  1.17it/s] 17%|â–ˆâ–‹        | 271/1605 [03:53<19:01,  1.17it/s] 17%|â–ˆâ–‹        | 272/1605 [03:54<18:57,  1.17it/s] 17%|â–ˆâ–‹        | 273/1605 [03:55<18:53,  1.17it/s] 17%|â–ˆâ–‹        | 274/1605 [03:56<18:54,  1.17it/s] 17%|â–ˆâ–‹        | 275/1605 [03:57<18:53,  1.17it/s] 17%|â–ˆâ–‹        | 276/1605 [03:57<18:49,  1.18it/s] 17%|â–ˆâ–‹        | 277/1605 [03:58<18:55,  1.17it/s] 17%|â–ˆâ–‹        | 278/1605 [03:59<18:48,  1.18it/s] 17%|â–ˆâ–‹        | 279/1605 [04:00<18:43,  1.18it/s] 17%|â–ˆâ–‹        | 280/1605 [04:01<18:44,  1.18it/s] 18%|â–ˆâ–Š        | 281/1605 [04:02<18:38,  1.18it/s] 18%|â–ˆâ–Š        | 282/1605 [04:02<18:39,  1.18it/s] 18%|â–ˆâ–Š        | 283/1605 [04:03<18:35,  1.19it/s] 18%|â–ˆâ–Š        | 284/1605 [04:04<18:35,  1.18it/s] 18%|â–ˆâ–Š        | 285/1605 [04:05<18:52,  1.17it/s] 18%|â–ˆâ–Š        | 286/1605 [04:06<18:51,  1.17it/s] 18%|â–ˆâ–Š        | 287/1605 [04:07<18:52,  1.16it/s] 18%|â–ˆâ–Š        | 288/1605 [04:08<18:45,  1.17it/s] 18%|â–ˆâ–Š        | 289/1605 [04:08<18:47,  1.17it/s] 18%|â–ˆâ–Š        | 290/1605 [04:09<18:47,  1.17it/s] 18%|â–ˆâ–Š        | 291/1605 [04:10<18:42,  1.17it/s] 18%|â–ˆâ–Š        | 292/1605 [04:11<18:54,  1.16it/s] 18%|â–ˆâ–Š        | 293/1605 [04:12<18:53,  1.16it/s] 18%|â–ˆâ–Š        | 294/1605 [04:13<18:44,  1.17it/s] 18%|â–ˆâ–Š        | 295/1605 [04:14<18:35,  1.17it/s] 18%|â–ˆâ–Š        | 296/1605 [04:14<18:43,  1.17it/s] 19%|â–ˆâ–Š        | 297/1605 [04:15<18:35,  1.17it/s] 19%|â–ˆâ–Š        | 298/1605 [04:16<18:29,  1.18it/s] 19%|â–ˆâ–Š        | 299/1605 [04:17<18:46,  1.16it/s] 19%|â–ˆâ–Š        | 300/1605 [04:18<18:41,  1.16it/s] 19%|â–ˆâ–‰        | 301/1605 [04:19<18:43,  1.16it/s] 19%|â–ˆâ–‰        | 302/1605 [04:20<18:37,  1.17it/s] 19%|â–ˆâ–‰        | 303/1605 [04:20<18:31,  1.17it/s] 19%|â–ˆâ–‰        | 304/1605 [04:21<18:32,  1.17it/s] 19%|â–ˆâ–‰        | 305/1605 [04:22<18:28,  1.17it/s] 19%|â–ˆâ–‰        | 306/1605 [04:23<18:39,  1.16it/s] 19%|â–ˆâ–‰        | 307/1605 [04:24<18:32,  1.17it/s] 19%|â–ˆâ–‰        | 308/1605 [04:25<18:40,  1.16it/s] 19%|â–ˆâ–‰        | 309/1605 [04:26<18:39,  1.16it/s] 19%|â–ˆâ–‰        | 310/1605 [04:26<18:30,  1.17it/s] 19%|â–ˆâ–‰        | 311/1605 [04:27<18:28,  1.17it/s] 19%|â–ˆâ–‰        | 312/1605 [04:28<18:29,  1.17it/s] 20%|â–ˆâ–‰        | 313/1605 [04:29<18:24,  1.17it/s] 20%|â–ˆâ–‰        | 314/1605 [04:30<18:22,  1.17it/s] 20%|â–ˆâ–‰        | 315/1605 [04:31<18:20,  1.17it/s] 20%|â–ˆâ–‰        | 316/1605 [04:32<18:18,  1.17it/s] 20%|â–ˆâ–‰        | 317/1605 [04:32<18:15,  1.18it/s] 20%|â–ˆâ–‰        | 318/1605 [04:33<18:18,  1.17it/s] 20%|â–ˆâ–‰        | 319/1605 [04:34<18:25,  1.16it/s] 20%|â–ˆâ–‰        | 320/1605 [04:35<18:18,  1.17it/s] 20%|â–ˆâ–ˆ        | 321/1605 [04:36<18:15,  1.17it/s] 20%|â–ˆâ–ˆ        | 322/1605 [04:37<18:14,  1.17it/s] 20%|â–ˆâ–ˆ        | 323/1605 [04:38<18:15,  1.17it/s] 20%|â–ˆâ–ˆ        | 324/1605 [04:38<18:10,  1.17it/s] 20%|â–ˆâ–ˆ        | 325/1605 [04:39<18:13,  1.17it/s] 20%|â–ˆâ–ˆ        | 326/1605 [04:40<18:04,  1.18it/s] 20%|â–ˆâ–ˆ        | 327/1605 [04:41<18:07,  1.17it/s] 20%|â–ˆâ–ˆ        | 328/1605 [04:42<18:17,  1.16it/s] 20%|â–ˆâ–ˆ        | 329/1605 [04:43<18:12,  1.17it/s] 21%|â–ˆâ–ˆ        | 330/1605 [04:44<18:08,  1.17it/s] 21%|â–ˆâ–ˆ        | 331/1605 [04:44<18:18,  1.16it/s] 21%|â–ˆâ–ˆ        | 332/1605 [04:45<18:18,  1.16it/s] 21%|â–ˆâ–ˆ        | 333/1605 [04:46<18:11,  1.17it/s] 21%|â–ˆâ–ˆ        | 334/1605 [04:47<18:03,  1.17it/s] 21%|â–ˆâ–ˆ        | 335/1605 [04:48<18:07,  1.17it/s] 21%|â–ˆâ–ˆ        | 336/1605 [04:49<18:08,  1.17it/s] 21%|â–ˆâ–ˆ        | 337/1605 [04:50<18:07,  1.17it/s] 21%|â–ˆâ–ˆ        | 338/1605 [04:50<17:58,  1.18it/s] 21%|â–ˆâ–ˆ        | 339/1605 [04:51<18:06,  1.17it/s] 21%|â–ˆâ–ˆ        | 340/1605 [04:52<17:55,  1.18it/s] 21%|â–ˆâ–ˆ        | 341/1605 [04:53<17:59,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 342/1605 [04:54<17:57,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 343/1605 [04:55<17:58,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 344/1605 [04:56<18:05,  1.16it/s] 21%|â–ˆâ–ˆâ–       | 345/1605 [04:56<17:55,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 346/1605 [04:57<18:00,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 347/1605 [04:58<17:57,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 348/1605 [04:59<18:10,  1.15it/s] 22%|â–ˆâ–ˆâ–       | 349/1605 [05:00<17:55,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 350/1605 [05:01<17:59,  1.16it/s] 22%|â–ˆâ–ˆâ–       | 351/1605 [05:02<17:56,  1.16it/s] 22%|â–ˆâ–ˆâ–       | 352/1605 [05:02<17:49,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 353/1605 [05:03<17:52,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 354/1605 [05:04<17:46,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 355/1605 [05:05<17:49,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 356/1605 [05:06<17:45,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 357/1605 [05:07<17:44,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 358/1605 [05:08<17:47,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 359/1605 [05:08<17:43,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 360/1605 [05:09<17:38,  1.18it/s] 22%|â–ˆâ–ˆâ–       | 361/1605 [05:10<17:45,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 362/1605 [05:11<17:51,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 363/1605 [05:12<17:54,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 364/1605 [05:13<17:49,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 365/1605 [05:14<17:44,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 366/1605 [05:14<17:50,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 367/1605 [05:15<17:44,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 368/1605 [05:16<17:47,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 369/1605 [05:17<17:44,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 370/1605 [05:18<17:39,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 371/1605 [05:19<17:33,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 372/1605 [05:20<17:34,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 373/1605 [05:20<17:35,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 374/1605 [05:21<17:28,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 375/1605 [05:22<17:31,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 376/1605 [05:23<17:27,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 377/1605 [05:24<17:27,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 378/1605 [05:25<17:27,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 379/1605 [05:26<17:24,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 380/1605 [05:26<17:26,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 381/1605 [05:27<17:31,  1.16it/s] 24%|â–ˆâ–ˆâ–       | 382/1605 [05:28<17:25,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 383/1605 [05:29<17:22,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 384/1605 [05:30<17:18,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 385/1605 [05:31<17:20,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 386/1605 [05:32<17:20,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 387/1605 [05:32<17:18,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 388/1605 [05:33<17:15,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 389/1605 [05:34<17:07,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 390/1605 [05:35<17:13,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 391/1605 [05:36<17:10,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 392/1605 [05:37<17:12,  1.18it/s] 24%|â–ˆâ–ˆâ–       | 393/1605 [05:37<17:12,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 394/1605 [05:38<17:15,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 395/1605 [05:39<17:07,  1.18it/s] 25%|â–ˆâ–ˆâ–       | 396/1605 [05:40<17:13,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 397/1605 [05:41<17:22,  1.16it/s] 25%|â–ˆâ–ˆâ–       | 398/1605 [05:42<17:13,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 399/1605 [05:43<17:15,  1.16it/s] 25%|â–ˆâ–ˆâ–       | 400/1605 [05:43<17:11,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 401/1605 [05:44<17:11,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 402/1605 [05:45<17:11,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 403/1605 [05:46<17:07,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 404/1605 [05:47<17:03,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 405/1605 [05:48<17:02,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 406/1605 [05:49<17:06,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 407/1605 [05:49<17:03,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 408/1605 [05:50<16:57,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 409/1605 [05:51<16:55,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 410/1605 [05:52<17:01,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 411/1605 [05:53<16:54,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 412/1605 [05:54<17:00,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 413/1605 [05:55<16:53,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 414/1605 [05:55<16:57,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 415/1605 [05:56<16:59,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 416/1605 [05:57<16:59,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 417/1605 [05:58<16:55,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 418/1605 [05:59<16:49,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 419/1605 [06:00<16:45,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 420/1605 [06:01<16:49,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 421/1605 [06:01<16:52,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 422/1605 [06:02<16:59,  1.16it/s] 26%|â–ˆâ–ˆâ–‹       | 423/1605 [06:03<16:54,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 424/1605 [06:04<16:48,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 425/1605 [06:05<16:47,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 426/1605 [06:06<16:44,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 427/1605 [06:07<16:45,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 428/1605 [06:07<16:38,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 429/1605 [06:08<16:42,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 430/1605 [06:09<16:40,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 431/1605 [06:10<16:38,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 432/1605 [06:11<16:42,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 433/1605 [06:12<16:41,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 434/1605 [06:12<16:36,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 435/1605 [06:13<16:40,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 436/1605 [06:14<16:33,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 437/1605 [06:15<16:33,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 438/1605 [06:16<16:33,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 439/1605 [06:17<16:33,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 440/1605 [06:18<16:30,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 441/1605 [06:18<16:27,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 442/1605 [06:19<16:34,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 443/1605 [06:20<16:27,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 444/1605 [06:21<16:27,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 445/1605 [06:22<16:29,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 446/1605 [06:23<16:25,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 447/1605 [06:24<16:25,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 448/1605 [06:24<16:24,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 449/1605 [06:25<16:24,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 450/1605 [06:26<16:23,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 451/1605 [06:27<16:20,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 452/1605 [06:28<16:18,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 453/1605 [06:29<16:18,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 454/1605 [06:29<16:17,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 455/1605 [06:30<16:17,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 456/1605 [06:31<16:16,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 457/1605 [06:32<16:17,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 458/1605 [06:33<16:14,  1.18it/s] 29%|â–ˆâ–ˆâ–Š       | 459/1605 [06:34<16:17,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 460/1605 [06:35<16:14,  1.18it/s] 29%|â–ˆâ–ˆâ–Š       | 461/1605 [06:35<16:08,  1.18it/s] 29%|â–ˆâ–ˆâ–‰       | 462/1605 [06:36<16:15,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 463/1605 [06:37<16:12,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 464/1605 [06:38<16:17,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 465/1605 [06:39<16:18,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 466/1605 [06:40<16:13,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 467/1605 [06:41<16:19,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 468/1605 [06:41<16:18,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 469/1605 [06:42<16:13,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 470/1605 [06:43<16:08,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 471/1605 [06:44<16:06,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 472/1605 [06:45<15:59,  1.18it/s] 29%|â–ˆâ–ˆâ–‰       | 473/1605 [06:46<16:05,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 474/1605 [06:47<15:58,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 475/1605 [06:47<16:00,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 476/1605 [06:48<16:03,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 477/1605 [06:49<16:03,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 478/1605 [06:50<15:59,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 479/1605 [06:51<16:03,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 480/1605 [06:52<15:58,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 481/1605 [06:53<15:59,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 482/1605 [06:53<15:54,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 483/1605 [06:54<15:54,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 484/1605 [06:55<15:55,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 485/1605 [06:56<15:56,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 486/1605 [06:57<15:52,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 487/1605 [06:58<15:51,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 488/1605 [06:58<15:50,  1.1{'loss': 0.7089, 'learning_rate': 3.442367601246106e-05, 'epoch': 0.93}
8it/s] 30%|â–ˆâ–ˆâ–ˆ       | 489/1605 [06:59<15:56,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 490/1605 [07:00<15:46,  1.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 491/1605 [07:01<15:41,  1.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 492/1605 [07:02<15:46,  1.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 493/1605 [07:03<15:58,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 494/1605 [07:04<15:53,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 495/1605 [07:04<15:50,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 496/1605 [07:05<15:47,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 497/1605 [07:06<15:47,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 498/1605 [07:07<15:51,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 499/1605 [07:08<15:47,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 500/1605 [07:09<15:45,  1.17it/s]                                                   31%|â–ˆâ–ˆâ–ˆ       | 500/1605 [07:09<15:45,  1.17it/s][INFO|trainer.py:2985] 2024-02-13 12:35:10,846 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-13 12:35:10,849 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 12:35:24,884 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 12:35:24,895 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 12:35:24,896 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-500/special_tokens_map.json
 31%|â–ˆâ–ˆâ–ˆ       | 501/1605 [07:47<3:40:40, 11.99s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 502/1605 [07:48<2:38:58,  8.65s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 503/1605 [07:48<1:55:48,  6.31s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 504/1605 [07:49<1:25:37,  4.67s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 505/1605 [07:50<1:04:32,  3.52s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 506/1605 [07:51<49:46,  2.72s/it]   32%|â–ˆâ–ˆâ–ˆâ–      | 507/1605 [07:52<39:31,  2.16s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 508/1605 [07:53<32:18,  1.77s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 509/1605 [07:53<27:12,  1.49s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 510/1605 [07:54<23:40,  1.30s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 511/1605 [07:55<21:14,  1.16s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 512/1605 [07:56<19:33,  1.07s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 513/1605 [07:57<18:13,  1.00s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 514/1605 [07:58<17:30,  1.04it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 515/1605 [07:59<16:54,  1.07it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 516/1605 [07:59<16:20,  1.11it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 517/1605 [08:00<16:06,  1.13it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 518/1605 [08:01<15:49,  1.14it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 519/1605 [08:02<15:46,  1.15it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 520/1605 [08:03<15:36,  1.16it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 521/1605 [08:04<15:29,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 522/1605 [08:05<15:30,  1.16it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 523/1605 [08:05<15:27,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 524/1605 [08:06<15:26,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 525/1605 [08:07<15:20,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 526/1605 [08:08<15:16,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 527/1605 [08:09<15:23,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 528/1605 [08:10<15:14,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 529/1605 [08:11<15:23,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 530/1605 [08:11<15:18,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 531/1605 [08:12<15:19,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 532/1605 [08:13<15:17,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 533/1605 [08:14<15:14,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 534/1605 [08:15<15:09,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 535/1605 [08:16<15:11,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 536/1605 [08:17<15:10,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 537/1605 [08:17<15:07,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 538/1605 [08:18<15:11,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 539/1605 [08:19<15:15,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 540/1605 [08:20<15:10,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 541/1605 [08:21<15:10,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 542/1605 [08:22<15:06,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 543/1605 [08:22<15:06,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 544/1605 [08:23<15:06,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 545/1605 [08:24<15:10,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 546/1605 [08:25<15:04,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 547/1605 [08:26<15:01,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 548/1605 [08:27<14:57,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 549/1605 [08:28<14:57,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 550/1605 [08:28<14:58,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 551/1605 [08:29<14:58,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 552/1605 [08:30<14:55,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 553/1605 [08:31<14:57,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 554/1605 [08:32<14:59,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 555/1605 [08:33<14:57,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 556/1605 [08:34<14:55,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 557/1605 [08:34<14:50,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 558/1605 [08:35<14:49,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 559/1605 [08:36<14:52,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 560/1605 [08:37<14:50,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 561/1605 [08:38<14:43,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 562/1605 [08:39<14:49,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 563/1605 [08:40<14:44,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 564/1605 [08:40<14:49,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 565/1605 [08:41<14:56,  1.16it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 566/1605 [08:42<14:49,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 567/1605 [08:43<14:44,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 568/1605 [08:44<14:36,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 569/1605 [08:45<14:40,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 570/1605 [08:45<14:37,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 571/1605 [08:46<14:45,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1605 [08:47<14:38,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 573/1605 [08:48<14:48,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 574/1605 [08:49<14:41,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 575/1605 [08:50<14:45,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 576/1605 [08:51<14:42,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 577/1605 [08:51<14:36,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 578/1605 [08:52<14:47,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 579/1605 [08:53<14:38,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 580/1605 [08:54<14:37,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 581/1605 [08:55<14:33,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 582/1605 [08:56<14:39,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 583/1605 [08:57<14:36,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 584/1605 [08:58<14:39,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 585/1605 [08:58<14:28,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 586/1605 [08:59<14:33,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 587/1605 [09:00<14:30,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 588/1605 [09:01<14:30,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 589/1605 [09:02<14:25,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 590/1605 [09:03<14:26,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 591/1605 [09:03<14:22,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 592/1605 [09:04<14:21,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 593/1605 [09:05<14:21,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 594/1605 [09:06<14:24,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 595/1605 [09:07<14:24,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 596/1605 [09:08<14:25,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 597/1605 [09:09<14:24,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 598/1605 [09:09<14:20,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 599/1605 [09:10<14:15,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 600/1605 [09:11<14:18,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 601/1605 [09:12<14:16,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 602/1605 [09:13<14:18,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 603/1605 [09:14<14:16,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 604/1605 [09:15<14:11,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 605/1605 [09:15<14:12,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 606/1605 [09:16<14:10,  1.18it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 607/1605 [09:17<14:15,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 608/1605 [09:18<14:11,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 609/1605 [09:19<14:08,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1605 [09:20<14:10,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 611/1605 [09:21<14:10,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 612/1605 [09:21<14:03,  1.18it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 613/1605 [09:22<14:07,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1605 [09:23<14:07,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 615/1605 [09:24<14:06,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 616/1605 [09:25<14:05,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 617/1605 [09:26<14:05,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1605 [09:27<14:01,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 619/1605 [09:27<14:03,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 620/1605 [09:28<13:59,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 621/1605 [09:29<14:02,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 622/1605 [09:30<14:05,  1.16it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 623/1605 [09:31<14:00,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1605 [09:32<13:55,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 625/1605 [09:33<13:55,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1605 [09:33<13:50,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1605 [09:34<13:56,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1605 [09:35<13:52,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1605 [09:36<13:47,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1605 [09:37<13:50,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1605 [09:38<13:53,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1605 [09:38<13:51,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 633/1605 [09:39<13:49,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 634/1605 [09:40<13:44,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 635/1605 [09:41<13:46,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1605 [09:42<13:45,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 637/1605 [09:43<13:52,  1.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1605 [09:44<13:50,  1.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 639/1605 [09:44<13:46,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 640/1605 [09:45<13:44,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 641/1605 [09:46<13:42,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 642/1605 [09:47<13:39,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 643/1605 [09:48<13:43,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 644/1605 [09:49<13:40,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 645/1605 [09:50<13:38,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 646/1605 [09:50<13:36,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 647/1605 [09:51<13:42,  1.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 648/1605 [09:52<13:37,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1605 [09:53<13:33,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 650/1605 [09:54<13:37,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 651/1605 [09:55<13:34,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 652/1605 [09:56<13:33,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1605 [09:56<13:28,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 654/1605 [09:57<13:26,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 655/1605 [09:58<13:28,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 656/1605 [09:59<13:27,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 657/1605 [10:00<13:33,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1605 [10:01<13:31,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1605 [10:02<13:31,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 660/1605 [10:02<13:29,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 661/1605 [10:03<13:24,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 662/1605 [10:04<13:22,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 663/1605 [10:05<13:22,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1605 [10:06<13:20,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 665/1605 [10:07<13:13,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 666/1605 [10:07<13:21,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 667/1605 [10:08<13:22,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 668/1605 [10:09<13:27,  1.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 669/1605 [10:10<13:24,  1.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1605 [10:11<13:22,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 671/1605 [10:12<13:17,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 672/1605 [10:13<13:16,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1605 [10:13<13:13,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1605 [10:14<13:14,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1605 [10:15<13:13,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 676/1605 [10:16<13:11,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 677/1605 [10:17<13:07,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 678/1605 [10:18<13:11,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 679/1605 [10:19<13:06,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 680/1605 [10:19<13:08,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 681/1605 [10:20<13:05,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 682/1605 [10:21<13:04,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 683/1605 [10:22<13:14,  1.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1605 [10:23<13:11,  1.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 685/1605 [10:24<13:08,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/1605 [10:25<13:07,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 687/1605 [10:25<13:06,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 688/1605 [10:26<13:05,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1605 [10:27<13:02,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1605 [10:28<13:01,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 691/1605 [10:29<12:58,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 692/1605 [10:30<13:01,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 693/1605 [10:31<12:55,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 694/1605 [10:31<12:52,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 695/1605 [10:32<12:52,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 696/1605 [10:33<12:52,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1605 [10:34<12:50,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 698/1605 [10:35<12:52,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 699/1605 [10:36<12:50,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 700/1605 [10:37<12:51,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 701/1605 [10:37<12:46,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 702/1605 [10:38<12:49,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 703/1605 [10:39<12:44,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 704/1605 [10:40<12:51,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 705/1605 [10:41<12:51,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 706/1605 [10:42<12:47,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 707/1605 [10:42<12:45,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 708/1605 [10:43<12:44,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1605 [10:44<12:46,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1605 [10:45<12:41,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 711/1605 [10:46<12:43,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 712/1605 [10:47<12:42,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1605 [10:48<12:41,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1605 [10:48<12:37,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 715/1605 [10:49<12:40,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 716/1605 [10:50<12:42,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1605 [10:51<12:37,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1605 [10:52<12:40,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 719/1605 [10:53<12:39,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 720/1605 [10:54<12:36,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 721/1605 [10:54<12:35,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 722/1605 [10:55<12:33,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 723/1605 [10:56<12:30,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 724/1605 [10:57<12:33,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 725/1605 [10:58<12:30,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 726/1605 [10:59<12:25,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 727/1605 [11:00<12:28,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 728/1605 [11:00<12:24,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 729/1605 [11:01<12:26,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 730/1605 [11:02<12:25,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 731/1605 [11:03<12:28,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 732/1605 [11:04<12:25,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 733/1605 [11:05<12:21,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 734/1605 [11:06<12:19,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 735/1605 [11:06<12:17,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 736/1605 [11:07<12:19,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1605 [11:08<12:16,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 738/1605 [11:09<12:17,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 739/1605 [11:10<12:12,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 740/1605 [11:11<12:15,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 741/1605 [11:11<12:12,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 742/1605 [11:12<12:11,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 743/1605 [11:13<12:09,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 744/1605 [11:14<12:15,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 745/1605 [11:15<12:12,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 746/1605 [11:16<12:09,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 747/1605 [11:17<12:17,  1.16it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 748/1605 [11:17<12:12,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 749/1605 [11:18<12:11,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 750/1605 [11:19<12:09,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1605 [11:20<12:04,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 752/1605 [11:21<12:11,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 753/1605 [11:22<12:04,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 754/1605 [11:23<12:07,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 755/1605 [11:23<12:03,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 756/1605 [11:24<12:04,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 757/1605 [11:25<12:07,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 758/1605 [11:26<11:57,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 759/1605 [11:27<12:06,  1.16it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 760/1605 [11:28<12:04,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 761/1605 [11:29<11:58,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 762/1605 [11:29<12:02,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 763/1605 [11:30<12:05,  1.16it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 764/1605 [11:31<12:04,  1.16it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 765/1605 [11:32<11:59,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 766/1605 [11:33<12:06,  1.16it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 767/1605 [11:34<11:58,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 768/1605 [11:35<11:55,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 769/1605 [11:35<11:52,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 770/1605 [11:36<11:55,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 771/1605 [11:37<11:51,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 772/1605 [11:38<11:49,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 773/1605 [11:39<11:48,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 774/1605 [11:40<11:48,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 775/1605 [11:40<11:46,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 776/1605 [11:41<11:47,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 777/1605 [11:42<11:44,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 778/1605 [11:43<11:47,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 779/1605 [11:44<11:42,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 780/1605 [11:45<11:40,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 781/1605 [11:46<11:42,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 782/1605 [11:46<11:35,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 783/1605 [11:47<11:40,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 784/1605 [11:48<11:38,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 785/1605 [11:49<11:35,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 786/1605 [11:50<11:33,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 787/1605 [11:51<11:33,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 788/1605 [11:52<11:28,  1.19it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 789/1605 [11:52<11:32,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 790/1605 [11:53<11:33,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 791/1605 [11:54<11:33,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 792/1605 [11:55<11:29,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 793/1605 [11:56<11:34,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 794/1605 [11:57<11:37,  1.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 795/1605 [11:58<11:36,  1.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 796/1605 [11:58<11:31,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 797/1605 [11:59<11:28,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 798/1605 [12:00<11:25,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 799/1605 [12:01<11:24,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 800/1605 [12:02<11:20,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 801/1605 [12:03<11:29,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 802/1605 [12:03<11:28,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 803/1605 [12:04<11:21,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 804/1605 [12:05<11:23,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 805/1605 [12:06<11:20,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 806/1605 [12:07<11:26,  1.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 807/1605 [12:08<11:22,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 808/1605 [12:09<11:24,  1.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 809/1605 [12:09<11:24,  1.16it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1605 [12:10<11:21,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 811/1605 [12:11<11:20,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 812/1605 [12:12<11:17,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 813/1605 [12:13<11:17,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 814/1605 [12:14<11:15,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 815/1605 [12:15<11:13,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 816/1605 [12:15<11:12,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 817/1605 [12:16<11:19,  1.16it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 818/1605 [12:17<11:11,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1605 [12:18<11:06,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 820/1605 [12:19<11:06,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 821/1605 [12:20<11:05,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 822/1605 [12:21<11:02,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 823/1605 [12:21<10:59,  1.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 824/1605 [12:22<11:01,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 825/1605 [12:23<11:06,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 826/1605 [12:24<11:05,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 827/1605 [12:25<11:02,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 828/1605 [12:26<11:01,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 829/1605 [12:27<11:06,  1.16it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1605 [12:27<11:02,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 831/1605 [12:28<10:58,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1605 [12:29<10:56,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 833/1605 [12:30<10:59,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1605 [12:31<11:01,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1605 [12:32<10:54,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1605 [12:33<10:57,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1605 [12:33<10:58,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1605 [12:34<10:56,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1605 [12:35<10:54,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 840/1605 [12:36<10:50,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 841/1605 [12:37<10:49,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 842/1605 [12:38<10:49,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 843/1605 [12:38<10:48,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1605 [12:39<10:47,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1605 [12:40<10:41,  1.19it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1605 [12:41<10:44,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 847/1605 [12:42<10:41,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1605 [12:43<10:43,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 849/1605 [12:44<10:45,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 850/1605 [12:44<10:45,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 851/1605 [12:45<10:42,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1605 [12:46<10:42,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 853/1605 [12:47<10:39,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 854/1605 [12:48<10:38,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 855/1605 [12:49<10:40,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 856/1605 [12:50<10:38,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 857/1605 [12:50<10:37,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 858/1605 [12:51<10:33,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 859/1605 [12:52<10:35,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 860/1605 [12:53<10:33,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 861/1605 [12:54<10:34,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 862/1605 [12:55<10:35,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 863/1605 [12:56<10:34,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1605 [12:56<10:31,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 865/1605 [12:57<10:29,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 866/1605 [12:58<10:32,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 867/1605 [12:59<10:30,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1605 [13:00<10:26,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 869/1605 [13:01<10:26,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 870/1605 [13:01<10:25,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 871/1605 [13:02<10:23,  1.18it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 872/1605 [13:03<10:28,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 873/1605 [13:04<10:25,  1.17it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1605 [13:05<10:26,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 875/1605 [13:06<10:23,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 876/1605 [13:07<10:24,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 877/1605 [13:07<10:23,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 878/1605 [13:08<10:16,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 879/1605 [13:09<10:19,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 880/1605 [13:10<10:18,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 881/1605 [13:11<10:14,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 882/1605 [13:12<10:15,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 883/1605 [13:13<10:12,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 884/1605 [13:13<10:12,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1605 [13:14<10:11,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 886/1605 [13:15<10:09,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1605 [13:16<10:11,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 888/1605 [13:17<10:09,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1605 [13:18<10:08,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 890/1605 [13:18<10:08,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 891/1605 [13:19<10:05,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 892/1605 [13:20<10:05,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 893/1605 [13:21<10:04,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 894/1605 [13:22<10:04,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1605 [13:23<10:03,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 896/1605 [13:24<10:02,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 897/1605 [13:24<10:05,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1605 [13:25<10:05,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 899/1605 [13:26<10:09,  1.16it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 900/1605 [13:27<10:04,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 901/1605 [13:28<10:00,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 902/1605 [13:29<10:02,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 903/1605 [13:30<09:59,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 904/1605 [13:30<09:55,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 905/1605 [13:31<09:57,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 906/1605 [13:32<09:56,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 907/1605 [13:33<09:54,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 908/1605 [13:34<09:50,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 909/1605 [13:35<09:48,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 910/1605 [13:36<09:52,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 911/1605 [13:36<09:48,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 912/1605 [13:37<09:50,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 913/1605 [13:38<09:51,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 914/1605 [13:39<09:48,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 915/1605 [13:40<09:47,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 916/1605 [13:41<09:49,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 917/1605 [13:42<09:45,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 918/1605 [13:42<09:46,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 919/1605 [13:43<09:43,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 920/1605 [13:44<09:42,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 921/1605 [13:45<09:39,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 922/1605 [13:46<09:39,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 923/1605 [13:47<09:39,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 924/1605 [13:47<09:39,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 925/1605 [13:48<09:40,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 926/1605 [13:49<09:41,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 927/1605 [13:50<09:38,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 928/1605 [13:51<09:35,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 929/1605 [13:52<09:34,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 930/1605 [13:53<09:34,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 931/1605 [13:53<09:36,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 932/1605 [13:54<09:35,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 933/1605 [13:55<09:31,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 934/1605 [13:56<09:34,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 935/1605 [13:57<09:31,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 936/1605 [13:58<09:26,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 937/1605 [13:59<09:32,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 938/1605 [13:59<09:32,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 939/1605 [14:00<09:29,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 940/1605 [14:01<09:26,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 941/1605 [14:02<09:29,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 942/1605 [14:03<09:26,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 943/1605 [14:04<09:26,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 944/1605 [14:05<09:22,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 945/1605 [14:05<09:24,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 946/1605 [14:06<09:21,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 947/1605 [14:07<09:20,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 948/1605 [14:08<09:17,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 949/1605 [14:09<09:17,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 950/1605 [14:10<09:16,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 951/1605 [14:10<09:16,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 952/1605 [14:11<09:17,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 953/1605 [14:12<09:17,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1605 [14:13<09:14,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 955/1605 [14:14<09:13,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1605 [14:15<09:12,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 957/1605 [14:16<09:11,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 958/1605 [14:16<09:08,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 959/1605 [14:17<09:08,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 960/1605 [14:18<09:09,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 961/1605 [14:19<09:09,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 962/1605 [14:20<09:09,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 963/1605 [14:21<09:04,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1605 [14:22<09:05,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 965/1605 [14:22<09:04,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 966/1605 [14:23<09:06,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 967/1605 [14:24<09:07,  1.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 968/1605 [14:25<09:01,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 969/1605 [14:26<09:04,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 970/1605 [14:27<09:03,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 971/1605 [14:28<09:06,  1.16it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 972/1605 [14:28<09:02,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 973/1605 [14:29<08:59,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 974/1605 [14:30<08:56,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 975/1605 [14:31<08:58,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 976/1605 [14:32<08:56,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1605 [14:33<08:53,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 978/1605 [14:34<08:50,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 979/1605 [14:34<08:52,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 980/1605 [14:35<09:00,  1.16it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 981/1605 [14:36<08:58,  1.16it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 982/1605 [14:37<08:54,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 983/1605 [14:38<08:54,  1.16it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 984/1605 [14:39<08:51,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 985/1605 [14:40<08:50,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 986/1605 [14:40<08:47,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   {'loss': 0.6254, 'learning_rate': 1.884735202492212e-05, 'epoch': 1.87}
| 987/1605 [14:41<08:48,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 988/1605 [14:42<08:46,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 989/1605 [14:43<08:45,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 990/1605 [14:44<08:41,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 991/1605 [14:45<08:43,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 992/1605 [14:45<08:40,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 993/1605 [14:46<08:42,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 994/1605 [14:47<08:40,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 995/1605 [14:48<08:41,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 996/1605 [14:49<08:39,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 997/1605 [14:50<08:37,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1605 [14:51<08:35,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 999/1605 [14:51<08:32,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1000/1605 [14:52<08:35,  1.17it/s]                                                    62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1000/1605 [14:52<08:35,  1.17it/s][INFO|trainer.py:2985] 2024-02-13 12:42:54,417 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1000
[INFO|configuration_utils.py:473] 2024-02-13 12:42:54,433 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1000/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 12:43:08,789 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 12:43:08,791 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 12:43:08,792 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1000/special_tokens_map.json
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1001/1605 [15:31<2:02:07, 12.13s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1002/1605 [15:32<1:27:50,  8.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1003/1605 [15:32<1:03:55,  6.37s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1004/1605 [15:33<47:12,  4.71s/it]   63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1005/1605 [15:34<35:33,  3.56s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1006/1605 [15:35<27:21,  2.74s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1007/1605 [15:36<21:40,  2.18s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1008/1605 [15:37<17:42,  1.78s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1009/1605 [15:38<14:52,  1.50s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1010/1605 [15:38<12:57,  1.31s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1011/1605 [15:39<11:33,  1.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1012/1605 [15:40<10:35,  1.07s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1013/1605 [15:41<09:52,  1.00s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1014/1605 [15:42<09:25,  1.05it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1015/1605 [15:43<09:06,  1.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1016/1605 [15:43<08:52,  1.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1017/1605 [15:44<08:40,  1.13it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1018/1605 [15:45<08:32,  1.15it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1019/1605 [15:46<08:27,  1.15it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1020/1605 [15:47<08:24,  1.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1021/1605 [15:48<08:25,  1.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1022/1605 [15:49<08:19,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1023/1605 [15:49<08:20,  1.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1024/1605 [15:50<08:16,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1025/1605 [15:51<08:16,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1026/1605 [15:52<08:17,  1.16it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1027/1605 [15:53<08:15,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1028/1605 [15:54<08:12,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1029/1605 [15:55<08:09,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1605 [15:55<08:10,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1031/1605 [15:56<08:06,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1032/1605 [15:57<08:09,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1605 [15:58<08:06,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1605 [15:59<08:05,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1035/1605 [16:00<08:03,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1605 [16:00<08:01,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/1605 [16:01<08:05,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1038/1605 [16:02<08:05,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1039/1605 [16:03<08:05,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1040/1605 [16:04<08:02,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1041/1605 [16:05<08:00,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1042/1605 [16:06<07:57,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1043/1605 [16:06<07:59,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1605 [16:07<07:56,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1045/1605 [16:08<07:57,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1605 [16:09<07:55,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1605 [16:10<07:56,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1605 [16:11<07:53,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1049/1605 [16:12<07:50,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1050/1605 [16:12<07:52,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1605 [16:13<07:52,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1052/1605 [16:14<07:50,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1053/1605 [16:15<07:50,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1605 [16:16<07:48,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1605 [16:17<07:50,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1605 [16:18<07:46,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1057/1605 [16:18<07:46,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1058/1605 [16:19<07:45,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1605 [16:20<07:44,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1060/1605 [16:21<07:41,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1061/1605 [16:22<07:45,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1062/1605 [16:23<07:42,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1063/1605 [16:24<07:43,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1064/1605 [16:24<07:42,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1065/1605 [16:25<07:41,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1066/1605 [16:26<07:40,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1067/1605 [16:27<07:37,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1068/1605 [16:28<07:38,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1069/1605 [16:29<07:35,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1070/1605 [16:29<07:33,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1071/1605 [16:30<07:35,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1072/1605 [16:31<07:33,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1073/1605 [16:32<07:32,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1074/1605 [16:33<07:34,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1075/1605 [16:34<07:31,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1076/1605 [16:35<07:32,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1077/1605 [16:35<07:31,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1078/1605 [16:36<07:35,  1.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1079/1605 [16:37<07:38,  1.15it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1080/1605 [16:38<07:32,  1.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1081/1605 [16:39<07:31,  1.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1082/1605 [16:40<07:27,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1083/1605 [16:41<07:25,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1084/1605 [16:41<07:23,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1085/1605 [16:42<07:21,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1086/1605 [16:43<07:20,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1087/1605 [16:44<07:19,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1088/1605 [16:45<07:20,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1089/1605 [16:46<07:20,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1090/1605 [16:47<07:19,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1091/1605 [16:47<07:17,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1092/1605 [16:48<07:16,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1093/1605 [16:49<07:16,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1094/1605 [16:50<07:14,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1095/1605 [16:51<07:14,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1096/1605 [16:52<07:13,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1097/1605 [16:53<07:14,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1098/1605 [16:53<07:13,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1099/1605 [16:54<07:16,  1.16it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1100/1605 [16:55<07:12,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1101/1605 [16:56<07:11,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1102/1605 [16:57<07:09,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1103/1605 [16:58<07:09,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1104/1605 [16:59<07:06,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1105/1605 [16:59<07:07,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1106/1605 [17:00<07:07,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1107/1605 [17:01<07:06,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1108/1605 [17:02<07:03,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1109/1605 [17:03<07:01,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1110/1605 [17:04<06:59,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1111/1605 [17:04<06:59,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1112/1605 [17:05<07:01,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1113/1605 [17:06<06:58,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1114/1605 [17:07<06:59,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1115/1605 [17:08<06:57,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1116/1605 [17:09<06:59,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1117/1605 [17:10<06:59,  1.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1118/1605 [17:10<06:59,  1.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1119/1605 [17:11<06:56,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1120/1605 [17:12<06:56,  1.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1121/1605 [17:13<06:53,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1122/1605 [17:14<06:54,  1.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1123/1605 [17:15<06:51,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1124/1605 [17:16<06:49,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1605 [17:16<06:48,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1126/1605 [17:17<06:46,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1127/1605 [17:18<06:44,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1128/1605 [17:19<06:49,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1129/1605 [17:20<06:48,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1130/1605 [17:21<06:44,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1131/1605 [17:22<06:44,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1132/1605 [17:22<06:43,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1133/1605 [17:23<06:43,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1134/1605 [17:24<06:41,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1135/1605 [17:25<06:41,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1136/1605 [17:26<06:40,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1137/1605 [17:27<06:36,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1138/1605 [17:28<06:35,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1139/1605 [17:28<06:42,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1140/1605 [17:29<06:39,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1141/1605 [17:30<06:40,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1142/1605 [17:31<06:37,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1143/1605 [17:32<06:34,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1144/1605 [17:33<06:37,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1145/1605 [17:34<06:36,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1146/1605 [17:34<06:35,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1147/1605 [17:35<06:35,  1.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1148/1605 [17:36<06:31,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1149/1605 [17:37<06:28,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1150/1605 [17:38<06:29,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1151/1605 [17:39<06:27,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1152/1605 [17:40<06:27,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1153/1605 [17:40<06:27,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1154/1605 [17:41<06:26,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1155/1605 [17:42<06:28,  1.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1156/1605 [17:43<06:25,  1.16it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1157/1605 [17:44<06:22,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1158/1605 [17:45<06:22,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1159/1605 [17:46<06:20,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1160/1605 [17:46<06:18,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1161/1605 [17:47<06:15,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1162/1605 [17:48<06:18,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1163/1605 [17:49<06:15,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1164/1605 [17:50<06:18,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1165/1605 [17:51<06:17,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1166/1605 [17:52<06:13,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1167/1605 [17:52<06:13,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1168/1605 [17:53<06:15,  1.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1169/1605 [17:54<06:13,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1170/1605 [17:55<06:12,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1171/1605 [17:56<06:11,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1172/1605 [17:57<06:10,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1173/1605 [17:58<06:08,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1174/1605 [17:58<06:08,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1175/1605 [17:59<06:05,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1176/1605 [18:00<06:06,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1177/1605 [18:01<06:06,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1605 [18:02<06:04,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1179/1605 [18:03<06:04,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1180/1605 [18:03<06:02,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1181/1605 [18:04<06:01,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1182/1605 [18:05<05:58,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1183/1605 [18:06<05:59,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1184/1605 [18:07<05:59,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1185/1605 [18:08<05:58,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1186/1605 [18:09<05:57,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1187/1605 [18:09<05:57,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1188/1605 [18:10<05:56,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1189/1605 [18:11<05:56,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1190/1605 [18:12<05:53,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1191/1605 [18:13<05:53,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1605 [18:14<05:51,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1193/1605 [18:15<05:51,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1194/1605 [18:15<05:51,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1605 [18:16<05:49,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1196/1605 [18:17<05:50,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1197/1605 [18:18<05:49,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1198/1605 [18:19<05:47,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1199/1605 [18:20<05:47,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1200/1605 [18:21<05:46,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1201/1605 [18:21<05:44,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1202/1605 [18:22<05:44,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1203/1605 [18:23<05:44,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1605 [18:24<05:43,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1605 [18:25<05:42,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1605 [18:26<05:40,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1605 [18:27<05:38,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1208/1605 [18:27<05:37,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1209/1605 [18:28<05:37,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1210/1605 [18:29<05:34,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1211/1605 [18:30<05:35,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1212/1605 [18:31<05:36,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1213/1605 [18:32<05:37,  1.16it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1214/1605 [18:33<05:35,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1215/1605 [18:33<05:33,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1605 [18:34<05:31,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1217/1605 [18:35<05:32,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1218/1605 [18:36<05:30,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1219/1605 [18:37<05:30,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1220/1605 [18:38<05:26,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1221/1605 [18:39<05:27,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1222/1605 [18:39<05:26,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1223/1605 [18:40<05:27,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1224/1605 [18:41<05:26,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1225/1605 [18:42<05:24,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1605 [18:43<05:25,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1227/1605 [18:44<05:23,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1228/1605 [18:45<05:23,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1605 [18:45<05:21,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1230/1605 [18:46<05:20,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1231/1605 [18:47<05:20,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1232/1605 [18:48<05:19,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1233/1605 [18:49<05:18,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1234/1605 [18:50<05:16,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1605 [18:50<05:14,  1.18it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1605 [18:51<05:16,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1237/1605 [18:52<05:17,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1238/1605 [18:53<05:14,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1605 [18:54<05:12,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1240/1605 [18:55<05:14,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1241/1605 [18:56<05:12,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1242/1605 [18:56<05:11,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1243/1605 [18:57<05:08,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1244/1605 [18:58<05:09,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1245/1605 [18:59<05:07,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1246/1605 [19:00<05:06,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1605 [19:01<05:05,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1605 [19:02<05:02,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1249/1605 [19:02<05:04,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1250/1605 [19:03<05:02,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1605 [19:04<05:01,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1605 [19:05<05:00,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1605 [19:06<05:01,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1605 [19:07<04:58,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1255/1605 [19:08<04:58,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1605 [19:08<04:56,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1257/1605 [19:09<04:56,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1258/1605 [19:10<04:56,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1259/1605 [19:11<04:55,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1260/1605 [19:12<04:52,  1.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1261/1605 [19:13<04:54,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1262/1605 [19:14<04:54,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1263/1605 [19:14<04:52,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1264/1605 [19:15<04:50,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1265/1605 [19:16<04:48,  1.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1605 [19:17<04:48,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1605 [19:18<04:46,  1.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1268/1605 [19:19<04:48,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1269/1605 [19:20<04:48,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1605 [19:20<04:46,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1271/1605 [19:21<04:49,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1605 [19:22<04:48,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1273/1605 [19:23<04:44,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1274/1605 [19:24<04:45,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1275/1605 [19:25<04:43,  1.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1276/1605 [19:26<04:41,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1277/1605 [19:26<04:39,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1278/1605 [19:27<04:39,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1279/1605 [19:28<04:39,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1280/1605 [19:29<04:39,  1.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1281/1605 [19:30<04:37,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1282/1605 [19:31<04:35,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1283/1605 [19:32<04:33,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1284/1605 [19:32<04:34,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1285/1605 [19:33<04:32,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1286/1605 [19:34<04:32,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1287/1605 [19:35<04:31,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1288/1605 [19:36<04:31,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1289/1605 [19:37<04:30,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1290/1605 [19:37<04:27,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1291/1605 [19:38<04:27,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1292/1605 [19:39<04:27,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1293/1605 [19:40<04:27,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1294/1605 [19:41<04:25,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1295/1605 [19:42<04:23,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1296/1605 [19:43<04:24,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1297/1605 [19:43<04:22,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1298/1605 [19:44<04:21,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1299/1605 [19:45<04:19,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1300/1605 [19:46<04:20,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1301/1605 [19:47<04:18,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1302/1605 [19:48<04:16,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1303/1605 [19:49<04:16,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1304/1605 [19:49<04:15,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1305/1605 [19:50<04:16,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1605 [19:51<04:15,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1307/1605 [19:52<04:15,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1308/1605 [19:53<04:12,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1309/1605 [19:54<04:13,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1310/1605 [19:55<04:13,  1.16it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1311/1605 [19:55<04:11,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1312/1605 [19:56<04:10,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1313/1605 [19:57<04:09,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1314/1605 [19:58<04:07,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1315/1605 [19:59<04:05,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1316/1605 [20:00<04:04,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1317/1605 [20:01<04:06,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1318/1605 [20:01<04:04,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1319/1605 [20:02<04:04,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1320/1605 [20:03<04:02,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1321/1605 [20:04<04:03,  1.16it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1322/1605 [20:05<04:01,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1323/1605 [20:06<04:01,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1324/1605 [20:06<03:59,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1325/1605 [20:07<03:57,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1326/1605 [20:08<03:57,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1327/1605 [20:09<03:57,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1328/1605 [20:10<03:56,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1329/1605 [20:11<03:53,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1330/1605 [20:12<03:54,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1331/1605 [20:12<03:52,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1332/1605 [20:13<03:50,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1333/1605 [20:14<03:49,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1334/1605 [20:15<03:48,  1.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1335/1605 [20:16<03:50,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1336/1605 [20:17<03:48,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1337/1605 [20:18<03:49,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1338/1605 [20:18<03:47,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1339/1605 [20:19<03:47,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1340/1605 [20:20<03:45,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1341/1605 [20:21<03:42,  1.19it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1342/1605 [20:22<03:42,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1343/1605 [20:23<03:42,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1344/1605 [20:23<03:41,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1345/1605 [20:24<03:39,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1346/1605 [20:25<03:39,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1347/1605 [20:26<03:39,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1605 [20:27<03:39,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1605 [20:28<03:39,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1350/1605 [20:29<03:39,  1.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1351/1605 [20:29<03:37,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1352/1605 [20:30<03:35,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1605 [20:31<03:36,  1.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1354/1605 [20:32<03:34,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1355/1605 [20:33<03:34,  1.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1356/1605 [20:34<03:33,  1.16it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1357/1605 [20:35<03:30,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1358/1605 [20:35<03:30,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1359/1605 [20:36<03:28,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1360/1605 [20:37<03:29,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1361/1605 [20:38<03:26,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1362/1605 [20:39<03:27,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1363/1605 [20:40<03:26,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1364/1605 [20:41<03:25,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1365/1605 [20:41<03:24,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1366/1605 [20:42<03:23,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1605 [20:43<03:21,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1368/1605 [20:44<03:22,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1369/1605 [20:45<03:20,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1605 [20:46<03:21,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1371/1605 [20:47<03:18,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1372/1605 [20:47<03:19,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1373/1605 [20:48<03:17,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1374/1605 [20:49<03:16,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1605 [20:50<03:15,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1376/1605 [20:51<03:14,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1377/1605 [20:52<03:13,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1378/1605 [20:52<03:13,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1379/1605 [20:53<03:11,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1380/1605 [20:54<03:11,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1381/1605 [20:55<03:10,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1382/1605 [20:56<03:09,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1383/1605 [20:57<03:10,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1384/1605 [20:58<03:09,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1385/1605 [20:58<03:08,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1386/1605 [20:59<03:06,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1605 [21:00<03:04,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1388/1605 [21:01<03:05,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1389/1605 [21:02<03:05,  1.16it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1605 [21:03<03:03,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1391/1605 [21:04<03:01,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1605 [21:04<03:02,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1393/1605 [21:05<03:01,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1394/1605 [21:06<02:59,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1395/1605 [21:07<02:59,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1396/1605 [21:08<02:57,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1605 [21:09<02:56,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1398/1605 [21:10<02:56,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1399/1605 [21:10<02:54,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1400/1605 [21:11<02:53,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1401/1605 [21:12<02:52,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1402/1605 [21:13<02:51,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1403/1605 [21:14<02:51,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1404/1605 [21:15<02:50,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1405/1605 [21:15<02:50,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1406/1605 [21:16<02:49,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1407/1605 [21:17<02:48,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1408/1605 [21:18<02:47,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1409/1605 [21:19<02:46,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1410/1605 [21:20<02:44,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1411/1605 [21:21<02:44,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1412/1605 [21:21<02:44,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1413/1605 [21:22<02:43,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1414/1605 [21:23<02:42,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1415/1605 [21:24<02:42,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1416/1605 [21:25<02:40,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1417/1605 [21:26<02:39,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1418/1605 [21:27<02:38,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1419/1605 [21:27<02:38,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1420/1605 [21:28<02:37,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1421/1605 [21:29<02:35,  1.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1422/1605 [21:30<02:36,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1423/1605 [21:31<02:35,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1424/1605 [21:32<02:34,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1425/1605 [21:32<02:33,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1426/1605 [21:33<02:33,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1427/1605 [21:34<02:31,  1.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1428/1605 [21:35<02:30,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1605 [21:36<02:30,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1430/1605 [21:37<02:29,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1431/1605 [21:38<02:28,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1432/1605 [21:38<02:28,  1.16it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1433/1605 [21:39<02:27,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1605 [21:40<02:25,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1435/1605 [21:41<02:24,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1436/1605 [21:42<02:23,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1605 [21:43<02:22,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1438/1605 [21:44<02:22,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1439/1605 [21:44<02:20,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1440/1605 [21:45<02:20,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1441/1605 [21:46<02:19,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1442/1605 [21:47<02:19,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1443/1605 [21:48<02:18,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1444/1605 [21:49<02:17,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1445/1605 [21:50<02:16,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1605 [21:50<02:15,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1605 [21:51<02:14,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1448/1605 [21:52<02:13,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1449/1605 [21:53<02:12,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1450/1605 [21:54<02:11,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1451/1605 [21:55<02:11,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1452/1605 [21:55<02:09,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1453/1605 [21:56<02:09,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1454/1605 [21:57<02:08,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1455/1605 [21:58<02:07,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1605 [21:59<02:06,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1457/1605 [22:00<02:05,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1458/1605 [22:01<02:06,  1.16it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1605 [22:01<02:04,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1460/1605 [22:02<02:04,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1461/1605 [22:03<02:02,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1462/1605 [22:04<02:01,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1463/1605 [22:05<02:00,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1464/1605 [22:06<01:59,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1465/1605 [22:07<02:00,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1605 [22:07<01:58,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1605 [22:08<01:57,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1605 [22:09<01:56,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1469/1605 [22:10<01:55,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1605 [22:11<01:54,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1471/1605 [22:12<01:54,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1605 [22:12<01:52,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1473/1605 [22:13<01:51,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1474/1605 [22:14<01:50,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1475/1605 [22:15<01:50,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1476/1605 [22:16<01:49,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1605 [22:17<01:48,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1478/1605 [22:18<01:48,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1479/1605 [22:18<01:47,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1480/1605 [22:19<01:48,  1.15it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1481/1605 [22:20<01:46,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1482/1605 [22:21<01:45,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1483/1605 [22:22<01:45,  1.16it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1484/1605 [22:23<01:43,  1.16it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1605 [22:24<01:42,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1486/1605 [22:24<01:41,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1487/1605 [22:25<01:41,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1488/1605 [22:26<01:40,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1489/1605 [22:27<01:39,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1490/1605 [22:28<01:37,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1491/1605 [22:29<01:36,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1492/1605 [22:30<01:35,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1493/1605 [22:30<01:34,  1.19it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1494/1605 [22:31<01:34,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| {'loss': 0.5751, 'learning_rate': 3.2710280373831774e-06, 'epoch': 2.8}
1495/1605 [22:32<01:33,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1496/1605 [22:33<01:32,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1497/1605 [22:34<01:31,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1498/1605 [22:35<01:30,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1605 [22:35<01:29,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1500/1605 [22:36<01:29,  1.18it/s]                                                    93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1500/1605 [22:37<01:29,  1.18it/s][INFO|trainer.py:2985] 2024-02-13 12:50:38,459 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1500
[INFO|configuration_utils.py:473] 2024-02-13 12:50:38,474 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 12:50:52,656 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 12:50:52,658 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 12:50:52,659 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tmp-checkpoint-1500/special_tokens_map.json
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1501/1605 [23:17<21:56, 12.66s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1502/1605 [23:17<15:38,  9.11s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1503/1605 [23:18<11:16,  6.64s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1504/1605 [23:19<08:15,  4.90s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1505/1605 [23:20<06:08,  3.68s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1506/1605 [23:21<04:40,  2.83s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1507/1605 [23:22<03:38,  2.23s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1508/1605 [23:22<02:56,  1.82s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1509/1605 [23:23<02:26,  1.53s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1510/1605 [23:24<02:05,  1.33s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1511/1605 [23:25<01:51,  1.18s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1512/1605 [23:26<01:40,  1.08s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1513/1605 [23:27<01:32,  1.01s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1514/1605 [23:28<01:27,  1.04it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1515/1605 [23:28<01:23,  1.07it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1605 [23:29<01:20,  1.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1517/1605 [23:30<01:18,  1.13it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1518/1605 [23:31<01:16,  1.14it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1519/1605 [23:32<01:14,  1.15it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1520/1605 [23:33<01:13,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1521/1605 [23:34<01:12,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1522/1605 [23:34<01:11,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1523/1605 [23:35<01:10,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1524/1605 [23:36<01:09,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1525/1605 [23:37<01:08,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1526/1605 [23:38<01:07,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1527/1605 [23:39<01:06,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1528/1605 [23:39<01:05,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1529/1605 [23:40<01:04,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1605 [23:41<01:03,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1531/1605 [23:42<01:03,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1532/1605 [23:43<01:02,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1533/1605 [23:44<01:01,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1534/1605 [23:45<01:00,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1535/1605 [23:45<00:59,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1536/1605 [23:46<00:58,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1537/1605 [23:47<00:58,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1538/1605 [23:48<00:56,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/1605 [23:49<00:56,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1540/1605 [23:50<00:55,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1541/1605 [23:51<00:54,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1542/1605 [23:51<00:53,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1543/1605 [23:52<00:52,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1544/1605 [23:53<00:51,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1545/1605 [23:54<00:51,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1605 [23:55<00:50,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1605 [23:56<00:49,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1605 [23:57<00:48,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1549/1605 [23:57<00:47,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1550/1605 [23:58<00:47,  1.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1551/1605 [23:59<00:46,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1605 [24:00<00:45,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1553/1605 [24:01<00:44,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1554/1605 [24:02<00:43,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1555/1605 [24:03<00:42,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1556/1605 [24:03<00:41,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1557/1605 [24:04<00:40,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1558/1605 [24:05<00:40,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1559/1605 [24:06<00:39,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1560/1605 [24:07<00:38,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1561/1605 [24:08<00:37,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1562/1605 [24:09<00:36,  1.16it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1563/1605 [24:09<00:35,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1564/1605 [24:10<00:34,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1565/1605 [24:11<00:33,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1566/1605 [24:12<00:33,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1567/1605 [24:13<00:32,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1568/1605 [24:14<00:31,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1569/1605 [24:14<00:30,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1605 [24:15<00:29,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1571/1605 [24:16<00:29,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1605 [24:17<00:28,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1573/1605 [24:18<00:27,  1.16it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1574/1605 [24:19<00:26,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1575/1605 [24:20<00:25,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1576/1605 [24:20<00:24,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1577/1605 [24:21<00:23,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1578/1605 [24:22<00:23,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1579/1605 [24:23<00:22,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1580/1605 [24:24<00:21,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1581/1605 [24:25<00:20,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1582/1605 [24:26<00:19,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1583/1605 [24:26<00:18,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1584/1605 [24:27<00:17,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1585/1605 [24:28<00:17,  1.16it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1586/1605 [24:29<00:16,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1587/1605 [24:30<00:15,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1588/1605 [24:31<00:14,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1589/1605 [24:32<00:13,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1590/1605 [24:32<00:12,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1591/1605 [24:33<00:11,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1592/1605 [24:34<00:11,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1605 [24:35<00:10,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1594/1605 [24:36<00:09,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1595/1605 [24:37<00:08,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1596/1605 [24:37<00:07,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1597/1605 [24:38<00:06,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1598/1605 [24:39<00:05,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1599/1605 [24:40<00:05,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1600/1605 [24:41<00:04,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1601/1605 [24:42<00:03,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1602/1605 [24:43<00:02,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1603/1605 [24:43<00:01,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1604/1605 [24:44<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1605/1605 [24:45<00:00,  1.17it/s][INFO|trainer.py:1988] 2024-02-13 12:52:47,114 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1485.855, 'train_samples_per_second': 17.265, 'train_steps_per_second': 1.08, 'train_loss': 0.6298493453646746, 'epoch': 3.0}
                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1605/1605 [24:45<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1605/1605 [24:45<00:00,  1.08it/s]
[INFO|trainer.py:2985] 2024-02-13 12:52:47,274 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola
[INFO|configuration_utils.py:473] 2024-02-13 12:52:47,276 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 12:53:00,920 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 12:53:00,922 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 12:53:00,923 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.6298
  train_runtime            = 0:24:45.85
  train_samples            =       8551
  train_samples_per_second =     17.265
  train_steps_per_second   =       1.08
02/13/2024 12:53:00 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 12:53:00,993 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 12:53:00,995 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 12:53:00,995 >>   Num examples = 1043
[INFO|trainer.py:3296] 2024-02-13 12:53:00,995 >>   Batch size = 8
  0%|          | 0/66 [00:00<?, ?it/s]  3%|â–Ž         | 2/66 [00:00<00:08,  8.00it/s]  5%|â–         | 3/66 [00:00<00:11,  5.61it/s]  6%|â–Œ         | 4/66 [00:00<00:12,  4.89it/s]  8%|â–Š         | 5/66 [00:01<00:13,  4.46it/s]  9%|â–‰         | 6/66 [00:01<00:14,  4.28it/s] 11%|â–ˆ         | 7/66 [00:01<00:14,  4.12it/s] 12%|â–ˆâ–        | 8/66 [00:01<00:14,  4.01it/s] 14%|â–ˆâ–Ž        | 9/66 [00:02<00:14,  4.01it/s] 15%|â–ˆâ–Œ        | 10/66 [00:02<00:14,  4.00it/s] 17%|â–ˆâ–‹        | 11/66 [00:02<00:13,  4.08it/s] 18%|â–ˆâ–Š        | 12/66 [00:02<00:13,  4.10it/s] 20%|â–ˆâ–‰        | 13/66 [00:03<00:13,  4.02it/s] 21%|â–ˆâ–ˆ        | 14/66 [00:03<00:13,  4.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 15/66 [00:03<00:12,  3.96it/s] 24%|â–ˆâ–ˆâ–       | 16/66 [00:03<00:12,  4.01it/s] 26%|â–ˆâ–ˆâ–Œ       | 17/66 [00:04<00:12,  3.89it/s] 27%|â–ˆâ–ˆâ–‹       | 18/66 [00:04<00:12,  3.98it/s] 29%|â–ˆâ–ˆâ–‰       | 19/66 [00:04<00:12,  3.83it/s] 30%|â–ˆâ–ˆâ–ˆ       | 20/66 [00:04<00:11,  3.91it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 21/66 [00:05<00:11,  3.83it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/66 [00:05<00:11,  3.88it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 23/66 [00:05<00:10,  3.97it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 24/66 [00:05<00:10,  3.92it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 25/66 [00:06<00:10,  3.88it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 26/66 [00:06<00:10,  3.92it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/66 [00:06<00:09,  3.94it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/66 [00:06<00:09,  3.96it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/66 [00:07<00:09,  3.96it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 30/66 [00:07<00:09,  3.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/66 [00:07<00:08,  3.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32/66 [00:07<00:08,  3.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 33/66 [00:08<00:08,  3.94it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/66 [00:08<00:08,  3.96it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 35/66 [00:08<00:07,  3.95it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 36/66 [00:08<00:07,  3.93it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/66 [00:09<00:07,  3.89it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 38/66 [00:09<00:07,  3.86it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 39/66 [00:09<00:06,  3.91it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 40/66 [00:09<00:06,  3.93it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/66 [00:10<00:06,  4.01it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/66 [00:10<00:06,  3.94it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 43/66 [00:10<00:05,  3.97it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 44/66 [00:10<00:05,  3.90it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 45/66 [00:11<00:05,  3.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 46/66 [00:11<00:05,  3.97it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 47/66 [00:11<00:04,  3.93it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 48/66 [00:11<00:04,  3.89it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 49/66 [00:12<00:04,  3.91it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 50/66 [00:12<00:04,  3.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 51/66 [00:12<00:03,  3.86it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 52/66 [00:13<00:03,  3.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 53/66 [00:13<00:03,  3.95it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/66 [00:13<00:03,  3.89it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 55/66 [00:13<00:02,  3.96it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 56/66 [00:13<00:02,  4.03it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 57/66 [00:14<00:02,  3.90it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 58/66 [00:14<00:02,  3.98it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 59/66 [00:14<00:01,  3.88it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 60/66 [00:15<00:01,  3.87it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 61/66 [00:15<00:01,  3.89it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62/66 [00:15<00:00,  4.01it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 63/66 [00:15<00:00,  4.04it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 64/66 [00:16<00:00,  4.02it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 65/66 [00:16<00:00,  3.90it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:16<00:00,  4.02it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:16<00:00,  3.97it/s]
***** eval metrics *****
  epoch                     =        3.0
  eval_loss                 =     0.6246
  eval_matthews_correlation =     0.0991
  eval_runtime              = 0:00:16.89
  eval_samples              =       1043
  eval_samples_per_second   =     61.748
  eval_steps_per_second     =      3.907
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/13/2024 13:03:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/13/2024 13:03:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/runs/Feb13_13-03-29_v007.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
02/13/2024 13:03:30 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Overwrite dataset info from restored data version if exists.
02/13/2024 13:03:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 13:03:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/13/2024 13:03:31 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/13/2024 13:03:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-13 13:03:32,308 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-13 13:03:32,310 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[WARNING|logging.py:314] 2024-02-13 13:03:32,630 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_utils_base.py:2029] 2024-02-13 13:03:32,710 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-13 13:03:32,711 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 13:03:32,711 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 13:03:32,711 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-13 13:03:32,711 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-13 13:03:32,774 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-13 13:03:32,837 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-13 13:03:55,444 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-13 13:03:55,445 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[WARNING|modeling_utils.py:3996] 2024-02-13 13:03:55,458 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b7dc2e0193cfa0c7.arrow
02/13/2024 13:03:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b7dc2e0193cfa0c7.arrow
Running tokenizer on dataset:   1%|â–         | 1000/67349 [00:00<00:10, 6156.19 examples/s]Running tokenizer on dataset:   3%|â–Ž         | 2000/67349 [00:00<00:12, 5161.52 examples/s]Running tokenizer on dataset:   6%|â–Œ         | 4000/67349 [00:00<00:07, 8152.85 examples/s]Running tokenizer on dataset:   9%|â–‰         | 6000/67349 [00:00<00:06, 9605.37 examples/s]Running tokenizer on dataset:  12%|â–ˆâ–        | 8000/67349 [00:00<00:05, 10522.36 examples/s]Running tokenizer on dataset:  15%|â–ˆâ–        | 10000/67349 [00:01<00:05, 10989.27 examples/s]Running tokenizer on dataset:  18%|â–ˆâ–Š        | 12000/67349 [00:01<00:04, 11304.25 examples/s]Running tokenizer on dataset:  21%|â–ˆâ–ˆ        | 14000/67349 [00:01<00:04, 11514.92 examples/s]Running tokenizer on dataset:  24%|â–ˆâ–ˆâ–       | 16000/67349 [00:01<00:04, 11674.88 examples/s]Running tokenizer on dataset:  27%|â–ˆâ–ˆâ–‹       | 18000/67349 [00:01<00:04, 11715.34 examples/s]Running tokenizer on dataset:  30%|â–ˆâ–ˆâ–‰       | 20000/67349 [00:01<00:03, 11859.32 examples/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22000/67349 [00:02<00:03, 11937.68 examples/s]Running tokenizer on dataset:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24000/67349 [00:02<00:03, 11872.72 examples/s]Running tokenizer on dataset:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 26000/67349 [00:02<00:04, 9967.85 examples/s] Running tokenizer on dataset:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28000/67349 [00:02<00:03, 10458.65 examples/s]Running tokenizer on dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30000/67349 [00:02<00:03, 10930.56 examples/s]Running tokenizer on dataset:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32000/67349 [00:02<00:03, 11284.27 examples/s]Running tokenizer on dataset:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34000/67349 [00:03<00:02, 11444.95 examples/s]Running tokenizer on dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36000/67349 [00:03<00:02, 11605.74 examples/s]Running tokenizer on dataset:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38000/67349 [00:03<00:02, 11766.76 examples/s]Running tokenizer on dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40000/67349 [00:03<00:02, 11843.92 examples/s]Running tokenizer on dataset:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42000/67349 [00:03<00:02, 11902.81 examples/s]Running tokenizer on dataset:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44000/67349 [00:03<00:01, 12001.97 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46000/67349 [00:04<00:01, 11980.60 examples/s]Running tokenizer on dataset:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48000/67349 [00:04<00:01, 12010.48 examples/s]Running tokenizer on dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50000/67349 [00:04<00:01, 9979.28 examples/s] Running tokenizer on dataset:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 52000/67349 [00:04<00:01, 10525.37 examples/s]Running tokenizer on dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54000/67349 [00:04<00:01, 10940.96 examples/s]Running tokenizer on dataset:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56000/67349 [00:05<00:01, 11218.80 examples/s]Running tokenizer on dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 58000/67349 [00:05<00:00, 11437.18 examples/s]Running tokenizer on dataset:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60000/67349 [00:05<00:00, 11599.69 examples/s]Running tokenizer on dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 62000/67349 [00:05<00:00, 11718.21 examples/s]Running tokenizer on dataset:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64000/67349 [00:05<00:00, 11830.98 examples/s]Running tokenizer on dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66000/67349 [00:05<00:00, 11884.75 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67349/67349 [00:06<00:00, 11743.57 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67349/67349 [00:06<00:00, 11145.19 examples/s]
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c44c8f7f49b19f7.arrow
02/13/2024 13:04:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c44c8f7f49b19f7.arrow
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 8214.28 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7823.29 examples/s]
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5b6d49d1dfefaa8f.arrow
02/13/2024 13:04:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5b6d49d1dfefaa8f.arrow
Running tokenizer on dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1000/1821 [00:00<00:00, 7454.22 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1821/1821 [00:00<00:00, 8029.85 examples/s]
02/13/2024 13:04:02 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'moving , and adventurous ', 'label': 1, 'idx': 1824, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 8401, 1919, 322, 17623, 332, 681, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 13:04:02 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'of our most flamboyant female comics ', 'label': 1, 'idx': 409, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 310, 1749, 1556, 1652, 314, 19415, 424, 12944, 419, 1199, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/13/2024 13:04:02 - INFO - __main__ - Sample 4506 of the training set: {'sentence': "'n safe as to often play like a milquetoast movie of the week blown up for the big screen . ", 'label': 0, 'idx': 4506, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 525, 29876, 9109, 408, 304, 4049, 1708, 763, 263, 2316, 339, 10896, 579, 14064, 310, 278, 4723, 13031, 29876, 701, 363, 278, 4802, 4315, 869, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7620.83 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [00:00<00:00, 7284.71 examples/s]
02/13/2024 13:04:03 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-13 13:04:05,829 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-13 13:04:06,243 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-13 13:04:06,244 >>   Num examples = 10,000
[INFO|trainer.py:1749] 2024-02-13 13:04:06,244 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-13 13:04:06,244 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-13 13:04:06,244 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1754] 2024-02-13 13:04:06,244 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-13 13:04:06,244 >>   Total optimization steps = 1,875
[INFO|trainer.py:1756] 2024-02-13 13:04:06,245 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/1875 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/1875 [00:02<1:20:41,  2.58s/it]  0%|          | 2/1875 [00:03<47:12,  1.51s/it]    0%|          | 3/1875 [00:04<37:55,  1.22s/it]  0%|          | 4/1875 [00:05<33:17,  1.07s/it]  0%|          | 5/1875 [00:05<30:58,  1.01it/s]  0%|          | 6/1875 [00:06<29:18,  1.06it/s]  0%|          | 7/1875 [00:07<28:30,  1.09it/s]  0%|          | 8/1875 [00:08<27:58,  1.11it/s]  0%|          | 9/1875 [00:09<27:23,  1.14it/s]  1%|          | 10/1875 [00:10<27:06,  1.15it/s]  1%|          | 11/1875 [00:11<26:48,  1.16it/s]  1%|          | 12/1875 [00:11<26:51,  1.16it/s]  1%|          | 13/1875 [00:12<26:45,  1.16it/s]  1%|          | 14/1875 [00:13<26:35,  1.17it/s]  1%|          | 15/1875 [00:14<26:39,  1.16it/s]  1%|          | 16/1875 [00:15<26:32,  1.17it/s]  1%|          | 17/1875 [00:16<26:25,  1.17it/s]  1%|          | 18/1875 [00:16<26:18,  1.18it/s]  1%|          | 19/1875 [00:17<26:22,  1.17it/s]  1%|          | 20/1875 [00:18<26:23,  1.17it/s]  1%|          | 21/1875 [00:19<26:12,  1.18it/s]  1%|          | 22/1875 [00:20<26:20,  1.17it/s]  1%|          | 23/1875 [00:21<26:17,  1.17it/s]  1%|â–         | 24/1875 [00:22<26:06,  1.18it/s]  1%|â–         | 25/1875 [00:22<26:10,  1.18it/s]  1%|â–         | 26/1875 [00:23<26:15,  1.17it/s]  1%|â–         | 27/1875 [00:24<26:15,  1.17it/s]  1%|â–         | 28/1875 [00:25<26:15,  1.17it/s]  2%|â–         | 29/1875 [00:26<26:15,  1.17it/s]  2%|â–         | 30/1875 [00:27<26:10,  1.17it/s]  2%|â–         | 31/1875 [00:28<26:06,  1.18it/s]  2%|â–         | 32/1875 [00:28<26:07,  1.18it/s]  2%|â–         | 33/1875 [00:29<26:06,  1.18it/s]  2%|â–         | 34/1875 [00:30<26:26,  1.16it/s]  2%|â–         | 35/1875 [00:31<26:25,  1.16it/s]  2%|â–         | 36/1875 [00:32<26:20,  1.16it/s]  2%|â–         | 37/1875 [00:33<26:15,  1.17it/s]  2%|â–         | 38/1875 [00:34<26:12,  1.17it/s]  2%|â–         | 39/1875 [00:34<26:14,  1.17it/s]  2%|â–         | 40/1875 [00:35<26:11,  1.17it/s]  2%|â–         | 41/1875 [00:36<26:08,  1.17it/s]  2%|â–         | 42/1875 [00:37<26:05,  1.17it/s]  2%|â–         | 43/1875 [00:38<26:04,  1.17it/s]  2%|â–         | 44/1875 [00:39<26:01,  1.17it/s]  2%|â–         | 45/1875 [00:40<25:58,  1.17it/s]  2%|â–         | 46/1875 [00:40<26:15,  1.16it/s]  3%|â–Ž         | 47/1875 [00:41<26:01,  1.17it/s]  3%|â–Ž         | 48/1875 [00:42<26:02,  1.17it/s]  3%|â–Ž         | 49/1875 [00:43<25:53,  1.18it/s]  3%|â–Ž         | 50/1875 [00:44<26:10,  1.16it/s]  3%|â–Ž         | 51/1875 [00:45<25:57,  1.17it/s]  3%|â–Ž         | 52/1875 [00:46<25:58,  1.17it/s]  3%|â–Ž         | 53/1875 [00:46<26:05,  1.16it/s]  3%|â–Ž         | 54/1875 [00:47<25:58,  1.17it/s]  3%|â–Ž         | 55/1875 [00:48<25:51,  1.17it/s]  3%|â–Ž         | 56/1875 [00:49<25:48,  1.17it/s]  3%|â–Ž         | 57/1875 [00:50<25:56,  1.17it/s]  3%|â–Ž         | 58/1875 [00:51<25:51,  1.17it/s]  3%|â–Ž         | 59/1875 [00:52<25:48,  1.17it/s]  3%|â–Ž         | 60/1875 [00:52<25:47,  1.17it/s]  3%|â–Ž         | 61/1875 [00:53<25:55,  1.17it/s]  3%|â–Ž         | 62/1875 [00:54<25:42,  1.18it/s]  3%|â–Ž         | 63/1875 [00:55<25:46,  1.17it/s]  3%|â–Ž         | 64/1875 [00:56<25:38,  1.18it/s]  3%|â–Ž         | 65/1875 [00:57<25:32,  1.18it/s]  4%|â–Ž         | 66/1875 [00:57<25:40,  1.17it/s]  4%|â–Ž         | 67/1875 [00:58<25:42,  1.17it/s]  4%|â–Ž         | 68/1875 [00:59<25:43,  1.17it/s]  4%|â–Ž         | 69/1875 [01:00<25:50,  1.16it/s]  4%|â–Ž         | 70/1875 [01:01<25:43,  1.17it/s]  4%|â–         | 71/1875 [01:02<25:36,  1.17it/s]  4%|â–         | 72/1875 [01:03<25:40,  1.17it/s]  4%|â–         | 73/1875 [01:03<25:43,  1.17it/s]  4%|â–         | 74/1875 [01:04<25:45,  1.17it/s]  4%|â–         | 75/1875 [01:05<25:32,  1.17it/s]  4%|â–         | 76/1875 [01:06<25:24,  1.18it/s]  4%|â–         | 77/1875 [01:07<25:29,  1.18it/s]  4%|â–         | 78/1875 [01:08<25:31,  1.17it/s]  4%|â–         | 79/1875 [01:09<25:28,  1.17it/s]  4%|â–         | 80/1875 [01:09<25:31,  1.17it/s]  4%|â–         | 81/1875 [01:10<25:30,  1.17it/s]  4%|â–         | 82/1875 [01:11<25:40,  1.16it/s]  4%|â–         | 83/1875 [01:12<25:32,  1.17it/s]  4%|â–         | 84/1875 [01:13<25:33,  1.17it/s]  5%|â–         | 85/1875 [01:14<25:32,  1.17it/s]  5%|â–         | 86/1875 [01:15<25:38,  1.16it/s]  5%|â–         | 87/1875 [01:15<25:27,  1.17it/s]  5%|â–         | 88/1875 [01:16<25:30,  1.17it/s]  5%|â–         | 89/1875 [01:17<25:24,  1.17it/s]  5%|â–         | 90/1875 [01:18<25:26,  1.17it/s]  5%|â–         | 91/1875 [01:19<25:19,  1.17it/s]  5%|â–         | 92/1875 [01:20<25:23,  1.17it/s]  5%|â–         | 93/1875 [01:21<25:16,  1.18it/s]  5%|â–Œ         | 94/1875 [01:21<25:17,  1.17it/s]  5%|â–Œ         | 95/1875 [01:22<25:18,  1.17it/s]  5%|â–Œ         | 96/1875 [01:23<25:16,  1.17it/s]  5%|â–Œ         | 97/1875 [01:24<25:12,  1.18it/s]  5%|â–Œ         | 98/1875 [01:25<25:07,  1.18it/s]  5%|â–Œ         | 99/1875 [01:26<25:05,  1.18it/s]  5%|â–Œ         | 100/1875 [01:26<25:12,  1.17it/s]  5%|â–Œ         | 101/1875 [01:27<25:16,  1.17it/s]  5%|â–Œ         | 102/1875 [01:28<25:20,  1.17it/s]  5%|â–Œ         | 103/1875 [01:29<25:33,  1.16it/s]  6%|â–Œ         | 104/1875 [01:30<25:15,  1.17it/s]  6%|â–Œ         | 105/1875 [01:31<25:24,  1.16it/s]  6%|â–Œ         | 106/1875 [01:32<25:22,  1.16it/s]  6%|â–Œ         | 107/1875 [01:33<25:21,  1.16it/s]  6%|â–Œ         | 108/1875 [01:33<25:08,  1.17it/s]  6%|â–Œ         | 109/1875 [01:34<25:17,  1.16it/s]  6%|â–Œ         | 110/1875 [01:35<25:16,  1.16it/s]  6%|â–Œ         | 111/1875 [01:36<25:13,  1.17it/s]  6%|â–Œ         | 112/1875 [01:37<25:16,  1.16it/s]  6%|â–Œ         | 113/1875 [01:38<25:15,  1.16it/s]  6%|â–Œ         | 114/1875 [01:39<25:15,  1.16it/s]  6%|â–Œ         | 115/1875 [01:39<25:14,  1.16it/s]  6%|â–Œ         | 116/1875 [01:40<25:04,  1.17it/s]  6%|â–Œ         | 117/1875 [01:41<25:04,  1.17it/s]  6%|â–‹         | 118/1875 [01:42<25:05,  1.17it/s]  6%|â–‹         | 119/1875 [01:43<25:10,  1.16it/s]  6%|â–‹         | 120/1875 [01:44<24:55,  1.17it/s]  6%|â–‹         | 121/1875 [01:45<24:54,  1.17it/s]  7%|â–‹         | 122/1875 [01:45<24:52,  1.17it/s]  7%|â–‹         | 123/1875 [01:46<24:49,  1.18it/s]  7%|â–‹         | 124/1875 [01:47<24:56,  1.17it/s]  7%|â–‹         | 125/1875 [01:48<25:05,  1.16it/s]  7%|â–‹         | 126/1875 [01:49<24:50,  1.17it/s]  7%|â–‹         | 127/1875 [01:50<24:55,  1.17it/s]  7%|â–‹         | 128/1875 [01:51<24:58,  1.17it/s]  7%|â–‹         | 129/1875 [01:51<24:46,  1.17it/s]  7%|â–‹         | 130/1875 [01:52<24:45,  1.17it/s]  7%|â–‹         | 131/1875 [01:53<24:50,  1.17it/s]  7%|â–‹         | 132/1875 [01:54<24:48,  1.17it/s]  7%|â–‹         | 133/1875 [01:55<24:43,  1.17it/s]  7%|â–‹         | 134/1875 [01:56<24:52,  1.17it/s]  7%|â–‹         | 135/1875 [01:56<24:37,  1.18it/s]  7%|â–‹         | 136/1875 [01:57<24:48,  1.17it/s]  7%|â–‹         | 137/1875 [01:58<24:50,  1.17it/s]  7%|â–‹         | 138/1875 [01:59<24:42,  1.17it/s]  7%|â–‹         | 139/1875 [02:00<24:34,  1.18it/s]  7%|â–‹         | 140/1875 [02:01<24:32,  1.18it/s]  8%|â–Š         | 141/1875 [02:02<24:36,  1.17it/s]  8%|â–Š         | 142/1875 [02:02<24:41,  1.17it/s]  8%|â–Š         | 143/1875 [02:03<24:44,  1.17it/s]  8%|â–Š         | 144/1875 [02:04<24:39,  1.17it/s]  8%|â–Š         | 145/1875 [02:05<24:32,  1.17it/s]  8%|â–Š         | 146/1875 [02:06<24:34,  1.17it/s]  8%|â–Š         | 147/1875 [02:07<24:49,  1.16it/s]  8%|â–Š         | 148/1875 [02:08<24:42,  1.17it/s]  8%|â–Š         | 149/1875 [02:08<24:40,  1.17it/s]  8%|â–Š         | 150/1875 [02:09<24:33,  1.17it/s]  8%|â–Š         | 151/1875 [02:10<24:24,  1.18it/s]  8%|â–Š         | 152/1875 [02:11<24:26,  1.17it/s]  8%|â–Š         | 153/1875 [02:12<24:27,  1.17it/s]  8%|â–Š         | 154/1875 [02:13<24:25,  1.17it/s]  8%|â–Š         | 155/1875 [02:14<24:34,  1.17it/s]  8%|â–Š         | 156/1875 [02:14<24:29,  1.17it/s]  8%|â–Š         | 157/1875 [02:15<24:25,  1.17it/s]  8%|â–Š         | 158/1875 [02:16<24:18,  1.18it/s]  8%|â–Š         | 159/1875 [02:17<24:21,  1.17it/s]  9%|â–Š         | 160/1875 [02:18<24:17,  1.18it/s]  9%|â–Š         | 161/1875 [02:19<24:34,  1.16it/s]  9%|â–Š         | 162/1875 [02:20<24:18,  1.17it/s]  9%|â–Š         | 163/1875 [02:20<24:26,  1.17it/s]  9%|â–Š         | 164/1875 [02:21<24:18,  1.17it/s]  9%|â–‰         | 165/1875 [02:22<24:08,  1.18it/s]  9%|â–‰         | 166/1875 [02:23<24:15,  1.17it/s]  9%|â–‰         | 167/1875 [02:24<24:17,  1.17it/s]  9%|â–‰         | 168/1875 [02:25<24:16,  1.17it/s]  9%|â–‰         | 169/1875 [02:25<24:15,  1.17it/s]  9%|â–‰         | 170/1875 [02:26<24:19,  1.17it/s]  9%|â–‰         | 171/1875 [02:27<24:13,  1.17it/s]  9%|â–‰         | 172/1875 [02:28<24:15,  1.17it/s]  9%|â–‰         | 173/1875 [02:29<24:15,  1.17it/s]  9%|â–‰         | 174/1875 [02:30<24:16,  1.17it/s]  9%|â–‰         | 175/1875 [02:31<24:12,  1.17it/s]  9%|â–‰         | 176/1875 [02:31<24:10,  1.17it/s]  9%|â–‰         | 177/1875 [02:32<24:00,  1.18it/s]  9%|â–‰         | 178/1875 [02:33<23:58,  1.18it/s] 10%|â–‰         | 179/1875 [02:34<24:15,  1.17it/s] 10%|â–‰         | 180/1875 [02:35<24:05,  1.17it/s] 10%|â–‰         | 181/1875 [02:36<24:03,  1.17it/s] 10%|â–‰         | 182/1875 [02:37<24:04,  1.17it/s] 10%|â–‰         | 183/1875 [02:37<24:00,  1.17it/s] 10%|â–‰         | 184/1875 [02:38<24:04,  1.17it/s] 10%|â–‰         | 185/1875 [02:39<23:54,  1.18it/s] 10%|â–‰         | 186/1875 [02:40<23:54,  1.18it/s] 10%|â–‰         | 187/1875 [02:41<24:03,  1.17it/s] 10%|â–ˆ         | 188/1875 [02:42<23:59,  1.17it/s] 10%|â–ˆ         | 189/1875 [02:43<23:54,  1.17it/s] 10%|â–ˆ         | 190/1875 [02:43<23:51,  1.18it/s] 10%|â–ˆ         | 191/1875 [02:44<24:01,  1.17it/s] 10%|â–ˆ         | 192/1875 [02:45<23:56,  1.17it/s] 10%|â–ˆ         | 193/1875 [02:46<23:49,  1.18it/s] 10%|â–ˆ         | 194/1875 [02:47<23:50,  1.18it/s] 10%|â–ˆ         | 195/1875 [02:48<23:48,  1.18it/s] 10%|â–ˆ         | 196/1875 [02:49<23:49,  1.17it/s] 11%|â–ˆ         | 197/1875 [02:49<23:49,  1.17it/s] 11%|â–ˆ         | 198/1875 [02:50<23:42,  1.18it/s] 11%|â–ˆ         | 199/1875 [02:51<23:45,  1.18it/s] 11%|â–ˆ         | 200/1875 [02:52<23:46,  1.17it/s] 11%|â–ˆ         | 201/1875 [02:53<23:39,  1.18it/s] 11%|â–ˆ         | 202/1875 [02:54<23:48,  1.17it/s] 11%|â–ˆ         | 203/1875 [02:54<23:59,  1.16it/s] 11%|â–ˆ         | 204/1875 [02:55<23:48,  1.17it/s] 11%|â–ˆ         | 205/1875 [02:56<23:50,  1.17it/s] 11%|â–ˆ         | 206/1875 [02:57<23:48,  1.17it/s] 11%|â–ˆ         | 207/1875 [02:58<23:43,  1.17it/s] 11%|â–ˆ         | 208/1875 [02:59<23:43,  1.17it/s] 11%|â–ˆ         | 209/1875 [03:00<23:44,  1.17it/s] 11%|â–ˆ         | 210/1875 [03:00<23:37,  1.17it/s] 11%|â–ˆâ–        | 211/1875 [03:01<23:33,  1.18it/s] 11%|â–ˆâ–        | 212/1875 [03:02<23:42,  1.17it/s] 11%|â–ˆâ–        | 213/1875 [03:03<23:40,  1.17it/s] 11%|â–ˆâ–        | 214/1875 [03:04<23:37,  1.17it/s] 11%|â–ˆâ–        | 215/1875 [03:05<23:35,  1.17it/s] 12%|â–ˆâ–        | 216/1875 [03:06<23:29,  1.18it/s] 12%|â–ˆâ–        | 217/1875 [03:06<23:30,  1.18it/s] 12%|â–ˆâ–        | 218/1875 [03:07<23:33,  1.17it/s] 12%|â–ˆâ–        | 219/1875 [03:08<23:29,  1.17it/s] 12%|â–ˆâ–        | 220/1875 [03:09<23:33,  1.17it/s] 12%|â–ˆâ–        | 221/1875 [03:10<23:32,  1.17it/s] 12%|â–ˆâ–        | 222/1875 [03:11<23:21,  1.18it/s] 12%|â–ˆâ–        | 223/1875 [03:12<23:23,  1.18it/s] 12%|â–ˆâ–        | 224/1875 [03:12<23:17,  1.18it/s] 12%|â–ˆâ–        | 225/1875 [03:13<23:21,  1.18it/s] 12%|â–ˆâ–        | 226/1875 [03:14<23:17,  1.18it/s] 12%|â–ˆâ–        | 227/1875 [03:15<23:14,  1.18it/s] 12%|â–ˆâ–        | 228/1875 [03:16<23:17,  1.18it/s] 12%|â–ˆâ–        | 229/1875 [03:17<23:17,  1.18it/s] 12%|â–ˆâ–        | 230/1875 [03:17<23:18,  1.18it/s] 12%|â–ˆâ–        | 231/1875 [03:18<23:16,  1.18it/s] 12%|â–ˆâ–        | 232/1875 [03:19<23:28,  1.17it/s] 12%|â–ˆâ–        | 233/1875 [03:20<23:20,  1.17it/s] 12%|â–ˆâ–        | 234/1875 [03:21<23:27,  1.17it/s] 13%|â–ˆâ–Ž        | 235/1875 [03:22<23:19,  1.17it/s] 13%|â–ˆâ–Ž        | 236/1875 [03:23<23:18,  1.17it/s] 13%|â–ˆâ–Ž        | 237/1875 [03:23<23:15,  1.17it/s] 13%|â–ˆâ–Ž        | 238/1875 [03:24<23:13,  1.18it/s] 13%|â–ˆâ–Ž        | 239/1875 [03:25<23:16,  1.17it/s] 13%|â–ˆâ–Ž        | 240/1875 [03:26<23:05,  1.18it/s] 13%|â–ˆâ–Ž        | 241/1875 [03:27<23:12,  1.17it/s] 13%|â–ˆâ–Ž        | 242/1875 [03:28<23:16,  1.17it/s] 13%|â–ˆâ–Ž        | 243/1875 [03:29<23:17,  1.17it/s] 13%|â–ˆâ–Ž        | 244/1875 [03:29<23:12,  1.17it/s] 13%|â–ˆâ–Ž        | 245/1875 [03:30<23:16,  1.17it/s] 13%|â–ˆâ–Ž        | 246/1875 [03:31<23:02,  1.18it/s] 13%|â–ˆâ–Ž        | 247/1875 [03:32<23:14,  1.17it/s] 13%|â–ˆâ–Ž        | 248/1875 [03:33<23:14,  1.17it/s] 13%|â–ˆâ–Ž        | 249/1875 [03:34<23:03,  1.18it/s] 13%|â–ˆâ–Ž        | 250/1875 [03:35<23:05,  1.17it/s] 13%|â–ˆâ–Ž        | 251/1875 [03:35<23:04,  1.17it/s] 13%|â–ˆâ–Ž        | 252/1875 [03:36<23:08,  1.17it/s] 13%|â–ˆâ–Ž        | 253/1875 [03:37<23:04,  1.17it/s] 14%|â–ˆâ–Ž        | 254/1875 [03:38<22:58,  1.18it/s] 14%|â–ˆâ–Ž        | 255/1875 [03:39<23:13,  1.16it/s] 14%|â–ˆâ–Ž        | 256/1875 [03:40<23:04,  1.17it/s] 14%|â–ˆâ–Ž        | 257/1875 [03:41<23:01,  1.17it/s] 14%|â–ˆâ–        | 258/1875 [03:41<23:01,  1.17it/s] 14%|â–ˆâ–        | 259/1875 [03:42<23:04,  1.17it/s] 14%|â–ˆâ–        | 260/1875 [03:43<23:01,  1.17it/s] 14%|â–ˆâ–        | 261/1875 [03:44<22:57,  1.17it/s] 14%|â–ˆâ–        | 262/1875 [03:45<22:56,  1.17it/s] 14%|â–ˆâ–        | 263/1875 [03:46<22:55,  1.17it/s] 14%|â–ˆâ–        | 264/1875 [03:46<22:49,  1.18it/s] 14%|â–ˆâ–        | 265/1875 [03:47<22:53,  1.17it/s] 14%|â–ˆâ–        | 266/1875 [03:48<23:03,  1.16it/s] 14%|â–ˆâ–        | 267/1875 [03:49<22:54,  1.17it/s] 14%|â–ˆâ–        | 268/1875 [03:50<22:54,  1.17it/s] 14%|â–ˆâ–        | 269/1875 [03:51<23:02,  1.16it/s] 14%|â–ˆâ–        | 270/1875 [03:52<22:45,  1.18it/s] 14%|â–ˆâ–        | 271/1875 [03:52<22:50,  1.17it/s] 15%|â–ˆâ–        | 272/1875 [03:53<22:47,  1.17it/s] 15%|â–ˆâ–        | 273/1875 [03:54<22:45,  1.17it/s] 15%|â–ˆâ–        | 274/1875 [03:55<22:49,  1.17it/s] 15%|â–ˆâ–        | 275/1875 [03:56<22:53,  1.16it/s] 15%|â–ˆâ–        | 276/1875 [03:57<22:48,  1.17it/s] 15%|â–ˆâ–        | 277/1875 [03:58<22:48,  1.17it/s] 15%|â–ˆâ–        | 278/1875 [03:58<22:41,  1.17it/s] 15%|â–ˆâ–        | 279/1875 [03:59<22:30,  1.18it/s] 15%|â–ˆâ–        | 280/1875 [04:00<22:43,  1.17it/s] 15%|â–ˆâ–        | 281/1875 [04:01<22:44,  1.17it/s] 15%|â–ˆâ–Œ        | 282/1875 [04:02<22:41,  1.17it/s] 15%|â–ˆâ–Œ        | 283/1875 [04:03<22:45,  1.17it/s] 15%|â–ˆâ–Œ        | 284/1875 [04:04<22:35,  1.17it/s] 15%|â–ˆâ–Œ        | 285/1875 [04:04<22:28,  1.18it/s] 15%|â–ˆâ–Œ        | 286/1875 [04:05<22:32,  1.18it/s] 15%|â–ˆâ–Œ        | 287/1875 [04:06<22:37,  1.17it/s] 15%|â–ˆâ–Œ        | 288/1875 [04:07<22:32,  1.17it/s] 15%|â–ˆâ–Œ        | 289/1875 [04:08<22:26,  1.18it/s] 15%|â–ˆâ–Œ        | 290/1875 [04:09<22:33,  1.17it/s] 16%|â–ˆâ–Œ        | 291/1875 [04:10<22:35,  1.17it/s] 16%|â–ˆâ–Œ        | 292/1875 [04:10<22:23,  1.18it/s] 16%|â–ˆâ–Œ        | 293/1875 [04:11<22:26,  1.17it/s] 16%|â–ˆâ–Œ        | 294/1875 [04:12<22:27,  1.17it/s] 16%|â–ˆâ–Œ        | 295/1875 [04:13<22:26,  1.17it/s] 16%|â–ˆâ–Œ        | 296/1875 [04:14<22:39,  1.16it/s] 16%|â–ˆâ–Œ        | 297/1875 [04:15<22:30,  1.17it/s] 16%|â–ˆâ–Œ        | 298/1875 [04:16<22:32,  1.17it/s] 16%|â–ˆâ–Œ        | 299/1875 [04:16<22:27,  1.17it/s] 16%|â–ˆâ–Œ        | 300/1875 [04:17<22:27,  1.17it/s] 16%|â–ˆâ–Œ        | 301/1875 [04:18<22:22,  1.17it/s] 16%|â–ˆâ–Œ        | 302/1875 [04:19<22:22,  1.17it/s] 16%|â–ˆâ–Œ        | 303/1875 [04:20<22:19,  1.17it/s] 16%|â–ˆâ–Œ        | 304/1875 [04:21<22:33,  1.16it/s] 16%|â–ˆâ–‹        | 305/1875 [04:22<22:24,  1.17it/s] 16%|â–ˆâ–‹        | 306/1875 [04:22<22:20,  1.17it/s] 16%|â–ˆâ–‹        | 307/1875 [04:23<22:12,  1.18it/s] 16%|â–ˆâ–‹        | 308/1875 [04:24<22:27,  1.16it/s] 16%|â–ˆâ–‹        | 309/1875 [04:25<22:19,  1.17it/s] 17%|â–ˆâ–‹        | 310/1875 [04:26<22:26,  1.16it/s] 17%|â–ˆâ–‹        | 311/1875 [04:27<22:26,  1.16it/s] 17%|â–ˆâ–‹        | 312/1875 [04:28<22:18,  1.17it/s] 17%|â–ˆâ–‹        | 313/1875 [04:28<22:10,  1.17it/s] 17%|â–ˆâ–‹        | 314/1875 [04:29<22:07,  1.18it/s] 17%|â–ˆâ–‹        | 315/1875 [04:30<22:16,  1.17it/s] 17%|â–ˆâ–‹        | 316/1875 [04:31<22:10,  1.17it/s] 17%|â–ˆâ–‹        | 317/1875 [04:32<22:03,  1.18it/s] 17%|â–ˆâ–‹        | 318/1875 [04:33<22:09,  1.17it/s] 17%|â–ˆâ–‹        | 319/1875 [04:33<22:11,  1.17it/s] 17%|â–ˆâ–‹        | 320/1875 [04:34<22:05,  1.17it/s] 17%|â–ˆâ–‹        | 321/1875 [04:35<22:03,  1.17it/s] 17%|â–ˆâ–‹        | 322/1875 [04:36<21:55,  1.18it/s] 17%|â–ˆâ–‹        | 323/1875 [04:37<21:55,  1.18it/s] 17%|â–ˆâ–‹        | 324/1875 [04:38<21:51,  1.18it/s] 17%|â–ˆâ–‹        | 325/1875 [04:39<21:49,  1.18it/s] 17%|â–ˆâ–‹        | 326/1875 [04:39<21:53,  1.18it/s] 17%|â–ˆâ–‹        | 327/1875 [04:40<21:50,  1.18it/s] 17%|â–ˆâ–‹        | 328/1875 [04:41<21:48,  1.18it/s] 18%|â–ˆâ–Š        | 329/1875 [04:42<21:49,  1.18it/s] 18%|â–ˆâ–Š        | 330/1875 [04:43<21:39,  1.19it/s] 18%|â–ˆâ–Š        | 331/1875 [04:44<21:51,  1.18it/s] 18%|â–ˆâ–Š        | 332/1875 [04:44<21:42,  1.18it/s] 18%|â–ˆâ–Š        | 333/1875 [04:45<21:42,  1.18it/s] 18%|â–ˆâ–Š        | 334/1875 [04:46<21:52,  1.17it/s] 18%|â–ˆâ–Š        | 335/1875 [04:47<21:55,  1.17it/s] 18%|â–ˆâ–Š        | 336/1875 [04:48<21:48,  1.18it/s] 18%|â–ˆâ–Š        | 337/1875 [04:49<21:48,  1.18it/s] 18%|â–ˆâ–Š        | 338/1875 [04:50<21:54,  1.17it/s] 18%|â–ˆâ–Š        | 339/1875 [04:50<21:50,  1.17it/s] 18%|â–ˆâ–Š        | 340/1875 [04:51<21:52,  1.17it/s] 18%|â–ˆâ–Š        | 341/1875 [04:52<22:03,  1.16it/s] 18%|â–ˆâ–Š        | 342/1875 [04:53<21:52,  1.17it/s] 18%|â–ˆâ–Š        | 343/1875 [04:54<21:55,  1.16it/s] 18%|â–ˆâ–Š        | 344/1875 [04:55<21:49,  1.17it/s] 18%|â–ˆâ–Š        | 345/1875 [04:56<21:59,  1.16it/s] 18%|â–ˆâ–Š        | 346/1875 [04:56<21:50,  1.17it/s] 19%|â–ˆâ–Š        | 347/1875 [04:57<21:40,  1.18it/s] 19%|â–ˆâ–Š        | 348/1875 [04:58<21:36,  1.18it/s] 19%|â–ˆâ–Š        | 349/1875 [04:59<21:45,  1.17it/s] 19%|â–ˆâ–Š        | 350/1875 [05:00<21:37,  1.18it/s] 19%|â–ˆâ–Š        | 351/1875 [05:01<21:35,  1.18it/s] 19%|â–ˆâ–‰        | 352/1875 [05:02<21:32,  1.18it/s] 19%|â–ˆâ–‰        | 353/1875 [05:02<21:31,  1.18it/s] 19%|â–ˆâ–‰        | 354/1875 [05:03<21:31,  1.18it/s] 19%|â–ˆâ–‰        | 355/1875 [05:04<21:24,  1.18it/s] 19%|â–ˆâ–‰        | 356/1875 [05:05<21:26,  1.18it/s] 19%|â–ˆâ–‰        | 357/1875 [05:06<21:34,  1.17it/s] 19%|â–ˆâ–‰        | 358/1875 [05:07<21:35,  1.17it/s] 19%|â–ˆâ–‰        | 359/1875 [05:08<21:36,  1.17it/s] 19%|â–ˆâ–‰        | 360/1875 [05:08<21:35,  1.17it/s] 19%|â–ˆâ–‰        | 361/1875 [05:09<21:36,  1.17it/s] 19%|â–ˆâ–‰        | 362/1875 [05:10<21:25,  1.18it/s] 19%|â–ˆâ–‰        | 363/1875 [05:11<21:29,  1.17it/s] 19%|â–ˆâ–‰        | 364/1875 [05:12<21:27,  1.17it/s] 19%|â–ˆâ–‰        | 365/1875 [05:13<21:17,  1.18it/s] 20%|â–ˆâ–‰        | 366/1875 [05:13<21:28,  1.17it/s] 20%|â–ˆâ–‰        | 367/1875 [05:14<21:25,  1.17it/s] 20%|â–ˆâ–‰        | 368/1875 [05:15<21:13,  1.18it/s] 20%|â–ˆâ–‰        | 369/1875 [05:16<21:14,  1.18it/s] 20%|â–ˆâ–‰        | 370/1875 [05:17<21:21,  1.17it/s] 20%|â–ˆâ–‰        | 371/1875 [05:18<21:20,  1.17it/s] 20%|â–ˆâ–‰        | 372/1875 [05:19<21:14,  1.18it/s] 20%|â–ˆâ–‰        | 373/1875 [05:19<21:12,  1.18it/s] 20%|â–ˆâ–‰        | 374/1875 [05:20<21:09,  1.18it/s] 20%|â–ˆâ–ˆ        | 375/1875 [05:21<21:08,  1.18it/s] 20%|â–ˆâ–ˆ        | 376/1875 [05:22<21:21,  1.17it/s] 20%|â–ˆâ–ˆ        | 377/1875 [05:23<21:16,  1.17it/s] 20%|â–ˆâ–ˆ        | 378/1875 [05:24<21:18,  1.17it/s] 20%|â–ˆâ–ˆ        | 379/1875 [05:25<21:13,  1.17it/s] 20%|â–ˆâ–ˆ        | 380/1875 [05:25<21:05,  1.18it/s] 20%|â–ˆâ–ˆ        | 381/1875 [05:26<21:13,  1.17it/s] 20%|â–ˆâ–ˆ        | 382/1875 [05:27<21:11,  1.17it/s] 20%|â–ˆâ–ˆ        | 383/1875 [05:28<21:06,  1.18it/s] 20%|â–ˆâ–ˆ        | 384/1875 [05:29<21:01,  1.18it/s] 21%|â–ˆâ–ˆ        | 385/1875 [05:30<21:05,  1.18it/s] 21%|â–ˆâ–ˆ        | 386/1875 [05:30<21:09,  1.17it/s] 21%|â–ˆâ–ˆ        | 387/1875 [05:31<20:58,  1.18it/s] 21%|â–ˆâ–ˆ        | 388/1875 [05:32<21:16,  1.17it/s] 21%|â–ˆâ–ˆ        | 389/1875 [05:33<21:03,  1.18it/s] 21%|â–ˆâ–ˆ        | 390/1875 [05:34<21:02,  1.18it/s] 21%|â–ˆâ–ˆ        | 391/1875 [05:35<21:02,  1.18it/s] 21%|â–ˆâ–ˆ        | 392/1875 [05:36<21:04,  1.17it/s] 21%|â–ˆâ–ˆ        | 393/1875 [05:36<20:58,  1.18it/s] 21%|â–ˆâ–ˆ        | 394/1875 [05:37<20:57,  1.18it/s] 21%|â–ˆâ–ˆ        | 395/1875 [05:38<20:55,  1.18it/s] 21%|â–ˆâ–ˆ        | 396/1875 [05:39<20:55,  1.18it/s] 21%|â–ˆâ–ˆ        | 397/1875 [05:40<20:54,  1.18it/s] 21%|â–ˆâ–ˆ        | 398/1875 [05:41<20:53,  1.18it/s] 21%|â–ˆâ–ˆâ–       | 399/1875 [05:42<20:51,  1.18it/s] 21%|â–ˆâ–ˆâ–       | 400/1875 [05:42<20:59,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 401/1875 [05:43<20:56,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 402/1875 [05:44<21:01,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 403/1875 [05:45<20:56,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 404/1875 [05:46<21:04,  1.16it/s] 22%|â–ˆâ–ˆâ–       | 405/1875 [05:47<21:05,  1.16it/s] 22%|â–ˆâ–ˆâ–       | 406/1875 [05:48<20:51,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 407/1875 [05:48<20:56,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 408/1875 [05:49<20:49,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 409/1875 [05:50<20:50,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 410/1875 [05:51<20:52,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 411/1875 [05:52<20:52,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 412/1875 [05:53<20:42,  1.18it/s] 22%|â–ˆâ–ˆâ–       | 413/1875 [05:53<20:44,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 414/1875 [05:54<20:43,  1.18it/s] 22%|â–ˆâ–ˆâ–       | 415/1875 [05:55<20:38,  1.18it/s] 22%|â–ˆâ–ˆâ–       | 416/1875 [05:56<20:48,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 417/1875 [05:57<20:45,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 418/1875 [05:58<20:38,  1.18it/s] 22%|â–ˆâ–ˆâ–       | 419/1875 [05:59<20:52,  1.16it/s] 22%|â–ˆâ–ˆâ–       | 420/1875 [05:59<20:40,  1.17it/s] 22%|â–ˆâ–ˆâ–       | 421/1875 [06:00<20:37,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 422/1875 [06:01<20:41,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 423/1875 [06:02<20:43,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 424/1875 [06:03<20:51,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 425/1875 [06:04<20:41,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 426/1875 [06:05<20:41,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 427/1875 [06:05<20:37,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 428/1875 [06:06<20:41,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 429/1875 [06:07<20:47,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 430/1875 [06:08<20:40,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 431/1875 [06:09<20:43,  1.16it/s] 23%|â–ˆâ–ˆâ–Ž       | 432/1875 [06:10<20:27,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 433/1875 [06:11<20:30,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 434/1875 [06:11<20:27,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 435/1875 [06:12<20:18,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 436/1875 [06:13<20:21,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 437/1875 [06:14<20:25,  1.17it/s] 23%|â–ˆâ–ˆâ–Ž       | 438/1875 [06:15<20:16,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 439/1875 [06:16<20:18,  1.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 440/1875 [06:17<20:20,  1.18it/s] 24%|â–ˆâ–ˆâ–Ž       | 441/1875 [06:17<20:27,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 442/1875 [06:18<20:23,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 443/1875 [06:19<20:32,  1.16it/s] 24%|â–ˆâ–ˆâ–Ž       | 444/1875 [06:20<20:19,  1.17it/s] 24%|â–ˆâ–ˆâ–Ž       | 445/1875 [06:21<20:35,  1.16it/s] 24%|â–ˆâ–ˆâ–       | 446/1875 [06:22<20:25,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 447/1875 [06:23<20:23,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 448/1875 [06:23<20:18,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 449/1875 [06:24<20:21,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 450/1875 [06:25<20:33,  1.16it/s] 24%|â–ˆâ–ˆâ–       | 451/1875 [06:26<20:24,  1.16it/s] 24%|â–ˆâ–ˆâ–       | 452/1875 [06:27<20:17,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 453/1875 [06:28<20:14,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 454/1875 [06:29<20:12,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 455/1875 [06:29<20:08,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 456/1875 [06:30<20:09,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 457/1875 [06:31<20:08,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 458/1875 [06:32<20:15,  1.17it/s] 24%|â–ˆâ–ˆâ–       | 459/1875 [06:33<20:05,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 460/1875 [06:34<20:07,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 461/1875 [06:35<20:10,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 462/1875 [06:35<20:06,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 463/1875 [06:36<20:07,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 464/1875 [06:37<20:07,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 465/1875 [06:38<20:01,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 466/1875 [06:39<20:05,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 467/1875 [06:40<20:07,  1.17it/s] 25%|â–ˆâ–ˆâ–       | 468/1875 [06:40<19:55,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 469/1875 [06:41<20:05,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 470/1875 [06:42<19:58,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 471/1875 [06:43<19:59,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 472/1875 [06:44<19:54,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 473/1875 [06:45<19:51,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 474/1875 [06:46<19:58,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 475/1875 [06:46<19:49,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 476/1875 [06:47<19:53,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 477/1875 [06:48<19:51,  1.17it/s] 25%|â–ˆâ–ˆâ–Œ       | 478/1875 [06:49<19:50,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 479/1875 [06:50<19:52,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 480/1875 [06:51<19:45,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 481/1875 [06:52<19:43,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 482/1875 [06:52<19:50,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 483/1875 [06:53<19:48,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 484/1875 [06:54<19:45,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 485/1875 [06:55<19:52,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 486/1875 [06:56<19:36,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 487/1875 [06:57<19:41,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 488/1875 [06:58<19:40,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 489/1875 [06:58<19:39,  1.18it/s] 26%|â–ˆâ–ˆâ–Œ       | 490/1875 [06:59<19:38,  1.17it/s] 26%|â–ˆâ–ˆâ–Œ       | 491/1875 [07:00<19:47,  1.17it/s] 2{'loss': 0.5837, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.8}
6%|â–ˆâ–ˆâ–Œ       | 492/1875 [07:01<19:39,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 493/1875 [07:02<19:33,  1.18it/s] 26%|â–ˆâ–ˆâ–‹       | 494/1875 [07:03<19:39,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 495/1875 [07:04<19:40,  1.17it/s] 26%|â–ˆâ–ˆâ–‹       | 496/1875 [07:04<19:25,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 497/1875 [07:05<19:26,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 498/1875 [07:06<19:27,  1.18it/s] 27%|â–ˆâ–ˆâ–‹       | 499/1875 [07:07<19:34,  1.17it/s] 27%|â–ˆâ–ˆâ–‹       | 500/1875 [07:08<19:26,  1.18it/s]                                                   27%|â–ˆâ–ˆâ–‹       | 500/1875 [07:08<19:26,  1.18it/s][INFO|trainer.py:2985] 2024-02-13 13:11:14,715 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500
[INFO|configuration_utils.py:473] 2024-02-13 13:11:14,739 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 13:11:35,816 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 13:11:35,818 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 13:11:35,819 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-500/special_tokens_map.json
 27%|â–ˆâ–ˆâ–‹       | 501/1875 [08:13<7:44:44, 20.29s/it] 27%|â–ˆâ–ˆâ–‹       | 502/1875 [08:14<5:30:56, 14.46s/it] 27%|â–ˆâ–ˆâ–‹       | 503/1875 [08:15<3:57:16, 10.38s/it] 27%|â–ˆâ–ˆâ–‹       | 504/1875 [08:16<2:51:41,  7.51s/it] 27%|â–ˆâ–ˆâ–‹       | 505/1875 [08:17<2:05:57,  5.52s/it] 27%|â–ˆâ–ˆâ–‹       | 506/1875 [08:18<1:33:49,  4.11s/it] 27%|â–ˆâ–ˆâ–‹       | 507/1875 [08:18<1:11:25,  3.13s/it] 27%|â–ˆâ–ˆâ–‹       | 508/1875 [08:19<55:48,  2.45s/it]   27%|â–ˆâ–ˆâ–‹       | 509/1875 [08:20<44:51,  1.97s/it] 27%|â–ˆâ–ˆâ–‹       | 510/1875 [08:21<37:03,  1.63s/it] 27%|â–ˆâ–ˆâ–‹       | 511/1875 [08:22<31:44,  1.40s/it] 27%|â–ˆâ–ˆâ–‹       | 512/1875 [08:23<28:00,  1.23s/it] 27%|â–ˆâ–ˆâ–‹       | 513/1875 [08:24<25:18,  1.11s/it] 27%|â–ˆâ–ˆâ–‹       | 514/1875 [08:24<23:34,  1.04s/it] 27%|â–ˆâ–ˆâ–‹       | 515/1875 [08:25<22:15,  1.02it/s] 28%|â–ˆâ–ˆâ–Š       | 516/1875 [08:26<21:21,  1.06it/s] 28%|â–ˆâ–ˆâ–Š       | 517/1875 [08:27<20:41,  1.09it/s] 28%|â–ˆâ–ˆâ–Š       | 518/1875 [08:28<20:12,  1.12it/s] 28%|â–ˆâ–ˆâ–Š       | 519/1875 [08:29<20:01,  1.13it/s] 28%|â–ˆâ–ˆâ–Š       | 520/1875 [08:30<19:49,  1.14it/s] 28%|â–ˆâ–ˆâ–Š       | 521/1875 [08:30<19:35,  1.15it/s] 28%|â–ˆâ–ˆâ–Š       | 522/1875 [08:31<19:19,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 523/1875 [08:32<19:24,  1.16it/s] 28%|â–ˆâ–ˆâ–Š       | 524/1875 [08:33<19:15,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 525/1875 [08:34<19:07,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 526/1875 [08:35<19:09,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 527/1875 [08:35<19:09,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 528/1875 [08:36<19:09,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 529/1875 [08:37<19:01,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 530/1875 [08:38<19:05,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 531/1875 [08:39<19:02,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 532/1875 [08:40<19:01,  1.18it/s] 28%|â–ˆâ–ˆâ–Š       | 533/1875 [08:41<19:05,  1.17it/s] 28%|â–ˆâ–ˆâ–Š       | 534/1875 [08:41<19:04,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 535/1875 [08:42<19:01,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 536/1875 [08:43<19:01,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 537/1875 [08:44<18:59,  1.17it/s] 29%|â–ˆâ–ˆâ–Š       | 538/1875 [08:45<18:53,  1.18it/s] 29%|â–ˆâ–ˆâ–Š       | 539/1875 [08:46<19:02,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 540/1875 [08:47<18:53,  1.18it/s] 29%|â–ˆâ–ˆâ–‰       | 541/1875 [08:47<19:11,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 542/1875 [08:48<19:01,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 543/1875 [08:49<19:04,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 544/1875 [08:50<19:02,  1.16it/s] 29%|â–ˆâ–ˆâ–‰       | 545/1875 [08:51<18:52,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 546/1875 [08:52<18:57,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 547/1875 [08:53<18:54,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 548/1875 [08:53<18:52,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 549/1875 [08:54<18:46,  1.18it/s] 29%|â–ˆâ–ˆâ–‰       | 550/1875 [08:55<18:50,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 551/1875 [08:56<18:48,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 552/1875 [08:57<18:50,  1.17it/s] 29%|â–ˆâ–ˆâ–‰       | 553/1875 [08:58<18:49,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 554/1875 [08:59<18:47,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 555/1875 [08:59<18:47,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 556/1875 [09:00<18:39,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 557/1875 [09:01<18:43,  1.17it/s] 30%|â–ˆâ–ˆâ–‰       | 558/1875 [09:02<18:36,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 559/1875 [09:03<18:37,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 560/1875 [09:04<18:35,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 561/1875 [09:04<18:33,  1.18it/s] 30%|â–ˆâ–ˆâ–‰       | 562/1875 [09:05<18:39,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 563/1875 [09:06<18:36,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 564/1875 [09:07<18:37,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 565/1875 [09:08<18:34,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 566/1875 [09:09<18:37,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 567/1875 [09:10<18:32,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 568/1875 [09:10<18:35,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 569/1875 [09:11<18:33,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 570/1875 [09:12<18:31,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 571/1875 [09:13<18:43,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 572/1875 [09:14<18:35,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 573/1875 [09:15<18:31,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 574/1875 [09:16<18:34,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 575/1875 [09:16<18:30,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 576/1875 [09:17<18:32,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 577/1875 [09:18<18:24,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 578/1875 [09:19<18:23,  1.18it/s] 31%|â–ˆâ–ˆâ–ˆ       | 579/1875 [09:20<18:31,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 580/1875 [09:21<18:31,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 581/1875 [09:22<18:34,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 582/1875 [09:22<18:25,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 583/1875 [09:23<18:23,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆ       | 584/1875 [09:24<18:28,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆ       | 585/1875 [09:25<18:25,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 586/1875 [09:26<18:21,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 587/1875 [09:27<18:30,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 588/1875 [09:28<18:28,  1.16it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 589/1875 [09:28<18:23,  1.17it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 590/1875 [09:29<18:22,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 591/1875 [09:30<18:19,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 592/1875 [09:31<18:20,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 593/1875 [09:32<18:19,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 594/1875 [09:33<18:17,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 595/1875 [09:34<18:17,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 596/1875 [09:34<18:16,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 597/1875 [09:35<18:13,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 598/1875 [09:36<18:07,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 599/1875 [09:37<18:15,  1.16it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 600/1875 [09:38<18:13,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 601/1875 [09:39<18:06,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 602/1875 [09:40<18:08,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 603/1875 [09:40<18:06,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 604/1875 [09:41<17:58,  1.18it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 605/1875 [09:42<17:59,  1.18it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 606/1875 [09:43<18:08,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 607/1875 [09:44<18:02,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 608/1875 [09:45<17:58,  1.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 609/1875 [09:46<17:56,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 610/1875 [09:46<18:01,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 611/1875 [09:47<18:01,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 612/1875 [09:48<17:56,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 613/1875 [09:49<17:53,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 614/1875 [09:50<17:50,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 615/1875 [09:51<17:50,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 616/1875 [09:51<17:49,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 617/1875 [09:52<18:03,  1.16it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 618/1875 [09:53<17:56,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 619/1875 [09:54<17:53,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 620/1875 [09:55<17:54,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 621/1875 [09:56<17:54,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 622/1875 [09:57<17:54,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 623/1875 [09:57<17:50,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 624/1875 [09:58<17:46,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 625/1875 [09:59<17:43,  1.18it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 626/1875 [10:00<17:46,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 627/1875 [10:01<17:50,  1.17it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 628/1875 [10:02<17:50,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 629/1875 [10:03<17:46,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 630/1875 [10:03<17:42,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 631/1875 [10:04<17:39,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 632/1875 [10:05<17:39,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 633/1875 [10:06<17:49,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 634/1875 [10:07<17:46,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 635/1875 [10:08<17:42,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 636/1875 [10:09<17:36,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 637/1875 [10:09<17:31,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 638/1875 [10:10<17:34,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 639/1875 [10:11<17:32,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 640/1875 [10:12<17:35,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 641/1875 [10:13<17:28,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 642/1875 [10:14<17:29,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 643/1875 [10:15<17:37,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 644/1875 [10:15<17:32,  1.17it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 645/1875 [10:16<17:24,  1.18it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 646/1875 [10:17<17:34,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 647/1875 [10:18<17:26,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 648/1875 [10:19<17:28,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 649/1875 [10:20<17:25,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 650/1875 [10:21<17:22,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 651/1875 [10:21<17:24,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 652/1875 [10:22<17:23,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 653/1875 [10:23<17:23,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 654/1875 [10:24<17:21,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 655/1875 [10:25<17:13,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 656/1875 [10:26<17:17,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 657/1875 [10:26<17:16,  1.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 658/1875 [10:27<17:19,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 659/1875 [10:28<17:21,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 660/1875 [10:29<17:27,  1.16it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 661/1875 [10:30<17:18,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 662/1875 [10:31<17:22,  1.16it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 663/1875 [10:32<17:14,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 664/1875 [10:33<17:16,  1.17it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 665/1875 [10:33<17:14,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 666/1875 [10:34<17:11,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 667/1875 [10:35<17:11,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 668/1875 [10:36<17:17,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 669/1875 [10:37<17:21,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 670/1875 [10:38<17:26,  1.15it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 671/1875 [10:39<17:18,  1.16it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 672/1875 [10:39<17:11,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 673/1875 [10:40<17:08,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 674/1875 [10:41<17:05,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 675/1875 [10:42<16:57,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 676/1875 [10:43<17:00,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 677/1875 [10:44<17:01,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 678/1875 [10:44<16:58,  1.18it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 679/1875 [10:45<16:58,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 680/1875 [10:46<16:59,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 681/1875 [10:47<17:01,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 682/1875 [10:48<16:57,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 683/1875 [10:49<17:00,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 684/1875 [10:50<16:58,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 685/1875 [10:50<16:55,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 686/1875 [10:51<16:48,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 687/1875 [10:52<16:51,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 688/1875 [10:53<16:51,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 689/1875 [10:54<16:52,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 690/1875 [10:55<16:50,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 691/1875 [10:56<16:46,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 692/1875 [10:56<16:47,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 693/1875 [10:57<16:44,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 694/1875 [10:58<16:47,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 695/1875 [10:59<16:47,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 696/1875 [11:00<16:45,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 697/1875 [11:01<16:45,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 698/1875 [11:02<16:42,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 699/1875 [11:02<16:40,  1.18it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 700/1875 [11:03<16:45,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 701/1875 [11:04<16:42,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 702/1875 [11:05<16:44,  1.17it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 703/1875 [11:06<16:46,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 704/1875 [11:07<16:44,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 705/1875 [11:08<16:43,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 706/1875 [11:08<16:37,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 707/1875 [11:09<16:34,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 708/1875 [11:10<16:34,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 709/1875 [11:11<16:34,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 710/1875 [11:12<16:31,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 711/1875 [11:13<16:32,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 712/1875 [11:14<16:43,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 713/1875 [11:14<16:38,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 714/1875 [11:15<16:43,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 715/1875 [11:16<16:39,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 716/1875 [11:17<16:35,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 717/1875 [11:18<16:31,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 718/1875 [11:19<16:37,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 719/1875 [11:20<16:33,  1.16it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 720/1875 [11:20<16:29,  1.17it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 721/1875 [11:21<16:30,  1.16it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 722/1875 [11:22<16:24,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 723/1875 [11:23<16:19,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 724/1875 [11:24<16:17,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 725/1875 [11:25<16:23,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 726/1875 [11:25<16:16,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 727/1875 [11:26<16:19,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 728/1875 [11:27<16:19,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 729/1875 [11:28<16:19,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 730/1875 [11:29<16:19,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 731/1875 [11:30<16:19,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 732/1875 [11:31<16:10,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 733/1875 [11:31<16:15,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 734/1875 [11:32<16:16,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 735/1875 [11:33<16:09,  1.18it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 736/1875 [11:34<16:16,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 737/1875 [11:35<16:12,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 738/1875 [11:36<16:08,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 739/1875 [11:37<16:13,  1.17it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 740/1875 [11:37<16:12,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 741/1875 [11:38<16:04,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 742/1875 [11:39<16:07,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 743/1875 [11:40<16:03,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 744/1875 [11:41<15:58,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 745/1875 [11:42<16:03,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 746/1875 [11:43<15:59,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 747/1875 [11:43<15:56,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 748/1875 [11:44<16:02,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 749/1875 [11:45<16:03,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 750/1875 [11:46<16:01,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 751/1875 [11:47<15:58,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 752/1875 [11:48<15:59,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 753/1875 [11:49<15:54,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 754/1875 [11:49<15:57,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 755/1875 [11:50<15:54,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 756/1875 [11:51<15:54,  1.17it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 757/1875 [11:52<15:49,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 758/1875 [11:53<15:48,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 759/1875 [11:54<15:51,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 760/1875 [11:54<15:49,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 761/1875 [11:55<15:46,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 762/1875 [11:56<15:49,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 763/1875 [11:57<15:52,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 764/1875 [11:58<15:45,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 765/1875 [11:59<15:48,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 766/1875 [12:00<15:47,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 767/1875 [12:00<15:41,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 768/1875 [12:01<15:37,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 769/1875 [12:02<15:41,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 770/1875 [12:03<15:45,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 771/1875 [12:04<15:39,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 772/1875 [12:05<15:37,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 773/1875 [12:06<15:36,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 774/1875 [12:06<15:34,  1.18it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 775/1875 [12:07<15:44,  1.16it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 776/1875 [12:08<15:36,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 777/1875 [12:09<15:34,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 778/1875 [12:10<15:36,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 779/1875 [12:11<15:34,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 780/1875 [12:12<15:31,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 781/1875 [12:12<15:36,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 782/1875 [12:13<15:31,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 783/1875 [12:14<15:30,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 784/1875 [12:15<15:31,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 785/1875 [12:16<15:25,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 786/1875 [12:17<15:26,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 787/1875 [12:17<15:25,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 788/1875 [12:18<15:25,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 789/1875 [12:19<15:24,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 790/1875 [12:20<15:23,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 791/1875 [12:21<15:25,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 792/1875 [12:22<15:17,  1.18it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 793/1875 [12:23<15:24,  1.17it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 794/1875 [12:23<15:30,  1.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 795/1875 [12:24<15:28,  1.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 796/1875 [12:25<15:22,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 797/1875 [12:26<15:20,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 798/1875 [12:27<15:19,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 799/1875 [12:28<15:21,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 800/1875 [12:29<15:22,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 801/1875 [12:29<15:17,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 802/1875 [12:30<15:14,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 803/1875 [12:31<15:12,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 804/1875 [12:32<15:13,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 805/1875 [12:33<15:09,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 806/1875 [12:34<15:13,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 807/1875 [12:35<15:09,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 808/1875 [12:35<15:07,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 809/1875 [12:36<15:03,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 810/1875 [12:37<15:06,  1.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 811/1875 [12:38<15:04,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 812/1875 [12:39<15:01,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 813/1875 [12:40<14:59,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 814/1875 [12:40<14:57,  1.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 815/1875 [12:41<14:56,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 816/1875 [12:42<15:00,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 817/1875 [12:43<15:03,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 818/1875 [12:44<14:59,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 819/1875 [12:45<15:03,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 820/1875 [12:46<14:58,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 821/1875 [12:46<15:03,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 822/1875 [12:47<15:03,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 823/1875 [12:48<14:59,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 824/1875 [12:49<14:55,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 825/1875 [12:50<14:54,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 826/1875 [12:51<14:50,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 827/1875 [12:52<14:53,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 828/1875 [12:52<14:46,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 829/1875 [12:53<14:49,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 830/1875 [12:54<14:46,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 831/1875 [12:55<14:47,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 832/1875 [12:56<14:45,  1.18it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 833/1875 [12:57<14:48,  1.17it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 834/1875 [12:58<14:47,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 835/1875 [12:58<14:45,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 836/1875 [12:59<14:43,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 837/1875 [13:00<14:45,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 838/1875 [13:01<14:45,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 839/1875 [13:02<14:41,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 840/1875 [13:03<14:40,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 841/1875 [13:03<14:36,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 842/1875 [13:04<14:32,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 843/1875 [13:05<14:34,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 844/1875 [13:06<14:35,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 845/1875 [13:07<14:33,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 846/1875 [13:08<14:30,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 847/1875 [13:09<14:34,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 848/1875 [13:09<14:37,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 849/1875 [13:10<14:31,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 850/1875 [13:11<14:28,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 851/1875 [13:12<14:29,  1.18it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 852/1875 [13:13<14:32,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 853/1875 [13:14<14:25,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 854/1875 [13:15<14:29,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 855/1875 [13:15<14:26,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 856/1875 [13:16<14:24,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 857/1875 [13:17<14:24,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 858/1875 [13:18<14:24,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 859/1875 [13:19<14:21,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 860/1875 [13:20<14:23,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 861/1875 [13:20<14:25,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 862/1875 [13:21<14:28,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 863/1875 [13:22<14:24,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 864/1875 [13:23<14:21,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 865/1875 [13:24<14:18,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 866/1875 [13:25<14:13,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 867/1875 [13:26<14:19,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 868/1875 [13:26<14:15,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 869/1875 [13:27<14:12,  1.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 870/1875 [13:28<14:15,  1.17it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 871/1875 [13:29<14:13,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 872/1875 [13:30<14:11,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 873/1875 [13:31<14:11,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 874/1875 [13:32<14:14,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 875/1875 [13:32<14:07,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 876/1875 [13:33<14:09,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 877/1875 [13:34<14:07,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 878/1875 [13:35<14:08,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 879/1875 [13:36<14:06,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 880/1875 [13:37<14:06,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 881/1875 [13:37<14:02,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 882/1875 [13:38<14:06,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 883/1875 [13:39<14:04,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 884/1875 [13:40<14:04,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 885/1875 [13:41<13:59,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 886/1875 [13:42<14:02,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 887/1875 [13:43<13:59,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 888/1875 [13:43<14:01,  1.17it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 889/1875 [13:44<13:54,  1.18it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 890/1875 [13:45<13:56,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 891/1875 [13:46<13:56,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 892/1875 [13:47<13:59,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 893/1875 [13:48<13:54,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 894/1875 [13:49<13:56,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 895/1875 [13:49<13:53,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 896/1875 [13:50<13:57,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 897/1875 [13:51<13:55,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 898/1875 [13:52<13:56,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 899/1875 [13:53<13:50,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 900/1875 [13:54<13:52,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 901/1875 [13:55<13:49,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 902/1875 [13:55<13:49,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 903/1875 [13:56<13:41,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 904/1875 [13:57<13:46,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 905/1875 [13:58<13:47,  1.17it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 906/1875 [13:59<13:43,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 907/1875 [14:00<13:42,  1.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 908/1875 [14:00<13:36,  1.19it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 909/1875 [14:01<13:40,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 910/1875 [14:02<13:38,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 911/1875 [14:03<13:34,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 912/1875 [14:04<13:37,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 913/1875 [14:05<13:35,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 914/1875 [14:06<13:39,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 915/1875 [14:06<13:39,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 916/1875 [14:07<13:36,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 917/1875 [14:08<13:34,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 918/1875 [14:09<13:33,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 919/1875 [14:10<13:27,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 920/1875 [14:11<13:33,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 921/1875 [14:12<13:28,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 922/1875 [14:12<13:29,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 923/1875 [14:13<13:32,  1.17it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 924/1875 [14:14<13:28,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 925/1875 [14:15<13:22,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 926/1875 [14:16<13:22,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 927/1875 [14:17<13:23,  1.18it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 928/1875 [14:17<13:23,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 929/1875 [14:18<13:24,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 930/1875 [14:19<13:23,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 931/1875 [14:20<13:27,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 932/1875 [14:21<13:27,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 933/1875 [14:22<13:26,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 934/1875 [14:23<13:18,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 935/1875 [14:23<13:18,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 936/1875 [14:24<13:14,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 937/1875 [14:25<13:14,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 938/1875 [14:26<13:12,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 939/1875 [14:27<13:18,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 940/1875 [14:28<13:11,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 941/1875 [14:29<13:14,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 942/1875 [14:29<13:11,  1.18it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 943/1875 [14:30<13:16,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 944/1875 [14:31<13:15,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 945/1875 [14:32<13:12,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 946/1875 [14:33<13:09,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 947/1875 [14:34<13:10,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 948/1875 [14:34<13:07,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 949/1875 [14:35<13:07,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 950/1875 [14:36<13:06,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 951/1875 [14:37<13:05,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 952/1875 [14:38<13:01,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 953/1875 [14:39<13:02,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 954/1875 [14:40<13:01,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 955/1875 [14:40<12:55,  1.19it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 956/1875 [14:41<13:02,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 957/1875 [14:42<12:59,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 958/1875 [14:43<12:58,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 959/1875 [14:44<12:58,  1.18it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 960/1875 [14:45<13:03,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 961/1875 [14:46<13:04,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 962/1875 [14:46<13:00,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 963/1875 [14:47<13:02,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 964/1875 [14:48<12:59,  1.17it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 965/1875 [14:49<12:57,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 966/1875 [14:50<12:59,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 967/1875 [14:51<12:53,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 968/1875 [14:52<12:55,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 969/1875 [14:52<12:50,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 970/1875 [14:53<12:52,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 971/1875 [14:54<12:56,  1.16it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 972/1875 [14:55<12:51,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 973/1875 [14:56<12:49,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 974/1875 [14:57<12:46,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 975/1875 [14:57<12:47,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 976/1875 [14:58<12:42,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 977/1875 [14:59<12:42,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 978/1875 [15:00<12:43,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 979/1875 [15:01<12:41,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 980/1875 [15:02<12:42,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 981/1875 [15:03<12:39,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 982/1875 [15:03<12:37,  1.18it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 983/1875 [15:04<12:39,  1.17it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 984/1875 [15:05<12:39,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 985/1875 [15:06<12:40,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 986/1875 [15:07<12:33,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 987/1875 [15:08<12:38,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 988/1875 [15:09<12:36,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 989/1875 [15:09<12:32,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 990/1875 [15:10<12:36,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 991/1875 [15:11<12:31,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 992/1875 [15:12<12:31,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 993/1875 [15:13<12:31,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 994/1875 [15:14<12:30,  1.17it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 995/1875 [15:14<12:26,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 996/1875 [15:15<12:27,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 997/1875 [15:16<12:24,  1.18it/s] 53%{'loss': 0.2973, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.6}
|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 998/1875 [15:17<12:20,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 999/1875 [15:18<12:21,  1.18it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1000/1875 [15:19<12:24,  1.18it/s]                                                    53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1000/1875 [15:19<12:24,  1.18it/s][INFO|trainer.py:2985] 2024-02-13 13:19:25,720 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000
[INFO|configuration_utils.py:473] 2024-02-13 13:19:25,723 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 13:19:38,571 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 13:19:38,573 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 13:19:38,574 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1000/special_tokens_map.json
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1001/1875 [15:57<2:55:22, 12.04s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1002/1875 [15:58<2:06:22,  8.69s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1003/1875 [15:59<1:32:03,  6.33s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1004/1875 [15:59<1:08:00,  4.69s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1005/1875 [16:00<51:09,  3.53s/it]   54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1006/1875 [16:01<39:25,  2.72s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1007/1875 [16:02<31:19,  2.17s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1008/1875 [16:03<25:36,  1.77s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1009/1875 [16:04<21:34,  1.49s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1010/1875 [16:05<18:43,  1.30s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1011/1875 [16:05<16:48,  1.17s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1012/1875 [16:06<15:26,  1.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1013/1875 [16:07<14:25,  1.00s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1014/1875 [16:08<13:42,  1.05it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1015/1875 [16:09<13:16,  1.08it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1016/1875 [16:10<12:51,  1.11it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1017/1875 [16:10<12:38,  1.13it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1018/1875 [16:11<12:34,  1.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1019/1875 [16:12<12:27,  1.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1020/1875 [16:13<12:24,  1.15it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1021/1875 [16:14<12:16,  1.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1022/1875 [16:15<12:12,  1.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1023/1875 [16:16<12:06,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1024/1875 [16:16<12:11,  1.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1025/1875 [16:17<12:05,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1026/1875 [16:18<12:07,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1027/1875 [16:19<12:04,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1028/1875 [16:20<12:01,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1029/1875 [16:21<12:02,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1030/1875 [16:22<12:00,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1031/1875 [16:22<11:53,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1032/1875 [16:23<11:58,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1033/1875 [16:24<11:53,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1034/1875 [16:25<11:53,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1035/1875 [16:26<11:54,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1036/1875 [16:27<11:51,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1037/1875 [16:27<11:50,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1038/1875 [16:28<11:46,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1039/1875 [16:29<11:47,  1.18it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1040/1875 [16:30<11:45,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1041/1875 [16:31<11:50,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1042/1875 [16:32<11:46,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1043/1875 [16:33<11:49,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1044/1875 [16:33<11:43,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1045/1875 [16:34<11:46,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1046/1875 [16:35<11:42,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1047/1875 [16:36<11:44,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1048/1875 [16:37<11:44,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1049/1875 [16:38<11:46,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1050/1875 [16:39<11:42,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1051/1875 [16:39<11:42,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1052/1875 [16:40<11:37,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1053/1875 [16:41<11:38,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1054/1875 [16:42<11:36,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1055/1875 [16:43<11:35,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1056/1875 [16:44<11:33,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1057/1875 [16:44<11:31,  1.18it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1058/1875 [16:45<11:36,  1.17it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1059/1875 [16:46<11:32,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1060/1875 [16:47<11:32,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1061/1875 [16:48<11:33,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1062/1875 [16:49<11:34,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1063/1875 [16:50<11:30,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1064/1875 [16:50<11:35,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1065/1875 [16:51<11:32,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1066/1875 [16:52<11:34,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1067/1875 [16:53<11:32,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1068/1875 [16:54<11:29,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1069/1875 [16:55<11:23,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1070/1875 [16:56<11:23,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1071/1875 [16:56<11:24,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1072/1875 [16:57<11:23,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1073/1875 [16:58<11:17,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1074/1875 [16:59<11:23,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1075/1875 [17:00<11:23,  1.17it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1076/1875 [17:01<11:19,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1077/1875 [17:02<11:18,  1.18it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1078/1875 [17:02<11:18,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1079/1875 [17:03<11:17,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1080/1875 [17:04<11:18,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1081/1875 [17:05<11:13,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1082/1875 [17:06<11:15,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1083/1875 [17:07<11:12,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1084/1875 [17:07<11:13,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1085/1875 [17:08<11:11,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1086/1875 [17:09<11:11,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1087/1875 [17:10<11:11,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1088/1875 [17:11<11:09,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1089/1875 [17:12<11:09,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1090/1875 [17:13<11:07,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1091/1875 [17:13<11:02,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1092/1875 [17:14<11:07,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1093/1875 [17:15<11:06,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1094/1875 [17:16<11:02,  1.18it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1095/1875 [17:17<11:05,  1.17it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1096/1875 [17:18<11:04,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1097/1875 [17:19<11:13,  1.15it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1098/1875 [17:19<11:05,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1099/1875 [17:20<11:07,  1.16it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1100/1875 [17:21<11:05,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1101/1875 [17:22<11:04,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1102/1875 [17:23<11:03,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1103/1875 [17:24<11:02,  1.16it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1104/1875 [17:25<10:59,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1105/1875 [17:25<10:58,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1106/1875 [17:26<10:58,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1107/1875 [17:27<10:55,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1108/1875 [17:28<10:58,  1.16it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1109/1875 [17:29<10:55,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1110/1875 [17:30<10:51,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1111/1875 [17:31<10:51,  1.17it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1112/1875 [17:31<10:49,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1113/1875 [17:32<10:45,  1.18it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1114/1875 [17:33<10:41,  1.19it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1115/1875 [17:34<10:45,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1116/1875 [17:35<10:44,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1117/1875 [17:36<10:46,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1118/1875 [17:37<10:48,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1119/1875 [17:37<10:45,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1120/1875 [17:38<10:44,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1121/1875 [17:39<10:39,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1122/1875 [17:40<10:40,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1123/1875 [17:41<10:39,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1124/1875 [17:42<10:39,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1125/1875 [17:42<10:38,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1126/1875 [17:43<10:42,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1127/1875 [17:44<10:45,  1.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1128/1875 [17:45<10:39,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1129/1875 [17:46<10:37,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1130/1875 [17:47<10:35,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1131/1875 [17:48<10:32,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1132/1875 [17:48<10:31,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1133/1875 [17:49<10:34,  1.17it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1134/1875 [17:50<10:29,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1135/1875 [17:51<10:31,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1136/1875 [17:52<10:26,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1137/1875 [17:53<10:27,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1138/1875 [17:54<10:28,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1139/1875 [17:54<10:25,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1140/1875 [17:55<10:26,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1141/1875 [17:56<10:25,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1142/1875 [17:57<10:19,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1143/1875 [17:58<10:20,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1144/1875 [17:59<10:21,  1.18it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1145/1875 [18:00<10:23,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1146/1875 [18:00<10:20,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1147/1875 [18:01<10:22,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1148/1875 [18:02<10:21,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1149/1875 [18:03<10:18,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1150/1875 [18:04<10:17,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1151/1875 [18:05<10:19,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1152/1875 [18:05<10:16,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1153/1875 [18:06<10:16,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1154/1875 [18:07<10:14,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1155/1875 [18:08<10:14,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1156/1875 [18:09<10:14,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1157/1875 [18:10<10:15,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1158/1875 [18:11<10:09,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1159/1875 [18:11<10:12,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1160/1875 [18:12<10:11,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1161/1875 [18:13<10:09,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1162/1875 [18:14<10:04,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1163/1875 [18:15<10:06,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1164/1875 [18:16<10:06,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1165/1875 [18:17<10:06,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1166/1875 [18:17<10:02,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1167/1875 [18:18<10:04,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1168/1875 [18:19<10:03,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1169/1875 [18:20<10:01,  1.17it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1170/1875 [18:21<09:58,  1.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1171/1875 [18:22<09:55,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1172/1875 [18:23<09:56,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1173/1875 [18:23<09:59,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1174/1875 [18:24<09:58,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1175/1875 [18:25<09:54,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1176/1875 [18:26<09:56,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1177/1875 [18:27<10:00,  1.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1178/1875 [18:28<09:59,  1.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1179/1875 [18:29<09:55,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1180/1875 [18:29<09:55,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1181/1875 [18:30<09:53,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1182/1875 [18:31<09:56,  1.16it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1183/1875 [18:32<09:51,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1184/1875 [18:33<09:49,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1185/1875 [18:34<09:45,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1186/1875 [18:34<09:45,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1187/1875 [18:35<09:48,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1188/1875 [18:36<09:48,  1.17it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1189/1875 [18:37<09:41,  1.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1190/1875 [18:38<09:43,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1191/1875 [18:39<09:43,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1192/1875 [18:40<09:40,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1193/1875 [18:40<09:38,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1194/1875 [18:41<09:40,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1195/1875 [18:42<09:36,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1196/1875 [18:43<09:39,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1197/1875 [18:44<09:36,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1198/1875 [18:45<09:32,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1199/1875 [18:46<09:34,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1200/1875 [18:46<09:30,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1201/1875 [18:47<09:29,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1202/1875 [18:48<09:30,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1203/1875 [18:49<09:28,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1204/1875 [18:50<09:30,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1205/1875 [18:51<09:32,  1.17it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1206/1875 [18:51<09:28,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1207/1875 [18:52<09:24,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1208/1875 [18:53<09:26,  1.18it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1209/1875 [18:54<09:26,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1210/1875 [18:55<09:24,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1211/1875 [18:56<09:24,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1212/1875 [18:57<09:22,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1213/1875 [18:57<09:21,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1214/1875 [18:58<09:23,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1215/1875 [18:59<09:19,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1216/1875 [19:00<09:18,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1217/1875 [19:01<09:21,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1218/1875 [19:02<09:20,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1219/1875 [19:03<09:18,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1220/1875 [19:03<09:20,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1221/1875 [19:04<09:18,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1222/1875 [19:05<09:17,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1223/1875 [19:06<09:16,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1224/1875 [19:07<09:14,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1225/1875 [19:08<09:11,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1226/1875 [19:09<09:13,  1.17it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1227/1875 [19:09<09:10,  1.18it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1228/1875 [19:10<09:09,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1229/1875 [19:11<09:09,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1230/1875 [19:12<09:08,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1231/1875 [19:13<09:09,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1232/1875 [19:14<09:11,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1233/1875 [19:14<09:07,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1234/1875 [19:15<09:05,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1235/1875 [19:16<09:08,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1236/1875 [19:17<09:04,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1237/1875 [19:18<09:03,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1238/1875 [19:19<09:03,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1239/1875 [19:20<09:01,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1240/1875 [19:20<09:04,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1241/1875 [19:21<09:02,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1242/1875 [19:22<09:05,  1.16it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1243/1875 [19:23<09:01,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1244/1875 [19:24<08:55,  1.18it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1245/1875 [19:25<08:57,  1.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1246/1875 [19:26<09:00,  1.16it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1247/1875 [19:26<08:56,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1248/1875 [19:27<08:55,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1249/1875 [19:28<08:54,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1250/1875 [19:29<08:50,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1251/1875 [19:30<08:49,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1252/1875 [19:31<08:49,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1253/1875 [19:32<08:50,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1254/1875 [19:32<08:47,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1255/1875 [19:33<08:45,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1256/1875 [19:34<08:44,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1257/1875 [19:35<08:44,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1258/1875 [19:36<08:45,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1259/1875 [19:37<08:42,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1260/1875 [19:37<08:42,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1261/1875 [19:38<08:39,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1262/1875 [19:39<08:43,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1263/1875 [19:40<08:41,  1.17it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1264/1875 [19:41<08:39,  1.18it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1265/1875 [19:42<08:37,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1266/1875 [19:43<08:35,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1267/1875 [19:43<08:36,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1268/1875 [19:44<08:40,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1269/1875 [19:45<08:39,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1270/1875 [19:46<08:38,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1271/1875 [19:47<08:36,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1272/1875 [19:48<08:38,  1.16it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1273/1875 [19:49<08:32,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1274/1875 [19:49<08:34,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1275/1875 [19:50<08:31,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1276/1875 [19:51<08:29,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1277/1875 [19:52<08:28,  1.18it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1278/1875 [19:53<08:30,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1279/1875 [19:54<08:29,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1280/1875 [19:55<08:27,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1281/1875 [19:55<08:29,  1.16it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1282/1875 [19:56<08:26,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1283/1875 [19:57<08:24,  1.17it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1284/1875 [19:58<08:24,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1285/1875 [19:59<08:22,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1286/1875 [20:00<08:20,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1287/1875 [20:01<08:18,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1288/1875 [20:01<08:18,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1289/1875 [20:02<08:15,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1290/1875 [20:03<08:18,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1291/1875 [20:04<08:17,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1292/1875 [20:05<08:17,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1293/1875 [20:06<08:16,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1294/1875 [20:06<08:14,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1295/1875 [20:07<08:14,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1296/1875 [20:08<08:12,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1297/1875 [20:09<08:11,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1298/1875 [20:10<08:14,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1299/1875 [20:11<08:10,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1300/1875 [20:12<08:11,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1301/1875 [20:12<08:08,  1.17it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1302/1875 [20:13<08:04,  1.18it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1303/1875 [20:14<08:05,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1304/1875 [20:15<08:05,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1305/1875 [20:16<08:06,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1306/1875 [20:17<08:04,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1307/1875 [20:18<08:01,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1308/1875 [20:18<08:06,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1309/1875 [20:19<08:04,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1310/1875 [20:20<08:02,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1311/1875 [20:21<08:01,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1312/1875 [20:22<08:00,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1313/1875 [20:23<07:57,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1314/1875 [20:24<07:58,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1315/1875 [20:24<07:56,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1316/1875 [20:25<07:55,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1317/1875 [20:26<07:53,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1318/1875 [20:27<07:52,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1319/1875 [20:28<07:50,  1.18it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1320/1875 [20:29<07:52,  1.17it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1321/1875 [20:29<07:50,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1322/1875 [20:30<07:53,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1323/1875 [20:31<07:51,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1324/1875 [20:32<07:50,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1325/1875 [20:33<07:49,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1326/1875 [20:34<07:45,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1327/1875 [20:35<07:50,  1.16it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1328/1875 [20:35<07:47,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1329/1875 [20:36<07:45,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1330/1875 [20:37<07:42,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1331/1875 [20:38<07:41,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1332/1875 [20:39<07:40,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1333/1875 [20:40<07:41,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1334/1875 [20:41<07:41,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1335/1875 [20:41<07:37,  1.18it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1336/1875 [20:42<07:39,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1337/1875 [20:43<07:38,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1338/1875 [20:44<07:40,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1339/1875 [20:45<07:37,  1.17it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1340/1875 [20:46<07:36,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1341/1875 [20:47<07:38,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1342/1875 [20:47<07:35,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1343/1875 [20:48<07:32,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1344/1875 [20:49<07:35,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1345/1875 [20:50<07:30,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1346/1875 [20:51<07:31,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1347/1875 [20:52<07:28,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1348/1875 [20:52<07:26,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1349/1875 [20:53<07:24,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1350/1875 [20:54<07:29,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1351/1875 [20:55<07:27,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1352/1875 [20:56<07:27,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1353/1875 [20:57<07:23,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1354/1875 [20:58<07:20,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1355/1875 [20:58<07:20,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1356/1875 [20:59<07:22,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1357/1875 [21:00<07:21,  1.17it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1358/1875 [21:01<07:18,  1.18it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1359/1875 [21:02<07:18,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1360/1875 [21:03<07:15,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1361/1875 [21:04<07:16,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1362/1875 [21:04<07:13,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1363/1875 [21:05<07:16,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1364/1875 [21:06<07:15,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1365/1875 [21:07<07:11,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1366/1875 [21:08<07:13,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1367/1875 [21:09<07:13,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1368/1875 [21:09<07:10,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1369/1875 [21:10<07:10,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1370/1875 [21:11<07:12,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1371/1875 [21:12<07:10,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1372/1875 [21:13<07:09,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1373/1875 [21:14<07:09,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1374/1875 [21:15<07:05,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1375/1875 [21:15<07:10,  1.16it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1376/1875 [21:16<07:06,  1.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1377/1875 [21:17<07:03,  1.18it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1378/1875 [21:18<07:05,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1379/1875 [21:19<07:04,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1380/1875 [21:20<07:01,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1381/1875 [21:21<07:00,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1382/1875 [21:21<06:59,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1383/1875 [21:22<06:59,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1384/1875 [21:23<06:55,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1385/1875 [21:24<06:56,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1386/1875 [21:25<06:55,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1387/1875 [21:26<06:51,  1.19it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1388/1875 [21:27<06:53,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1389/1875 [21:27<06:52,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1390/1875 [21:28<06:50,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1391/1875 [21:29<06:51,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1392/1875 [21:30<06:49,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1393/1875 [21:31<06:50,  1.17it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1394/1875 [21:32<06:48,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1395/1875 [21:32<06:47,  1.18it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1396/1875 [21:33<06:45,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1397/1875 [21:34<06:43,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1398/1875 [21:35<06:47,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1399/1875 [21:36<06:42,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1400/1875 [21:37<06:42,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1401/1875 [21:38<06:45,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1402/1875 [21:38<06:44,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1403/1875 [21:39<06:41,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1404/1875 [21:40<06:43,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1405/1875 [21:41<06:40,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1406/1875 [21:42<06:40,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1407/1875 [21:43<06:41,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1408/1875 [21:44<06:37,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1409/1875 [21:44<06:35,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1410/1875 [21:45<06:34,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1411/1875 [21:46<06:32,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1412/1875 [21:47<06:34,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1413/1875 [21:48<06:31,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1414/1875 [21:49<06:31,  1.18it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1415/1875 [21:49<06:30,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1416/1875 [21:50<06:29,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1417/1875 [21:51<06:29,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1418/1875 [21:52<06:28,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1419/1875 [21:53<06:27,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1420/1875 [21:54<06:27,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1421/1875 [21:55<06:25,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1422/1875 [21:55<06:22,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1423/1875 [21:56<06:23,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1424/1875 [21:57<06:26,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1425/1875 [21:58<06:22,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1426/1875 [21:59<06:23,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1427/1875 [22:00<06:19,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1428/1875 [22:01<06:19,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1429/1875 [22:01<06:22,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1430/1875 [22:02<06:20,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1431/1875 [22:03<06:18,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1432/1875 [22:04<06:18,  1.17it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1433/1875 [22:05<06:14,  1.18it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1434/1875 [22:06<06:16,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1435/1875 [22:07<06:16,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1436/1875 [22:07<06:14,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1437/1875 [22:08<06:14,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1438/1875 [22:09<06:16,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1439/1875 [22:10<06:15,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1440/1875 [22:11<06:11,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1441/1875 [22:12<06:10,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1442/1875 [22:13<06:10,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1443/1875 [22:13<06:08,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1444/1875 [22:14<06:07,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1445/1875 [22:15<06:05,  1.18it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1446/1875 [22:16<06:06,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1447/1875 [22:17<06:05,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1448/1875 [22:18<06:06,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1449/1875 [22:18<06:03,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1450/1875 [22:19<06:03,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1451/1875 [22:20<06:04,  1.16it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1452/1875 [22:21<06:01,  1.17it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1453/1875 [22:22<05:59,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1454/1875 [22:23<05:57,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1455/1875 [22:24<06:00,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1456/1875 [22:24<05:57,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1457/1875 [22:25<05:59,  1.16it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1458/1875 [22:26<05:57,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1459/1875 [22:27<05:54,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1460/1875 [22:28<05:54,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1461/1875 [22:29<05:51,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1462/1875 [22:30<05:51,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1463/1875 [22:30<05:48,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1464/1875 [22:31<05:50,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1465/1875 [22:32<05:49,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1466/1875 [22:33<05:49,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1467/1875 [22:34<05:47,  1.18it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1468/1875 [22:35<05:47,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1469/1875 [22:36<05:47,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1470/1875 [22:36<05:45,  1.17it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1471/1875 [22:37<05:44,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1472/1875 [22:38<05:46,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1473/1875 [22:39<05:43,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1474/1875 [22:40<05:41,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1475/1875 [22:41<05:41,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1476/1875 [22:42<05:42,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1477/1875 [22:42<05:41,  1.16it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1478/1875 [22:43<05:40,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1479/1875 [22:44<05:38,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1480/1875 [22:45<05:35,  1.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1481/1875 [22:46<05:37,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1482/1875 [22:47<05:36,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1483/1875 [22:48<05:35,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1484/1875 [22:48<05:34,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1485/1875 [22:49<05:32,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1486/1875 [22:50<05:31,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1487/1875 [22:51<05:29,  1.18it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1488/1875 [22:52<05:29,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1489/1875 [22:53<05:30,  1.17it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1490/1875 [22:53<05:29,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1491/1875 [22:54<05:29,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1492/1875 [22:55<05:27,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1493/1875 [22:56<05:24,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1494/1875 [22:57<05:25,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1495/1875 [22:58<05:24,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1496/1875 [22:59<05:21,  1.18i{'loss': 0.2133, 'learning_rate': 1e-05, 'epoch': 2.4}
t/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1497/1875 [22:59<05:20,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1498/1875 [23:00<05:19,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1499/1875 [23:01<05:21,  1.17it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1500/1875 [23:02<05:19,  1.17it/s]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1500/1875 [23:02<05:19,  1.17it/s][INFO|trainer.py:2985] 2024-02-13 13:27:08,993 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500
[INFO|configuration_utils.py:473] 2024-02-13 13:27:09,009 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 13:27:22,038 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 13:27:22,040 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 13:27:22,041 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tmp-checkpoint-1500/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1501/1875 [23:40<1:14:57, 12.02s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1502/1875 [23:41<53:54,  8.67s/it]   80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1503/1875 [23:42<39:13,  6.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1504/1875 [23:43<28:57,  4.68s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1505/1875 [23:44<21:49,  3.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1506/1875 [23:44<16:46,  2.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1507/1875 [23:45<13:15,  2.16s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1508/1875 [23:46<10:47,  1.76s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1509/1875 [23:47<09:03,  1.49s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1510/1875 [23:48<07:50,  1.29s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1511/1875 [23:49<07:01,  1.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1512/1875 [23:49<06:27,  1.07s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1513/1875 [23:50<06:02,  1.00s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1514/1875 [23:51<05:45,  1.05it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1515/1875 [23:52<05:32,  1.08it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1516/1875 [23:53<05:25,  1.10it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1517/1875 [23:54<05:19,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1518/1875 [23:55<05:12,  1.14it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1519/1875 [23:55<05:08,  1.16it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1520/1875 [23:56<05:05,  1.16it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1521/1875 [23:57<05:05,  1.16it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1522/1875 [23:58<05:02,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1523/1875 [23:59<05:00,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1524/1875 [24:00<04:58,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1525/1875 [24:00<04:57,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1526/1875 [24:01<04:57,  1.17it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1527/1875 [24:02<04:55,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1528/1875 [24:03<04:55,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1529/1875 [24:04<04:53,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1530/1875 [24:05<04:54,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1531/1875 [24:06<04:55,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1532/1875 [24:06<04:53,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1533/1875 [24:07<04:52,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1534/1875 [24:08<04:50,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1535/1875 [24:09<04:50,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1536/1875 [24:10<04:50,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1537/1875 [24:11<04:49,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1538/1875 [24:12<04:47,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1539/1875 [24:12<04:45,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1540/1875 [24:13<04:43,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1541/1875 [24:14<04:46,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1542/1875 [24:15<04:43,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1543/1875 [24:16<04:42,  1.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1544/1875 [24:17<04:39,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1545/1875 [24:18<04:40,  1.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1546/1875 [24:18<04:38,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1547/1875 [24:19<04:36,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1548/1875 [24:20<04:35,  1.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1549/1875 [24:21<04:37,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1550/1875 [24:22<04:35,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1551/1875 [24:23<04:35,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1552/1875 [24:23<04:34,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1553/1875 [24:24<04:33,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1554/1875 [24:25<04:32,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1555/1875 [24:26<04:29,  1.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1556/1875 [24:27<04:30,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1557/1875 [24:28<04:28,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1558/1875 [24:29<04:28,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1559/1875 [24:29<04:28,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1560/1875 [24:30<04:28,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1561/1875 [24:31<04:27,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1562/1875 [24:32<04:26,  1.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1563/1875 [24:33<04:25,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1564/1875 [24:34<04:23,  1.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1565/1875 [24:34<04:23,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1566/1875 [24:35<04:23,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1567/1875 [24:36<04:25,  1.16it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1568/1875 [24:37<04:22,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1569/1875 [24:38<04:19,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1570/1875 [24:39<04:18,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1571/1875 [24:40<04:20,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1572/1875 [24:40<04:19,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1573/1875 [24:41<04:16,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1574/1875 [24:42<04:17,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1575/1875 [24:43<04:16,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1576/1875 [24:44<04:16,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1577/1875 [24:45<04:14,  1.17it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1578/1875 [24:46<04:12,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1579/1875 [24:46<04:09,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1580/1875 [24:47<04:10,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1581/1875 [24:48<04:09,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1582/1875 [24:49<04:08,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1583/1875 [24:50<04:08,  1.18it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1584/1875 [24:51<04:08,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1585/1875 [24:52<04:05,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1586/1875 [24:52<04:04,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1587/1875 [24:53<04:05,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1588/1875 [24:54<04:03,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1589/1875 [24:55<04:05,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1590/1875 [24:56<04:02,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1591/1875 [24:57<04:02,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1592/1875 [24:57<04:01,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1593/1875 [24:58<04:00,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1594/1875 [24:59<03:59,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1595/1875 [25:00<03:58,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1596/1875 [25:01<03:56,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1597/1875 [25:02<03:56,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1598/1875 [25:03<03:55,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1599/1875 [25:03<03:54,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1600/1875 [25:04<03:54,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1601/1875 [25:05<03:53,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1602/1875 [25:06<03:52,  1.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1603/1875 [25:07<03:50,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1604/1875 [25:08<03:51,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1605/1875 [25:09<03:49,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1606/1875 [25:09<03:48,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1607/1875 [25:10<03:48,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1608/1875 [25:11<03:48,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1609/1875 [25:12<03:47,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1610/1875 [25:13<03:44,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1611/1875 [25:14<03:44,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1612/1875 [25:14<03:43,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1613/1875 [25:15<03:43,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1614/1875 [25:16<03:43,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1615/1875 [25:17<03:44,  1.16it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1616/1875 [25:18<03:41,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1617/1875 [25:19<03:39,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1618/1875 [25:20<03:38,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1619/1875 [25:20<03:39,  1.17it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1620/1875 [25:21<03:36,  1.18it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1621/1875 [25:22<03:35,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1622/1875 [25:23<03:34,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1623/1875 [25:24<03:35,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1624/1875 [25:25<03:34,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1625/1875 [25:26<03:33,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1626/1875 [25:26<03:34,  1.16it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1627/1875 [25:27<03:32,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1628/1875 [25:28<03:31,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1629/1875 [25:29<03:30,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1630/1875 [25:30<03:29,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1631/1875 [25:31<03:28,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1632/1875 [25:32<03:26,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1633/1875 [25:32<03:26,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1634/1875 [25:33<03:24,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1635/1875 [25:34<03:24,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1636/1875 [25:35<03:22,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1637/1875 [25:36<03:21,  1.18it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1638/1875 [25:37<03:22,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1639/1875 [25:38<03:21,  1.17it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1640/1875 [25:38<03:20,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1641/1875 [25:39<03:20,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1642/1875 [25:40<03:19,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1643/1875 [25:41<03:19,  1.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1644/1875 [25:42<03:18,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1645/1875 [25:43<03:16,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1646/1875 [25:44<03:15,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1647/1875 [25:44<03:15,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1648/1875 [25:45<03:13,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1649/1875 [25:46<03:14,  1.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1650/1875 [25:47<03:13,  1.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1651/1875 [25:48<03:12,  1.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1652/1875 [25:49<03:11,  1.16it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1653/1875 [25:50<03:10,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1654/1875 [25:50<03:09,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1655/1875 [25:51<03:07,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1656/1875 [25:52<03:07,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1657/1875 [25:53<03:04,  1.18it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1658/1875 [25:54<03:05,  1.17it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1659/1875 [25:55<03:04,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1660/1875 [25:56<03:03,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1661/1875 [25:56<03:02,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1662/1875 [25:57<03:02,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1663/1875 [25:58<03:01,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1664/1875 [25:59<02:59,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1665/1875 [26:00<02:57,  1.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1666/1875 [26:01<02:58,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1667/1875 [26:01<02:57,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1668/1875 [26:02<02:56,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1669/1875 [26:03<02:56,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1670/1875 [26:04<02:55,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1671/1875 [26:05<02:54,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1672/1875 [26:06<02:53,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1673/1875 [26:07<02:52,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1674/1875 [26:07<02:51,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1675/1875 [26:08<02:50,  1.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1676/1875 [26:09<02:48,  1.18it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1677/1875 [26:10<02:49,  1.17it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1678/1875 [26:11<02:48,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1679/1875 [26:12<02:46,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1680/1875 [26:13<02:46,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1681/1875 [26:13<02:44,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1682/1875 [26:14<02:45,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1683/1875 [26:15<02:42,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1684/1875 [26:16<02:43,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1685/1875 [26:17<02:41,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1686/1875 [26:18<02:41,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1687/1875 [26:19<02:40,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1688/1875 [26:19<02:39,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1689/1875 [26:20<02:38,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1690/1875 [26:21<02:37,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1691/1875 [26:22<02:35,  1.19it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1692/1875 [26:23<02:35,  1.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1693/1875 [26:24<02:35,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1694/1875 [26:24<02:34,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1695/1875 [26:25<02:33,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1696/1875 [26:26<02:32,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1697/1875 [26:27<02:31,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1698/1875 [26:28<02:30,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1699/1875 [26:29<02:29,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1700/1875 [26:30<02:28,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1701/1875 [26:30<02:27,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1702/1875 [26:31<02:27,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1703/1875 [26:32<02:25,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1704/1875 [26:33<02:25,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1705/1875 [26:34<02:24,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1706/1875 [26:35<02:24,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1707/1875 [26:36<02:23,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1708/1875 [26:36<02:22,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1709/1875 [26:37<02:20,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1710/1875 [26:38<02:19,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1711/1875 [26:39<02:19,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1712/1875 [26:40<02:19,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1713/1875 [26:41<02:17,  1.18it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1714/1875 [26:42<02:18,  1.17it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1715/1875 [26:42<02:16,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1716/1875 [26:43<02:15,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1717/1875 [26:44<02:13,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1718/1875 [26:45<02:14,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1719/1875 [26:46<02:12,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1720/1875 [26:47<02:11,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1721/1875 [26:47<02:11,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1722/1875 [26:48<02:10,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1723/1875 [26:49<02:08,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1724/1875 [26:50<02:08,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1725/1875 [26:51<02:07,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1726/1875 [26:52<02:06,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1727/1875 [26:53<02:05,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1728/1875 [26:53<02:05,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1729/1875 [26:54<02:04,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1730/1875 [26:55<02:03,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1731/1875 [26:56<02:02,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1732/1875 [26:57<02:01,  1.18it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1733/1875 [26:58<02:00,  1.17it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1734/1875 [26:59<02:00,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1735/1875 [26:59<01:58,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1736/1875 [27:00<01:59,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1737/1875 [27:01<01:58,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1738/1875 [27:02<01:56,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1739/1875 [27:03<01:55,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1740/1875 [27:04<01:55,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1741/1875 [27:04<01:54,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1742/1875 [27:05<01:53,  1.17it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1743/1875 [27:06<01:51,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1744/1875 [27:07<01:50,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1745/1875 [27:08<01:50,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1746/1875 [27:09<01:49,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1747/1875 [27:10<01:48,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1748/1875 [27:10<01:47,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1749/1875 [27:11<01:46,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1750/1875 [27:12<01:46,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1751/1875 [27:13<01:45,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1752/1875 [27:14<01:44,  1.18it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1753/1875 [27:15<01:44,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1754/1875 [27:16<01:42,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1755/1875 [27:16<01:41,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1756/1875 [27:17<01:41,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1757/1875 [27:18<01:40,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1758/1875 [27:19<01:39,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1759/1875 [27:20<01:38,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1760/1875 [27:21<01:37,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1761/1875 [27:21<01:37,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1762/1875 [27:22<01:36,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1763/1875 [27:23<01:35,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1764/1875 [27:24<01:34,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1765/1875 [27:25<01:33,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1766/1875 [27:26<01:34,  1.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1767/1875 [27:27<01:32,  1.16it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1768/1875 [27:27<01:31,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1769/1875 [27:28<01:30,  1.17it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1770/1875 [27:29<01:29,  1.18it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1771/1875 [27:30<01:29,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1772/1875 [27:31<01:27,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1773/1875 [27:32<01:27,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1774/1875 [27:33<01:26,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1775/1875 [27:33<01:25,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1776/1875 [27:34<01:24,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1777/1875 [27:35<01:23,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1778/1875 [27:36<01:22,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1779/1875 [27:37<01:21,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1780/1875 [27:38<01:20,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1781/1875 [27:39<01:20,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1782/1875 [27:39<01:19,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1783/1875 [27:40<01:18,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1784/1875 [27:41<01:17,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1785/1875 [27:42<01:16,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1786/1875 [27:43<01:15,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1787/1875 [27:44<01:14,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1788/1875 [27:45<01:14,  1.17it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1789/1875 [27:45<01:13,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1790/1875 [27:46<01:12,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1791/1875 [27:47<01:11,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1792/1875 [27:48<01:10,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1793/1875 [27:49<01:09,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1794/1875 [27:50<01:09,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1795/1875 [27:50<01:08,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1796/1875 [27:51<01:07,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1797/1875 [27:52<01:06,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1798/1875 [27:53<01:05,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1799/1875 [27:54<01:04,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1800/1875 [27:55<01:04,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1801/1875 [27:56<01:03,  1.17it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1802/1875 [27:56<01:02,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1803/1875 [27:57<01:01,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1804/1875 [27:58<01:00,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1805/1875 [27:59<00:59,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1806/1875 [28:00<00:58,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1807/1875 [28:01<00:57,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1808/1875 [28:02<00:56,  1.18it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1809/1875 [28:02<00:56,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1810/1875 [28:03<00:55,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1811/1875 [28:04<00:54,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1812/1875 [28:05<00:53,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1813/1875 [28:06<00:52,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1814/1875 [28:07<00:52,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1815/1875 [28:08<00:51,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1816/1875 [28:08<00:50,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1817/1875 [28:09<00:49,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1818/1875 [28:10<00:48,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1819/1875 [28:11<00:48,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1820/1875 [28:12<00:47,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1821/1875 [28:13<00:46,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1822/1875 [28:14<00:45,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1823/1875 [28:14<00:44,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1824/1875 [28:15<00:43,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1825/1875 [28:16<00:42,  1.17it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1826/1875 [28:17<00:41,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1827/1875 [28:18<00:40,  1.18it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1828/1875 [28:19<00:40,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1829/1875 [28:19<00:39,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1830/1875 [28:20<00:38,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1831/1875 [28:21<00:37,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1832/1875 [28:22<00:36,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1833/1875 [28:23<00:35,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1834/1875 [28:24<00:34,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1835/1875 [28:25<00:34,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1836/1875 [28:25<00:33,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1837/1875 [28:26<00:32,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1838/1875 [28:27<00:31,  1.16it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1839/1875 [28:28<00:30,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1840/1875 [28:29<00:29,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1841/1875 [28:30<00:28,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1842/1875 [28:31<00:28,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1843/1875 [28:31<00:27,  1.18it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1844/1875 [28:32<00:26,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1845/1875 [28:33<00:25,  1.17it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1846/1875 [28:34<00:24,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1847/1875 [28:35<00:23,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1848/1875 [28:36<00:23,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1849/1875 [28:37<00:22,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1850/1875 [28:37<00:21,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1851/1875 [28:38<00:20,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1852/1875 [28:39<00:19,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1853/1875 [28:40<00:18,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1854/1875 [28:41<00:17,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1855/1875 [28:42<00:17,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1856/1875 [28:42<00:16,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1857/1875 [28:43<00:15,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1858/1875 [28:44<00:14,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1859/1875 [28:45<00:13,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1860/1875 [28:46<00:12,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1861/1875 [28:47<00:11,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1862/1875 [28:48<00:11,  1.17it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1863/1875 [28:48<00:10,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1864/1875 [28:49<00:09,  1.18it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1865/1875 [28:50<00:08,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1866/1875 [28:51<00:07,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1867/1875 [28:52<00:06,  1.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1868/1875 [28:53<00:05,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1869/1875 [28:54<00:05,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1870/1875 [28:54<00:04,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1871/1875 [28:55<00:03,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1872/1875 [28:56<00:02,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1873/1875 [28:57<00:01,  1.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1874/1875 [28:58<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [28:59<00:00,  1.17it/s][INFO|trainer.py:1988] 2024-02-13 13:33:05,492 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1739.3802, 'train_samples_per_second': 17.248, 'train_steps_per_second': 1.078, 'train_loss': 0.3202986124674479, 'epoch': 3.0}
                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [28:59<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [28:59<00:00,  1.08it/s]
[INFO|trainer.py:2985] 2024-02-13 13:33:05,629 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2
[INFO|configuration_utils.py:473] 2024-02-13 13:33:05,640 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/config.json
[INFO|modeling_utils.py:2462] 2024-02-13 13:33:18,768 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-13 13:33:18,770 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-13 13:33:18,771 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.3203
  train_runtime            = 0:28:59.38
  train_samples            =      10000
  train_samples_per_second =     17.248
  train_steps_per_second   =      1.078
02/13/2024 13:33:18 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-13 13:33:18,825 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-13 13:33:18,827 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-13 13:33:18,827 >>   Num examples = 872
[INFO|trainer.py:3296] 2024-02-13 13:33:18,827 >>   Batch size = 8
  0%|          | 0/55 [00:00<?, ?it/s]  4%|â–Ž         | 2/55 [00:00<00:06,  7.64it/s]  5%|â–Œ         | 3/55 [00:00<00:09,  5.53it/s]  7%|â–‹         | 4/55 [00:00<00:10,  4.77it/s]  9%|â–‰         | 5/55 [00:01<00:11,  4.45it/s] 11%|â–ˆ         | 6/55 [00:01<00:11,  4.36it/s] 13%|â–ˆâ–Ž        | 7/55 [00:01<00:11,  4.17it/s] 15%|â–ˆâ–        | 8/55 [00:01<00:11,  4.21it/s] 16%|â–ˆâ–‹        | 9/55 [00:02<00:11,  4.11it/s] 18%|â–ˆâ–Š        | 10/55 [00:02<00:11,  4.01it/s] 20%|â–ˆâ–ˆ        | 11/55 [00:02<00:10,  4.07it/s] 22%|â–ˆâ–ˆâ–       | 12/55 [00:02<00:10,  3.98it/s] 24%|â–ˆâ–ˆâ–Ž       | 13/55 [00:03<00:10,  4.00it/s] 25%|â–ˆâ–ˆâ–Œ       | 14/55 [00:03<00:09,  4.12it/s] 27%|â–ˆâ–ˆâ–‹       | 15/55 [00:03<00:09,  4.02it/s] 29%|â–ˆâ–ˆâ–‰       | 16/55 [00:03<00:09,  4.11it/s] 31%|â–ˆâ–ˆâ–ˆ       | 17/55 [00:04<00:09,  4.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/55 [00:04<00:09,  3.96it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 19/55 [00:04<00:09,  3.98it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 20/55 [00:04<00:08,  4.02it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 21/55 [00:04<00:08,  4.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/55 [00:05<00:08,  4.00it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 23/55 [00:05<00:07,  4.00it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 24/55 [00:05<00:07,  4.04it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 25/55 [00:06<00:07,  3.98it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 26/55 [00:06<00:07,  3.99it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 27/55 [00:06<00:07,  3.99it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 28/55 [00:06<00:06,  3.94it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/55 [00:07<00:06,  3.89it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 30/55 [00:07<00:06,  3.92it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/55 [00:07<00:05,  4.00it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 32/55 [00:07<00:05,  3.99it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/55 [00:08<00:05,  3.94it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 34/55 [00:08<00:05,  4.01it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 35/55 [00:08<00:05,  3.96it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 36/55 [00:08<00:04,  3.96it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 37/55 [00:09<00:04,  4.03it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 38/55 [00:09<00:04,  4.02it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 39/55 [00:09<00:03,  4.02it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 40/55 [00:09<00:03,  4.01it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 41/55 [00:10<00:03,  3.95it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:10<00:03,  4.02it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 43/55 [00:10<00:02,  4.02it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 44/55 [00:10<00:02,  3.98it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 45/55 [00:11<00:02,  3.90it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 46/55 [00:11<00:02,  3.98it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 47/55 [00:11<00:02,  3.99it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 48/55 [00:11<00:01,  3.99it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 49/55 [00:12<00:01,  3.94it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 50/55 [00:12<00:01,  3.94it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 51/55 [00:12<00:01,  3.91it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 52/55 [00:12<00:00,  3.92it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 53/55 [00:13<00:00,  3.99it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 54/55 [00:13<00:00,  3.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:13<00:00,  3.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:13<00:00,  4.04it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.7729
  eval_loss               =     0.8304
  eval_runtime            = 0:00:13.84
  eval_samples            =        872
  eval_samples_per_second =     62.986
  eval_steps_per_second   =      3.973
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:6000 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:6000 (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    result = agent.run()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 727, in run
    result = self._invoke_run(role)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 862, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 699, in _initialize_workers
    self._rendezvous(worker_group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 542, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:6000 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:6000 (errno: 98 - Address already in use).
srun: error: v007: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:6000 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:6000 (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    result = agent.run()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 727, in run
    result = self._invoke_run(role)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 862, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 699, in _initialize_workers
    self._rendezvous(worker_group)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 542, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:6000 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:6000 (errno: 98 - Address already in use).
srun: error: v007: task 0: Exited with exit code 1
