/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:21:22 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:21:22 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/runs/Feb18_21-21-21_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:21:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:21:23 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:21:23 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:21:23 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:21:23,632 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:21:23,635 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:21:23,702 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:21:23,703 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:21:23,703 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:21:23,703 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:21:23,703 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:21:23,797 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:21:23,922 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:22:12,717 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:22:12,717 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
02/18/2024 21:22:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0fe9f7c1b160ff74.arrow
02/18/2024 21:22:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0fe9f7c1b160ff74.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ab0d9d9940fc51bd.arrow
02/18/2024 21:22:13 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ab0d9d9940fc51bd.arrow
02/18/2024 21:22:13 - INFO - __main__ - Sample 81 of the training set: {'sentence1': 'But Mitsubishi Tokyo Financial ( JP : 8306 : news , chart , profile ) declined 3,000 yen , or 0.65 percent , to 456,000 yen .', 'sentence2': 'Sumitomo Mitsui Financial ( JP : 8316 : news , chart , profile ) was down 2.5 percent at 198,000 yen .', 'label': 0, 'idx': 94, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1205, 341, 1169, 431, 29603, 20377, 4231, 273, 1455, 313, 435, 29925, 584, 29871, 29947, 29941, 29900, 29953, 584, 9763, 1919, 8727, 1919, 8722, 1723, 4845, 1312, 29871, 29941, 29892, 29900, 29900, 29900, 343, 264, 1919, 470, 29871, 29900, 29889, 29953, 29945, 10151, 1919, 304, 29871, 29946, 29945, 29953, 29892, 29900, 29900, 29900, 343, 264, 869, 1, 6991, 277, 10730, 341, 1169, 1481, 4231, 273, 1455, 313, 435, 29925, 584, 29871, 29947, 29941, 29896, 29953, 584, 9763, 1919, 8727, 1919, 8722, 1723, 471, 1623, 29871, 29906, 29889, 29945, 10151, 472, 29871, 29896, 29929, 29947, 29892, 29900, 29900, 29900, 343, 264, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:22:13 - INFO - __main__ - Sample 14 of the training set: {'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .', 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .', 'label': 0, 'idx': 15, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 12842, 990, 29891, 940, 466, 1358, 1919, 2343, 310, 278, 1887, 766, 1901, 5190, 1919, 1497, 278, 11182, 471, 19436, 29871, 29941, 29947, 28134, 869, 1, 450, 2343, 310, 278, 1887, 766, 1901, 5190, 1919, 12842, 990, 29891, 940, 466, 1358, 1919, 1497, 278, 11182, 7156, 750, 5229, 304, 540, 287, 2654, 5040, 26068, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:22:13 - INFO - __main__ - Sample 3 of the training set: {'sentence1': 'Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .', 'sentence2': 'Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 826, 618, 29871, 29900, 29941, 29941, 29945, 402, 11490, 1919, 11090, 29358, 892, 701, 29871, 29896, 29929, 274, 1237, 1919, 470, 29871, 29946, 29889, 29946, 1273, 1919, 472, 319, 395, 29871, 29946, 29889, 29945, 29953, 1919, 2534, 8859, 731, 263, 2407, 1880, 310, 319, 395, 29871, 29946, 29889, 29945, 29955, 869, 1, 11090, 29358, 12500, 287, 29871, 29906, 29900, 274, 1237, 1919, 470, 29871, 29946, 29889, 29953, 1273, 1919, 304, 731, 263, 2407, 14382, 1880, 472, 319, 395, 29871, 29946, 29889, 29945, 29955, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:22:14 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:22:15,660 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:22:16,048 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:22:16,048 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:22:16,048 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:22:16,048 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:22:16,049 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:22:16,049 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:22:16,049 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:22:16,050 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:38,  2.60s/it]  5%|▌         | 2/39 [00:03<01:07,  1.83s/it]  8%|▊         | 3/39 [00:05<00:58,  1.62s/it] 10%|█         | 4/39 [00:06<00:53,  1.53s/it] 13%|█▎        | 5/39 [00:08<00:50,  1.47s/it] 15%|█▌        | 6/39 [00:09<00:47,  1.44s/it] 18%|█▊        | 7/39 [00:10<00:45,  1.42s/it] 21%|██        | 8/39 [00:12<00:43,  1.41s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.40s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:16<00:38,  1.39s/it] 31%|███       | 12/39 [00:17<00:37,  1.39s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.21s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:21<00:31,  1.30s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:24<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:25<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:28<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:32<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:35<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:37<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.30s/it] 74%|███████▍  | 29/39 [00:40<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:41<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:44<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:48<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:51<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:52<00:01,  1.38s/it]100%|██████████| 39/39 [00:53<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:23:09,291 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 53.3726, 'train_samples_per_second': 5.621, 'train_steps_per_second': 0.731, 'train_loss': 1.0611893091446314, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:53<00:00,  1.20s/it]100%|██████████| 39/39 [00:53<00:00,  1.37s/it]
[INFO|trainer.py:2985] 2024-02-18 21:23:09,427 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:23:09,429 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:23:19,368 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:23:19,370 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:23:19,371 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.0612
  train_runtime            = 0:00:53.37
  train_samples            =        100
  train_samples_per_second =      5.621
  train_steps_per_second   =      0.731
02/18/2024 21:23:19 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:23:19,438 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:23:19,440 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:23:19,441 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:23:19,441 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.73it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.34it/s] 31%|███       | 4/13 [00:01<00:03,  2.89it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.68it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.57it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.50it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.45it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.42it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.40it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.39it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.38it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.65it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.69
  eval_combined_score     =      0.741
  eval_f1                 =     0.7919
  eval_loss               =       0.63
  eval_runtime            = 0:00:05.39
  eval_samples            =        100
  eval_samples_per_second =     18.535
  eval_steps_per_second   =       2.41
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:25:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:25:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1/runs/Feb18_21-25-44_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:25:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:25:46 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:25:46 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:25:46 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:25:47,115 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:25:47,117 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "cola",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:25:47,168 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:25:47,168 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:25:47,168 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:25:47,168 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:25:47,168 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:25:47,230 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:25:47,290 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:25:50,384 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:25:50,384 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c05ac417c8accd89.arrow
02/18/2024 21:25:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c05ac417c8accd89.arrow
Running tokenizer on dataset:  12%|█▏        | 1000/8551 [00:00<00:04, 1854.18 examples/s]Running tokenizer on dataset:  23%|██▎       | 2000/8551 [00:00<00:02, 2687.95 examples/s]Running tokenizer on dataset:  35%|███▌      | 3000/8551 [00:00<00:01, 3880.76 examples/s]Running tokenizer on dataset:  47%|████▋     | 4000/8551 [00:01<00:00, 4893.00 examples/s]Running tokenizer on dataset:  58%|█████▊    | 5000/8551 [00:01<00:00, 5715.12 examples/s]Running tokenizer on dataset:  70%|███████   | 6000/8551 [00:01<00:00, 6324.97 examples/s]Running tokenizer on dataset:  82%|████████▏ | 7000/8551 [00:01<00:00, 6767.29 examples/s]Running tokenizer on dataset:  94%|█████████▎| 8000/8551 [00:01<00:00, 6877.94 examples/s]Running tokenizer on dataset: 100%|██████████| 8551/8551 [00:01<00:00, 5236.90 examples/s]
Running tokenizer on dataset:   0%|          | 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-73d12882f823e6f3.arrow
02/18/2024 21:25:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-73d12882f823e6f3.arrow
Running tokenizer on dataset:  96%|█████████▌| 1000/1043 [00:00<00:00, 8179.94 examples/s]Running tokenizer on dataset: 100%|██████████| 1043/1043 [00:00<00:00, 7253.36 examples/s]
Running tokenizer on dataset:   0%|          | 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2406b9a3f9add4c9.arrow
02/18/2024 21:25:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2406b9a3f9add4c9.arrow
Running tokenizer on dataset:  94%|█████████▍| 1000/1063 [00:00<00:00, 7056.96 examples/s]Running tokenizer on dataset: 100%|██████████| 1063/1063 [00:00<00:00, 6385.22 examples/s]
02/18/2024 21:25:52 - INFO - __main__ - Sample 81 of the training set: {'sentence': 'Bill squeezed the puppet through the hole.', 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 6682, 269, 802, 6096, 287, 278, 2653, 7988, 1549, 278, 16188, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:25:52 - INFO - __main__ - Sample 14 of the training set: {'sentence': 'The gardener watered the flowers.', 'label': 1, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 17161, 759, 4094, 287, 278, 18281, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:25:52 - INFO - __main__ - Sample 3 of the training set: {'sentence': 'The more we study verbs, the crazier they get.', 'label': 1, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 901, 591, 6559, 1147, 5824, 29892, 278, 274, 9504, 631, 896, 679, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:25:53 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:25:54,717 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:25:55,041 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:25:55,041 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:25:55,041 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:25:55,041 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:25:55,041 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:25:55,041 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:25:55,042 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:25:55,042 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:22,  2.17s/it]  5%|▌         | 2/39 [00:03<01:01,  1.65s/it]  8%|▊         | 3/39 [00:04<00:54,  1.53s/it] 10%|█         | 4/39 [00:06<00:51,  1.47s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.44s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.40s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.38s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.20s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:51<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:26:47,790 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.8797, 'train_samples_per_second': 5.673, 'train_steps_per_second': 0.738, 'train_loss': 1.2989163521008613, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:26:47,925 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:26:47,928 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:27:02,045 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:27:02,047 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:27:02,048 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/cola256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.2989
  train_runtime            = 0:00:52.87
  train_samples            =        100
  train_samples_per_second =      5.673
  train_steps_per_second   =      0.738
02/18/2024 21:27:02 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:27:02,083 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:27:02,085 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:27:02,085 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:27:02,085 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.76it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.36it/s] 31%|███       | 4/13 [00:01<00:03,  2.91it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.70it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.58it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.52it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.47it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.44it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.42it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.41it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.39it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.67it/s]
***** eval metrics *****
  epoch                     =        3.0
  eval_loss                 =     1.0502
  eval_matthews_correlation =     -0.166
  eval_runtime              = 0:00:05.31
  eval_samples              =        100
  eval_samples_per_second   =     18.819
  eval_steps_per_second     =      2.446
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:28:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:28:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1/runs/Feb18_21-28-37_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:28:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:28:38 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:28:38 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:28:38 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:28:39,292 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:28:39,294 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "sst2",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:28:39,329 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:28:39,330 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:28:39,330 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:28:39,330 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:28:39,330 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:28:39,393 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:28:39,452 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:28:42,551 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:28:42,552 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-290af534648c70f9.arrow
02/18/2024 21:28:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-290af534648c70f9.arrow
Running tokenizer on dataset:   1%|▏         | 1000/67349 [00:00<00:11, 5883.90 examples/s]Running tokenizer on dataset:   3%|▎         | 2000/67349 [00:00<00:14, 4489.82 examples/s]Running tokenizer on dataset:   4%|▍         | 3000/67349 [00:00<00:11, 5583.24 examples/s]Running tokenizer on dataset:   6%|▌         | 4000/67349 [00:00<00:10, 6267.18 examples/s]Running tokenizer on dataset:   7%|▋         | 5000/67349 [00:00<00:09, 6714.66 examples/s]Running tokenizer on dataset:   9%|▉         | 6000/67349 [00:00<00:09, 6809.88 examples/s]Running tokenizer on dataset:  10%|█         | 7000/67349 [00:01<00:08, 7099.82 examples/s]Running tokenizer on dataset:  12%|█▏        | 8000/67349 [00:01<00:08, 7240.87 examples/s]Running tokenizer on dataset:  13%|█▎        | 9000/67349 [00:01<00:08, 7038.21 examples/s]Running tokenizer on dataset:  15%|█▍        | 10000/67349 [00:01<00:08, 7151.75 examples/s]Running tokenizer on dataset:  16%|█▋        | 11000/67349 [00:01<00:07, 7253.63 examples/s]Running tokenizer on dataset:  18%|█▊        | 12000/67349 [00:01<00:07, 7267.10 examples/s]Running tokenizer on dataset:  19%|█▉        | 13000/67349 [00:01<00:07, 7335.69 examples/s]Running tokenizer on dataset:  21%|██        | 14000/67349 [00:02<00:07, 7405.30 examples/s]Running tokenizer on dataset:  22%|██▏       | 15000/67349 [00:02<00:07, 7415.24 examples/s]Running tokenizer on dataset:  24%|██▍       | 16000/67349 [00:02<00:06, 7377.59 examples/s]Running tokenizer on dataset:  25%|██▌       | 17000/67349 [00:02<00:06, 7413.67 examples/s]Running tokenizer on dataset:  27%|██▋       | 18000/67349 [00:02<00:06, 7472.17 examples/s]Running tokenizer on dataset:  28%|██▊       | 19000/67349 [00:02<00:06, 7496.29 examples/s]Running tokenizer on dataset:  30%|██▉       | 20000/67349 [00:02<00:06, 7499.27 examples/s]Running tokenizer on dataset:  31%|███       | 21000/67349 [00:02<00:06, 7483.24 examples/s]Running tokenizer on dataset:  33%|███▎      | 22000/67349 [00:03<00:06, 7507.21 examples/s]Running tokenizer on dataset:  34%|███▍      | 23000/67349 [00:03<00:05, 7427.16 examples/s]Running tokenizer on dataset:  36%|███▌      | 24000/67349 [00:03<00:05, 7431.51 examples/s]Running tokenizer on dataset:  37%|███▋      | 25000/67349 [00:03<00:05, 7485.03 examples/s]Running tokenizer on dataset:  39%|███▊      | 26000/67349 [00:03<00:06, 5995.77 examples/s]Running tokenizer on dataset:  40%|████      | 27000/67349 [00:03<00:06, 6346.21 examples/s]Running tokenizer on dataset:  42%|████▏     | 28000/67349 [00:04<00:05, 6616.94 examples/s]Running tokenizer on dataset:  43%|████▎     | 29000/67349 [00:04<00:05, 6877.85 examples/s]Running tokenizer on dataset:  45%|████▍     | 30000/67349 [00:04<00:05, 7050.88 examples/s]Running tokenizer on dataset:  46%|████▌     | 31000/67349 [00:04<00:05, 7164.02 examples/s]Running tokenizer on dataset:  48%|████▊     | 32000/67349 [00:04<00:04, 7242.37 examples/s]Running tokenizer on dataset:  49%|████▉     | 33000/67349 [00:04<00:04, 7273.64 examples/s]Running tokenizer on dataset:  50%|█████     | 34000/67349 [00:04<00:04, 7292.54 examples/s]Running tokenizer on dataset:  52%|█████▏    | 35000/67349 [00:04<00:04, 7317.11 examples/s]Running tokenizer on dataset:  53%|█████▎    | 36000/67349 [00:05<00:04, 7375.44 examples/s]Running tokenizer on dataset:  55%|█████▍    | 37000/67349 [00:05<00:04, 7357.87 examples/s]Running tokenizer on dataset:  56%|█████▋    | 38000/67349 [00:05<00:03, 7375.61 examples/s]Running tokenizer on dataset:  58%|█████▊    | 39000/67349 [00:05<00:03, 7367.89 examples/s]Running tokenizer on dataset:  59%|█████▉    | 40000/67349 [00:05<00:03, 7390.70 examples/s]Running tokenizer on dataset:  61%|██████    | 41000/67349 [00:05<00:03, 7394.64 examples/s]Running tokenizer on dataset:  62%|██████▏   | 42000/67349 [00:05<00:03, 7345.97 examples/s]Running tokenizer on dataset:  64%|██████▍   | 43000/67349 [00:06<00:03, 7346.77 examples/s]Running tokenizer on dataset:  65%|██████▌   | 44000/67349 [00:06<00:03, 7435.21 examples/s]Running tokenizer on dataset:  67%|██████▋   | 45000/67349 [00:06<00:02, 7454.57 examples/s]Running tokenizer on dataset:  68%|██████▊   | 46000/67349 [00:06<00:02, 7453.45 examples/s]Running tokenizer on dataset:  70%|██████▉   | 47000/67349 [00:06<00:02, 7493.00 examples/s]Running tokenizer on dataset:  71%|███████▏  | 48000/67349 [00:06<00:02, 7543.75 examples/s]Running tokenizer on dataset:  73%|███████▎  | 49000/67349 [00:06<00:02, 7538.12 examples/s]Running tokenizer on dataset:  74%|███████▍  | 50000/67349 [00:07<00:02, 5993.21 examples/s]Running tokenizer on dataset:  76%|███████▌  | 51000/67349 [00:07<00:02, 6383.38 examples/s]Running tokenizer on dataset:  77%|███████▋  | 52000/67349 [00:07<00:02, 6643.01 examples/s]Running tokenizer on dataset:  79%|███████▊  | 53000/67349 [00:07<00:02, 6854.62 examples/s]Running tokenizer on dataset:  80%|████████  | 54000/67349 [00:07<00:01, 6988.07 examples/s]Running tokenizer on dataset:  82%|████████▏ | 55000/67349 [00:07<00:01, 7106.34 examples/s]Running tokenizer on dataset:  83%|████████▎ | 56000/67349 [00:07<00:01, 7211.77 examples/s]Running tokenizer on dataset:  85%|████████▍ | 57000/67349 [00:08<00:01, 7281.32 examples/s]Running tokenizer on dataset:  86%|████████▌ | 58000/67349 [00:08<00:01, 7340.41 examples/s]Running tokenizer on dataset:  88%|████████▊ | 59000/67349 [00:08<00:01, 7432.58 examples/s]Running tokenizer on dataset:  89%|████████▉ | 60000/67349 [00:08<00:00, 7414.45 examples/s]Running tokenizer on dataset:  91%|█████████ | 61000/67349 [00:08<00:00, 7419.45 examples/s]Running tokenizer on dataset:  92%|█████████▏| 62000/67349 [00:08<00:00, 7495.04 examples/s]Running tokenizer on dataset:  94%|█████████▎| 63000/67349 [00:08<00:00, 7502.61 examples/s]Running tokenizer on dataset:  95%|█████████▌| 64000/67349 [00:08<00:00, 7426.06 examples/s]Running tokenizer on dataset:  97%|█████████▋| 65000/67349 [00:09<00:00, 7454.12 examples/s]Running tokenizer on dataset:  98%|█████████▊| 66000/67349 [00:09<00:00, 7459.10 examples/s]Running tokenizer on dataset:  99%|█████████▉| 67000/67349 [00:09<00:00, 7463.58 examples/s]Running tokenizer on dataset: 100%|██████████| 67349/67349 [00:09<00:00, 7133.40 examples/s]
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-adf982cfd4e88288.arrow
02/18/2024 21:28:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-adf982cfd4e88288.arrow
Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 6071.75 examples/s]Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 5787.76 examples/s]
Running tokenizer on dataset:   0%|          | 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-47c6a3a5bd956e96.arrow
02/18/2024 21:28:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-47c6a3a5bd956e96.arrow
Running tokenizer on dataset:  55%|█████▍    | 1000/1821 [00:00<00:00, 4532.82 examples/s]Running tokenizer on dataset: 100%|██████████| 1821/1821 [00:00<00:00, 5302.99 examples/s]Running tokenizer on dataset: 100%|██████████| 1821/1821 [00:00<00:00, 5042.56 examples/s]
02/18/2024 21:28:52 - INFO - __main__ - Sample 81 of the training set: {'sentence': 'generates ', 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 16785, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]}.
02/18/2024 21:28:52 - INFO - __main__ - Sample 14 of the training set: {'sentence': 'lend some dignity to a dumb story ', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 301, 355, 777, 18085, 537, 304, 263, 270, 3774, 5828, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:28:52 - INFO - __main__ - Sample 3 of the training set: {'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 9242, 14401, 368, 15787, 304, 3933, 278, 1021, 10106, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:28:52 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:28:54,535 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:28:54,878 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:28:54,878 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:28:54,878 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:28:54,878 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:28:54,879 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:28:54,879 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:28:54,879 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:28:54,880 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:24,  2.23s/it]  5%|▌         | 2/39 [00:03<01:02,  1.68s/it]  8%|▊         | 3/39 [00:04<00:55,  1.54s/it] 10%|█         | 4/39 [00:06<00:51,  1.48s/it] 13%|█▎        | 5/39 [00:07<00:49,  1.44s/it] 15%|█▌        | 6/39 [00:09<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:45,  1.41s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.39s/it] 31%|███       | 12/39 [00:17<00:37,  1.39s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.21s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.30s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:25<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:41<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:52<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:29:47,747 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.9992, 'train_samples_per_second': 5.66, 'train_steps_per_second': 0.736, 'train_loss': 1.095460940630008, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:29:47,882 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:29:47,885 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:30:06,294 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:30:06,296 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:30:06,297 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/sst2256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.0955
  train_runtime            = 0:00:52.99
  train_samples            =        100
  train_samples_per_second =       5.66
  train_steps_per_second   =      0.736
02/18/2024 21:30:06 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:30:06,331 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:30:06,333 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:30:06,333 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:30:06,333 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.75it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.35it/s] 31%|███       | 4/13 [00:01<00:03,  2.90it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.69it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.58it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.50it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.46it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.43it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.41it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.40it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.39it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:05<00:00,  2.59it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.49
  eval_loss               =     0.7398
  eval_runtime            = 0:00:05.46
  eval_samples            =        100
  eval_samples_per_second =     18.295
  eval_steps_per_second   =      2.378
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:30:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:30:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1/runs/Feb18_21-30-57_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:30:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:30:59 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:30:59 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:30:59 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:30:59,537 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:30:59,539 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "stsb",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:30:59,570 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:30:59,570 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:30:59,571 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:30:59,571 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:30:59,571 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:30:59,631 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:30:59,690 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:31:02,759 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:31:02,759 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/5749 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-84b0cfda338d22c6.arrow
02/18/2024 21:31:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-84b0cfda338d22c6.arrow
Running tokenizer on dataset:  17%|█▋        | 1000/5749 [00:00<00:00, 5789.95 examples/s]Running tokenizer on dataset:  35%|███▍      | 2000/5749 [00:00<00:00, 5957.45 examples/s]Running tokenizer on dataset:  52%|█████▏    | 3000/5749 [00:00<00:00, 3893.25 examples/s]Running tokenizer on dataset:  70%|██████▉   | 4000/5749 [00:00<00:00, 4145.13 examples/s]Running tokenizer on dataset:  87%|████████▋ | 5000/5749 [00:01<00:00, 4737.85 examples/s]Running tokenizer on dataset: 100%|██████████| 5749/5749 [00:01<00:00, 4863.81 examples/s]Running tokenizer on dataset: 100%|██████████| 5749/5749 [00:01<00:00, 4273.72 examples/s]
Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4c4ce6805ee7f950.arrow
02/18/2024 21:31:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4c4ce6805ee7f950.arrow
Running tokenizer on dataset:  67%|██████▋   | 1000/1500 [00:00<00:00, 3981.01 examples/s]Running tokenizer on dataset: 100%|██████████| 1500/1500 [00:00<00:00, 4308.69 examples/s]Running tokenizer on dataset: 100%|██████████| 1500/1500 [00:00<00:00, 4164.92 examples/s]
Running tokenizer on dataset:   0%|          | 0/1379 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a8a2b0679688665b.arrow
02/18/2024 21:31:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a8a2b0679688665b.arrow
Running tokenizer on dataset:  73%|███████▎  | 1000/1379 [00:00<00:00, 5659.32 examples/s]Running tokenizer on dataset: 100%|██████████| 1379/1379 [00:00<00:00, 5205.42 examples/s]
02/18/2024 21:31:04 - INFO - __main__ - Sample 81 of the training set: {'sentence1': 'A dog is eating water melon.', 'sentence2': 'A dog is eating a piece of watermelon.', 'label': 4.25, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 319, 11203, 338, 321, 1218, 4094, 9232, 265, 29889, 1, 319, 11203, 338, 321, 1218, 263, 8424, 310, 16699, 837, 295, 265, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:31:04 - INFO - __main__ - Sample 14 of the training set: {'sentence1': 'A man is running on the road.', 'sentence2': 'A panda dog is running on the road.', 'label': 1.6670000553131104, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 319, 767, 338, 2734, 373, 278, 6520, 29889, 1, 319, 282, 5863, 11203, 338, 2734, 373, 278, 6520, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:31:04 - INFO - __main__ - Sample 3 of the training set: {'sentence1': 'Three men are playing chess.', 'sentence2': 'Two men are playing chess.', 'label': 2.5999999046325684, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 12753, 1757, 526, 8743, 521, 404, 29889, 1, 7803, 1757, 526, 8743, 521, 404, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:31:05 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:31:06,703 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:31:07,045 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:31:07,045 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:31:07,045 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:31:07,045 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:31:07,045 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:31:07,045 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:31:07,045 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:31:07,046 >>   Number of trainable parameters = 1,280,151,552
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:21,  2.14s/it]  5%|▌         | 2/39 [00:03<01:00,  1.64s/it]  8%|▊         | 3/39 [00:04<00:54,  1.52s/it] 10%|█         | 4/39 [00:06<00:51,  1.47s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.44s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.40s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.39s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.21s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:51<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:31:59,793 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.8787, 'train_samples_per_second': 5.673, 'train_steps_per_second': 0.738, 'train_loss': 13.643900553385416, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:31:59,928 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:31:59,931 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:32:17,908 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:32:17,910 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:32:17,911 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/stsb256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =    13.6439
  train_runtime            = 0:00:52.87
  train_samples            =        100
  train_samples_per_second =      5.673
  train_steps_per_second   =      0.738
02/18/2024 21:32:17 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:32:17,943 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:32:17,945 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:32:17,945 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:32:17,946 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.76it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.36it/s] 31%|███       | 4/13 [00:01<00:03,  2.91it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.70it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.58it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.51it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.47it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.44it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.42it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.40it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.39it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.66it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_combined_score     =     0.3161
  eval_loss               =      3.616
  eval_pearson            =     0.3258
  eval_runtime            = 0:00:05.32
  eval_samples            =        100
  eval_samples_per_second =     18.768
  eval_spearmanr          =     0.3065
  eval_steps_per_second   =       2.44
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:33:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:33:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1/runs/Feb18_21-33-06_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:33:08 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:33:08 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:33:08 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:33:08 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:33:10,838 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:33:10,840 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "qqp",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:33:10,876 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:33:10,876 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:33:10,877 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:33:10,877 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:33:10,877 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:33:10,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:33:10,992 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:33:14,093 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:33:14,094 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/363846 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-70b897cd6483a24c.arrow
02/18/2024 21:33:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-70b897cd6483a24c.arrow
Running tokenizer on dataset:   0%|          | 1000/363846 [00:00<02:58, 2028.32 examples/s]Running tokenizer on dataset:   1%|          | 2000/363846 [00:00<01:48, 3337.76 examples/s]Running tokenizer on dataset:   1%|          | 3000/363846 [00:00<01:47, 3347.77 examples/s]Running tokenizer on dataset:   1%|          | 4000/363846 [00:01<01:31, 3921.47 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/363846 [00:01<01:23, 4277.05 examples/s]Running tokenizer on dataset:   2%|▏         | 6000/363846 [00:01<01:15, 4726.15 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/363846 [00:01<01:14, 4790.40 examples/s]Running tokenizer on dataset:   2%|▏         | 8000/363846 [00:01<01:09, 5113.95 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/363846 [00:02<01:06, 5338.04 examples/s]Running tokenizer on dataset:   3%|▎         | 10000/363846 [00:02<01:04, 5515.80 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/363846 [00:02<01:02, 5641.88 examples/s]Running tokenizer on dataset:   3%|▎         | 12000/363846 [00:02<01:01, 5721.03 examples/s]Running tokenizer on dataset:   4%|▎         | 13000/363846 [00:02<01:00, 5772.96 examples/s]Running tokenizer on dataset:   4%|▍         | 14000/363846 [00:02<01:00, 5799.71 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/363846 [00:03<01:00, 5785.82 examples/s]Running tokenizer on dataset:   4%|▍         | 16000/363846 [00:03<00:59, 5833.51 examples/s]Running tokenizer on dataset:   5%|▍         | 17000/363846 [00:03<00:58, 5880.41 examples/s]Running tokenizer on dataset:   5%|▍         | 18000/363846 [00:03<00:58, 5914.88 examples/s]Running tokenizer on dataset:   5%|▌         | 19000/363846 [00:03<00:58, 5889.42 examples/s]Running tokenizer on dataset:   5%|▌         | 20000/363846 [00:03<00:57, 5936.81 examples/s]Running tokenizer on dataset:   6%|▌         | 21000/363846 [00:04<00:57, 5923.62 examples/s]Running tokenizer on dataset:   6%|▌         | 22000/363846 [00:04<00:58, 5893.26 examples/s]Running tokenizer on dataset:   6%|▋         | 23000/363846 [00:04<00:57, 5877.20 examples/s]Running tokenizer on dataset:   7%|▋         | 24000/363846 [00:04<00:58, 5840.83 examples/s]Running tokenizer on dataset:   7%|▋         | 25000/363846 [00:04<00:58, 5818.42 examples/s]Running tokenizer on dataset:   7%|▋         | 26000/363846 [00:04<00:57, 5889.43 examples/s]Running tokenizer on dataset:   7%|▋         | 27000/363846 [00:05<00:57, 5891.14 examples/s]Running tokenizer on dataset:   8%|▊         | 28000/363846 [00:05<00:56, 5900.98 examples/s]Running tokenizer on dataset:   8%|▊         | 29000/363846 [00:05<00:56, 5916.81 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/363846 [00:05<00:56, 5889.57 examples/s]Running tokenizer on dataset:   9%|▊         | 31000/363846 [00:05<00:56, 5877.61 examples/s]Running tokenizer on dataset:   9%|▉         | 32000/363846 [00:05<00:56, 5896.50 examples/s]Running tokenizer on dataset:   9%|▉         | 33000/363846 [00:06<00:56, 5838.32 examples/s]Running tokenizer on dataset:   9%|▉         | 34000/363846 [00:06<00:56, 5870.91 examples/s]Running tokenizer on dataset:  10%|▉         | 35000/363846 [00:06<00:55, 5892.27 examples/s]Running tokenizer on dataset:  10%|▉         | 36000/363846 [00:06<00:55, 5887.45 examples/s]Running tokenizer on dataset:  10%|█         | 37000/363846 [00:06<00:55, 5888.25 examples/s]Running tokenizer on dataset:  10%|█         | 38000/363846 [00:06<00:55, 5855.61 examples/s]Running tokenizer on dataset:  11%|█         | 39000/363846 [00:07<01:09, 4644.07 examples/s]Running tokenizer on dataset:  11%|█         | 40000/363846 [00:07<01:05, 4958.68 examples/s]Running tokenizer on dataset:  11%|█▏        | 41000/363846 [00:07<01:01, 5239.32 examples/s]Running tokenizer on dataset:  12%|█▏        | 42000/363846 [00:07<00:59, 5384.11 examples/s]Running tokenizer on dataset:  12%|█▏        | 43000/363846 [00:07<00:58, 5528.16 examples/s]Running tokenizer on dataset:  12%|█▏        | 44000/363846 [00:08<00:56, 5615.20 examples/s]Running tokenizer on dataset:  12%|█▏        | 45000/363846 [00:08<00:55, 5703.98 examples/s]Running tokenizer on dataset:  13%|█▎        | 46000/363846 [00:08<00:55, 5757.78 examples/s]Running tokenizer on dataset:  13%|█▎        | 47000/363846 [00:08<00:54, 5835.41 examples/s]Running tokenizer on dataset:  13%|█▎        | 48000/363846 [00:08<00:54, 5789.86 examples/s]Running tokenizer on dataset:  13%|█▎        | 49000/363846 [00:08<00:54, 5808.81 examples/s]Running tokenizer on dataset:  14%|█▎        | 50000/363846 [00:09<00:53, 5830.50 examples/s]Running tokenizer on dataset:  14%|█▍        | 51000/363846 [00:09<00:53, 5796.49 examples/s]Running tokenizer on dataset:  14%|█▍        | 52000/363846 [00:09<00:53, 5842.56 examples/s]Running tokenizer on dataset:  15%|█▍        | 53000/363846 [00:09<00:52, 5897.20 examples/s]Running tokenizer on dataset:  15%|█▍        | 54000/363846 [00:09<00:52, 5849.80 examples/s]Running tokenizer on dataset:  15%|█▌        | 55000/363846 [00:10<00:52, 5880.17 examples/s]Running tokenizer on dataset:  15%|█▌        | 56000/363846 [00:10<00:52, 5908.09 examples/s]Running tokenizer on dataset:  16%|█▌        | 57000/363846 [00:10<00:52, 5861.84 examples/s]Running tokenizer on dataset:  16%|█▌        | 58000/363846 [00:10<00:51, 5892.68 examples/s]Running tokenizer on dataset:  16%|█▌        | 59000/363846 [00:10<00:52, 5850.36 examples/s]Running tokenizer on dataset:  16%|█▋        | 60000/363846 [00:10<00:52, 5808.01 examples/s]Running tokenizer on dataset:  17%|█▋        | 61000/363846 [00:11<00:51, 5857.63 examples/s]Running tokenizer on dataset:  17%|█▋        | 62000/363846 [00:11<00:51, 5891.27 examples/s]Running tokenizer on dataset:  17%|█▋        | 63000/363846 [00:11<00:51, 5848.33 examples/s]Running tokenizer on dataset:  18%|█▊        | 64000/363846 [00:11<00:51, 5868.36 examples/s]Running tokenizer on dataset:  18%|█▊        | 65000/363846 [00:11<00:51, 5858.11 examples/s]Running tokenizer on dataset:  18%|█▊        | 66000/363846 [00:11<00:51, 5835.40 examples/s]Running tokenizer on dataset:  18%|█▊        | 67000/363846 [00:12<00:50, 5854.86 examples/s]Running tokenizer on dataset:  19%|█▊        | 68000/363846 [00:12<00:50, 5888.44 examples/s]Running tokenizer on dataset:  19%|█▉        | 69000/363846 [00:12<01:00, 4848.67 examples/s]Running tokenizer on dataset:  19%|█▉        | 70000/363846 [00:12<00:57, 5130.41 examples/s]Running tokenizer on dataset:  20%|█▉        | 71000/363846 [00:12<00:55, 5322.06 examples/s]Running tokenizer on dataset:  20%|█▉        | 72000/363846 [00:13<00:53, 5472.90 examples/s]Running tokenizer on dataset:  20%|██        | 73000/363846 [00:13<00:52, 5572.93 examples/s]Running tokenizer on dataset:  20%|██        | 74000/363846 [00:13<00:51, 5633.86 examples/s]Running tokenizer on dataset:  21%|██        | 75000/363846 [00:13<00:51, 5614.01 examples/s]Running tokenizer on dataset:  21%|██        | 76000/363846 [00:13<00:50, 5705.81 examples/s]Running tokenizer on dataset:  21%|██        | 77000/363846 [00:13<00:49, 5790.30 examples/s]Running tokenizer on dataset:  21%|██▏       | 78000/363846 [00:14<00:49, 5774.32 examples/s]Running tokenizer on dataset:  22%|██▏       | 79000/363846 [00:14<00:48, 5860.24 examples/s]Running tokenizer on dataset:  22%|██▏       | 80000/363846 [00:14<00:48, 5902.65 examples/s]Running tokenizer on dataset:  22%|██▏       | 81000/363846 [00:14<00:48, 5846.46 examples/s]Running tokenizer on dataset:  23%|██▎       | 82000/363846 [00:14<00:47, 5911.08 examples/s]Running tokenizer on dataset:  23%|██▎       | 83000/363846 [00:14<00:47, 5916.13 examples/s]Running tokenizer on dataset:  23%|██▎       | 84000/363846 [00:15<00:47, 5879.40 examples/s]Running tokenizer on dataset:  23%|██▎       | 85000/363846 [00:15<00:47, 5890.34 examples/s]Running tokenizer on dataset:  24%|██▎       | 86000/363846 [00:15<00:47, 5856.15 examples/s]Running tokenizer on dataset:  24%|██▍       | 87000/363846 [00:15<00:47, 5826.69 examples/s]Running tokenizer on dataset:  24%|██▍       | 88000/363846 [00:15<00:46, 5887.85 examples/s]Running tokenizer on dataset:  24%|██▍       | 89000/363846 [00:15<00:46, 5883.69 examples/s]Running tokenizer on dataset:  25%|██▍       | 90000/363846 [00:16<00:46, 5855.16 examples/s]Running tokenizer on dataset:  25%|██▌       | 91000/363846 [00:16<00:46, 5859.12 examples/s]Running tokenizer on dataset:  25%|██▌       | 92000/363846 [00:16<00:46, 5876.90 examples/s]Running tokenizer on dataset:  26%|██▌       | 93000/363846 [00:16<00:46, 5816.15 examples/s]Running tokenizer on dataset:  26%|██▌       | 94000/363846 [00:16<00:54, 4949.92 examples/s]Running tokenizer on dataset:  26%|██▌       | 95000/363846 [00:17<00:51, 5204.84 examples/s]Running tokenizer on dataset:  26%|██▋       | 96000/363846 [00:17<00:49, 5395.00 examples/s]Running tokenizer on dataset:  27%|██▋       | 97000/363846 [00:17<00:48, 5520.39 examples/s]Running tokenizer on dataset:  27%|██▋       | 98000/363846 [00:17<00:47, 5634.73 examples/s]Running tokenizer on dataset:  27%|██▋       | 99000/363846 [00:17<00:46, 5685.43 examples/s]Running tokenizer on dataset:  27%|██▋       | 100000/363846 [00:17<00:45, 5775.11 examples/s]Running tokenizer on dataset:  28%|██▊       | 101000/363846 [00:18<00:45, 5803.88 examples/s]Running tokenizer on dataset:  28%|██▊       | 102000/363846 [00:18<00:45, 5794.09 examples/s]Running tokenizer on dataset:  28%|██▊       | 103000/363846 [00:18<00:44, 5827.03 examples/s]Running tokenizer on dataset:  29%|██▊       | 104000/363846 [00:18<00:44, 5830.13 examples/s]Running tokenizer on dataset:  29%|██▉       | 105000/363846 [00:18<00:44, 5800.17 examples/s]Running tokenizer on dataset:  29%|██▉       | 106000/363846 [00:18<00:44, 5856.77 examples/s]Running tokenizer on dataset:  29%|██▉       | 107000/363846 [00:19<00:44, 5824.90 examples/s]Running tokenizer on dataset:  30%|██▉       | 108000/363846 [00:19<00:44, 5799.81 examples/s]Running tokenizer on dataset:  30%|██▉       | 109000/363846 [00:19<00:43, 5839.53 examples/s]Running tokenizer on dataset:  30%|███       | 110000/363846 [00:19<00:43, 5850.24 examples/s]Running tokenizer on dataset:  31%|███       | 111000/363846 [00:19<00:43, 5848.20 examples/s]Running tokenizer on dataset:  31%|███       | 112000/363846 [00:19<00:42, 5937.62 examples/s]Running tokenizer on dataset:  31%|███       | 113000/363846 [00:20<00:42, 5914.35 examples/s]Running tokenizer on dataset:  31%|███▏      | 114000/363846 [00:20<00:42, 5896.48 examples/s]Running tokenizer on dataset:  32%|███▏      | 115000/363846 [00:20<00:41, 5938.43 examples/s]Running tokenizer on dataset:  32%|███▏      | 116000/363846 [00:20<00:41, 5911.63 examples/s]Running tokenizer on dataset:  32%|███▏      | 117000/363846 [00:20<00:42, 5869.50 examples/s]Running tokenizer on dataset:  32%|███▏      | 118000/363846 [00:21<00:50, 4907.48 examples/s]Running tokenizer on dataset:  33%|███▎      | 119000/363846 [00:21<00:47, 5152.45 examples/s]Running tokenizer on dataset:  33%|███▎      | 120000/363846 [00:21<00:45, 5366.17 examples/s]Running tokenizer on dataset:  33%|███▎      | 121000/363846 [00:21<00:43, 5535.44 examples/s]Running tokenizer on dataset:  34%|███▎      | 122000/363846 [00:21<00:42, 5636.66 examples/s]Running tokenizer on dataset:  34%|███▍      | 123000/363846 [00:21<00:42, 5692.37 examples/s]Running tokenizer on dataset:  34%|███▍      | 124000/363846 [00:22<00:41, 5786.47 examples/s]Running tokenizer on dataset:  34%|███▍      | 125000/363846 [00:22<00:41, 5790.26 examples/s]Running tokenizer on dataset:  35%|███▍      | 126000/363846 [00:22<00:41, 5784.41 examples/s]Running tokenizer on dataset:  35%|███▍      | 127000/363846 [00:22<00:40, 5861.09 examples/s]Running tokenizer on dataset:  35%|███▌      | 128000/363846 [00:22<00:40, 5840.06 examples/s]Running tokenizer on dataset:  35%|███▌      | 129000/363846 [00:22<00:40, 5834.52 examples/s]Running tokenizer on dataset:  36%|███▌      | 130000/363846 [00:23<00:39, 5891.67 examples/s]Running tokenizer on dataset:  36%|███▌      | 131000/363846 [00:23<00:39, 5847.22 examples/s]Running tokenizer on dataset:  36%|███▋      | 132000/363846 [00:23<00:39, 5835.97 examples/s]Running tokenizer on dataset:  37%|███▋      | 133000/363846 [00:23<00:39, 5855.13 examples/s]Running tokenizer on dataset:  37%|███▋      | 134000/363846 [00:23<00:39, 5844.93 examples/s]Running tokenizer on dataset:  37%|███▋      | 135000/363846 [00:23<00:39, 5854.67 examples/s]Running tokenizer on dataset:  37%|███▋      | 136000/363846 [00:24<00:38, 5882.71 examples/s]Running tokenizer on dataset:  38%|███▊      | 137000/363846 [00:24<00:38, 5848.20 examples/s]Running tokenizer on dataset:  38%|███▊      | 138000/363846 [00:24<00:38, 5824.99 examples/s]Running tokenizer on dataset:  38%|███▊      | 139000/363846 [00:24<00:38, 5829.56 examples/s]Running tokenizer on dataset:  38%|███▊      | 140000/363846 [00:24<00:38, 5838.00 examples/s]Running tokenizer on dataset:  39%|███▉      | 141000/363846 [00:25<00:38, 5800.85 examples/s]Running tokenizer on dataset:  39%|███▉      | 142000/363846 [00:25<00:45, 4860.76 examples/s]Running tokenizer on dataset:  39%|███▉      | 143000/363846 [00:25<00:43, 5111.19 examples/s]Running tokenizer on dataset:  40%|███▉      | 144000/363846 [00:25<00:41, 5283.79 examples/s]Running tokenizer on dataset:  40%|███▉      | 145000/363846 [00:25<00:39, 5471.70 examples/s]Running tokenizer on dataset:  40%|████      | 146000/363846 [00:25<00:38, 5593.69 examples/s]Running tokenizer on dataset:  40%|████      | 147000/363846 [00:26<00:38, 5648.72 examples/s]Running tokenizer on dataset:  41%|████      | 148000/363846 [00:26<00:37, 5762.88 examples/s]Running tokenizer on dataset:  41%|████      | 149000/363846 [00:26<00:36, 5817.55 examples/s]Running tokenizer on dataset:  41%|████      | 150000/363846 [00:26<00:36, 5793.03 examples/s]Running tokenizer on dataset:  42%|████▏     | 151000/363846 [00:26<00:36, 5872.07 examples/s]Running tokenizer on dataset:  42%|████▏     | 152000/363846 [00:26<00:36, 5879.46 examples/s]Running tokenizer on dataset:  42%|████▏     | 153000/363846 [00:27<00:36, 5806.81 examples/s]Running tokenizer on dataset:  42%|████▏     | 154000/363846 [00:27<00:35, 5832.11 examples/s]Running tokenizer on dataset:  43%|████▎     | 155000/363846 [00:27<00:35, 5819.78 examples/s]Running tokenizer on dataset:  43%|████▎     | 156000/363846 [00:27<00:35, 5825.62 examples/s]Running tokenizer on dataset:  43%|████▎     | 157000/363846 [00:27<00:35, 5870.21 examples/s]Running tokenizer on dataset:  43%|████▎     | 158000/363846 [00:28<00:34, 5883.31 examples/s]Running tokenizer on dataset:  44%|████▎     | 159000/363846 [00:28<00:34, 5858.40 examples/s]Running tokenizer on dataset:  44%|████▍     | 160000/363846 [00:28<00:34, 5896.03 examples/s]Running tokenizer on dataset:  44%|████▍     | 161000/363846 [00:28<00:34, 5864.16 examples/s]Running tokenizer on dataset:  45%|████▍     | 162000/363846 [00:28<00:34, 5816.11 examples/s]Running tokenizer on dataset:  45%|████▍     | 163000/363846 [00:28<00:34, 5840.52 examples/s]Running tokenizer on dataset:  45%|████▌     | 164000/363846 [00:29<00:33, 5888.91 examples/s]Running tokenizer on dataset:  45%|████▌     | 165000/363846 [00:29<00:33, 5864.26 examples/s]Running tokenizer on dataset:  46%|████▌     | 166000/363846 [00:29<00:39, 4967.03 examples/s]Running tokenizer on dataset:  46%|████▌     | 167000/363846 [00:29<00:38, 5165.89 examples/s]Running tokenizer on dataset:  46%|████▌     | 168000/363846 [00:29<00:36, 5364.94 examples/s]Running tokenizer on dataset:  46%|████▋     | 169000/363846 [00:30<00:35, 5551.57 examples/s]Running tokenizer on dataset:  47%|████▋     | 170000/363846 [00:30<00:34, 5640.66 examples/s]Running tokenizer on dataset:  47%|████▋     | 171000/363846 [00:30<00:34, 5661.89 examples/s]Running tokenizer on dataset:  47%|████▋     | 172000/363846 [00:30<00:33, 5737.38 examples/s]Running tokenizer on dataset:  48%|████▊     | 173000/363846 [00:30<00:33, 5769.88 examples/s]Running tokenizer on dataset:  48%|████▊     | 174000/363846 [00:30<00:32, 5760.45 examples/s]Running tokenizer on dataset:  48%|████▊     | 175000/363846 [00:31<00:32, 5836.91 examples/s]Running tokenizer on dataset:  48%|████▊     | 176000/363846 [00:31<00:32, 5814.49 examples/s]Running tokenizer on dataset:  49%|████▊     | 177000/363846 [00:31<00:32, 5780.36 examples/s]Running tokenizer on dataset:  49%|████▉     | 178000/363846 [00:31<00:32, 5791.32 examples/s]Running tokenizer on dataset:  49%|████▉     | 179000/363846 [00:31<00:32, 5771.94 examples/s]Running tokenizer on dataset:  49%|████▉     | 180000/363846 [00:31<00:31, 5789.62 examples/s]Running tokenizer on dataset:  50%|████▉     | 181000/363846 [00:32<00:31, 5874.12 examples/s]Running tokenizer on dataset:  50%|█████     | 182000/363846 [00:32<00:31, 5864.75 examples/s]Running tokenizer on dataset:  50%|█████     | 183000/363846 [00:32<00:30, 5860.83 examples/s]Running tokenizer on dataset:  51%|█████     | 184000/363846 [00:32<00:30, 5913.68 examples/s]Running tokenizer on dataset:  51%|█████     | 185000/363846 [00:32<00:30, 5891.26 examples/s]Running tokenizer on dataset:  51%|█████     | 186000/363846 [00:32<00:30, 5856.69 examples/s]Running tokenizer on dataset:  51%|█████▏    | 187000/363846 [00:33<00:30, 5863.94 examples/s]Running tokenizer on dataset:  52%|█████▏    | 188000/363846 [00:33<00:29, 5875.47 examples/s]Running tokenizer on dataset:  52%|█████▏    | 189000/363846 [00:33<00:29, 5834.01 examples/s]Running tokenizer on dataset:  52%|█████▏    | 190000/363846 [00:33<00:35, 4894.65 examples/s]Running tokenizer on dataset:  52%|█████▏    | 191000/363846 [00:33<00:33, 5157.77 examples/s]Running tokenizer on dataset:  53%|█████▎    | 192000/363846 [00:34<00:32, 5351.85 examples/s]Running tokenizer on dataset:  53%|█████▎    | 193000/363846 [00:34<00:31, 5510.25 examples/s]Running tokenizer on dataset:  53%|█████▎    | 194000/363846 [00:34<00:30, 5615.10 examples/s]Running tokenizer on dataset:  54%|█████▎    | 195000/363846 [00:34<00:29, 5630.13 examples/s]Running tokenizer on dataset:  54%|█████▍    | 196000/363846 [00:34<00:29, 5708.65 examples/s]Running tokenizer on dataset:  54%|█████▍    | 197000/363846 [00:34<00:29, 5749.10 examples/s]Running tokenizer on dataset:  54%|█████▍    | 198000/363846 [00:35<00:28, 5756.54 examples/s]Running tokenizer on dataset:  55%|█████▍    | 199000/363846 [00:35<00:28, 5789.39 examples/s]Running tokenizer on dataset:  55%|█████▍    | 200000/363846 [00:35<00:28, 5765.28 examples/s]Running tokenizer on dataset:  55%|█████▌    | 201000/363846 [00:35<00:28, 5729.61 examples/s]Running tokenizer on dataset:  56%|█████▌    | 202000/363846 [00:35<00:28, 5764.95 examples/s]Running tokenizer on dataset:  56%|█████▌    | 203000/363846 [00:35<00:28, 5726.74 examples/s]Running tokenizer on dataset:  56%|█████▌    | 204000/363846 [00:36<00:27, 5753.36 examples/s]Running tokenizer on dataset:  56%|█████▋    | 205000/363846 [00:36<00:27, 5794.14 examples/s]Running tokenizer on dataset:  57%|█████▋    | 206000/363846 [00:36<00:27, 5756.51 examples/s]Running tokenizer on dataset:  57%|█████▋    | 207000/363846 [00:36<00:27, 5783.25 examples/s]Running tokenizer on dataset:  57%|█████▋    | 208000/363846 [00:36<00:26, 5841.16 examples/s]Running tokenizer on dataset:  57%|█████▋    | 209000/363846 [00:36<00:26, 5791.49 examples/s]Running tokenizer on dataset:  58%|█████▊    | 210000/363846 [00:37<00:26, 5780.87 examples/s]Running tokenizer on dataset:  58%|█████▊    | 211000/363846 [00:37<00:26, 5749.10 examples/s]Running tokenizer on dataset:  58%|█████▊    | 212000/363846 [00:37<00:26, 5749.71 examples/s]Running tokenizer on dataset:  59%|█████▊    | 213000/363846 [00:37<00:25, 5810.29 examples/s]Running tokenizer on dataset:  59%|█████▉    | 214000/363846 [00:37<00:30, 4876.92 examples/s]Running tokenizer on dataset:  59%|█████▉    | 215000/363846 [00:38<00:28, 5159.01 examples/s]Running tokenizer on dataset:  59%|█████▉    | 216000/363846 [00:38<00:27, 5383.18 examples/s]Running tokenizer on dataset:  60%|█████▉    | 217000/363846 [00:38<00:26, 5490.71 examples/s]Running tokenizer on dataset:  60%|█████▉    | 218000/363846 [00:38<00:26, 5594.91 examples/s]Running tokenizer on dataset:  60%|██████    | 219000/363846 [00:38<00:25, 5620.77 examples/s]Running tokenizer on dataset:  60%|██████    | 220000/363846 [00:38<00:25, 5707.47 examples/s]Running tokenizer on dataset:  61%|██████    | 221000/363846 [00:39<00:24, 5729.39 examples/s]Running tokenizer on dataset:  61%|██████    | 222000/363846 [00:39<00:24, 5760.22 examples/s]Running tokenizer on dataset:  61%|██████▏   | 223000/363846 [00:39<00:24, 5828.65 examples/s]Running tokenizer on dataset:  62%|██████▏   | 224000/363846 [00:39<00:23, 5838.08 examples/s]Running tokenizer on dataset:  62%|██████▏   | 225000/363846 [00:39<00:23, 5815.54 examples/s]Running tokenizer on dataset:  62%|██████▏   | 226000/363846 [00:39<00:23, 5851.15 examples/s]Running tokenizer on dataset:  62%|██████▏   | 227000/363846 [00:40<00:23, 5815.44 examples/s]Running tokenizer on dataset:  63%|██████▎   | 228000/363846 [00:40<00:23, 5762.55 examples/s]Running tokenizer on dataset:  63%|██████▎   | 229000/363846 [00:40<00:23, 5793.66 examples/s]Running tokenizer on dataset:  63%|██████▎   | 230000/363846 [00:40<00:23, 5808.85 examples/s]Running tokenizer on dataset:  63%|██████▎   | 231000/363846 [00:40<00:22, 5847.82 examples/s]Running tokenizer on dataset:  64%|██████▍   | 232000/363846 [00:41<00:22, 5892.28 examples/s]Running tokenizer on dataset:  64%|██████▍   | 233000/363846 [00:41<00:22, 5837.23 examples/s]Running tokenizer on dataset:  64%|██████▍   | 234000/363846 [00:41<00:22, 5815.92 examples/s]Running tokenizer on dataset:  65%|██████▍   | 235000/363846 [00:41<00:22, 5828.14 examples/s]Running tokenizer on dataset:  65%|██████▍   | 236000/363846 [00:41<00:21, 5859.40 examples/s]Running tokenizer on dataset:  65%|██████▌   | 237000/363846 [00:41<00:21, 5850.64 examples/s]Running tokenizer on dataset:  65%|██████▌   | 238000/363846 [00:42<00:25, 4883.68 examples/s]Running tokenizer on dataset:  66%|██████▌   | 239000/363846 [00:42<00:24, 5128.12 examples/s]Running tokenizer on dataset:  66%|██████▌   | 240000/363846 [00:42<00:23, 5318.79 examples/s]Running tokenizer on dataset:  66%|██████▌   | 241000/363846 [00:42<00:22, 5442.91 examples/s]Running tokenizer on dataset:  67%|██████▋   | 242000/363846 [00:42<00:21, 5573.45 examples/s]Running tokenizer on dataset:  67%|██████▋   | 243000/363846 [00:43<00:21, 5589.45 examples/s]Running tokenizer on dataset:  67%|██████▋   | 244000/363846 [00:43<00:21, 5630.39 examples/s]Running tokenizer on dataset:  67%|██████▋   | 245000/363846 [00:43<00:20, 5705.27 examples/s]Running tokenizer on dataset:  68%|██████▊   | 246000/363846 [00:43<00:20, 5747.08 examples/s]Running tokenizer on dataset:  68%|██████▊   | 247000/363846 [00:43<00:20, 5809.07 examples/s]Running tokenizer on dataset:  68%|██████▊   | 248000/363846 [00:43<00:19, 5870.49 examples/s]Running tokenizer on dataset:  68%|██████▊   | 249000/363846 [00:44<00:19, 5856.38 examples/s]Running tokenizer on dataset:  69%|██████▊   | 250000/363846 [00:44<00:19, 5890.07 examples/s]Running tokenizer on dataset:  69%|██████▉   | 251000/363846 [00:44<00:19, 5875.62 examples/s]Running tokenizer on dataset:  69%|██████▉   | 252000/363846 [00:44<00:19, 5858.85 examples/s]Running tokenizer on dataset:  70%|██████▉   | 253000/363846 [00:44<00:18, 5871.92 examples/s]Running tokenizer on dataset:  70%|██████▉   | 254000/363846 [00:44<00:18, 5838.37 examples/s]Running tokenizer on dataset:  70%|███████   | 255000/363846 [00:45<00:18, 5832.87 examples/s]Running tokenizer on dataset:  70%|███████   | 256000/363846 [00:45<00:18, 5847.82 examples/s]Running tokenizer on dataset:  71%|███████   | 257000/363846 [00:45<00:18, 5833.23 examples/s]Running tokenizer on dataset:  71%|███████   | 258000/363846 [00:45<00:18, 5815.58 examples/s]Running tokenizer on dataset:  71%|███████   | 259000/363846 [00:45<00:17, 5825.31 examples/s]Running tokenizer on dataset:  71%|███████▏  | 260000/363846 [00:45<00:17, 5879.38 examples/s]Running tokenizer on dataset:  72%|███████▏  | 261000/363846 [00:46<00:17, 5848.23 examples/s]Running tokenizer on dataset:  72%|███████▏  | 262000/363846 [00:46<00:20, 4911.56 examples/s]Running tokenizer on dataset:  72%|███████▏  | 263000/363846 [00:46<00:19, 5187.61 examples/s]Running tokenizer on dataset:  73%|███████▎  | 264000/363846 [00:46<00:18, 5352.62 examples/s]Running tokenizer on dataset:  73%|███████▎  | 265000/363846 [00:46<00:18, 5490.77 examples/s]Running tokenizer on dataset:  73%|███████▎  | 266000/363846 [00:47<00:17, 5584.57 examples/s]Running tokenizer on dataset:  73%|███████▎  | 267000/363846 [00:47<00:17, 5645.62 examples/s]Running tokenizer on dataset:  74%|███████▎  | 268000/363846 [00:47<00:16, 5733.31 examples/s]Running tokenizer on dataset:  74%|███████▍  | 269000/363846 [00:47<00:16, 5762.71 examples/s]Running tokenizer on dataset:  74%|███████▍  | 270000/363846 [00:47<00:16, 5712.15 examples/s]Running tokenizer on dataset:  74%|███████▍  | 271000/363846 [00:47<00:16, 5788.05 examples/s]Running tokenizer on dataset:  75%|███████▍  | 272000/363846 [00:48<00:15, 5814.44 examples/s]Running tokenizer on dataset:  75%|███████▌  | 273000/363846 [00:48<00:15, 5753.74 examples/s]Running tokenizer on dataset:  75%|███████▌  | 274000/363846 [00:48<00:15, 5797.91 examples/s]Running tokenizer on dataset:  76%|███████▌  | 275000/363846 [00:48<00:15, 5764.24 examples/s]Running tokenizer on dataset:  76%|███████▌  | 276000/363846 [00:48<00:15, 5737.91 examples/s]Running tokenizer on dataset:  76%|███████▌  | 277000/363846 [00:48<00:15, 5770.46 examples/s]Running tokenizer on dataset:  76%|███████▋  | 278000/363846 [00:49<00:14, 5828.33 examples/s]Running tokenizer on dataset:  77%|███████▋  | 279000/363846 [00:49<00:14, 5805.47 examples/s]Running tokenizer on dataset:  77%|███████▋  | 280000/363846 [00:49<00:14, 5837.52 examples/s]Running tokenizer on dataset:  77%|███████▋  | 281000/363846 [00:49<00:14, 5834.90 examples/s]Running tokenizer on dataset:  78%|███████▊  | 282000/363846 [00:49<00:14, 5824.18 examples/s]Running tokenizer on dataset:  78%|███████▊  | 283000/363846 [00:49<00:13, 5881.50 examples/s]Running tokenizer on dataset:  78%|███████▊  | 284000/363846 [00:50<00:13, 5887.28 examples/s]Running tokenizer on dataset:  78%|███████▊  | 285000/363846 [00:50<00:13, 5822.26 examples/s]Running tokenizer on dataset:  79%|███████▊  | 286000/363846 [00:50<00:16, 4858.61 examples/s]Running tokenizer on dataset:  79%|███████▉  | 287000/363846 [00:50<00:15, 5116.85 examples/s]Running tokenizer on dataset:  79%|███████▉  | 288000/363846 [00:50<00:14, 5324.92 examples/s]Running tokenizer on dataset:  79%|███████▉  | 289000/363846 [00:51<00:13, 5489.37 examples/s]Running tokenizer on dataset:  80%|███████▉  | 290000/363846 [00:51<00:13, 5586.08 examples/s]Running tokenizer on dataset:  80%|███████▉  | 291000/363846 [00:51<00:13, 5557.48 examples/s]Running tokenizer on dataset:  80%|████████  | 292000/363846 [00:51<00:12, 5611.37 examples/s]Running tokenizer on dataset:  81%|████████  | 293000/363846 [00:51<00:12, 5653.02 examples/s]Running tokenizer on dataset:  81%|████████  | 294000/363846 [00:52<00:12, 5695.12 examples/s]Running tokenizer on dataset:  81%|████████  | 295000/363846 [00:52<00:11, 5793.78 examples/s]Running tokenizer on dataset:  81%|████████▏ | 296000/363846 [00:52<00:11, 5835.56 examples/s]Running tokenizer on dataset:  82%|████████▏ | 297000/363846 [00:52<00:11, 5823.61 examples/s]Running tokenizer on dataset:  82%|████████▏ | 298000/363846 [00:52<00:11, 5864.16 examples/s]Running tokenizer on dataset:  82%|████████▏ | 299000/363846 [00:52<00:11, 5790.82 examples/s]Running tokenizer on dataset:  82%|████████▏ | 300000/363846 [00:53<00:10, 5828.56 examples/s]Running tokenizer on dataset:  83%|████████▎ | 301000/363846 [00:53<00:10, 5914.63 examples/s]Running tokenizer on dataset:  83%|████████▎ | 302000/363846 [00:53<00:10, 5830.72 examples/s]Running tokenizer on dataset:  83%|████████▎ | 303000/363846 [00:53<00:10, 5829.60 examples/s]Running tokenizer on dataset:  84%|████████▎ | 304000/363846 [00:53<00:10, 5909.33 examples/s]Running tokenizer on dataset:  84%|████████▍ | 305000/363846 [00:53<00:10, 5853.61 examples/s]Running tokenizer on dataset:  84%|████████▍ | 306000/363846 [00:54<00:09, 5851.89 examples/s]Running tokenizer on dataset:  84%|████████▍ | 307000/363846 [00:54<00:09, 5865.92 examples/s]Running tokenizer on dataset:  85%|████████▍ | 308000/363846 [00:54<00:09, 5809.89 examples/s]Running tokenizer on dataset:  85%|████████▍ | 309000/363846 [00:54<00:09, 5737.46 examples/s]Running tokenizer on dataset:  85%|████████▌ | 310000/363846 [00:54<00:11, 4627.24 examples/s]Running tokenizer on dataset:  85%|████████▌ | 311000/363846 [00:55<00:10, 4964.50 examples/s]Running tokenizer on dataset:  86%|████████▌ | 312000/363846 [00:55<00:09, 5229.11 examples/s]Running tokenizer on dataset:  86%|████████▌ | 313000/363846 [00:55<00:09, 5369.51 examples/s]Running tokenizer on dataset:  86%|████████▋ | 314000/363846 [00:55<00:08, 5539.36 examples/s]Running tokenizer on dataset:  87%|████████▋ | 315000/363846 [00:55<00:08, 5633.16 examples/s]Running tokenizer on dataset:  87%|████████▋ | 316000/363846 [00:55<00:08, 5714.58 examples/s]Running tokenizer on dataset:  87%|████████▋ | 317000/363846 [00:56<00:08, 5757.13 examples/s]Running tokenizer on dataset:  87%|████████▋ | 318000/363846 [00:56<00:07, 5775.87 examples/s]Running tokenizer on dataset:  88%|████████▊ | 319000/363846 [00:56<00:07, 5748.67 examples/s]Running tokenizer on dataset:  88%|████████▊ | 320000/363846 [00:56<00:07, 5773.71 examples/s]Running tokenizer on dataset:  88%|████████▊ | 321000/363846 [00:56<00:07, 5795.86 examples/s]Running tokenizer on dataset:  88%|████████▊ | 322000/363846 [00:56<00:07, 5841.99 examples/s]Running tokenizer on dataset:  89%|████████▉ | 323000/363846 [00:57<00:07, 5820.97 examples/s]Running tokenizer on dataset:  89%|████████▉ | 324000/363846 [00:57<00:06, 5861.38 examples/s]Running tokenizer on dataset:  89%|████████▉ | 325000/363846 [00:57<00:06, 5869.89 examples/s]Running tokenizer on dataset:  90%|████████▉ | 326000/363846 [00:57<00:06, 5851.89 examples/s]Running tokenizer on dataset:  90%|████████▉ | 327000/363846 [00:57<00:06, 5891.57 examples/s]Running tokenizer on dataset:  90%|█████████ | 328000/363846 [00:57<00:06, 5890.76 examples/s]Running tokenizer on dataset:  90%|█████████ | 329000/363846 [00:58<00:05, 5821.31 examples/s]Running tokenizer on dataset:  91%|█████████ | 330000/363846 [00:58<00:05, 5891.51 examples/s]Running tokenizer on dataset:  91%|█████████ | 331000/363846 [00:58<00:05, 5857.12 examples/s]Running tokenizer on dataset:  91%|█████████ | 332000/363846 [00:58<00:05, 5877.98 examples/s]Running tokenizer on dataset:  92%|█████████▏| 333000/363846 [00:58<00:05, 5895.45 examples/s]Running tokenizer on dataset:  92%|█████████▏| 334000/363846 [00:59<00:06, 4902.63 examples/s]Running tokenizer on dataset:  92%|█████████▏| 335000/363846 [00:59<00:05, 5166.12 examples/s]Running tokenizer on dataset:  92%|█████████▏| 336000/363846 [00:59<00:05, 5323.04 examples/s]Running tokenizer on dataset:  93%|█████████▎| 337000/363846 [00:59<00:04, 5451.15 examples/s]Running tokenizer on dataset:  93%|█████████▎| 338000/363846 [00:59<00:04, 5553.50 examples/s]Running tokenizer on dataset:  93%|█████████▎| 339000/363846 [00:59<00:04, 5640.28 examples/s]Running tokenizer on dataset:  93%|█████████▎| 340000/363846 [01:00<00:04, 5699.27 examples/s]Running tokenizer on dataset:  94%|█████████▎| 341000/363846 [01:00<00:03, 5747.72 examples/s]Running tokenizer on dataset:  94%|█████████▍| 342000/363846 [01:00<00:03, 5709.43 examples/s]Running tokenizer on dataset:  94%|█████████▍| 343000/363846 [01:00<00:03, 5748.23 examples/s]Running tokenizer on dataset:  95%|█████████▍| 344000/363846 [01:00<00:03, 5770.69 examples/s]Running tokenizer on dataset:  95%|█████████▍| 345000/363846 [01:00<00:03, 5812.65 examples/s]Running tokenizer on dataset:  95%|█████████▌| 346000/363846 [01:01<00:03, 5850.28 examples/s]Running tokenizer on dataset:  95%|█████████▌| 347000/363846 [01:01<00:02, 5818.89 examples/s]Running tokenizer on dataset:  96%|█████████▌| 348000/363846 [01:01<00:02, 5853.22 examples/s]Running tokenizer on dataset:  96%|█████████▌| 349000/363846 [01:01<00:02, 5883.70 examples/s]Running tokenizer on dataset:  96%|█████████▌| 350000/363846 [01:01<00:02, 5872.26 examples/s]Running tokenizer on dataset:  96%|█████████▋| 351000/363846 [01:01<00:02, 5857.46 examples/s]Running tokenizer on dataset:  97%|█████████▋| 352000/363846 [01:02<00:02, 5744.52 examples/s]Running tokenizer on dataset:  97%|█████████▋| 353000/363846 [01:02<00:01, 5761.37 examples/s]Running tokenizer on dataset:  97%|█████████▋| 354000/363846 [01:02<00:01, 5741.35 examples/s]Running tokenizer on dataset:  98%|█████████▊| 355000/363846 [01:02<00:01, 5751.74 examples/s]Running tokenizer on dataset:  98%|█████████▊| 356000/363846 [01:02<00:01, 5789.86 examples/s]Running tokenizer on dataset:  98%|█████████▊| 357000/363846 [01:03<00:01, 5810.84 examples/s]Running tokenizer on dataset:  98%|█████████▊| 358000/363846 [01:03<00:01, 4737.52 examples/s]Running tokenizer on dataset:  99%|█████████▊| 359000/363846 [01:03<00:00, 5031.53 examples/s]Running tokenizer on dataset:  99%|█████████▉| 360000/363846 [01:03<00:00, 5244.51 examples/s]Running tokenizer on dataset:  99%|█████████▉| 361000/363846 [01:03<00:00, 5388.23 examples/s]Running tokenizer on dataset:  99%|█████████▉| 362000/363846 [01:04<00:00, 5575.32 examples/s]Running tokenizer on dataset: 100%|█████████▉| 363000/363846 [01:04<00:00, 5651.49 examples/s]Running tokenizer on dataset: 100%|██████████| 363846/363846 [01:04<00:00, 5626.41 examples/s]Running tokenizer on dataset: 100%|██████████| 363846/363846 [01:04<00:00, 5651.02 examples/s]
Running tokenizer on dataset:   0%|          | 0/40430 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4823967296169288.arrow
02/18/2024 21:34:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4823967296169288.arrow
Running tokenizer on dataset:   2%|▏         | 1000/40430 [00:00<00:08, 4824.51 examples/s]Running tokenizer on dataset:   5%|▍         | 2000/40430 [00:00<00:07, 5328.66 examples/s]Running tokenizer on dataset:   7%|▋         | 3000/40430 [00:00<00:06, 5570.78 examples/s]Running tokenizer on dataset:  10%|▉         | 4000/40430 [00:00<00:06, 5701.08 examples/s]Running tokenizer on dataset:  12%|█▏        | 5000/40430 [00:00<00:06, 5715.51 examples/s]Running tokenizer on dataset:  15%|█▍        | 6000/40430 [00:01<00:05, 5751.18 examples/s]Running tokenizer on dataset:  17%|█▋        | 7000/40430 [00:01<00:05, 5768.48 examples/s]Running tokenizer on dataset:  20%|█▉        | 8000/40430 [00:01<00:05, 5726.34 examples/s]Running tokenizer on dataset:  22%|██▏       | 9000/40430 [00:01<00:05, 5757.28 examples/s]Running tokenizer on dataset:  25%|██▍       | 10000/40430 [00:01<00:05, 5779.89 examples/s]Running tokenizer on dataset:  27%|██▋       | 11000/40430 [00:01<00:05, 5691.98 examples/s]Running tokenizer on dataset:  30%|██▉       | 12000/40430 [00:02<00:04, 5721.92 examples/s]Running tokenizer on dataset:  32%|███▏      | 13000/40430 [00:02<00:04, 5779.50 examples/s]Running tokenizer on dataset:  35%|███▍      | 14000/40430 [00:02<00:04, 5755.05 examples/s]Running tokenizer on dataset:  37%|███▋      | 15000/40430 [00:02<00:04, 5791.20 examples/s]Running tokenizer on dataset:  40%|███▉      | 16000/40430 [00:02<00:04, 5851.38 examples/s]Running tokenizer on dataset:  42%|████▏     | 17000/40430 [00:02<00:04, 5839.45 examples/s]Running tokenizer on dataset:  45%|████▍     | 18000/40430 [00:03<00:04, 4876.99 examples/s]Running tokenizer on dataset:  47%|████▋     | 19000/40430 [00:03<00:04, 5137.28 examples/s]Running tokenizer on dataset:  49%|████▉     | 20000/40430 [00:03<00:03, 5304.43 examples/s]Running tokenizer on dataset:  52%|█████▏    | 21000/40430 [00:03<00:03, 5466.77 examples/s]Running tokenizer on dataset:  54%|█████▍    | 22000/40430 [00:03<00:03, 5558.50 examples/s]Running tokenizer on dataset:  57%|█████▋    | 23000/40430 [00:04<00:03, 5607.96 examples/s]Running tokenizer on dataset:  59%|█████▉    | 24000/40430 [00:04<00:02, 5739.94 examples/s]Running tokenizer on dataset:  62%|██████▏   | 25000/40430 [00:04<00:02, 5786.81 examples/s]Running tokenizer on dataset:  64%|██████▍   | 26000/40430 [00:04<00:02, 5785.13 examples/s]Running tokenizer on dataset:  67%|██████▋   | 27000/40430 [00:04<00:02, 5830.85 examples/s]Running tokenizer on dataset:  69%|██████▉   | 28000/40430 [00:04<00:02, 5869.40 examples/s]Running tokenizer on dataset:  72%|███████▏  | 29000/40430 [00:05<00:01, 5843.58 examples/s]Running tokenizer on dataset:  74%|███████▍  | 30000/40430 [00:05<00:01, 5857.29 examples/s]Running tokenizer on dataset:  77%|███████▋  | 31000/40430 [00:05<00:01, 5855.46 examples/s]Running tokenizer on dataset:  79%|███████▉  | 32000/40430 [00:05<00:01, 5842.18 examples/s]Running tokenizer on dataset:  82%|████████▏ | 33000/40430 [00:05<00:01, 5845.80 examples/s]Running tokenizer on dataset:  84%|████████▍ | 34000/40430 [00:05<00:01, 5882.91 examples/s]Running tokenizer on dataset:  87%|████████▋ | 35000/40430 [00:06<00:00, 5825.41 examples/s]Running tokenizer on dataset:  89%|████████▉ | 36000/40430 [00:06<00:00, 5837.19 examples/s]Running tokenizer on dataset:  92%|█████████▏| 37000/40430 [00:06<00:00, 5882.40 examples/s]Running tokenizer on dataset:  94%|█████████▍| 38000/40430 [00:06<00:00, 5836.18 examples/s]Running tokenizer on dataset:  96%|█████████▋| 39000/40430 [00:06<00:00, 5873.65 examples/s]Running tokenizer on dataset:  99%|█████████▉| 40000/40430 [00:07<00:00, 5880.48 examples/s]Running tokenizer on dataset: 100%|██████████| 40430/40430 [00:07<00:00, 5696.30 examples/s]
Running tokenizer on dataset:   0%|          | 0/390965 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-61851de63dcaf816.arrow
02/18/2024 21:34:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-61851de63dcaf816.arrow
Running tokenizer on dataset:   0%|          | 1000/390965 [00:00<01:17, 5010.04 examples/s]Running tokenizer on dataset:   1%|          | 2000/390965 [00:00<01:10, 5531.81 examples/s]Running tokenizer on dataset:   1%|          | 3000/390965 [00:00<01:29, 4343.12 examples/s]Running tokenizer on dataset:   1%|          | 4000/390965 [00:00<01:20, 4807.01 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/390965 [00:01<01:15, 5127.91 examples/s]Running tokenizer on dataset:   2%|▏         | 6000/390965 [00:01<01:15, 5106.40 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/390965 [00:01<01:11, 5361.71 examples/s]Running tokenizer on dataset:   2%|▏         | 8000/390965 [00:01<01:09, 5529.82 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/390965 [00:01<01:08, 5585.41 examples/s]Running tokenizer on dataset:   3%|▎         | 10000/390965 [00:01<01:06, 5695.44 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/390965 [00:02<01:06, 5755.76 examples/s]Running tokenizer on dataset:   3%|▎         | 12000/390965 [00:02<01:05, 5774.38 examples/s]Running tokenizer on dataset:   3%|▎         | 13000/390965 [00:02<01:05, 5811.17 examples/s]Running tokenizer on dataset:   4%|▎         | 14000/390965 [00:02<01:04, 5821.38 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/390965 [00:02<01:04, 5825.83 examples/s]Running tokenizer on dataset:   4%|▍         | 16000/390965 [00:02<01:04, 5824.64 examples/s]Running tokenizer on dataset:   4%|▍         | 17000/390965 [00:03<01:04, 5817.05 examples/s]Running tokenizer on dataset:   5%|▍         | 18000/390965 [00:03<01:04, 5825.81 examples/s]Running tokenizer on dataset:   5%|▍         | 19000/390965 [00:03<01:03, 5839.87 examples/s]Running tokenizer on dataset:   5%|▌         | 20000/390965 [00:03<01:03, 5857.06 examples/s]Running tokenizer on dataset:   5%|▌         | 21000/390965 [00:03<01:03, 5841.18 examples/s]Running tokenizer on dataset:   6%|▌         | 22000/390965 [00:03<01:03, 5845.95 examples/s]Running tokenizer on dataset:   6%|▌         | 23000/390965 [00:04<01:02, 5869.02 examples/s]Running tokenizer on dataset:   6%|▌         | 24000/390965 [00:04<01:02, 5828.22 examples/s]Running tokenizer on dataset:   6%|▋         | 25000/390965 [00:04<01:02, 5849.96 examples/s]Running tokenizer on dataset:   7%|▋         | 26000/390965 [00:04<01:02, 5886.36 examples/s]Running tokenizer on dataset:   7%|▋         | 27000/390965 [00:04<01:02, 5839.63 examples/s]Running tokenizer on dataset:   7%|▋         | 28000/390965 [00:04<01:01, 5860.14 examples/s]Running tokenizer on dataset:   7%|▋         | 29000/390965 [00:05<01:01, 5881.15 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/390965 [00:05<01:01, 5873.56 examples/s]Running tokenizer on dataset:   8%|▊         | 31000/390965 [00:05<01:01, 5870.20 examples/s]Running tokenizer on dataset:   8%|▊         | 32000/390965 [00:05<01:01, 5853.23 examples/s]Running tokenizer on dataset:   8%|▊         | 33000/390965 [00:05<01:01, 5796.10 examples/s]Running tokenizer on dataset:   9%|▊         | 34000/390965 [00:05<01:01, 5829.09 examples/s]Running tokenizer on dataset:   9%|▉         | 35000/390965 [00:06<01:01, 5793.52 examples/s]Running tokenizer on dataset:   9%|▉         | 36000/390965 [00:06<01:01, 5798.23 examples/s]Running tokenizer on dataset:   9%|▉         | 37000/390965 [00:06<01:01, 5801.00 examples/s]Running tokenizer on dataset:  10%|▉         | 38000/390965 [00:06<01:02, 5646.45 examples/s]Running tokenizer on dataset:  10%|▉         | 39000/390965 [00:06<01:02, 5671.79 examples/s]Running tokenizer on dataset:  10%|█         | 40000/390965 [00:07<01:01, 5695.72 examples/s]Running tokenizer on dataset:  10%|█         | 41000/390965 [00:07<01:00, 5762.03 examples/s]Running tokenizer on dataset:  11%|█         | 42000/390965 [00:07<01:00, 5796.50 examples/s]Running tokenizer on dataset:  11%|█         | 43000/390965 [00:07<01:00, 5793.77 examples/s]Running tokenizer on dataset:  11%|█▏        | 44000/390965 [00:07<00:59, 5851.24 examples/s]Running tokenizer on dataset:  12%|█▏        | 45000/390965 [00:07<00:58, 5866.94 examples/s]Running tokenizer on dataset:  12%|█▏        | 46000/390965 [00:08<00:58, 5847.73 examples/s]Running tokenizer on dataset:  12%|█▏        | 47000/390965 [00:08<00:58, 5893.43 examples/s]Running tokenizer on dataset:  12%|█▏        | 48000/390965 [00:08<00:58, 5869.20 examples/s]Running tokenizer on dataset:  13%|█▎        | 49000/390965 [00:08<01:12, 4691.76 examples/s]Running tokenizer on dataset:  13%|█▎        | 50000/390965 [00:08<01:08, 4980.59 examples/s]Running tokenizer on dataset:  13%|█▎        | 51000/390965 [00:09<01:05, 5203.02 examples/s]Running tokenizer on dataset:  13%|█▎        | 52000/390965 [00:09<01:02, 5400.30 examples/s]Running tokenizer on dataset:  14%|█▎        | 53000/390965 [00:09<01:01, 5518.43 examples/s]Running tokenizer on dataset:  14%|█▍        | 54000/390965 [00:09<01:00, 5608.85 examples/s]Running tokenizer on dataset:  14%|█▍        | 55000/390965 [00:09<00:58, 5711.40 examples/s]Running tokenizer on dataset:  14%|█▍        | 56000/390965 [00:09<00:58, 5719.88 examples/s]Running tokenizer on dataset:  15%|█▍        | 57000/390965 [00:10<00:58, 5744.01 examples/s]Running tokenizer on dataset:  15%|█▍        | 58000/390965 [00:10<00:57, 5814.49 examples/s]Running tokenizer on dataset:  15%|█▌        | 59000/390965 [00:10<00:56, 5841.39 examples/s]Running tokenizer on dataset:  15%|█▌        | 60000/390965 [00:10<00:57, 5785.76 examples/s]Running tokenizer on dataset:  16%|█▌        | 61000/390965 [00:10<00:56, 5811.08 examples/s]Running tokenizer on dataset:  16%|█▌        | 62000/390965 [00:10<00:56, 5803.16 examples/s]Running tokenizer on dataset:  16%|█▌        | 63000/390965 [00:11<00:56, 5819.83 examples/s]Running tokenizer on dataset:  16%|█▋        | 64000/390965 [00:11<00:55, 5877.69 examples/s]Running tokenizer on dataset:  17%|█▋        | 65000/390965 [00:11<00:55, 5851.14 examples/s]Running tokenizer on dataset:  17%|█▋        | 66000/390965 [00:11<00:55, 5819.08 examples/s]Running tokenizer on dataset:  17%|█▋        | 67000/390965 [00:11<00:55, 5820.29 examples/s]Running tokenizer on dataset:  17%|█▋        | 68000/390965 [00:11<00:55, 5840.98 examples/s]Running tokenizer on dataset:  18%|█▊        | 69000/390965 [00:12<00:55, 5834.94 examples/s]Running tokenizer on dataset:  18%|█▊        | 70000/390965 [00:12<00:55, 5820.51 examples/s]Running tokenizer on dataset:  18%|█▊        | 71000/390965 [00:12<00:54, 5827.58 examples/s]Running tokenizer on dataset:  18%|█▊        | 72000/390965 [00:12<00:54, 5805.59 examples/s]Running tokenizer on dataset:  19%|█▊        | 73000/390965 [00:12<00:55, 5760.80 examples/s]Running tokenizer on dataset:  19%|█▉        | 74000/390965 [00:12<00:54, 5782.27 examples/s]Running tokenizer on dataset:  19%|█▉        | 75000/390965 [00:13<00:54, 5774.09 examples/s]Running tokenizer on dataset:  19%|█▉        | 76000/390965 [00:13<00:54, 5820.40 examples/s]Running tokenizer on dataset:  20%|█▉        | 77000/390965 [00:13<00:53, 5886.63 examples/s]Running tokenizer on dataset:  20%|█▉        | 78000/390965 [00:13<00:53, 5866.02 examples/s]Running tokenizer on dataset:  20%|██        | 79000/390965 [00:13<00:53, 5864.50 examples/s]Running tokenizer on dataset:  20%|██        | 80000/390965 [00:14<00:52, 5912.36 examples/s]Running tokenizer on dataset:  21%|██        | 81000/390965 [00:14<00:52, 5880.94 examples/s]Running tokenizer on dataset:  21%|██        | 82000/390965 [00:14<00:52, 5838.04 examples/s]Running tokenizer on dataset:  21%|██        | 83000/390965 [00:14<00:52, 5843.98 examples/s]Running tokenizer on dataset:  21%|██▏       | 84000/390965 [00:14<00:52, 5821.53 examples/s]Running tokenizer on dataset:  22%|██▏       | 85000/390965 [00:14<01:03, 4807.26 examples/s]Running tokenizer on dataset:  22%|██▏       | 86000/390965 [00:15<01:00, 5081.61 examples/s]Running tokenizer on dataset:  22%|██▏       | 87000/390965 [00:15<00:57, 5276.28 examples/s]Running tokenizer on dataset:  23%|██▎       | 88000/390965 [00:15<00:55, 5466.27 examples/s]Running tokenizer on dataset:  23%|██▎       | 89000/390965 [00:15<00:54, 5567.16 examples/s]Running tokenizer on dataset:  23%|██▎       | 90000/390965 [00:15<00:53, 5612.62 examples/s]Running tokenizer on dataset:  23%|██▎       | 91000/390965 [00:16<00:52, 5689.22 examples/s]Running tokenizer on dataset:  24%|██▎       | 92000/390965 [00:16<00:52, 5738.08 examples/s]Running tokenizer on dataset:  24%|██▍       | 93000/390965 [00:16<00:51, 5772.14 examples/s]Running tokenizer on dataset:  24%|██▍       | 94000/390965 [00:16<00:51, 5799.67 examples/s]Running tokenizer on dataset:  24%|██▍       | 95000/390965 [00:16<00:50, 5811.20 examples/s]Running tokenizer on dataset:  25%|██▍       | 96000/390965 [00:16<00:50, 5826.67 examples/s]Running tokenizer on dataset:  25%|██▍       | 97000/390965 [00:17<00:50, 5853.22 examples/s]Running tokenizer on dataset:  25%|██▌       | 98000/390965 [00:17<00:50, 5838.48 examples/s]Running tokenizer on dataset:  25%|██▌       | 99000/390965 [00:17<00:50, 5777.70 examples/s]Running tokenizer on dataset:  26%|██▌       | 100000/390965 [00:17<00:50, 5791.23 examples/s]Running tokenizer on dataset:  26%|██▌       | 101000/390965 [00:17<00:49, 5807.20 examples/s]Running tokenizer on dataset:  26%|██▌       | 102000/390965 [00:17<00:49, 5787.41 examples/s]Running tokenizer on dataset:  26%|██▋       | 103000/390965 [00:18<00:49, 5826.64 examples/s]Running tokenizer on dataset:  27%|██▋       | 104000/390965 [00:18<00:49, 5824.41 examples/s]Running tokenizer on dataset:  27%|██▋       | 105000/390965 [00:18<00:49, 5763.20 examples/s]Running tokenizer on dataset:  27%|██▋       | 106000/390965 [00:18<00:49, 5738.27 examples/s]Running tokenizer on dataset:  27%|██▋       | 107000/390965 [00:18<00:49, 5724.07 examples/s]Running tokenizer on dataset:  28%|██▊       | 108000/390965 [00:18<00:48, 5777.96 examples/s]Running tokenizer on dataset:  28%|██▊       | 109000/390965 [00:19<00:48, 5832.09 examples/s]Running tokenizer on dataset:  28%|██▊       | 110000/390965 [00:19<00:48, 5819.75 examples/s]Running tokenizer on dataset:  28%|██▊       | 111000/390965 [00:19<00:47, 5868.31 examples/s]Running tokenizer on dataset:  29%|██▊       | 112000/390965 [00:19<00:47, 5903.67 examples/s]Running tokenizer on dataset:  29%|██▉       | 113000/390965 [00:19<00:47, 5878.33 examples/s]Running tokenizer on dataset:  29%|██▉       | 114000/390965 [00:19<00:47, 5870.14 examples/s]Running tokenizer on dataset:  29%|██▉       | 115000/390965 [00:20<00:57, 4838.83 examples/s]Running tokenizer on dataset:  30%|██▉       | 116000/390965 [00:20<00:54, 5067.04 examples/s]Running tokenizer on dataset:  30%|██▉       | 117000/390965 [00:20<00:51, 5293.20 examples/s]Running tokenizer on dataset:  30%|███       | 118000/390965 [00:20<00:50, 5417.22 examples/s]Running tokenizer on dataset:  30%|███       | 119000/390965 [00:20<00:49, 5536.64 examples/s]Running tokenizer on dataset:  31%|███       | 120000/390965 [00:21<00:48, 5640.44 examples/s]Running tokenizer on dataset:  31%|███       | 121000/390965 [00:21<00:48, 5623.94 examples/s]Running tokenizer on dataset:  31%|███       | 122000/390965 [00:21<00:47, 5680.76 examples/s]Running tokenizer on dataset:  31%|███▏      | 123000/390965 [00:21<00:47, 5700.02 examples/s]Running tokenizer on dataset:  32%|███▏      | 124000/390965 [00:21<00:46, 5724.96 examples/s]Running tokenizer on dataset:  32%|███▏      | 125000/390965 [00:21<00:46, 5756.98 examples/s]Running tokenizer on dataset:  32%|███▏      | 126000/390965 [00:22<00:46, 5649.64 examples/s]Running tokenizer on dataset:  32%|███▏      | 127000/390965 [00:22<00:46, 5655.27 examples/s]Running tokenizer on dataset:  33%|███▎      | 128000/390965 [00:22<00:46, 5701.80 examples/s]Running tokenizer on dataset:  33%|███▎      | 129000/390965 [00:22<00:45, 5757.48 examples/s]Running tokenizer on dataset:  33%|███▎      | 130000/390965 [00:22<00:45, 5789.09 examples/s]Running tokenizer on dataset:  34%|███▎      | 131000/390965 [00:23<00:45, 5769.97 examples/s]Running tokenizer on dataset:  34%|███▍      | 132000/390965 [00:23<00:44, 5777.43 examples/s]Running tokenizer on dataset:  34%|███▍      | 133000/390965 [00:23<00:44, 5766.23 examples/s]Running tokenizer on dataset:  34%|███▍      | 134000/390965 [00:23<00:44, 5743.75 examples/s]Running tokenizer on dataset:  35%|███▍      | 135000/390965 [00:23<00:44, 5794.15 examples/s]Running tokenizer on dataset:  35%|███▍      | 136000/390965 [00:23<00:44, 5774.69 examples/s]Running tokenizer on dataset:  35%|███▌      | 137000/390965 [00:24<00:44, 5734.88 examples/s]Running tokenizer on dataset:  35%|███▌      | 138000/390965 [00:24<00:44, 5745.31 examples/s]Running tokenizer on dataset:  36%|███▌      | 139000/390965 [00:24<00:44, 5649.40 examples/s]Running tokenizer on dataset:  36%|███▌      | 140000/390965 [00:24<00:53, 4702.51 examples/s]Running tokenizer on dataset:  36%|███▌      | 141000/390965 [00:24<00:49, 5014.89 examples/s]Running tokenizer on dataset:  36%|███▋      | 142000/390965 [00:25<00:47, 5187.10 examples/s]Running tokenizer on dataset:  37%|███▋      | 143000/390965 [00:25<00:46, 5390.32 examples/s]Running tokenizer on dataset:  37%|███▋      | 144000/390965 [00:25<00:44, 5507.17 examples/s]Running tokenizer on dataset:  37%|███▋      | 145000/390965 [00:25<00:44, 5556.59 examples/s]Running tokenizer on dataset:  37%|███▋      | 146000/390965 [00:25<00:43, 5679.29 examples/s]Running tokenizer on dataset:  38%|███▊      | 147000/390965 [00:25<00:42, 5698.25 examples/s]Running tokenizer on dataset:  38%|███▊      | 148000/390965 [00:26<00:42, 5672.57 examples/s]Running tokenizer on dataset:  38%|███▊      | 149000/390965 [00:26<00:42, 5751.16 examples/s]Running tokenizer on dataset:  38%|███▊      | 150000/390965 [00:26<00:42, 5731.80 examples/s]Running tokenizer on dataset:  39%|███▊      | 151000/390965 [00:26<00:41, 5721.25 examples/s]Running tokenizer on dataset:  39%|███▉      | 152000/390965 [00:26<00:41, 5781.24 examples/s]Running tokenizer on dataset:  39%|███▉      | 153000/390965 [00:26<00:41, 5789.00 examples/s]Running tokenizer on dataset:  39%|███▉      | 154000/390965 [00:27<00:41, 5770.27 examples/s]Running tokenizer on dataset:  40%|███▉      | 155000/390965 [00:27<00:41, 5742.08 examples/s]Running tokenizer on dataset:  40%|███▉      | 156000/390965 [00:27<00:40, 5744.86 examples/s]Running tokenizer on dataset:  40%|████      | 157000/390965 [00:27<00:40, 5739.93 examples/s]Running tokenizer on dataset:  40%|████      | 158000/390965 [00:27<00:40, 5790.27 examples/s]Running tokenizer on dataset:  41%|████      | 159000/390965 [00:28<00:39, 5809.38 examples/s]Running tokenizer on dataset:  41%|████      | 160000/390965 [00:28<00:39, 5787.31 examples/s]Running tokenizer on dataset:  41%|████      | 161000/390965 [00:28<00:39, 5785.54 examples/s]Running tokenizer on dataset:  41%|████▏     | 162000/390965 [00:28<00:39, 5791.68 examples/s]Running tokenizer on dataset:  42%|████▏     | 163000/390965 [00:28<00:39, 5752.42 examples/s]Running tokenizer on dataset:  42%|████▏     | 164000/390965 [00:29<00:47, 4779.49 examples/s]Running tokenizer on dataset:  42%|████▏     | 165000/390965 [00:29<00:44, 5025.28 examples/s]Running tokenizer on dataset:  42%|████▏     | 166000/390965 [00:29<00:43, 5195.36 examples/s]Running tokenizer on dataset:  43%|████▎     | 167000/390965 [00:29<00:41, 5389.27 examples/s]Running tokenizer on dataset:  43%|████▎     | 168000/390965 [00:29<00:40, 5511.83 examples/s]Running tokenizer on dataset:  43%|████▎     | 169000/390965 [00:29<00:40, 5526.53 examples/s]Running tokenizer on dataset:  43%|████▎     | 170000/390965 [00:30<00:39, 5617.45 examples/s]Running tokenizer on dataset:  44%|████▎     | 171000/390965 [00:30<00:39, 5611.08 examples/s]Running tokenizer on dataset:  44%|████▍     | 172000/390965 [00:30<00:38, 5669.04 examples/s]Running tokenizer on dataset:  44%|████▍     | 173000/390965 [00:30<00:38, 5733.91 examples/s]Running tokenizer on dataset:  45%|████▍     | 174000/390965 [00:30<00:37, 5729.33 examples/s]Running tokenizer on dataset:  45%|████▍     | 175000/390965 [00:30<00:37, 5773.60 examples/s]Running tokenizer on dataset:  45%|████▌     | 176000/390965 [00:31<00:36, 5838.23 examples/s]Running tokenizer on dataset:  45%|████▌     | 177000/390965 [00:31<00:36, 5803.85 examples/s]Running tokenizer on dataset:  46%|████▌     | 178000/390965 [00:31<00:36, 5817.47 examples/s]Running tokenizer on dataset:  46%|████▌     | 179000/390965 [00:31<00:36, 5822.93 examples/s]Running tokenizer on dataset:  46%|████▌     | 180000/390965 [00:31<00:36, 5824.02 examples/s]Running tokenizer on dataset:  46%|████▋     | 181000/390965 [00:31<00:36, 5800.37 examples/s]Running tokenizer on dataset:  47%|████▋     | 182000/390965 [00:32<00:36, 5788.35 examples/s]Running tokenizer on dataset:  47%|████▋     | 183000/390965 [00:32<00:35, 5799.03 examples/s]Running tokenizer on dataset:  47%|████▋     | 184000/390965 [00:32<00:35, 5844.07 examples/s]Running tokenizer on dataset:  47%|████▋     | 185000/390965 [00:32<00:35, 5832.01 examples/s]Running tokenizer on dataset:  48%|████▊     | 186000/390965 [00:32<00:35, 5826.17 examples/s]Running tokenizer on dataset:  48%|████▊     | 187000/390965 [00:32<00:34, 5827.80 examples/s]Running tokenizer on dataset:  48%|████▊     | 188000/390965 [00:33<00:41, 4852.50 examples/s]Running tokenizer on dataset:  48%|████▊     | 189000/390965 [00:33<00:39, 5138.26 examples/s]Running tokenizer on dataset:  49%|████▊     | 190000/390965 [00:33<00:37, 5327.81 examples/s]Running tokenizer on dataset:  49%|████▉     | 191000/390965 [00:33<00:36, 5464.31 examples/s]Running tokenizer on dataset:  49%|████▉     | 192000/390965 [00:33<00:35, 5592.40 examples/s]Running tokenizer on dataset:  49%|████▉     | 193000/390965 [00:34<00:35, 5606.15 examples/s]Running tokenizer on dataset:  50%|████▉     | 194000/390965 [00:34<00:34, 5688.24 examples/s]Running tokenizer on dataset:  50%|████▉     | 195000/390965 [00:34<00:34, 5733.15 examples/s]Running tokenizer on dataset:  50%|█████     | 196000/390965 [00:34<00:34, 5724.87 examples/s]Running tokenizer on dataset:  50%|█████     | 197000/390965 [00:34<00:33, 5750.21 examples/s]Running tokenizer on dataset:  51%|█████     | 198000/390965 [00:34<00:33, 5736.72 examples/s]Running tokenizer on dataset:  51%|█████     | 199000/390965 [00:35<00:33, 5743.31 examples/s]Running tokenizer on dataset:  51%|█████     | 200000/390965 [00:35<00:33, 5766.24 examples/s]Running tokenizer on dataset:  51%|█████▏    | 201000/390965 [00:35<00:32, 5770.00 examples/s]Running tokenizer on dataset:  52%|█████▏    | 202000/390965 [00:35<00:32, 5751.82 examples/s]Running tokenizer on dataset:  52%|█████▏    | 203000/390965 [00:35<00:32, 5725.96 examples/s]Running tokenizer on dataset:  52%|█████▏    | 204000/390965 [00:36<00:32, 5743.77 examples/s]Running tokenizer on dataset:  52%|█████▏    | 205000/390965 [00:36<00:32, 5777.88 examples/s]Running tokenizer on dataset:  53%|█████▎    | 206000/390965 [00:36<00:32, 5779.63 examples/s]Running tokenizer on dataset:  53%|█████▎    | 207000/390965 [00:36<00:31, 5841.84 examples/s]Running tokenizer on dataset:  53%|█████▎    | 208000/390965 [00:36<00:31, 5839.09 examples/s]Running tokenizer on dataset:  53%|█████▎    | 209000/390965 [00:36<00:31, 5766.33 examples/s]Running tokenizer on dataset:  54%|█████▎    | 210000/390965 [00:37<00:30, 5843.44 examples/s]Running tokenizer on dataset:  54%|█████▍    | 211000/390965 [00:37<00:31, 5796.97 examples/s]Running tokenizer on dataset:  54%|█████▍    | 212000/390965 [00:37<00:36, 4864.79 examples/s]Running tokenizer on dataset:  54%|█████▍    | 213000/390965 [00:37<00:34, 5085.64 examples/s]Running tokenizer on dataset:  55%|█████▍    | 214000/390965 [00:37<00:33, 5248.50 examples/s]Running tokenizer on dataset:  55%|█████▍    | 215000/390965 [00:38<00:32, 5421.68 examples/s]Running tokenizer on dataset:  55%|█████▌    | 216000/390965 [00:38<00:31, 5575.82 examples/s]Running tokenizer on dataset:  56%|█████▌    | 217000/390965 [00:38<00:31, 5589.34 examples/s]Running tokenizer on dataset:  56%|█████▌    | 218000/390965 [00:38<00:30, 5682.90 examples/s]Running tokenizer on dataset:  56%|█████▌    | 219000/390965 [00:38<00:29, 5734.12 examples/s]Running tokenizer on dataset:  56%|█████▋    | 220000/390965 [00:38<00:30, 5698.08 examples/s]Running tokenizer on dataset:  57%|█████▋    | 221000/390965 [00:39<00:29, 5730.63 examples/s]Running tokenizer on dataset:  57%|█████▋    | 222000/390965 [00:39<00:29, 5789.25 examples/s]Running tokenizer on dataset:  57%|█████▋    | 223000/390965 [00:39<00:29, 5766.66 examples/s]Running tokenizer on dataset:  57%|█████▋    | 224000/390965 [00:39<00:28, 5803.11 examples/s]Running tokenizer on dataset:  58%|█████▊    | 225000/390965 [00:39<00:28, 5796.29 examples/s]Running tokenizer on dataset:  58%|█████▊    | 226000/390965 [00:39<00:28, 5760.80 examples/s]Running tokenizer on dataset:  58%|█████▊    | 227000/390965 [00:40<00:28, 5783.77 examples/s]Running tokenizer on dataset:  58%|█████▊    | 228000/390965 [00:40<00:27, 5851.03 examples/s]Running tokenizer on dataset:  59%|█████▊    | 229000/390965 [00:40<00:27, 5802.25 examples/s]Running tokenizer on dataset:  59%|█████▉    | 230000/390965 [00:40<00:27, 5771.58 examples/s]Running tokenizer on dataset:  59%|█████▉    | 231000/390965 [00:40<00:27, 5795.26 examples/s]Running tokenizer on dataset:  59%|█████▉    | 232000/390965 [00:40<00:27, 5758.41 examples/s]Running tokenizer on dataset:  60%|█████▉    | 233000/390965 [00:41<00:27, 5796.70 examples/s]Running tokenizer on dataset:  60%|█████▉    | 234000/390965 [00:41<00:26, 5833.42 examples/s]Running tokenizer on dataset:  60%|██████    | 235000/390965 [00:41<00:27, 5736.65 examples/s]Running tokenizer on dataset:  60%|██████    | 236000/390965 [00:41<00:32, 4798.44 examples/s]Running tokenizer on dataset:  61%|██████    | 237000/390965 [00:41<00:30, 5069.01 examples/s]Running tokenizer on dataset:  61%|██████    | 238000/390965 [00:42<00:29, 5266.56 examples/s]Running tokenizer on dataset:  61%|██████    | 239000/390965 [00:42<00:27, 5436.48 examples/s]Running tokenizer on dataset:  61%|██████▏   | 240000/390965 [00:42<00:27, 5574.93 examples/s]Running tokenizer on dataset:  62%|██████▏   | 241000/390965 [00:42<00:26, 5610.22 examples/s]Running tokenizer on dataset:  62%|██████▏   | 242000/390965 [00:42<00:26, 5705.32 examples/s]Running tokenizer on dataset:  62%|██████▏   | 243000/390965 [00:42<00:25, 5737.85 examples/s]Running tokenizer on dataset:  62%|██████▏   | 244000/390965 [00:43<00:25, 5733.64 examples/s]Running tokenizer on dataset:  63%|██████▎   | 245000/390965 [00:43<00:25, 5804.53 examples/s]Running tokenizer on dataset:  63%|██████▎   | 246000/390965 [00:43<00:26, 5556.78 examples/s]Running tokenizer on dataset:  63%|██████▎   | 247000/390965 [00:43<00:25, 5670.57 examples/s]Running tokenizer on dataset:  63%|██████▎   | 248000/390965 [00:43<00:24, 5763.96 examples/s]Running tokenizer on dataset:  64%|██████▎   | 249000/390965 [00:44<00:24, 5783.26 examples/s]Running tokenizer on dataset:  64%|██████▍   | 250000/390965 [00:44<00:24, 5790.94 examples/s]Running tokenizer on dataset:  64%|██████▍   | 251000/390965 [00:44<00:24, 5798.19 examples/s]Running tokenizer on dataset:  64%|██████▍   | 252000/390965 [00:44<00:23, 5827.93 examples/s]Running tokenizer on dataset:  65%|██████▍   | 253000/390965 [00:44<00:24, 5728.04 examples/s]Running tokenizer on dataset:  65%|██████▍   | 254000/390965 [00:44<00:23, 5819.52 examples/s]Running tokenizer on dataset:  65%|██████▌   | 255000/390965 [00:45<00:23, 5834.03 examples/s]Running tokenizer on dataset:  65%|██████▌   | 256000/390965 [00:45<00:23, 5827.65 examples/s]Running tokenizer on dataset:  66%|██████▌   | 257000/390965 [00:45<00:22, 5857.99 examples/s]Running tokenizer on dataset:  66%|██████▌   | 258000/390965 [00:45<00:22, 5847.49 examples/s]Running tokenizer on dataset:  66%|██████▌   | 259000/390965 [00:45<00:22, 5795.54 examples/s]Running tokenizer on dataset:  67%|██████▋   | 260000/390965 [00:46<00:27, 4713.79 examples/s]Running tokenizer on dataset:  67%|██████▋   | 261000/390965 [00:46<00:25, 5038.25 examples/s]Running tokenizer on dataset:  67%|██████▋   | 262000/390965 [00:46<00:24, 5238.17 examples/s]Running tokenizer on dataset:  67%|██████▋   | 263000/390965 [00:46<00:23, 5418.92 examples/s]Running tokenizer on dataset:  68%|██████▊   | 264000/390965 [00:46<00:22, 5538.21 examples/s]Running tokenizer on dataset:  68%|██████▊   | 265000/390965 [00:46<00:22, 5578.01 examples/s]Running tokenizer on dataset:  68%|██████▊   | 266000/390965 [00:47<00:22, 5674.76 examples/s]Running tokenizer on dataset:  68%|██████▊   | 267000/390965 [00:47<00:21, 5666.19 examples/s]Running tokenizer on dataset:  69%|██████▊   | 268000/390965 [00:47<00:21, 5671.92 examples/s]Running tokenizer on dataset:  69%|██████▉   | 269000/390965 [00:47<00:21, 5742.06 examples/s]Running tokenizer on dataset:  69%|██████▉   | 270000/390965 [00:47<00:21, 5724.61 examples/s]Running tokenizer on dataset:  69%|██████▉   | 271000/390965 [00:47<00:20, 5773.51 examples/s]Running tokenizer on dataset:  70%|██████▉   | 272000/390965 [00:48<00:20, 5837.29 examples/s]Running tokenizer on dataset:  70%|██████▉   | 273000/390965 [00:48<00:20, 5811.47 examples/s]Running tokenizer on dataset:  70%|███████   | 274000/390965 [00:48<00:20, 5835.12 examples/s]Running tokenizer on dataset:  70%|███████   | 275000/390965 [00:48<00:19, 5809.30 examples/s]Running tokenizer on dataset:  71%|███████   | 276000/390965 [00:48<00:19, 5774.21 examples/s]Running tokenizer on dataset:  71%|███████   | 277000/390965 [00:48<00:19, 5752.17 examples/s]Running tokenizer on dataset:  71%|███████   | 278000/390965 [00:49<00:19, 5751.40 examples/s]Running tokenizer on dataset:  71%|███████▏  | 279000/390965 [00:49<00:19, 5761.57 examples/s]Running tokenizer on dataset:  72%|███████▏  | 280000/390965 [00:49<00:19, 5776.53 examples/s]Running tokenizer on dataset:  72%|███████▏  | 281000/390965 [00:49<00:18, 5813.93 examples/s]Running tokenizer on dataset:  72%|███████▏  | 282000/390965 [00:49<00:18, 5821.69 examples/s]Running tokenizer on dataset:  72%|███████▏  | 283000/390965 [00:50<00:18, 5799.54 examples/s]Running tokenizer on dataset:  73%|███████▎  | 284000/390965 [00:50<00:22, 4827.13 examples/s]Running tokenizer on dataset:  73%|███████▎  | 285000/390965 [00:50<00:20, 5066.33 examples/s]Running tokenizer on dataset:  73%|███████▎  | 286000/390965 [00:50<00:20, 5216.50 examples/s]Running tokenizer on dataset:  73%|███████▎  | 287000/390965 [00:50<00:19, 5468.92 examples/s]Running tokenizer on dataset:  74%|███████▎  | 288000/390965 [00:50<00:18, 5582.20 examples/s]Running tokenizer on dataset:  74%|███████▍  | 289000/390965 [00:51<00:18, 5651.36 examples/s]Running tokenizer on dataset:  74%|███████▍  | 290000/390965 [00:51<00:17, 5689.57 examples/s]Running tokenizer on dataset:  74%|███████▍  | 291000/390965 [00:51<00:17, 5695.89 examples/s]Running tokenizer on dataset:  75%|███████▍  | 292000/390965 [00:51<00:17, 5738.51 examples/s]Running tokenizer on dataset:  75%|███████▍  | 293000/390965 [00:51<00:16, 5766.96 examples/s]Running tokenizer on dataset:  75%|███████▌  | 294000/390965 [00:52<00:16, 5770.21 examples/s]Running tokenizer on dataset:  75%|███████▌  | 295000/390965 [00:52<00:16, 5792.01 examples/s]Running tokenizer on dataset:  76%|███████▌  | 296000/390965 [00:52<00:16, 5817.80 examples/s]Running tokenizer on dataset:  76%|███████▌  | 297000/390965 [00:52<00:16, 5788.96 examples/s]Running tokenizer on dataset:  76%|███████▌  | 298000/390965 [00:52<00:16, 5796.61 examples/s]Running tokenizer on dataset:  76%|███████▋  | 299000/390965 [00:52<00:15, 5756.88 examples/s]Running tokenizer on dataset:  77%|███████▋  | 300000/390965 [00:53<00:15, 5744.07 examples/s]Running tokenizer on dataset:  77%|███████▋  | 301000/390965 [00:53<00:15, 5688.59 examples/s]Running tokenizer on dataset:  77%|███████▋  | 302000/390965 [00:53<00:15, 5799.11 examples/s]Running tokenizer on dataset:  78%|███████▊  | 303000/390965 [00:53<00:15, 5796.96 examples/s]Running tokenizer on dataset:  78%|███████▊  | 304000/390965 [00:53<00:14, 5825.10 examples/s]Running tokenizer on dataset:  78%|███████▊  | 305000/390965 [00:53<00:14, 5766.42 examples/s]Running tokenizer on dataset:  78%|███████▊  | 306000/390965 [00:54<00:14, 5788.72 examples/s]Running tokenizer on dataset:  79%|███████▊  | 307000/390965 [00:54<00:14, 5797.26 examples/s]Running tokenizer on dataset:  79%|███████▉  | 308000/390965 [00:54<00:17, 4861.23 examples/s]Running tokenizer on dataset:  79%|███████▉  | 309000/390965 [00:54<00:15, 5123.30 examples/s]Running tokenizer on dataset:  79%|███████▉  | 310000/390965 [00:54<00:15, 5256.05 examples/s]Running tokenizer on dataset:  80%|███████▉  | 311000/390965 [00:55<00:14, 5414.37 examples/s]Running tokenizer on dataset:  80%|███████▉  | 312000/390965 [00:55<00:14, 5553.94 examples/s]Running tokenizer on dataset:  80%|████████  | 313000/390965 [00:55<00:13, 5621.58 examples/s]Running tokenizer on dataset:  80%|████████  | 314000/390965 [00:55<00:13, 5645.80 examples/s]Running tokenizer on dataset:  81%|████████  | 315000/390965 [00:55<00:13, 5699.65 examples/s]Running tokenizer on dataset:  81%|████████  | 316000/390965 [00:55<00:13, 5732.08 examples/s]Running tokenizer on dataset:  81%|████████  | 317000/390965 [00:56<00:12, 5739.56 examples/s]Running tokenizer on dataset:  81%|████████▏ | 318000/390965 [00:56<00:12, 5788.66 examples/s]Running tokenizer on dataset:  82%|████████▏ | 319000/390965 [00:56<00:12, 5806.54 examples/s]Running tokenizer on dataset:  82%|████████▏ | 320000/390965 [00:56<00:12, 5802.37 examples/s]Running tokenizer on dataset:  82%|████████▏ | 321000/390965 [00:56<00:12, 5811.60 examples/s]Running tokenizer on dataset:  82%|████████▏ | 322000/390965 [00:56<00:11, 5792.55 examples/s]Running tokenizer on dataset:  83%|████████▎ | 323000/390965 [00:57<00:11, 5775.64 examples/s]Running tokenizer on dataset:  83%|████████▎ | 324000/390965 [00:57<00:11, 5845.93 examples/s]Running tokenizer on dataset:  83%|████████▎ | 325000/390965 [00:57<00:11, 5844.11 examples/s]Running tokenizer on dataset:  83%|████████▎ | 326000/390965 [00:57<00:11, 5799.51 examples/s]Running tokenizer on dataset:  84%|████████▎ | 327000/390965 [00:57<00:11, 5735.66 examples/s]Running tokenizer on dataset:  84%|████████▍ | 328000/390965 [00:57<00:10, 5806.82 examples/s]Running tokenizer on dataset:  84%|████████▍ | 329000/390965 [00:58<00:10, 5793.28 examples/s]Running tokenizer on dataset:  84%|████████▍ | 330000/390965 [00:58<00:10, 5829.52 examples/s]Running tokenizer on dataset:  85%|████████▍ | 331000/390965 [00:58<00:10, 5779.22 examples/s]Running tokenizer on dataset:  85%|████████▍ | 332000/390965 [00:58<00:12, 4842.38 examples/s]Running tokenizer on dataset:  85%|████████▌ | 333000/390965 [00:58<00:11, 5099.42 examples/s]Running tokenizer on dataset:  85%|████████▌ | 334000/390965 [00:59<00:10, 5250.80 examples/s]Running tokenizer on dataset:  86%|████████▌ | 335000/390965 [00:59<00:10, 5439.26 examples/s]Running tokenizer on dataset:  86%|████████▌ | 336000/390965 [00:59<00:09, 5575.50 examples/s]Running tokenizer on dataset:  86%|████████▌ | 337000/390965 [00:59<00:09, 5589.64 examples/s]Running tokenizer on dataset:  86%|████████▋ | 338000/390965 [00:59<00:09, 5683.37 examples/s]Running tokenizer on dataset:  87%|████████▋ | 339000/390965 [01:00<00:09, 5733.81 examples/s]Running tokenizer on dataset:  87%|████████▋ | 340000/390965 [01:00<00:08, 5731.31 examples/s]Running tokenizer on dataset:  87%|████████▋ | 341000/390965 [01:00<00:08, 5787.28 examples/s]Running tokenizer on dataset:  87%|████████▋ | 342000/390965 [01:00<00:08, 5779.36 examples/s]Running tokenizer on dataset:  88%|████████▊ | 343000/390965 [01:00<00:08, 5692.02 examples/s]Running tokenizer on dataset:  88%|████████▊ | 344000/390965 [01:00<00:08, 5735.71 examples/s]Running tokenizer on dataset:  88%|████████▊ | 345000/390965 [01:01<00:07, 5770.63 examples/s]Running tokenizer on dataset:  88%|████████▊ | 346000/390965 [01:01<00:07, 5752.11 examples/s]Running tokenizer on dataset:  89%|████████▉ | 347000/390965 [01:01<00:07, 5772.88 examples/s]Running tokenizer on dataset:  89%|████████▉ | 348000/390965 [01:01<00:07, 5822.78 examples/s]Running tokenizer on dataset:  89%|████████▉ | 349000/390965 [01:01<00:07, 5790.33 examples/s]Running tokenizer on dataset:  90%|████████▉ | 350000/390965 [01:01<00:07, 5765.12 examples/s]Running tokenizer on dataset:  90%|████████▉ | 351000/390965 [01:02<00:06, 5793.27 examples/s]Running tokenizer on dataset:  90%|█████████ | 352000/390965 [01:02<00:06, 5778.31 examples/s]Running tokenizer on dataset:  90%|█████████ | 353000/390965 [01:02<00:06, 5789.61 examples/s]Running tokenizer on dataset:  91%|█████████ | 354000/390965 [01:02<00:06, 5841.81 examples/s]Running tokenizer on dataset:  91%|█████████ | 355000/390965 [01:02<00:06, 5744.35 examples/s]Running tokenizer on dataset:  91%|█████████ | 356000/390965 [01:03<00:07, 4914.01 examples/s]Running tokenizer on dataset:  91%|█████████▏| 357000/390965 [01:03<00:06, 5157.55 examples/s]Running tokenizer on dataset:  92%|█████████▏| 358000/390965 [01:03<00:06, 5222.52 examples/s]Running tokenizer on dataset:  92%|█████████▏| 359000/390965 [01:03<00:05, 5456.40 examples/s]Running tokenizer on dataset:  92%|█████████▏| 360000/390965 [01:03<00:05, 5552.59 examples/s]Running tokenizer on dataset:  92%|█████████▏| 361000/390965 [01:03<00:05, 5605.47 examples/s]Running tokenizer on dataset:  93%|█████████▎| 362000/390965 [01:04<00:05, 5712.54 examples/s]Running tokenizer on dataset:  93%|█████████▎| 363000/390965 [01:04<00:04, 5712.98 examples/s]Running tokenizer on dataset:  93%|█████████▎| 364000/390965 [01:04<00:04, 5701.20 examples/s]Running tokenizer on dataset:  93%|█████████▎| 365000/390965 [01:04<00:04, 5727.73 examples/s]Running tokenizer on dataset:  94%|█████████▎| 366000/390965 [01:04<00:04, 5570.53 examples/s]Running tokenizer on dataset:  94%|█████████▍| 367000/390965 [01:04<00:04, 5635.13 examples/s]Running tokenizer on dataset:  94%|█████████▍| 368000/390965 [01:05<00:04, 5739.57 examples/s]Running tokenizer on dataset:  94%|█████████▍| 369000/390965 [01:05<00:03, 5770.52 examples/s]Running tokenizer on dataset:  95%|█████████▍| 370000/390965 [01:05<00:03, 5758.49 examples/s]Running tokenizer on dataset:  95%|█████████▍| 371000/390965 [01:05<00:03, 5809.19 examples/s]Running tokenizer on dataset:  95%|█████████▌| 372000/390965 [01:05<00:03, 5859.48 examples/s]Running tokenizer on dataset:  95%|█████████▌| 373000/390965 [01:05<00:03, 5833.00 examples/s]Running tokenizer on dataset:  96%|█████████▌| 374000/390965 [01:06<00:02, 5838.95 examples/s]Running tokenizer on dataset:  96%|█████████▌| 375000/390965 [01:06<00:02, 5814.53 examples/s]Running tokenizer on dataset:  96%|█████████▌| 376000/390965 [01:06<00:02, 5758.81 examples/s]Running tokenizer on dataset:  96%|█████████▋| 377000/390965 [01:06<00:02, 5803.26 examples/s]Running tokenizer on dataset:  97%|█████████▋| 378000/390965 [01:06<00:02, 5835.67 examples/s]Running tokenizer on dataset:  97%|█████████▋| 379000/390965 [01:07<00:02, 5793.23 examples/s]Running tokenizer on dataset:  97%|█████████▋| 380000/390965 [01:07<00:02, 4776.73 examples/s]Running tokenizer on dataset:  97%|█████████▋| 381000/390965 [01:07<00:01, 4997.76 examples/s]Running tokenizer on dataset:  98%|█████████▊| 382000/390965 [01:07<00:01, 5254.81 examples/s]Running tokenizer on dataset:  98%|█████████▊| 383000/390965 [01:07<00:01, 5446.35 examples/s]Running tokenizer on dataset:  98%|█████████▊| 384000/390965 [01:08<00:01, 5558.69 examples/s]Running tokenizer on dataset:  98%|█████████▊| 385000/390965 [01:08<00:01, 5634.44 examples/s]Running tokenizer on dataset:  99%|█████████▊| 386000/390965 [01:08<00:00, 5735.45 examples/s]Running tokenizer on dataset:  99%|█████████▉| 387000/390965 [01:08<00:00, 5726.43 examples/s]Running tokenizer on dataset:  99%|█████████▉| 388000/390965 [01:08<00:00, 5745.60 examples/s]Running tokenizer on dataset:  99%|█████████▉| 389000/390965 [01:08<00:00, 5796.65 examples/s]Running tokenizer on dataset: 100%|█████████▉| 390000/390965 [01:09<00:00, 5785.66 examples/s]Running tokenizer on dataset: 100%|██████████| 390965/390965 [01:09<00:00, 5765.63 examples/s]Running tokenizer on dataset: 100%|██████████| 390965/390965 [01:09<00:00, 5648.25 examples/s]
02/18/2024 21:35:35 - INFO - __main__ - Sample 81 of the training set: {'question1': 'Which colour of Honda City 2016 is best?', 'question2': 'Which is better Honda City or Maruti Ciaz?', 'label': 0, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 8449, 12384, 310, 379, 14287, 4412, 29871, 29906, 29900, 29896, 29953, 338, 1900, 29973, 1, 8449, 338, 2253, 379, 14287, 4412, 470, 1085, 11321, 315, 423, 29920, 29973], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:35:35 - INFO - __main__ - Sample 14 of the training set: {'question1': 'How is being gay or lesbian less moral than divorce?', 'question2': '"Why do a lot of theists and agnostics confuse mainstream atheistic thought with ""positive atheism""?"', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1128, 338, 1641, 23852, 470, 966, 22863, 3109, 14731, 1135, 25074, 346, 29973, 1, 376, 11008, 437, 263, 3287, 310, 278, 2879, 322, 946, 6582, 1199, 1970, 1509, 1667, 5461, 472, 354, 4695, 2714, 411, 5124, 1066, 3321, 472, 354, 1608, 15945, 3026], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:35:35 - INFO - __main__ - Sample 3 of the training set: {'question1': 'What can one do after MBBS?', 'question2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1724, 508, 697, 437, 1156, 13232, 9851, 29973, 1, 1724, 437, 474, 437, 1156, 590, 13232, 9851, 1577], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:35:35 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:35:37,156 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: question1, idx, question2. If question1, idx, question2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:35:37,480 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:35:37,480 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:35:37,481 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:35:37,481 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:35:37,481 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:35:37,481 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:35:37,481 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:35:37,482 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:22,  2.17s/it]  5%|▌         | 2/39 [00:03<01:01,  1.65s/it]  8%|▊         | 3/39 [00:04<00:54,  1.53s/it] 10%|█         | 4/39 [00:06<00:51,  1.47s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.43s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.41s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.40s/it] 21%|██        | 8/39 [00:11<00:43,  1.39s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.38s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.38s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.20s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.25s/it] 38%|███▊      | 15/39 [00:20<00:30,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.36s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:21,  1.37s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.37s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.25s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:44<00:08,  1.36s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.37s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.37s/it] 97%|█████████▋| 38/39 [00:51<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:36:30,150 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.8, 'train_samples_per_second': 5.682, 'train_steps_per_second': 0.739, 'train_loss': 1.057376959385016, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.35s/it]
[INFO|trainer.py:2985] 2024-02-18 21:36:30,285 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:36:30,288 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:36:54,632 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:36:54,635 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:36:54,636 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qqp256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.0574
  train_runtime            = 0:00:52.79
  train_samples            =        100
  train_samples_per_second =      5.682
  train_steps_per_second   =      0.739
02/18/2024 21:36:54 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:36:54,671 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: question1, idx, question2. If question1, idx, question2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:36:54,674 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:36:54,674 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:36:54,674 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.80it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.38it/s] 31%|███       | 4/13 [00:01<00:03,  2.93it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.72it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.60it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.53it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.49it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.46it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.44it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.42it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.41it/s]100%|██████████| 13/13 [00:04<00:00,  2.82it/s]100%|██████████| 13/13 [00:04<00:00,  2.69it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.73
  eval_combined_score     =     0.4793
  eval_f1                 =     0.2286
  eval_loss               =      0.714
  eval_runtime            = 0:00:05.26
  eval_samples            =        100
  eval_samples_per_second =     18.996
  eval_steps_per_second   =      2.469
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:37:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:37:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1/runs/Feb18_21-37-34_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:37:36 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:37:36 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:37:37 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:37:37 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:37:40,655 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:37:40,658 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:37:40,690 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:37:40,690 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:37:40,690 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:37:40,690 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:37:40,691 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:37:40,750 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:37:40,809 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:37:43,893 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:37:43,894 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/392702 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2952bbeb8849c872.arrow
02/18/2024 21:37:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2952bbeb8849c872.arrow
Running tokenizer on dataset:   0%|          | 1000/392702 [00:00<04:49, 1352.56 examples/s]Running tokenizer on dataset:   1%|          | 2000/392702 [00:00<02:46, 2344.31 examples/s]Running tokenizer on dataset:   1%|          | 3000/392702 [00:01<02:39, 2436.45 examples/s]Running tokenizer on dataset:   1%|          | 4000/392702 [00:01<02:08, 3026.74 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/392702 [00:01<01:51, 3491.57 examples/s]Running tokenizer on dataset:   2%|▏         | 6000/392702 [00:01<01:38, 3918.99 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/392702 [00:02<01:30, 4254.70 examples/s]Running tokenizer on dataset:   2%|▏         | 8000/392702 [00:02<01:25, 4515.75 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/392702 [00:02<01:21, 4689.70 examples/s]Running tokenizer on dataset:   3%|▎         | 10000/392702 [00:02<01:18, 4860.78 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/392702 [00:02<01:16, 4964.37 examples/s]Running tokenizer on dataset:   3%|▎         | 12000/392702 [00:03<01:15, 5010.22 examples/s]Running tokenizer on dataset:   3%|▎         | 13000/392702 [00:03<01:14, 5075.11 examples/s]Running tokenizer on dataset:   4%|▎         | 14000/392702 [00:03<01:13, 5118.74 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/392702 [00:03<01:13, 5150.48 examples/s]Running tokenizer on dataset:   4%|▍         | 16000/392702 [00:03<01:12, 5183.88 examples/s]Running tokenizer on dataset:   4%|▍         | 17000/392702 [00:04<01:12, 5179.97 examples/s]Running tokenizer on dataset:   5%|▍         | 18000/392702 [00:04<01:12, 5173.44 examples/s]Running tokenizer on dataset:   5%|▍         | 19000/392702 [00:04<01:11, 5212.93 examples/s]Running tokenizer on dataset:   5%|▌         | 20000/392702 [00:04<01:11, 5209.39 examples/s]Running tokenizer on dataset:   5%|▌         | 21000/392702 [00:04<01:11, 5222.89 examples/s]Running tokenizer on dataset:   6%|▌         | 22000/392702 [00:05<01:10, 5238.15 examples/s]Running tokenizer on dataset:   6%|▌         | 23000/392702 [00:05<01:11, 5193.35 examples/s]Running tokenizer on dataset:   6%|▌         | 24000/392702 [00:05<01:10, 5207.34 examples/s]Running tokenizer on dataset:   6%|▋         | 25000/392702 [00:05<01:10, 5229.44 examples/s]Running tokenizer on dataset:   7%|▋         | 26000/392702 [00:05<01:10, 5216.81 examples/s]Running tokenizer on dataset:   7%|▋         | 27000/392702 [00:05<01:10, 5212.81 examples/s]Running tokenizer on dataset:   7%|▋         | 28000/392702 [00:06<01:10, 5184.56 examples/s]Running tokenizer on dataset:   7%|▋         | 29000/392702 [00:06<01:09, 5212.44 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/392702 [00:06<01:09, 5199.34 examples/s]Running tokenizer on dataset:   8%|▊         | 31000/392702 [00:06<01:10, 5164.43 examples/s]Running tokenizer on dataset:   8%|▊         | 32000/392702 [00:06<01:10, 5147.93 examples/s]Running tokenizer on dataset:   8%|▊         | 33000/392702 [00:07<01:09, 5196.35 examples/s]Running tokenizer on dataset:   9%|▊         | 34000/392702 [00:07<01:09, 5192.91 examples/s]Running tokenizer on dataset:   9%|▉         | 35000/392702 [00:07<01:08, 5224.87 examples/s]Running tokenizer on dataset:   9%|▉         | 36000/392702 [00:07<01:08, 5192.29 examples/s]Running tokenizer on dataset:   9%|▉         | 37000/392702 [00:07<01:08, 5222.63 examples/s]Running tokenizer on dataset:  10%|▉         | 38000/392702 [00:08<01:08, 5216.18 examples/s]Running tokenizer on dataset:  10%|▉         | 39000/392702 [00:08<01:20, 4403.10 examples/s]Running tokenizer on dataset:  10%|█         | 40000/392702 [00:08<01:15, 4644.00 examples/s]Running tokenizer on dataset:  10%|█         | 41000/392702 [00:08<01:13, 4770.82 examples/s]Running tokenizer on dataset:  11%|█         | 42000/392702 [00:08<01:11, 4888.90 examples/s]Running tokenizer on dataset:  11%|█         | 43000/392702 [00:09<01:10, 4983.86 examples/s]Running tokenizer on dataset:  11%|█         | 44000/392702 [00:09<01:09, 5034.81 examples/s]Running tokenizer on dataset:  11%|█▏        | 45000/392702 [00:09<01:09, 5005.66 examples/s]Running tokenizer on dataset:  12%|█▏        | 46000/392702 [00:09<01:07, 5114.74 examples/s]Running tokenizer on dataset:  12%|█▏        | 47000/392702 [00:09<01:07, 5150.42 examples/s]Running tokenizer on dataset:  12%|█▏        | 48000/392702 [00:10<01:06, 5155.61 examples/s]Running tokenizer on dataset:  12%|█▏        | 49000/392702 [00:10<01:07, 5124.40 examples/s]Running tokenizer on dataset:  13%|█▎        | 50000/392702 [00:10<01:06, 5151.98 examples/s]Running tokenizer on dataset:  13%|█▎        | 51000/392702 [00:10<01:06, 5158.82 examples/s]Running tokenizer on dataset:  13%|█▎        | 52000/392702 [00:10<01:05, 5181.88 examples/s]Running tokenizer on dataset:  13%|█▎        | 53000/392702 [00:11<01:05, 5179.94 examples/s]Running tokenizer on dataset:  14%|█▍        | 54000/392702 [00:11<01:05, 5168.88 examples/s]Running tokenizer on dataset:  14%|█▍        | 55000/392702 [00:11<01:05, 5186.64 examples/s]Running tokenizer on dataset:  14%|█▍        | 56000/392702 [00:11<01:05, 5131.00 examples/s]Running tokenizer on dataset:  15%|█▍        | 57000/392702 [00:11<01:04, 5186.20 examples/s]Running tokenizer on dataset:  15%|█▍        | 58000/392702 [00:12<01:04, 5163.02 examples/s]Running tokenizer on dataset:  15%|█▌        | 59000/392702 [00:12<01:04, 5203.38 examples/s]Running tokenizer on dataset:  15%|█▌        | 60000/392702 [00:12<01:04, 5177.42 examples/s]Running tokenizer on dataset:  16%|█▌        | 61000/392702 [00:12<01:03, 5201.74 examples/s]Running tokenizer on dataset:  16%|█▌        | 62000/392702 [00:12<01:03, 5209.03 examples/s]Running tokenizer on dataset:  16%|█▌        | 63000/392702 [00:13<01:03, 5155.91 examples/s]Running tokenizer on dataset:  16%|█▋        | 64000/392702 [00:13<01:03, 5160.33 examples/s]Running tokenizer on dataset:  17%|█▋        | 65000/392702 [00:13<01:03, 5168.83 examples/s]Running tokenizer on dataset:  17%|█▋        | 66000/392702 [00:13<01:03, 5128.65 examples/s]Running tokenizer on dataset:  17%|█▋        | 67000/392702 [00:13<01:03, 5163.99 examples/s]Running tokenizer on dataset:  17%|█▋        | 68000/392702 [00:14<01:03, 5145.65 examples/s]Running tokenizer on dataset:  18%|█▊        | 69000/392702 [00:14<01:14, 4342.18 examples/s]Running tokenizer on dataset:  18%|█▊        | 70000/392702 [00:14<01:10, 4580.81 examples/s]Running tokenizer on dataset:  18%|█▊        | 71000/392702 [00:14<01:07, 4755.08 examples/s]Running tokenizer on dataset:  18%|█▊        | 72000/392702 [00:14<01:06, 4858.39 examples/s]Running tokenizer on dataset:  19%|█▊        | 73000/392702 [00:15<01:04, 4928.58 examples/s]Running tokenizer on dataset:  19%|█▉        | 74000/392702 [00:15<01:03, 5022.94 examples/s]Running tokenizer on dataset:  19%|█▉        | 75000/392702 [00:15<01:03, 5035.06 examples/s]Running tokenizer on dataset:  19%|█▉        | 76000/392702 [00:15<01:02, 5070.62 examples/s]Running tokenizer on dataset:  20%|█▉        | 77000/392702 [00:15<01:02, 5085.50 examples/s]Running tokenizer on dataset:  20%|█▉        | 78000/392702 [00:16<01:02, 5043.82 examples/s]Running tokenizer on dataset:  20%|██        | 79000/392702 [00:16<01:01, 5087.62 examples/s]Running tokenizer on dataset:  20%|██        | 80000/392702 [00:16<01:00, 5131.09 examples/s]Running tokenizer on dataset:  21%|██        | 81000/392702 [00:16<01:00, 5112.88 examples/s]Running tokenizer on dataset:  21%|██        | 82000/392702 [00:16<01:00, 5149.53 examples/s]Running tokenizer on dataset:  21%|██        | 83000/392702 [00:17<01:00, 5135.59 examples/s]Running tokenizer on dataset:  21%|██▏       | 84000/392702 [00:17<01:00, 5136.11 examples/s]Running tokenizer on dataset:  22%|██▏       | 85000/392702 [00:17<00:59, 5196.03 examples/s]Running tokenizer on dataset:  22%|██▏       | 86000/392702 [00:17<00:59, 5162.98 examples/s]Running tokenizer on dataset:  22%|██▏       | 87000/392702 [00:17<00:59, 5146.49 examples/s]Running tokenizer on dataset:  22%|██▏       | 88000/392702 [00:18<00:59, 5133.43 examples/s]Running tokenizer on dataset:  23%|██▎       | 89000/392702 [00:18<00:59, 5115.56 examples/s]Running tokenizer on dataset:  23%|██▎       | 90000/392702 [00:18<00:58, 5154.57 examples/s]Running tokenizer on dataset:  23%|██▎       | 91000/392702 [00:18<00:58, 5137.73 examples/s]Running tokenizer on dataset:  23%|██▎       | 92000/392702 [00:18<00:58, 5174.41 examples/s]Running tokenizer on dataset:  24%|██▎       | 93000/392702 [00:18<00:58, 5151.80 examples/s]Running tokenizer on dataset:  24%|██▍       | 94000/392702 [00:19<01:07, 4421.69 examples/s]Running tokenizer on dataset:  24%|██▍       | 95000/392702 [00:19<01:04, 4633.35 examples/s]Running tokenizer on dataset:  24%|██▍       | 96000/392702 [00:19<01:03, 4685.69 examples/s]Running tokenizer on dataset:  25%|██▍       | 97000/392702 [00:19<01:00, 4877.87 examples/s]Running tokenizer on dataset:  25%|██▍       | 98000/392702 [00:20<00:59, 4980.16 examples/s]Running tokenizer on dataset:  25%|██▌       | 99000/392702 [00:20<00:58, 5016.71 examples/s]Running tokenizer on dataset:  25%|██▌       | 100000/392702 [00:20<00:57, 5083.91 examples/s]Running tokenizer on dataset:  26%|██▌       | 101000/392702 [00:20<00:57, 5114.96 examples/s]Running tokenizer on dataset:  26%|██▌       | 102000/392702 [00:20<00:56, 5110.21 examples/s]Running tokenizer on dataset:  26%|██▌       | 103000/392702 [00:21<00:56, 5145.46 examples/s]Running tokenizer on dataset:  26%|██▋       | 104000/392702 [00:21<00:56, 5132.62 examples/s]Running tokenizer on dataset:  27%|██▋       | 105000/392702 [00:21<00:55, 5140.43 examples/s]Running tokenizer on dataset:  27%|██▋       | 106000/392702 [00:21<00:55, 5149.93 examples/s]Running tokenizer on dataset:  27%|██▋       | 107000/392702 [00:21<00:55, 5104.26 examples/s]Running tokenizer on dataset:  28%|██▊       | 108000/392702 [00:22<00:55, 5107.89 examples/s]Running tokenizer on dataset:  28%|██▊       | 109000/392702 [00:22<00:54, 5161.83 examples/s]Running tokenizer on dataset:  28%|██▊       | 110000/392702 [00:22<00:54, 5181.70 examples/s]Running tokenizer on dataset:  28%|██▊       | 111000/392702 [00:22<00:54, 5154.16 examples/s]Running tokenizer on dataset:  29%|██▊       | 112000/392702 [00:22<00:54, 5163.97 examples/s]Running tokenizer on dataset:  29%|██▉       | 113000/392702 [00:22<00:54, 5156.35 examples/s]Running tokenizer on dataset:  29%|██▉       | 114000/392702 [00:23<00:54, 5133.36 examples/s]Running tokenizer on dataset:  29%|██▉       | 115000/392702 [00:23<00:54, 5135.28 examples/s]Running tokenizer on dataset:  30%|██▉       | 116000/392702 [00:23<00:54, 5099.90 examples/s]Running tokenizer on dataset:  30%|██▉       | 117000/392702 [00:23<00:53, 5148.45 examples/s]Running tokenizer on dataset:  30%|███       | 118000/392702 [00:24<01:02, 4391.00 examples/s]Running tokenizer on dataset:  30%|███       | 119000/392702 [00:24<00:59, 4603.78 examples/s]Running tokenizer on dataset:  31%|███       | 120000/392702 [00:24<00:57, 4756.02 examples/s]Running tokenizer on dataset:  31%|███       | 121000/392702 [00:24<00:55, 4863.38 examples/s]Running tokenizer on dataset:  31%|███       | 122000/392702 [00:24<00:54, 4944.63 examples/s]Running tokenizer on dataset:  31%|███▏      | 123000/392702 [00:25<00:54, 4971.22 examples/s]Running tokenizer on dataset:  32%|███▏      | 124000/392702 [00:25<00:53, 5069.10 examples/s]Running tokenizer on dataset:  32%|███▏      | 125000/392702 [00:25<00:52, 5111.03 examples/s]Running tokenizer on dataset:  32%|███▏      | 126000/392702 [00:25<00:52, 5100.56 examples/s]Running tokenizer on dataset:  32%|███▏      | 127000/392702 [00:25<00:51, 5143.07 examples/s]Running tokenizer on dataset:  33%|███▎      | 128000/392702 [00:25<00:51, 5123.38 examples/s]Running tokenizer on dataset:  33%|███▎      | 129000/392702 [00:26<00:51, 5131.67 examples/s]Running tokenizer on dataset:  33%|███▎      | 130000/392702 [00:26<00:50, 5160.66 examples/s]Running tokenizer on dataset:  33%|███▎      | 131000/392702 [00:26<00:50, 5139.02 examples/s]Running tokenizer on dataset:  34%|███▎      | 132000/392702 [00:26<00:50, 5164.79 examples/s]Running tokenizer on dataset:  34%|███▍      | 133000/392702 [00:26<00:50, 5135.40 examples/s]Running tokenizer on dataset:  34%|███▍      | 134000/392702 [00:27<00:50, 5171.93 examples/s]Running tokenizer on dataset:  34%|███▍      | 135000/392702 [00:27<00:49, 5177.01 examples/s]Running tokenizer on dataset:  35%|███▍      | 136000/392702 [00:27<00:49, 5165.08 examples/s]Running tokenizer on dataset:  35%|███▍      | 137000/392702 [00:27<00:49, 5173.98 examples/s]Running tokenizer on dataset:  35%|███▌      | 138000/392702 [00:27<00:49, 5122.35 examples/s]Running tokenizer on dataset:  35%|███▌      | 139000/392702 [00:28<00:49, 5171.02 examples/s]Running tokenizer on dataset:  36%|███▌      | 140000/392702 [00:28<00:48, 5161.26 examples/s]Running tokenizer on dataset:  36%|███▌      | 141000/392702 [00:28<00:49, 5112.20 examples/s]Running tokenizer on dataset:  36%|███▌      | 142000/392702 [00:28<00:57, 4381.11 examples/s]Running tokenizer on dataset:  36%|███▋      | 143000/392702 [00:29<00:54, 4568.41 examples/s]Running tokenizer on dataset:  37%|███▋      | 144000/392702 [00:29<00:52, 4735.35 examples/s]Running tokenizer on dataset:  37%|███▋      | 145000/392702 [00:29<00:50, 4881.29 examples/s]Running tokenizer on dataset:  37%|███▋      | 146000/392702 [00:29<00:49, 4958.73 examples/s]Running tokenizer on dataset:  37%|███▋      | 147000/392702 [00:29<00:48, 5023.05 examples/s]Running tokenizer on dataset:  38%|███▊      | 148000/392702 [00:29<00:48, 5044.71 examples/s]Running tokenizer on dataset:  38%|███▊      | 149000/392702 [00:30<00:47, 5086.95 examples/s]Running tokenizer on dataset:  38%|███▊      | 150000/392702 [00:30<00:47, 5117.02 examples/s]Running tokenizer on dataset:  38%|███▊      | 151000/392702 [00:30<00:47, 5098.48 examples/s]Running tokenizer on dataset:  39%|███▊      | 152000/392702 [00:30<00:47, 5101.85 examples/s]Running tokenizer on dataset:  39%|███▉      | 153000/392702 [00:30<00:47, 5049.86 examples/s]Running tokenizer on dataset:  39%|███▉      | 154000/392702 [00:31<00:46, 5124.75 examples/s]Running tokenizer on dataset:  39%|███▉      | 155000/392702 [00:31<00:46, 5119.87 examples/s]Running tokenizer on dataset:  40%|███▉      | 156000/392702 [00:31<00:46, 5094.41 examples/s]Running tokenizer on dataset:  40%|███▉      | 157000/392702 [00:31<00:46, 5106.15 examples/s]Running tokenizer on dataset:  40%|████      | 158000/392702 [00:31<00:45, 5107.96 examples/s]Running tokenizer on dataset:  40%|████      | 159000/392702 [00:32<00:45, 5142.07 examples/s]Running tokenizer on dataset:  41%|████      | 160000/392702 [00:32<00:44, 5173.42 examples/s]Running tokenizer on dataset:  41%|████      | 161000/392702 [00:32<00:44, 5174.00 examples/s]Running tokenizer on dataset:  41%|████▏     | 162000/392702 [00:32<00:44, 5129.60 examples/s]Running tokenizer on dataset:  42%|████▏     | 163000/392702 [00:32<00:49, 4596.47 examples/s]Running tokenizer on dataset:  42%|████▏     | 164000/392702 [00:33<00:47, 4791.45 examples/s]Running tokenizer on dataset:  42%|████▏     | 165000/392702 [00:33<00:46, 4869.66 examples/s]Running tokenizer on dataset:  42%|████▏     | 166000/392702 [00:33<00:53, 4270.79 examples/s]Running tokenizer on dataset:  43%|████▎     | 167000/392702 [00:33<00:50, 4511.40 examples/s]Running tokenizer on dataset:  43%|████▎     | 168000/392702 [00:34<00:47, 4737.49 examples/s]Running tokenizer on dataset:  43%|████▎     | 169000/392702 [00:34<00:45, 4869.55 examples/s]Running tokenizer on dataset:  43%|████▎     | 170000/392702 [00:34<00:44, 4975.80 examples/s]Running tokenizer on dataset:  44%|████▎     | 171000/392702 [00:34<00:44, 5033.01 examples/s]Running tokenizer on dataset:  44%|████▍     | 172000/392702 [00:34<00:43, 5100.00 examples/s]Running tokenizer on dataset:  44%|████▍     | 173000/392702 [00:35<00:42, 5124.27 examples/s]Running tokenizer on dataset:  44%|████▍     | 174000/392702 [00:35<00:42, 5126.31 examples/s]Running tokenizer on dataset:  45%|████▍     | 175000/392702 [00:35<00:42, 5161.64 examples/s]Running tokenizer on dataset:  45%|████▍     | 176000/392702 [00:35<00:41, 5187.28 examples/s]Running tokenizer on dataset:  45%|████▌     | 177000/392702 [00:35<00:41, 5185.36 examples/s]Running tokenizer on dataset:  45%|████▌     | 178000/392702 [00:35<00:41, 5157.81 examples/s]Running tokenizer on dataset:  46%|████▌     | 179000/392702 [00:36<00:41, 5148.32 examples/s]Running tokenizer on dataset:  46%|████▌     | 180000/392702 [00:36<00:41, 5133.15 examples/s]Running tokenizer on dataset:  46%|████▌     | 181000/392702 [00:36<00:41, 5110.08 examples/s]Running tokenizer on dataset:  46%|████▋     | 182000/392702 [00:36<00:40, 5153.77 examples/s]Running tokenizer on dataset:  47%|████▋     | 183000/392702 [00:36<00:40, 5116.44 examples/s]Running tokenizer on dataset:  47%|████▋     | 184000/392702 [00:37<00:40, 5144.93 examples/s]Running tokenizer on dataset:  47%|████▋     | 185000/392702 [00:37<00:40, 5190.93 examples/s]Running tokenizer on dataset:  47%|████▋     | 186000/392702 [00:37<00:40, 5164.25 examples/s]Running tokenizer on dataset:  48%|████▊     | 187000/392702 [00:37<00:39, 5147.73 examples/s]Running tokenizer on dataset:  48%|████▊     | 188000/392702 [00:37<00:39, 5194.01 examples/s]Running tokenizer on dataset:  48%|████▊     | 189000/392702 [00:38<00:39, 5159.71 examples/s]Running tokenizer on dataset:  48%|████▊     | 190000/392702 [00:38<00:45, 4467.76 examples/s]Running tokenizer on dataset:  49%|████▊     | 191000/392702 [00:38<00:44, 4568.74 examples/s]Running tokenizer on dataset:  49%|████▉     | 192000/392702 [00:38<00:42, 4720.65 examples/s]Running tokenizer on dataset:  49%|████▉     | 193000/392702 [00:39<00:41, 4862.47 examples/s]Running tokenizer on dataset:  49%|████▉     | 194000/392702 [00:39<00:40, 4965.79 examples/s]Running tokenizer on dataset:  50%|████▉     | 195000/392702 [00:39<00:39, 5050.20 examples/s]Running tokenizer on dataset:  50%|████▉     | 196000/392702 [00:39<00:38, 5076.76 examples/s]Running tokenizer on dataset:  50%|█████     | 197000/392702 [00:39<00:38, 5123.40 examples/s]Running tokenizer on dataset:  50%|█████     | 198000/392702 [00:39<00:38, 5106.61 examples/s]Running tokenizer on dataset:  51%|█████     | 199000/392702 [00:40<00:37, 5141.14 examples/s]Running tokenizer on dataset:  51%|█████     | 200000/392702 [00:40<00:37, 5185.17 examples/s]Running tokenizer on dataset:  51%|█████     | 201000/392702 [00:40<00:37, 5126.40 examples/s]Running tokenizer on dataset:  51%|█████▏    | 202000/392702 [00:40<00:37, 5146.00 examples/s]Running tokenizer on dataset:  52%|█████▏    | 203000/392702 [00:40<00:36, 5151.40 examples/s]Running tokenizer on dataset:  52%|█████▏    | 204000/392702 [00:41<00:36, 5143.07 examples/s]Running tokenizer on dataset:  52%|█████▏    | 205000/392702 [00:41<00:36, 5181.31 examples/s]Running tokenizer on dataset:  52%|█████▏    | 206000/392702 [00:41<00:35, 5209.30 examples/s]Running tokenizer on dataset:  53%|█████▎    | 207000/392702 [00:41<00:35, 5188.08 examples/s]Running tokenizer on dataset:  53%|█████▎    | 208000/392702 [00:41<00:35, 5157.59 examples/s]Running tokenizer on dataset:  53%|█████▎    | 209000/392702 [00:42<00:35, 5228.97 examples/s]Running tokenizer on dataset:  53%|█████▎    | 210000/392702 [00:42<00:35, 5216.89 examples/s]Running tokenizer on dataset:  54%|█████▎    | 211000/392702 [00:42<00:34, 5226.18 examples/s]Running tokenizer on dataset:  54%|█████▍    | 212000/392702 [00:42<00:34, 5245.65 examples/s]Running tokenizer on dataset:  54%|█████▍    | 213000/392702 [00:42<00:34, 5204.72 examples/s]Running tokenizer on dataset:  54%|█████▍    | 214000/392702 [00:43<00:41, 4276.74 examples/s]Running tokenizer on dataset:  55%|█████▍    | 215000/392702 [00:43<00:39, 4503.15 examples/s]Running tokenizer on dataset:  55%|█████▌    | 216000/392702 [00:43<00:37, 4679.81 examples/s]Running tokenizer on dataset:  55%|█████▌    | 217000/392702 [00:43<00:36, 4789.96 examples/s]Running tokenizer on dataset:  56%|█████▌    | 218000/392702 [00:43<00:35, 4929.63 examples/s]Running tokenizer on dataset:  56%|█████▌    | 219000/392702 [00:44<00:34, 5029.34 examples/s]Running tokenizer on dataset:  56%|█████▌    | 220000/392702 [00:44<00:34, 5019.87 examples/s]Running tokenizer on dataset:  56%|█████▋    | 221000/392702 [00:44<00:33, 5134.93 examples/s]Running tokenizer on dataset:  57%|█████▋    | 222000/392702 [00:44<00:33, 5154.04 examples/s]Running tokenizer on dataset:  57%|█████▋    | 223000/392702 [00:44<00:33, 5132.66 examples/s]Running tokenizer on dataset:  57%|█████▋    | 224000/392702 [00:45<00:32, 5171.09 examples/s]Running tokenizer on dataset:  57%|█████▋    | 225000/392702 [00:45<00:32, 5169.31 examples/s]Running tokenizer on dataset:  58%|█████▊    | 226000/392702 [00:45<00:32, 5183.26 examples/s]Running tokenizer on dataset:  58%|█████▊    | 227000/392702 [00:45<00:31, 5194.80 examples/s]Running tokenizer on dataset:  58%|█████▊    | 228000/392702 [00:45<00:31, 5158.09 examples/s]Running tokenizer on dataset:  58%|█████▊    | 229000/392702 [00:46<00:31, 5208.13 examples/s]Running tokenizer on dataset:  59%|█████▊    | 230000/392702 [00:46<00:31, 5212.74 examples/s]Running tokenizer on dataset:  59%|█████▉    | 231000/392702 [00:46<00:31, 5202.82 examples/s]Running tokenizer on dataset:  59%|█████▉    | 232000/392702 [00:46<00:30, 5184.76 examples/s]Running tokenizer on dataset:  59%|█████▉    | 233000/392702 [00:46<00:30, 5245.05 examples/s]Running tokenizer on dataset:  60%|█████▉    | 234000/392702 [00:47<00:30, 5218.69 examples/s]Running tokenizer on dataset:  60%|█████▉    | 235000/392702 [00:47<00:29, 5262.16 examples/s]Running tokenizer on dataset:  60%|██████    | 236000/392702 [00:47<00:30, 5176.99 examples/s]Running tokenizer on dataset:  60%|██████    | 237000/392702 [00:47<00:29, 5260.39 examples/s]Running tokenizer on dataset:  61%|██████    | 238000/392702 [00:47<00:34, 4447.30 examples/s]Running tokenizer on dataset:  61%|██████    | 239000/392702 [00:48<00:32, 4687.17 examples/s]Running tokenizer on dataset:  61%|██████    | 240000/392702 [00:48<00:31, 4843.43 examples/s]Running tokenizer on dataset:  61%|██████▏   | 241000/392702 [00:48<00:30, 4947.19 examples/s]Running tokenizer on dataset:  62%|██████▏   | 242000/392702 [00:48<00:29, 5023.61 examples/s]Running tokenizer on dataset:  62%|██████▏   | 243000/392702 [00:48<00:29, 5034.34 examples/s]Running tokenizer on dataset:  62%|██████▏   | 244000/392702 [00:49<00:29, 5100.33 examples/s]Running tokenizer on dataset:  62%|██████▏   | 245000/392702 [00:49<00:28, 5154.71 examples/s]Running tokenizer on dataset:  63%|██████▎   | 246000/392702 [00:49<00:28, 5131.63 examples/s]Running tokenizer on dataset:  63%|██████▎   | 247000/392702 [00:49<00:27, 5212.48 examples/s]Running tokenizer on dataset:  63%|██████▎   | 248000/392702 [00:49<00:27, 5196.43 examples/s]Running tokenizer on dataset:  63%|██████▎   | 249000/392702 [00:50<00:28, 5066.13 examples/s]Running tokenizer on dataset:  64%|██████▎   | 250000/392702 [00:50<00:27, 5125.94 examples/s]Running tokenizer on dataset:  64%|██████▍   | 251000/392702 [00:50<00:27, 5118.85 examples/s]Running tokenizer on dataset:  64%|██████▍   | 252000/392702 [00:50<00:27, 5135.22 examples/s]Running tokenizer on dataset:  64%|██████▍   | 253000/392702 [00:50<00:26, 5190.14 examples/s]Running tokenizer on dataset:  65%|██████▍   | 254000/392702 [00:50<00:26, 5175.57 examples/s]Running tokenizer on dataset:  65%|██████▍   | 255000/392702 [00:51<00:26, 5196.17 examples/s]Running tokenizer on dataset:  65%|██████▌   | 256000/392702 [00:51<00:26, 5221.80 examples/s]Running tokenizer on dataset:  65%|██████▌   | 257000/392702 [00:51<00:26, 5204.93 examples/s]Running tokenizer on dataset:  66%|██████▌   | 258000/392702 [00:51<00:25, 5225.76 examples/s]Running tokenizer on dataset:  66%|██████▌   | 259000/392702 [00:51<00:25, 5233.63 examples/s]Running tokenizer on dataset:  66%|██████▌   | 260000/392702 [00:52<00:25, 5192.04 examples/s]Running tokenizer on dataset:  66%|██████▋   | 261000/392702 [00:52<00:25, 5147.81 examples/s]Running tokenizer on dataset:  67%|██████▋   | 262000/392702 [00:52<00:30, 4267.67 examples/s]Running tokenizer on dataset:  67%|██████▋   | 263000/392702 [00:52<00:29, 4467.36 examples/s]Running tokenizer on dataset:  67%|██████▋   | 264000/392702 [00:53<00:27, 4647.90 examples/s]Running tokenizer on dataset:  67%|██████▋   | 265000/392702 [00:53<00:26, 4835.28 examples/s]Running tokenizer on dataset:  68%|██████▊   | 266000/392702 [00:53<00:25, 4943.53 examples/s]Running tokenizer on dataset:  68%|██████▊   | 267000/392702 [00:53<00:25, 4991.54 examples/s]Running tokenizer on dataset:  68%|██████▊   | 268000/392702 [00:53<00:24, 5071.15 examples/s]Running tokenizer on dataset:  68%|██████▊   | 269000/392702 [00:54<00:24, 5096.32 examples/s]Running tokenizer on dataset:  69%|██████▉   | 270000/392702 [00:54<00:23, 5119.46 examples/s]Running tokenizer on dataset:  69%|██████▉   | 271000/392702 [00:54<00:23, 5167.15 examples/s]Running tokenizer on dataset:  69%|██████▉   | 272000/392702 [00:54<00:23, 5187.71 examples/s]Running tokenizer on dataset:  70%|██████▉   | 273000/392702 [00:54<00:23, 5194.38 examples/s]Running tokenizer on dataset:  70%|██████▉   | 274000/392702 [00:54<00:22, 5241.79 examples/s]Running tokenizer on dataset:  70%|███████   | 275000/392702 [00:55<00:22, 5242.15 examples/s]Running tokenizer on dataset:  70%|███████   | 276000/392702 [00:55<00:22, 5231.68 examples/s]Running tokenizer on dataset:  71%|███████   | 277000/392702 [00:55<00:22, 5222.96 examples/s]Running tokenizer on dataset:  71%|███████   | 278000/392702 [00:55<00:22, 5207.84 examples/s]Running tokenizer on dataset:  71%|███████   | 279000/392702 [00:55<00:21, 5217.38 examples/s]Running tokenizer on dataset:  71%|███████▏  | 280000/392702 [00:56<00:21, 5238.98 examples/s]Running tokenizer on dataset:  72%|███████▏  | 281000/392702 [00:56<00:21, 5229.14 examples/s]Running tokenizer on dataset:  72%|███████▏  | 282000/392702 [00:56<00:21, 5220.94 examples/s]Running tokenizer on dataset:  72%|███████▏  | 283000/392702 [00:56<00:21, 5193.17 examples/s]Running tokenizer on dataset:  72%|███████▏  | 284000/392702 [00:56<00:20, 5242.69 examples/s]Running tokenizer on dataset:  73%|███████▎  | 285000/392702 [00:57<00:20, 5190.28 examples/s]Running tokenizer on dataset:  73%|███████▎  | 286000/392702 [00:57<00:23, 4464.68 examples/s]Running tokenizer on dataset:  73%|███████▎  | 287000/392702 [00:57<00:22, 4675.03 examples/s]Running tokenizer on dataset:  73%|███████▎  | 288000/392702 [00:57<00:21, 4814.37 examples/s]Running tokenizer on dataset:  74%|███████▎  | 289000/392702 [00:57<00:20, 4963.91 examples/s]Running tokenizer on dataset:  74%|███████▍  | 290000/392702 [00:58<00:20, 5039.36 examples/s]Running tokenizer on dataset:  74%|███████▍  | 291000/392702 [00:58<00:20, 5071.24 examples/s]Running tokenizer on dataset:  74%|███████▍  | 292000/392702 [00:58<00:19, 5129.11 examples/s]Running tokenizer on dataset:  75%|███████▍  | 293000/392702 [00:58<00:19, 5169.50 examples/s]Running tokenizer on dataset:  75%|███████▍  | 294000/392702 [00:58<00:18, 5197.43 examples/s]Running tokenizer on dataset:  75%|███████▌  | 295000/392702 [00:59<00:18, 5214.41 examples/s]Running tokenizer on dataset:  75%|███████▌  | 296000/392702 [00:59<00:18, 5199.29 examples/s]Running tokenizer on dataset:  76%|███████▌  | 297000/392702 [00:59<00:18, 5221.94 examples/s]Running tokenizer on dataset:  76%|███████▌  | 298000/392702 [00:59<00:18, 5207.00 examples/s]Running tokenizer on dataset:  76%|███████▌  | 299000/392702 [00:59<00:17, 5212.53 examples/s]Running tokenizer on dataset:  76%|███████▋  | 300000/392702 [01:00<00:17, 5219.67 examples/s]Running tokenizer on dataset:  77%|███████▋  | 301000/392702 [01:00<00:17, 5213.73 examples/s]Running tokenizer on dataset:  77%|███████▋  | 302000/392702 [01:00<00:17, 5249.04 examples/s]Running tokenizer on dataset:  77%|███████▋  | 303000/392702 [01:00<00:17, 5196.49 examples/s]Running tokenizer on dataset:  77%|███████▋  | 304000/392702 [01:00<00:17, 5207.72 examples/s]Running tokenizer on dataset:  78%|███████▊  | 305000/392702 [01:01<00:16, 5245.56 examples/s]Running tokenizer on dataset:  78%|███████▊  | 306000/392702 [01:01<00:16, 5187.00 examples/s]Running tokenizer on dataset:  78%|███████▊  | 307000/392702 [01:01<00:16, 5213.29 examples/s]Running tokenizer on dataset:  78%|███████▊  | 308000/392702 [01:01<00:16, 5206.52 examples/s]Running tokenizer on dataset:  79%|███████▊  | 309000/392702 [01:01<00:16, 5175.46 examples/s]Running tokenizer on dataset:  79%|███████▉  | 310000/392702 [01:02<00:18, 4441.62 examples/s]Running tokenizer on dataset:  79%|███████▉  | 311000/392702 [01:02<00:17, 4633.38 examples/s]Running tokenizer on dataset:  79%|███████▉  | 312000/392702 [01:02<00:17, 4711.71 examples/s]Running tokenizer on dataset:  80%|███████▉  | 313000/392702 [01:02<00:16, 4834.09 examples/s]Running tokenizer on dataset:  80%|███████▉  | 314000/392702 [01:02<00:15, 4954.66 examples/s]Running tokenizer on dataset:  80%|████████  | 315000/392702 [01:03<00:15, 5016.64 examples/s]Running tokenizer on dataset:  80%|████████  | 316000/392702 [01:03<00:15, 5074.04 examples/s]Running tokenizer on dataset:  81%|████████  | 317000/392702 [01:03<00:14, 5141.36 examples/s]Running tokenizer on dataset:  81%|████████  | 318000/392702 [01:03<00:14, 5127.58 examples/s]Running tokenizer on dataset:  81%|████████  | 319000/392702 [01:03<00:14, 5157.91 examples/s]Running tokenizer on dataset:  81%|████████▏ | 320000/392702 [01:04<00:14, 5187.13 examples/s]Running tokenizer on dataset:  82%|████████▏ | 321000/392702 [01:04<00:13, 5141.92 examples/s]Running tokenizer on dataset:  82%|████████▏ | 322000/392702 [01:04<00:13, 5182.30 examples/s]Running tokenizer on dataset:  82%|████████▏ | 323000/392702 [01:04<00:13, 5176.24 examples/s]Running tokenizer on dataset:  83%|████████▎ | 324000/392702 [01:04<00:13, 5167.76 examples/s]Running tokenizer on dataset:  83%|████████▎ | 325000/392702 [01:04<00:12, 5215.05 examples/s]Running tokenizer on dataset:  83%|████████▎ | 326000/392702 [01:05<00:12, 5186.70 examples/s]Running tokenizer on dataset:  83%|████████▎ | 327000/392702 [01:05<00:12, 5128.43 examples/s]Running tokenizer on dataset:  84%|████████▎ | 328000/392702 [01:05<00:12, 5190.89 examples/s]Running tokenizer on dataset:  84%|████████▍ | 329000/392702 [01:05<00:12, 5177.27 examples/s]Running tokenizer on dataset:  84%|████████▍ | 330000/392702 [01:05<00:12, 5167.81 examples/s]Running tokenizer on dataset:  84%|████████▍ | 331000/392702 [01:06<00:11, 5208.41 examples/s]Running tokenizer on dataset:  85%|████████▍ | 332000/392702 [01:06<00:11, 5156.61 examples/s]Running tokenizer on dataset:  85%|████████▍ | 333000/392702 [01:06<00:11, 5213.89 examples/s]Running tokenizer on dataset:  85%|████████▌ | 334000/392702 [01:06<00:13, 4278.15 examples/s]Running tokenizer on dataset:  85%|████████▌ | 335000/392702 [01:07<00:12, 4535.48 examples/s]Running tokenizer on dataset:  86%|████████▌ | 336000/392702 [01:07<00:12, 4677.89 examples/s]Running tokenizer on dataset:  86%|████████▌ | 337000/392702 [01:07<00:11, 4823.46 examples/s]Running tokenizer on dataset:  86%|████████▌ | 338000/392702 [01:07<00:11, 4928.11 examples/s]Running tokenizer on dataset:  86%|████████▋ | 339000/392702 [01:07<00:10, 4964.45 examples/s]Running tokenizer on dataset:  87%|████████▋ | 340000/392702 [01:08<00:10, 5050.76 examples/s]Running tokenizer on dataset:  87%|████████▋ | 341000/392702 [01:08<00:10, 5051.33 examples/s]Running tokenizer on dataset:  87%|████████▋ | 342000/392702 [01:08<00:10, 5035.60 examples/s]Running tokenizer on dataset:  87%|████████▋ | 343000/392702 [01:08<00:09, 5114.48 examples/s]Running tokenizer on dataset:  88%|████████▊ | 344000/392702 [01:08<00:09, 5115.18 examples/s]Running tokenizer on dataset:  88%|████████▊ | 345000/392702 [01:09<00:09, 5093.30 examples/s]Running tokenizer on dataset:  88%|████████▊ | 346000/392702 [01:09<00:09, 5141.41 examples/s]Running tokenizer on dataset:  88%|████████▊ | 347000/392702 [01:09<00:08, 5180.43 examples/s]Running tokenizer on dataset:  89%|████████▊ | 348000/392702 [01:09<00:08, 5195.13 examples/s]Running tokenizer on dataset:  89%|████████▉ | 349000/392702 [01:09<00:08, 5167.43 examples/s]Running tokenizer on dataset:  89%|████████▉ | 350000/392702 [01:09<00:08, 5179.55 examples/s]Running tokenizer on dataset:  89%|████████▉ | 351000/392702 [01:10<00:08, 5167.05 examples/s]Running tokenizer on dataset:  90%|████████▉ | 352000/392702 [01:10<00:07, 5181.21 examples/s]Running tokenizer on dataset:  90%|████████▉ | 353000/392702 [01:10<00:07, 5176.39 examples/s]Running tokenizer on dataset:  90%|█████████ | 354000/392702 [01:10<00:07, 5115.82 examples/s]Running tokenizer on dataset:  90%|█████████ | 355000/392702 [01:10<00:07, 5098.05 examples/s]Running tokenizer on dataset:  91%|█████████ | 356000/392702 [01:11<00:07, 5148.22 examples/s]Running tokenizer on dataset:  91%|█████████ | 357000/392702 [01:11<00:06, 5152.78 examples/s]Running tokenizer on dataset:  91%|█████████ | 358000/392702 [01:11<00:07, 4440.08 examples/s]Running tokenizer on dataset:  91%|█████████▏| 359000/392702 [01:11<00:07, 4580.97 examples/s]Running tokenizer on dataset:  92%|█████████▏| 360000/392702 [01:12<00:06, 4797.46 examples/s]Running tokenizer on dataset:  92%|█████████▏| 361000/392702 [01:12<00:06, 4902.69 examples/s]Running tokenizer on dataset:  92%|█████████▏| 362000/392702 [01:12<00:06, 5008.67 examples/s]Running tokenizer on dataset:  92%|█████████▏| 363000/392702 [01:12<00:05, 5040.93 examples/s]Running tokenizer on dataset:  93%|█████████▎| 364000/392702 [01:12<00:05, 5056.51 examples/s]Running tokenizer on dataset:  93%|█████████▎| 365000/392702 [01:12<00:05, 5090.89 examples/s]Running tokenizer on dataset:  93%|█████████▎| 366000/392702 [01:13<00:05, 5054.14 examples/s]Running tokenizer on dataset:  93%|█████████▎| 367000/392702 [01:13<00:05, 5132.60 examples/s]Running tokenizer on dataset:  94%|█████████▎| 368000/392702 [01:13<00:04, 5149.27 examples/s]Running tokenizer on dataset:  94%|█████████▍| 369000/392702 [01:13<00:04, 5138.55 examples/s]Running tokenizer on dataset:  94%|█████████▍| 370000/392702 [01:13<00:04, 5158.20 examples/s]Running tokenizer on dataset:  94%|█████████▍| 371000/392702 [01:14<00:04, 5180.19 examples/s]Running tokenizer on dataset:  95%|█████████▍| 372000/392702 [01:14<00:04, 5128.67 examples/s]Running tokenizer on dataset:  95%|█████████▍| 373000/392702 [01:14<00:03, 5128.12 examples/s]Running tokenizer on dataset:  95%|█████████▌| 374000/392702 [01:14<00:03, 5148.41 examples/s]Running tokenizer on dataset:  95%|█████████▌| 375000/392702 [01:14<00:03, 5108.44 examples/s]Running tokenizer on dataset:  96%|█████████▌| 376000/392702 [01:15<00:03, 5077.02 examples/s]Running tokenizer on dataset:  96%|█████████▌| 377000/392702 [01:15<00:03, 5175.49 examples/s]Running tokenizer on dataset:  96%|█████████▋| 378000/392702 [01:15<00:02, 5136.31 examples/s]Running tokenizer on dataset:  97%|█████████▋| 379000/392702 [01:15<00:02, 5181.97 examples/s]Running tokenizer on dataset:  97%|█████████▋| 380000/392702 [01:15<00:02, 5117.74 examples/s]Running tokenizer on dataset:  97%|█████████▋| 381000/392702 [01:16<00:02, 5072.31 examples/s]Running tokenizer on dataset:  97%|█████████▋| 382000/392702 [01:16<00:02, 4405.31 examples/s]Running tokenizer on dataset:  98%|█████████▊| 383000/392702 [01:16<00:02, 4613.72 examples/s]Running tokenizer on dataset:  98%|█████████▊| 384000/392702 [01:16<00:01, 4745.95 examples/s]Running tokenizer on dataset:  98%|█████████▊| 385000/392702 [01:16<00:01, 4879.57 examples/s]Running tokenizer on dataset:  98%|█████████▊| 386000/392702 [01:17<00:01, 4952.34 examples/s]Running tokenizer on dataset:  99%|█████████▊| 387000/392702 [01:17<00:01, 4995.17 examples/s]Running tokenizer on dataset:  99%|█████████▉| 388000/392702 [01:17<00:00, 4924.56 examples/s]Running tokenizer on dataset:  99%|█████████▉| 389000/392702 [01:17<00:00, 4996.02 examples/s]Running tokenizer on dataset:  99%|█████████▉| 390000/392702 [01:17<00:00, 4993.77 examples/s]Running tokenizer on dataset: 100%|█████████▉| 391000/392702 [01:18<00:00, 5037.76 examples/s]Running tokenizer on dataset: 100%|█████████▉| 392000/392702 [01:18<00:00, 5104.14 examples/s]Running tokenizer on dataset: 100%|██████████| 392702/392702 [01:18<00:00, 5031.97 examples/s]Running tokenizer on dataset: 100%|██████████| 392702/392702 [01:18<00:00, 5001.82 examples/s]
Running tokenizer on dataset:   0%|          | 0/9815 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3551fe92b65873bf.arrow
02/18/2024 21:39:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3551fe92b65873bf.arrow
Running tokenizer on dataset:  10%|█         | 1000/9815 [00:00<00:01, 4926.64 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9815 [00:00<00:01, 5046.60 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9815 [00:00<00:01, 5099.87 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9815 [00:00<00:01, 4992.18 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9815 [00:01<00:00, 4820.07 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9815 [00:01<00:00, 4938.31 examples/s]Running tokenizer on dataset:  71%|███████▏  | 7000/9815 [00:01<00:00, 5027.78 examples/s]Running tokenizer on dataset:  82%|████████▏ | 8000/9815 [00:01<00:00, 5041.91 examples/s]Running tokenizer on dataset:  92%|█████████▏| 9000/9815 [00:01<00:00, 5106.62 examples/s]Running tokenizer on dataset: 100%|██████████| 9815/9815 [00:01<00:00, 5112.94 examples/s]Running tokenizer on dataset: 100%|██████████| 9815/9815 [00:02<00:00, 4893.32 examples/s]
Running tokenizer on dataset:   0%|          | 0/9832 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b0e35849acb6039c.arrow
02/18/2024 21:39:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b0e35849acb6039c.arrow
Running tokenizer on dataset:  10%|█         | 1000/9832 [00:00<00:02, 4329.36 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9832 [00:00<00:01, 4675.39 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9832 [00:00<00:01, 4864.94 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9832 [00:00<00:01, 4680.49 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9832 [00:01<00:01, 4806.52 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9832 [00:01<00:00, 4860.43 examples/s]Running tokenizer on dataset:  71%|███████   | 7000/9832 [00:01<00:00, 4917.37 examples/s]Running tokenizer on dataset:  81%|████████▏ | 8000/9832 [00:01<00:00, 4062.69 examples/s]Running tokenizer on dataset:  92%|█████████▏| 9000/9832 [00:01<00:00, 4342.15 examples/s]Running tokenizer on dataset: 100%|██████████| 9832/9832 [00:02<00:00, 4476.75 examples/s]Running tokenizer on dataset: 100%|██████████| 9832/9832 [00:02<00:00, 4539.96 examples/s]
Running tokenizer on dataset:   0%|          | 0/9796 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-575dc5fbe0b6ee56.arrow
02/18/2024 21:39:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-575dc5fbe0b6ee56.arrow
Running tokenizer on dataset:  10%|█         | 1000/9796 [00:00<00:02, 4056.34 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9796 [00:00<00:01, 4662.17 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9796 [00:00<00:01, 4870.53 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9796 [00:00<00:01, 4624.17 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9796 [00:01<00:01, 4669.44 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9796 [00:01<00:00, 4839.01 examples/s]Running tokenizer on dataset:  71%|███████▏  | 7000/9796 [00:01<00:00, 4957.21 examples/s]Running tokenizer on dataset:  82%|████████▏ | 8000/9796 [00:01<00:00, 5002.90 examples/s]Running tokenizer on dataset:  92%|█████████▏| 9000/9796 [00:01<00:00, 5069.84 examples/s]Running tokenizer on dataset: 100%|██████████| 9796/9796 [00:02<00:00, 4999.85 examples/s]Running tokenizer on dataset: 100%|██████████| 9796/9796 [00:02<00:00, 4845.20 examples/s]
Running tokenizer on dataset:   0%|          | 0/9847 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f9e4bd447fd1bb4e.arrow
02/18/2024 21:39:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f9e4bd447fd1bb4e.arrow
Running tokenizer on dataset:  10%|█         | 1000/9847 [00:00<00:02, 4254.70 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9847 [00:00<00:01, 4704.59 examples/s]Running tokenizer on dataset:  30%|███       | 3000/9847 [00:00<00:01, 4817.99 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9847 [00:00<00:01, 4688.57 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9847 [00:01<00:01, 4794.48 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9847 [00:01<00:00, 4904.78 examples/s]Running tokenizer on dataset:  71%|███████   | 7000/9847 [00:01<00:00, 4961.93 examples/s]Running tokenizer on dataset:  81%|████████  | 8000/9847 [00:01<00:00, 4997.21 examples/s]Running tokenizer on dataset:  91%|█████████▏| 9000/9847 [00:01<00:00, 5026.36 examples/s]Running tokenizer on dataset: 100%|██████████| 9847/9847 [00:02<00:00, 4911.47 examples/s]Running tokenizer on dataset: 100%|██████████| 9847/9847 [00:02<00:00, 4842.79 examples/s]
02/18/2024 21:39:10 - INFO - __main__ - Sample 81 of the training set: {'premise': 'The man should have died instantly.', 'hypothesis': 'The man was perfectly fine. ', 'label': 2, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 767, 881, 505, 6423, 26232, 29889, 1, 450, 767, 471, 7970, 2691, 29889, 29871], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:39:10 - INFO - __main__ - Sample 14 of the training set: {'premise': "I don't mean to be glib about your concerns, but if I were you, I might be more concerned about the near-term rate implications of this $1.", 'hypothesis': 'I am concerned more about your issues than the near-term rate implications.', 'label': 2, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 1016, 29915, 29873, 2099, 304, 367, 330, 1982, 1048, 596, 21838, 29892, 541, 565, 306, 892, 366, 29892, 306, 1795, 367, 901, 15041, 1048, 278, 2978, 29899, 8489, 6554, 2411, 5795, 310, 445, 395, 29896, 29889, 1, 306, 626, 15041, 901, 1048, 596, 5626, 1135, 278, 2978, 29899, 8489, 6554, 2411, 5795, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:39:10 - INFO - __main__ - Sample 3 of the training set: {'premise': 'How do you know? All this is their information again.', 'hypothesis': 'This information belongs to them.', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1128, 437, 366, 1073, 29973, 2178, 445, 338, 1009, 2472, 1449, 29889, 1, 910, 2472, 14393, 304, 963, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:39:11 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:39:12,991 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx. If premise, hypothesis, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:39:13,311 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:39:13,312 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:39:13,312 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:39:13,312 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:39:13,312 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:39:13,312 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:39:13,312 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:39:13,313 >>   Number of trainable parameters = 1,280,155,648
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:21,  2.14s/it]  5%|▌         | 2/39 [00:03<01:00,  1.64s/it]  8%|▊         | 3/39 [00:04<00:54,  1.52s/it] 10%|█         | 4/39 [00:06<00:51,  1.46s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.43s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.41s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.40s/it] 21%|██        | 8/39 [00:11<00:43,  1.39s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.38s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.20s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:21,  1.37s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:51<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:40:06,045 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.8633, 'train_samples_per_second': 5.675, 'train_steps_per_second': 0.738, 'train_loss': 1.4961227025741186, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:40:06,180 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:40:06,182 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:40:26,024 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:40:26,026 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:40:26,027 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mnli256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.4961
  train_runtime            = 0:00:52.86
  train_samples            =        100
  train_samples_per_second =      5.675
  train_steps_per_second   =      0.738
02/18/2024 21:40:26 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:40:26,067 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx. If premise, hypothesis, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:40:26,069 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:40:26,069 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:40:26,069 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.77it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.36it/s] 31%|███       | 4/13 [00:01<00:03,  2.92it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.70it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.59it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.52it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.47it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.44it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.42it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.41it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.40it/s]100%|██████████| 13/13 [00:04<00:00,  2.80it/s]100%|██████████| 13/13 [00:04<00:00,  2.66it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.36
  eval_loss               =     1.7374
  eval_runtime            = 0:00:05.32
  eval_samples            =        100
  eval_samples_per_second =     18.792
  eval_steps_per_second   =      2.443
[INFO|trainer.py:737] 2024-02-18 21:40:31,394 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: premise, hypothesis, idx. If premise, hypothesis, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:40:31,396 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:40:31,396 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:40:31,396 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.74it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.35it/s] 31%|███       | 4/13 [00:01<00:03,  2.90it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.69it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.57it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.50it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.45it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.43it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.40it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.39it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.39it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.67it/s]
***** eval metrics *****
  epoch_mm                   =        3.0
  eval_accuracy_mm           =        0.5
  eval_loss_mm               =     1.2957
  eval_runtime_mm            = 0:00:05.29
  eval_samples_mm            =        100
  eval_samples_per_second_mm =     18.872
  eval_steps_per_second_mm   =      2.453
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:42:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:42:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1/runs/Feb18_21-42-10_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:42:12 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:42:12 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:42:12 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:42:12 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:42:12,887 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:42:12,889 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:42:12,937 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:42:12,937 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:42:12,937 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:42:12,937 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:42:12,937 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:42:12,996 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:42:13,054 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:42:16,134 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:42:16,134 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/104743 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7bdee6b47e701dd0.arrow
02/18/2024 21:42:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7bdee6b47e701dd0.arrow
Running tokenizer on dataset:   1%|          | 1000/104743 [00:00<00:26, 3933.95 examples/s]Running tokenizer on dataset:   2%|▏         | 2000/104743 [00:00<00:23, 4390.78 examples/s]Running tokenizer on dataset:   3%|▎         | 3000/104743 [00:00<00:28, 3508.73 examples/s]Running tokenizer on dataset:   4%|▍         | 4000/104743 [00:01<00:26, 3814.45 examples/s]Running tokenizer on dataset:   5%|▍         | 5000/104743 [00:01<00:24, 4067.58 examples/s]Running tokenizer on dataset:   6%|▌         | 6000/104743 [00:01<00:23, 4169.92 examples/s]Running tokenizer on dataset:   7%|▋         | 7000/104743 [00:01<00:22, 4368.17 examples/s]Running tokenizer on dataset:   8%|▊         | 8000/104743 [00:01<00:21, 4440.33 examples/s]Running tokenizer on dataset:   9%|▊         | 9000/104743 [00:02<00:21, 4528.03 examples/s]Running tokenizer on dataset:  10%|▉         | 10000/104743 [00:02<00:20, 4578.32 examples/s]Running tokenizer on dataset:  11%|█         | 11000/104743 [00:02<00:20, 4649.64 examples/s]Running tokenizer on dataset:  11%|█▏        | 12000/104743 [00:02<00:20, 4624.91 examples/s]Running tokenizer on dataset:  12%|█▏        | 13000/104743 [00:02<00:19, 4669.86 examples/s]Running tokenizer on dataset:  13%|█▎        | 14000/104743 [00:03<00:19, 4683.77 examples/s]Running tokenizer on dataset:  14%|█▍        | 15000/104743 [00:03<00:19, 4670.83 examples/s]Running tokenizer on dataset:  15%|█▌        | 16000/104743 [00:03<00:18, 4730.11 examples/s]Running tokenizer on dataset:  16%|█▌        | 17000/104743 [00:03<00:18, 4717.99 examples/s]Running tokenizer on dataset:  17%|█▋        | 18000/104743 [00:04<00:18, 4684.39 examples/s]Running tokenizer on dataset:  18%|█▊        | 19000/104743 [00:04<00:18, 4713.91 examples/s]Running tokenizer on dataset:  19%|█▉        | 20000/104743 [00:04<00:18, 4682.82 examples/s]Running tokenizer on dataset:  20%|██        | 21000/104743 [00:04<00:17, 4681.71 examples/s]Running tokenizer on dataset:  21%|██        | 22000/104743 [00:04<00:17, 4692.64 examples/s]Running tokenizer on dataset:  22%|██▏       | 23000/104743 [00:05<00:17, 4692.73 examples/s]Running tokenizer on dataset:  23%|██▎       | 24000/104743 [00:05<00:17, 4687.17 examples/s]Running tokenizer on dataset:  24%|██▍       | 25000/104743 [00:05<00:16, 4701.44 examples/s]Running tokenizer on dataset:  25%|██▍       | 26000/104743 [00:05<00:16, 4705.64 examples/s]Running tokenizer on dataset:  26%|██▌       | 27000/104743 [00:05<00:16, 4681.21 examples/s]Running tokenizer on dataset:  27%|██▋       | 28000/104743 [00:06<00:16, 4709.15 examples/s]Running tokenizer on dataset:  28%|██▊       | 29000/104743 [00:06<00:16, 4693.85 examples/s]Running tokenizer on dataset:  29%|██▊       | 30000/104743 [00:06<00:16, 4649.16 examples/s]Running tokenizer on dataset:  30%|██▉       | 31000/104743 [00:06<00:15, 4673.69 examples/s]Running tokenizer on dataset:  31%|███       | 32000/104743 [00:07<00:15, 4694.94 examples/s]Running tokenizer on dataset:  32%|███▏      | 33000/104743 [00:07<00:17, 3992.63 examples/s]Running tokenizer on dataset:  32%|███▏      | 34000/104743 [00:07<00:16, 4196.73 examples/s]Running tokenizer on dataset:  33%|███▎      | 35000/104743 [00:07<00:16, 4317.95 examples/s]Running tokenizer on dataset:  34%|███▍      | 36000/104743 [00:08<00:15, 4400.78 examples/s]Running tokenizer on dataset:  35%|███▌      | 37000/104743 [00:08<00:14, 4526.22 examples/s]Running tokenizer on dataset:  36%|███▋      | 38000/104743 [00:08<00:14, 4564.86 examples/s]Running tokenizer on dataset:  37%|███▋      | 39000/104743 [00:08<00:14, 4595.71 examples/s]Running tokenizer on dataset:  38%|███▊      | 40000/104743 [00:08<00:13, 4627.70 examples/s]Running tokenizer on dataset:  39%|███▉      | 41000/104743 [00:09<00:13, 4656.52 examples/s]Running tokenizer on dataset:  40%|████      | 42000/104743 [00:09<00:13, 4673.69 examples/s]Running tokenizer on dataset:  41%|████      | 43000/104743 [00:09<00:13, 4653.15 examples/s]Running tokenizer on dataset:  42%|████▏     | 44000/104743 [00:09<00:12, 4692.13 examples/s]Running tokenizer on dataset:  43%|████▎     | 45000/104743 [00:09<00:12, 4668.40 examples/s]Running tokenizer on dataset:  44%|████▍     | 46000/104743 [00:10<00:12, 4691.97 examples/s]Running tokenizer on dataset:  45%|████▍     | 47000/104743 [00:10<00:12, 4723.30 examples/s]Running tokenizer on dataset:  46%|████▌     | 48000/104743 [00:10<00:12, 4685.55 examples/s]Running tokenizer on dataset:  47%|████▋     | 49000/104743 [00:10<00:11, 4717.37 examples/s]Running tokenizer on dataset:  48%|████▊     | 50000/104743 [00:10<00:11, 4721.35 examples/s]Running tokenizer on dataset:  49%|████▊     | 51000/104743 [00:11<00:11, 4675.70 examples/s]Running tokenizer on dataset:  50%|████▉     | 52000/104743 [00:11<00:11, 4712.26 examples/s]Running tokenizer on dataset:  51%|█████     | 53000/104743 [00:11<00:11, 4693.61 examples/s]Running tokenizer on dataset:  52%|█████▏    | 54000/104743 [00:11<00:10, 4694.74 examples/s]Running tokenizer on dataset:  53%|█████▎    | 55000/104743 [00:12<00:10, 4684.68 examples/s]Running tokenizer on dataset:  53%|█████▎    | 56000/104743 [00:12<00:10, 4678.65 examples/s]Running tokenizer on dataset:  54%|█████▍    | 57000/104743 [00:12<00:10, 4684.43 examples/s]Running tokenizer on dataset:  55%|█████▌    | 58000/104743 [00:12<00:11, 4059.19 examples/s]Running tokenizer on dataset:  56%|█████▋    | 59000/104743 [00:12<00:10, 4257.97 examples/s]Running tokenizer on dataset:  57%|█████▋    | 60000/104743 [00:13<00:10, 4339.85 examples/s]Running tokenizer on dataset:  58%|█████▊    | 61000/104743 [00:13<00:09, 4467.31 examples/s]Running tokenizer on dataset:  59%|█████▉    | 62000/104743 [00:13<00:09, 4533.69 examples/s]Running tokenizer on dataset:  60%|██████    | 63000/104743 [00:13<00:09, 4547.67 examples/s]Running tokenizer on dataset:  61%|██████    | 64000/104743 [00:14<00:08, 4613.30 examples/s]Running tokenizer on dataset:  62%|██████▏   | 65000/104743 [00:14<00:08, 4607.00 examples/s]Running tokenizer on dataset:  63%|██████▎   | 66000/104743 [00:14<00:08, 4641.70 examples/s]Running tokenizer on dataset:  64%|██████▍   | 67000/104743 [00:14<00:08, 4681.00 examples/s]Running tokenizer on dataset:  65%|██████▍   | 68000/104743 [00:14<00:07, 4687.52 examples/s]Running tokenizer on dataset:  66%|██████▌   | 69000/104743 [00:15<00:07, 4697.53 examples/s]Running tokenizer on dataset:  67%|██████▋   | 70000/104743 [00:15<00:07, 4697.76 examples/s]Running tokenizer on dataset:  68%|██████▊   | 71000/104743 [00:15<00:07, 4722.76 examples/s]Running tokenizer on dataset:  69%|██████▊   | 72000/104743 [00:15<00:07, 4665.35 examples/s]Running tokenizer on dataset:  70%|██████▉   | 73000/104743 [00:15<00:06, 4689.85 examples/s]Running tokenizer on dataset:  71%|███████   | 74000/104743 [00:16<00:06, 4686.69 examples/s]Running tokenizer on dataset:  72%|███████▏  | 75000/104743 [00:16<00:06, 4646.48 examples/s]Running tokenizer on dataset:  73%|███████▎  | 76000/104743 [00:16<00:06, 4661.54 examples/s]Running tokenizer on dataset:  74%|███████▎  | 77000/104743 [00:16<00:05, 4664.58 examples/s]Running tokenizer on dataset:  74%|███████▍  | 78000/104743 [00:17<00:05, 4650.55 examples/s]Running tokenizer on dataset:  75%|███████▌  | 79000/104743 [00:17<00:05, 4661.33 examples/s]Running tokenizer on dataset:  76%|███████▋  | 80000/104743 [00:17<00:05, 4652.09 examples/s]Running tokenizer on dataset:  77%|███████▋  | 81000/104743 [00:17<00:05, 4683.68 examples/s]Running tokenizer on dataset:  78%|███████▊  | 82000/104743 [00:18<00:05, 4060.06 examples/s]Running tokenizer on dataset:  79%|███████▉  | 83000/104743 [00:18<00:05, 4207.92 examples/s]Running tokenizer on dataset:  80%|████████  | 84000/104743 [00:18<00:04, 4324.22 examples/s]Running tokenizer on dataset:  81%|████████  | 85000/104743 [00:18<00:04, 4438.01 examples/s]Running tokenizer on dataset:  82%|████████▏ | 86000/104743 [00:18<00:04, 4499.23 examples/s]Running tokenizer on dataset:  83%|████████▎ | 87000/104743 [00:19<00:03, 4552.42 examples/s]Running tokenizer on dataset:  84%|████████▍ | 88000/104743 [00:19<00:03, 4610.86 examples/s]Running tokenizer on dataset:  85%|████████▍ | 89000/104743 [00:19<00:03, 4638.19 examples/s]Running tokenizer on dataset:  86%|████████▌ | 90000/104743 [00:19<00:03, 4653.00 examples/s]Running tokenizer on dataset:  87%|████████▋ | 91000/104743 [00:19<00:02, 4690.28 examples/s]Running tokenizer on dataset:  88%|████████▊ | 92000/104743 [00:20<00:02, 4702.95 examples/s]Running tokenizer on dataset:  89%|████████▉ | 93000/104743 [00:20<00:02, 4640.43 examples/s]Running tokenizer on dataset:  90%|████████▉ | 94000/104743 [00:20<00:02, 4676.46 examples/s]Running tokenizer on dataset:  91%|█████████ | 95000/104743 [00:20<00:02, 4678.57 examples/s]Running tokenizer on dataset:  92%|█████████▏| 96000/104743 [00:21<00:01, 4644.50 examples/s]Running tokenizer on dataset:  93%|█████████▎| 97000/104743 [00:21<00:01, 4644.18 examples/s]Running tokenizer on dataset:  94%|█████████▎| 98000/104743 [00:21<00:01, 4645.21 examples/s]Running tokenizer on dataset:  95%|█████████▍| 99000/104743 [00:21<00:01, 4654.33 examples/s]Running tokenizer on dataset:  95%|█████████▌| 100000/104743 [00:21<00:01, 4643.64 examples/s]Running tokenizer on dataset:  96%|█████████▋| 101000/104743 [00:22<00:00, 4673.14 examples/s]Running tokenizer on dataset:  97%|█████████▋| 102000/104743 [00:22<00:00, 4708.85 examples/s]Running tokenizer on dataset:  98%|█████████▊| 103000/104743 [00:22<00:00, 4704.52 examples/s]Running tokenizer on dataset:  99%|█████████▉| 104000/104743 [00:22<00:00, 4706.52 examples/s]Running tokenizer on dataset: 100%|██████████| 104743/104743 [00:22<00:00, 4664.54 examples/s]Running tokenizer on dataset: 100%|██████████| 104743/104743 [00:22<00:00, 4574.33 examples/s]
Running tokenizer on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5798215d8ab16665.arrow
02/18/2024 21:42:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5798215d8ab16665.arrow
Running tokenizer on dataset:  18%|█▊        | 1000/5463 [00:00<00:01, 3933.55 examples/s]Running tokenizer on dataset:  37%|███▋      | 2000/5463 [00:00<00:01, 3099.65 examples/s]Running tokenizer on dataset:  55%|█████▍    | 3000/5463 [00:00<00:00, 3454.51 examples/s]Running tokenizer on dataset:  73%|███████▎  | 4000/5463 [00:01<00:00, 3661.43 examples/s]Running tokenizer on dataset:  92%|█████████▏| 5000/5463 [00:01<00:00, 3939.94 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:01<00:00, 3995.02 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:01<00:00, 3748.66 examples/s]
Running tokenizer on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-15d22d8e54d696a4.arrow
02/18/2024 21:42:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/qnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-15d22d8e54d696a4.arrow
Running tokenizer on dataset:  18%|█▊        | 1000/5463 [00:00<00:01, 4053.63 examples/s]Running tokenizer on dataset:  37%|███▋      | 2000/5463 [00:00<00:00, 4075.69 examples/s]Running tokenizer on dataset:  55%|█████▍    | 3000/5463 [00:00<00:00, 4002.15 examples/s]Running tokenizer on dataset:  73%|███████▎  | 4000/5463 [00:00<00:00, 3993.08 examples/s]Running tokenizer on dataset:  92%|█████████▏| 5000/5463 [00:01<00:00, 4150.95 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:01<00:00, 4153.70 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:01<00:00, 4082.55 examples/s]
02/18/2024 21:42:42 - INFO - __main__ - Sample 81 of the training set: {'question': 'When was USB Battery Charging Specification Revision 1.2 released?', 'sentence': 'The USB Battery Charging Specification Revision 1.2 (released in 2010) makes clear that there are safety limits to the rated current at 5 A coming from USB 2.0.', 'label': 0, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1932, 471, 12951, 350, 2620, 29891, 678, 1191, 292, 12048, 2450, 830, 4924, 29871, 29896, 29889, 29906, 5492, 29973, 1, 450, 12951, 350, 2620, 29891, 678, 1191, 292, 12048, 2450, 830, 4924, 29871, 29896, 29889, 29906, 313, 276, 4611, 297, 29871, 29906, 29900, 29896, 29900, 29897, 3732, 2821, 393, 727, 526, 15332, 13071, 304, 278, 364, 630, 1857, 472, 29871, 29945, 319, 6421, 515, 12951, 29871, 29906, 29889, 29900, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:42:42 - INFO - __main__ - Sample 14 of the training set: {'question': 'While looking for bugs, what else can testing do?', 'sentence': 'Although testing can determine the correctness of software under the assumption of some specific hypotheses (see hierarchy of testing difficulty below), testing cannot identify all the defects within software.', 'label': 1, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 5806, 3063, 363, 24557, 29892, 825, 1683, 508, 6724, 437, 29973, 1, 8512, 6724, 508, 8161, 278, 1959, 2264, 310, 7047, 1090, 278, 11833, 310, 777, 2702, 13752, 21523, 313, 4149, 21277, 310, 6724, 14656, 2400, 511, 6724, 2609, 12439, 599, 278, 23503, 29879, 2629, 7047, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:42:42 - INFO - __main__ - Sample 3 of the training set: {'question': 'What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?', 'sentence': 'On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1724, 338, 278, 1024, 310, 278, 5720, 29871, 29929, 7800, 6641, 310, 3037, 2142, 271, 988, 278, 13476, 2480, 8249, 22630, 278, 11688, 1039, 550, 29973, 1, 1551, 29871, 29941, 29896, 5846, 29871, 29896, 29947, 29945, 29941, 29892, 278, 13476, 2480, 8249, 472, 3037, 2142, 271, 6153, 2750, 278, 10637, 4889, 472, 678, 300, 403, 29874, 470, 15018, 403, 29892, 263, 2319, 5720, 14183, 7800, 6641, 310, 3037, 2142, 271, 29892, 322, 17785, 963, 373, 29871, 29953, 5490, 29871, 29896, 29947, 29945, 29946, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:42:42 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:42:43,870 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx, question. If sentence, idx, question are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:42:44,190 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:42:44,191 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:42:44,191 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:42:44,191 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:42:44,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:42:44,191 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:42:44,191 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:42:44,192 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:22,  2.18s/it]  5%|▌         | 2/39 [00:03<01:01,  1.66s/it]  8%|▊         | 3/39 [00:04<00:55,  1.53s/it] 10%|█         | 4/39 [00:06<00:51,  1.47s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.44s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.41s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.39s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.21s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:52<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 21:43:36,999 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.9388, 'train_samples_per_second': 5.667, 'train_steps_per_second': 0.737, 'train_loss': 0.9836237002641727, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:43:37,134 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:43:37,137 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:43:55,581 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:43:55,585 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:43:55,587 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/qnli256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.9836
  train_runtime            = 0:00:52.93
  train_samples            =        100
  train_samples_per_second =      5.667
  train_steps_per_second   =      0.737
02/18/2024 21:43:55 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:43:55,621 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence, idx, question. If sentence, idx, question are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:43:55,623 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:43:55,623 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:43:55,623 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.74it/s] 23%|██▎       | 3/13 [00:00<00:02,  3.36it/s] 31%|███       | 4/13 [00:01<00:03,  2.91it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.69it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.57it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.51it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.46it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.43it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.41it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.40it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.39it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.67it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.49
  eval_loss               =     0.8058
  eval_runtime            = 0:00:05.32
  eval_samples            =        100
  eval_samples_per_second =     18.784
  eval_steps_per_second   =      2.442
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:44:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:44:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/runs/Feb18_21-44-30_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:44:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:44:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:44:31 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:44:31 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:44:31,858 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:44:31,861 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:44:31,892 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:44:31,893 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:44:31,893 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:44:31,893 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:44:31,893 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:44:31,955 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:44:32,014 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:44:35,105 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:44:35,105 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ed92b3eeb01c931f.arrow
02/18/2024 21:44:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ed92b3eeb01c931f.arrow
Running tokenizer on dataset:  40%|████      | 1000/2490 [00:00<00:00, 3324.89 examples/s]Running tokenizer on dataset:  80%|████████  | 2000/2490 [00:00<00:00, 3285.69 examples/s]Running tokenizer on dataset: 100%|██████████| 2490/2490 [00:00<00:00, 3294.74 examples/s]Running tokenizer on dataset: 100%|██████████| 2490/2490 [00:00<00:00, 3263.68 examples/s]
Running tokenizer on dataset:   0%|          | 0/277 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-13b5add1f6d4905a.arrow
02/18/2024 21:44:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-13b5add1f6d4905a.arrow
Running tokenizer on dataset: 100%|██████████| 277/277 [00:00<00:00, 2735.43 examples/s]
Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c6c5e3c98889b02.arrow
02/18/2024 21:44:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c6c5e3c98889b02.arrow
Running tokenizer on dataset:  33%|███▎      | 1000/3000 [00:00<00:00, 2847.93 examples/s]Running tokenizer on dataset:  67%|██████▋   | 2000/3000 [00:00<00:00, 3514.48 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:00<00:00, 2932.57 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:01<00:00, 2984.33 examples/s]
02/18/2024 21:44:37 - INFO - __main__ - Sample 654 of the training set: {'sentence1': 'On Wednesday night people in the Bahçelievler district of Yenibosna in Istanbul, Turkey, claimed that they had seen a UFO flying in the sky, the Turkish newspaper Sabah reports. They said that the UFO was glowing with white lights and that it revolved around itself. Istanbul has had previous UFO incidents in the past, and in 2002, the Istanbul UFO Museum was opened to the public.', 'sentence2': 'A UFO might have been seen in Turkey.', 'label': 0, 'idx': 654, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 1551, 15050, 4515, 3250, 4646, 2305, 297, 278, 15174, 30019, 295, 10384, 1358, 6474, 310, 612, 264, 747, 359, 1056, 297, 11066, 273, 8645, 29892, 26459, 29892, 17049, 393, 896, 750, 3595, 263, 501, 5800, 22764, 297, 278, 14744, 29892, 278, 24682, 19656, 11775, 801, 13676, 29889, 2688, 1497, 393, 278, 501, 5800, 471, 330, 677, 292, 411, 4796, 26068, 322, 393, 372, 13819, 1490, 2820, 3528, 29889, 11066, 273, 8645, 756, 750, 3517, 501, 5800, 5528, 16719, 297, 278, 4940, 29892, 322, 297, 29871, 29906, 29900, 29900, 29906, 29892, 278, 11066, 273, 8645, 501, 5800, 6838, 471, 6496, 304, 278, 970, 29889, 1, 319, 501, 5800, 1795, 505, 1063, 3595, 297, 26459, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:44:37 - INFO - __main__ - Sample 114 of the training set: {'sentence1': 'Friday, March 25, 2005 Terri Schiavo\'s father made a last-minute appeal to have his daughter\'s feeding tube reinserted Friday. "Terri is weakening. She\'s down to her last hours," said Bob Schindler, Schiavo\'s father. "Something has to be done and done quick." Bob Schindler and his wife Mary filed an emergency appeal Friday to have a feeding tube reinserted in their 41-year-old daughter. Earlier Friday, U.S. District Judge James Whittemore rejected the family\'s previous appeal. On Wednesday, he rejected a similar request. The new appeal will be filed in the 11th Circuit Court of Appeals in Atlanta, although that court has twice before rejected the family\'s appeal. The tube was removed on Friday, March 18, 2005.', 'sentence2': 'Bob Schindler married Mary.', 'label': 0, 'idx': 114, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 28728, 29892, 4779, 29871, 29906, 29945, 29892, 29871, 29906, 29900, 29900, 29945, 5061, 374, 1102, 423, 1365, 29915, 29879, 4783, 1754, 263, 1833, 29899, 1195, 1082, 25530, 304, 505, 670, 8750, 29915, 29879, 8343, 292, 260, 4003, 337, 7851, 287, 28728, 29889, 376, 29911, 261, 374, 338, 8062, 8333, 29889, 2296, 29915, 29879, 1623, 304, 902, 1833, 6199, 1699, 1497, 7991, 1102, 513, 1358, 29892, 1102, 423, 1365, 29915, 29879, 4783, 29889, 376, 16804, 756, 304, 367, 2309, 322, 2309, 4996, 1213, 7991, 1102, 513, 1358, 322, 670, 6532, 6182, 934, 29881, 385, 11176, 14703, 25530, 28728, 304, 505, 263, 8343, 292, 260, 4003, 337, 7851, 287, 297, 1009, 29871, 29946, 29896, 29899, 6360, 29899, 1025, 8750, 29889, 5290, 4926, 28728, 29892, 501, 29889, 29903, 29889, 7457, 26817, 5011, 806, 986, 331, 487, 22225, 278, 3942, 29915, 29879, 3517, 25530, 29889, 1551, 15050, 4515, 3250, 29892, 540, 22225, 263, 2788, 2009, 29889, 450, 716, 25530, 674, 367, 934, 29881, 297, 278, 29871, 29896, 29896, 386, 12594, 3121, 9245, 310, 2401, 29872, 1338, 297, 26484, 29892, 5998, 393, 8973, 756, 8951, 1434, 22225, 278, 3942, 29915, 29879, 25530, 29889, 450, 260, 4003, 471, 6206, 373, 28728, 29892, 4779, 29871, 29896, 29947, 29892, 29871, 29906, 29900, 29900, 29945, 29889, 1, 7991, 1102, 513, 1358, 8300, 6182, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:44:37 - INFO - __main__ - Sample 25 of the training set: {'sentence1': 'The two young leaders of the coup, Pibul Songgram and Pridi Phanomyang, both educated in Europe and influenced by Western ideas, came to dominate Thai politics in the ensuing years.', 'sentence2': 'Pibul was a young leader.', 'label': 0, 'idx': 25, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 1023, 4123, 20251, 310, 278, 17103, 29892, 349, 747, 352, 9362, 1393, 322, 1588, 8819, 1963, 273, 16103, 574, 29892, 1716, 27729, 297, 4092, 322, 28482, 491, 10504, 7014, 29892, 2996, 304, 8022, 403, 498, 1794, 22661, 297, 278, 427, 2146, 292, 2440, 29889, 1, 349, 747, 352, 471, 263, 4123, 11822, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:44:37 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:44:38,929 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:44:39,261 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:44:39,261 >>   Num examples = 1,000
[INFO|trainer.py:1749] 2024-02-18 21:44:39,261 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:44:39,262 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:44:39,262 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:44:39,262 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:44:39,262 >>   Total optimization steps = 375
[INFO|trainer.py:1756] 2024-02-18 21:44:39,263 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/375 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/375 [00:02<13:22,  2.14s/it]  1%|          | 2/375 [00:03<10:13,  1.65s/it]  1%|          | 3/375 [00:04<09:26,  1.52s/it]  1%|          | 4/375 [00:06<09:04,  1.47s/it]  1%|▏         | 5/375 [00:07<08:51,  1.44s/it]  2%|▏         | 6/375 [00:08<08:43,  1.42s/it]  2%|▏         | 7/375 [00:10<08:37,  1.41s/it]  2%|▏         | 8/375 [00:11<08:32,  1.40s/it]  2%|▏         | 9/375 [00:13<08:29,  1.39s/it]  3%|▎         | 10/375 [00:14<08:26,  1.39s/it]  3%|▎         | 11/375 [00:15<08:24,  1.39s/it]  3%|▎         | 12/375 [00:17<08:23,  1.39s/it]  3%|▎         | 13/375 [00:18<08:21,  1.38s/it]  4%|▎         | 14/375 [00:20<08:19,  1.38s/it]  4%|▍         | 15/375 [00:21<08:17,  1.38s/it]  4%|▍         | 16/375 [00:22<08:16,  1.38s/it]  5%|▍         | 17/375 [00:24<08:15,  1.38s/it]  5%|▍         | 18/375 [00:25<08:13,  1.38s/it]  5%|▌         | 19/375 [00:26<08:12,  1.38s/it]  5%|▌         | 20/375 [00:28<08:11,  1.38s/it]  6%|▌         | 21/375 [00:29<08:10,  1.38s/it]  6%|▌         | 22/375 [00:31<08:08,  1.38s/it]  6%|▌         | 23/375 [00:32<08:07,  1.38s/it]  6%|▋         | 24/375 [00:33<08:05,  1.38s/it]  7%|▋         | 25/375 [00:35<08:04,  1.38s/it]  7%|▋         | 26/375 [00:36<08:03,  1.38s/it]  7%|▋         | 27/375 [00:38<08:01,  1.38s/it]  7%|▋         | 28/375 [00:39<07:59,  1.38s/it]  8%|▊         | 29/375 [00:40<07:58,  1.38s/it]  8%|▊         | 30/375 [00:42<07:57,  1.38s/it]  8%|▊         | 31/375 [00:43<07:55,  1.38s/it]  9%|▊         | 32/375 [00:44<07:54,  1.38s/it]  9%|▉         | 33/375 [00:46<07:53,  1.38s/it]  9%|▉         | 34/375 [00:47<07:52,  1.38s/it]  9%|▉         | 35/375 [00:49<07:50,  1.38s/it] 10%|▉         | 36/375 [00:50<07:49,  1.38s/it] 10%|▉         | 37/375 [00:51<07:47,  1.38s/it] 10%|█         | 38/375 [00:53<07:46,  1.38s/it] 10%|█         | 39/375 [00:54<07:45,  1.38s/it] 11%|█         | 40/375 [00:55<07:43,  1.38s/it] 11%|█         | 41/375 [00:57<07:42,  1.38s/it] 11%|█         | 42/375 [00:58<07:40,  1.38s/it] 11%|█▏        | 43/375 [01:00<07:39,  1.38s/it] 12%|█▏        | 44/375 [01:01<07:37,  1.38s/it] 12%|█▏        | 45/375 [01:02<07:36,  1.38s/it] 12%|█▏        | 46/375 [01:04<07:35,  1.39s/it] 13%|█▎        | 47/375 [01:05<07:34,  1.39s/it] 13%|█▎        | 48/375 [01:07<07:33,  1.39s/it] 13%|█▎        | 49/375 [01:08<07:31,  1.39s/it] 13%|█▎        | 50/375 [01:09<07:30,  1.39s/it] 14%|█▎        | 51/375 [01:11<07:29,  1.39s/it] 14%|█▍        | 52/375 [01:12<07:27,  1.39s/it] 14%|█▍        | 53/375 [01:13<07:26,  1.39s/it] 14%|█▍        | 54/375 [01:15<07:24,  1.39s/it] 15%|█▍        | 55/375 [01:16<07:23,  1.39s/it] 15%|█▍        | 56/375 [01:18<07:22,  1.39s/it] 15%|█▌        | 57/375 [01:19<07:21,  1.39s/it] 15%|█▌        | 58/375 [01:20<07:19,  1.39s/it] 16%|█▌        | 59/375 [01:22<07:18,  1.39s/it] 16%|█▌        | 60/375 [01:23<07:17,  1.39s/it] 16%|█▋        | 61/375 [01:25<07:15,  1.39s/it] 17%|█▋        | 62/375 [01:26<07:13,  1.39s/it] 17%|█▋        | 63/375 [01:27<07:12,  1.39s/it] 17%|█▋        | 64/375 [01:29<07:11,  1.39s/it] 17%|█▋        | 65/375 [01:30<07:10,  1.39s/it] 18%|█▊        | 66/375 [01:32<07:08,  1.39s/it] 18%|█▊        | 67/375 [01:33<07:07,  1.39s/it] 18%|█▊        | 68/375 [01:34<07:06,  1.39s/it] 18%|█▊        | 69/375 [01:36<07:04,  1.39s/it] 19%|█▊        | 70/375 [01:37<07:03,  1.39s/it] 19%|█▉        | 71/375 [01:38<07:01,  1.39s/it] 19%|█▉        | 72/375 [01:40<07:00,  1.39s/it] 19%|█▉        | 73/375 [01:41<06:59,  1.39s/it] 20%|█▉        | 74/375 [01:43<06:57,  1.39s/it] 20%|██        | 75/375 [01:44<06:56,  1.39s/it] 20%|██        | 76/375 [01:45<06:55,  1.39s/it] 21%|██        | 77/375 [01:47<06:53,  1.39s/it] 21%|██        | 78/375 [01:48<06:52,  1.39s/it] 21%|██        | 79/375 [01:50<06:51,  1.39s/it] 21%|██▏       | 80/375 [01:51<06:49,  1.39s/it] 22%|██▏       | 81/375 [01:52<06:48,  1.39s/it] 22%|██▏       | 82/375 [01:54<06:47,  1.39s/it] 22%|██▏       | 83/375 [01:55<06:45,  1.39s/it] 22%|██▏       | 84/375 [01:57<06:44,  1.39s/it] 23%|██▎       | 85/375 [01:58<06:43,  1.39s/it] 23%|██▎       | 86/375 [01:59<06:41,  1.39s/it] 23%|██▎       | 87/375 [02:01<06:40,  1.39s/it] 23%|██▎       | 88/375 [02:02<06:38,  1.39s/it] 24%|██▎       | 89/375 [02:03<06:37,  1.39s/it] 24%|██▍       | 90/375 [02:05<06:35,  1.39s/it] 24%|██▍       | 91/375 [02:06<06:34,  1.39s/it] 25%|██▍       | 92/375 [02:08<06:33,  1.39s/it] 25%|██▍       | 93/375 [02:09<06:31,  1.39s/it] 25%|██▌       | 94/375 [02:10<06:30,  1.39s/it] 25%|██▌       | 95/375 [02:12<06:28,  1.39s/it] 26%|██▌       | 96/375 [02:13<06:27,  1.39s/it] 26%|██▌       | 97/375 [02:15<06:26,  1.39s/it] 26%|██▌       | 98/375 [02:16<06:25,  1.39s/it] 26%|██▋       | 99/375 [02:17<06:23,  1.39s/it] 27%|██▋       | 100/375 [02:19<06:22,  1.39s/it] 27%|██▋       | 101/375 [02:20<06:20,  1.39s/it] 27%|██▋       | 102/375 [02:22<06:18,  1.39s/it] 27%|██▋       | 103/375 [02:23<06:17,  1.39s/it] 28%|██▊       | 104/375 [02:24<06:16,  1.39s/it] 28%|██▊       | 105/375 [02:26<06:14,  1.39s/it] 28%|██▊       | 106/375 [02:27<06:13,  1.39s/it] 29%|██▊       | 107/375 [02:28<06:12,  1.39s/it] 29%|██▉       | 108/375 [02:30<06:10,  1.39s/it] 29%|██▉       | 109/375 [02:31<06:09,  1.39s/it] 29%|██▉       | 110/375 [02:33<06:08,  1.39s/it] 30%|██▉       | 111/375 [02:34<06:06,  1.39s/it] 30%|██▉       | 112/375 [02:35<06:05,  1.39s/it] 30%|███       | 113/375 [02:37<06:04,  1.39s/it] 30%|███       | 114/375 [02:38<06:02,  1.39s/it] 31%|███       | 115/375 [02:40<06:01,  1.39s/it] 31%|███       | 116/375 [02:41<06:00,  1.39s/it] 31%|███       | 117/375 [02:42<05:59,  1.39s/it] 31%|███▏      | 118/375 [02:44<05:57,  1.39s/it] 32%|███▏      | 119/375 [02:45<05:56,  1.39s/it] 32%|███▏      | 120/375 [02:47<05:55,  1.39s/it] 32%|███▏      | 121/375 [02:48<05:53,  1.39s/it] 33%|███▎      | 122/375 [02:49<05:51,  1.39s/it] 33%|███▎      | 123/375 [02:51<05:50,  1.39s/it] 33%|███▎      | 124/375 [02:52<05:49,  1.39s/it] 33%|███▎      | 125/375 [02:54<05:47,  1.39s/it] 34%|███▎      | 126/375 [02:55<05:46,  1.39s/it] 34%|███▍      | 127/375 [02:56<05:44,  1.39s/it] 34%|███▍      | 128/375 [02:58<05:43,  1.39s/it] 34%|███▍      | 129/375 [02:59<05:42,  1.39s/it] 35%|███▍      | 130/375 [03:00<05:40,  1.39s/it] 35%|███▍      | 131/375 [03:02<05:38,  1.39s/it] 35%|███▌      | 132/375 [03:03<05:37,  1.39s/it] 35%|███▌      | 133/375 [03:05<05:36,  1.39s/it] 36%|███▌      | 134/375 [03:06<05:34,  1.39s/it] 36%|███▌      | 135/375 [03:07<05:33,  1.39s/it] 36%|███▋      | 136/375 [03:09<05:32,  1.39s/it] 37%|███▋      | 137/375 [03:10<05:30,  1.39s/it] 37%|███▋      | 138/375 [03:12<05:29,  1.39s/it] 37%|███▋      | 139/375 [03:13<05:27,  1.39s/it] 37%|███▋      | 140/375 [03:14<05:26,  1.39s/it] 38%|███▊      | 141/375 [03:16<05:24,  1.39s/it] 38%|███▊      | 142/375 [03:17<05:23,  1.39s/it] 38%|███▊      | 143/375 [03:19<05:22,  1.39s/it] 38%|███▊      | 144/375 [03:20<05:21,  1.39s/it] 39%|███▊      | 145/375 [03:21<05:19,  1.39s/it] 39%|███▉      | 146/375 [03:23<05:18,  1.39s/it] 39%|███▉      | 147/375 [03:24<05:16,  1.39s/it] 39%|███▉      | 148/375 [03:25<05:14,  1.39s/it] 40%|███▉      | 149/375 [03:27<05:13,  1.39s/it] 40%|████      | 150/375 [03:28<05:13,  1.39s/it] 40%|████      | 151/375 [03:30<05:11,  1.39s/it] 41%|████      | 152/375 [03:31<05:10,  1.39s/it] 41%|████      | 153/375 [03:32<05:08,  1.39s/it] 41%|████      | 154/375 [03:34<05:07,  1.39s/it] 41%|████▏     | 155/375 [03:35<05:06,  1.39s/it] 42%|████▏     | 156/375 [03:37<05:04,  1.39s/it] 42%|████▏     | 157/375 [03:38<05:03,  1.39s/it] 42%|████▏     | 158/375 [03:39<05:02,  1.39s/it] 42%|████▏     | 159/375 [03:41<05:00,  1.39s/it] 43%|████▎     | 160/375 [03:42<04:59,  1.39s/it] 43%|████▎     | 161/375 [03:44<04:57,  1.39s/it] 43%|████▎     | 162/375 [03:45<04:56,  1.39s/it] 43%|████▎     | 163/375 [03:46<04:55,  1.39s/it] 44%|████▎     | 164/375 [03:48<04:53,  1.39s/it] 44%|████▍     | 165/375 [03:49<04:52,  1.39s/it] 44%|████▍     | 166/375 [03:51<04:51,  1.40s/it] 45%|████▍     | 167/375 [03:52<04:50,  1.40s/it] 45%|████▍     | 168/375 [03:53<04:48,  1.39s/it] 45%|████▌     | 169/375 [03:55<04:47,  1.39s/it] 45%|████▌     | 170/375 [03:56<04:45,  1.39s/it] 46%|████▌     | 171/375 [03:57<04:44,  1.39s/it] 46%|████▌     | 172/375 [03:59<04:42,  1.39s/it] 46%|████▌     | 173/375 [04:00<04:41,  1.39s/it] 46%|████▋     | 174/375 [04:02<04:39,  1.39s/it] 47%|████▋     | 175/375 [04:03<04:38,  1.39s/it] 47%|████▋     | 176/375 [04:04<04:37,  1.39s/it] 47%|████▋     | 177/375 [04:06<04:35,  1.39s/it] 47%|████▋     | 178/375 [04:07<04:34,  1.39s/it] 48%|████▊     | 179/375 [04:09<04:32,  1.39s/it] 48%|████▊     | 180/375 [04:10<04:31,  1.39s/it] 48%|████▊     | 181/375 [04:11<04:30,  1.39s/it] 49%|████▊     | 182/375 [04:13<04:28,  1.39s/it] 49%|████▉     | 183/375 [04:14<04:26,  1.39s/it] 49%|████▉     | 184/375 [04:16<04:26,  1.39s/it] 49%|████▉     | 185/375 [04:17<04:24,  1.39s/it] 50%|████▉     | 186/375 [04:18<04:23,  1.39s/it] 50%|████▉     | 187/375 [04:20<04:21,  1.39s/it] 50%|█████     | 188/375 [04:21<04:20,  1.39s/it] 50%|█████     | 189/375 [04:23<04:18,  1.39s/it] 51%|█████     | 190/375 [04:24<04:17,  1.39s/it] 51%|█████     | 191/375 [04:25<04:16,  1.39s/it] 51%|█████     | 192/375 [04:27<04:14,  1.39s/it] 51%|█████▏    | 193/375 [04:28<04:13,  1.39s/it] 52%|█████▏    | 194/375 [04:30<04:12,  1.39s/it] 52%|█████▏    | 195/375 [04:31<04:10,  1.39s/it] 52%|█████▏    | 196/375 [04:32<04:08,  1.39s/it] 53%|█████▎    | 197/375 [04:34<04:07,  1.39s/it] 53%|█████▎    | 198/375 [04:35<04:05,  1.39s/it] 53%|█████▎    | 199/375 [04:36<04:04,  1.39s/it] 53%|█████▎    | 200/375 [04:38<04:03,  1.39s/it] 54%|█████▎    | 201/375 [04:39<04:02,  1.39s/it] 54%|█████▍    | 202/375 [04:41<04:00,  1.39s/it] 54%|█████▍    | 203/375 [04:42<03:59,  1.39s/it] 54%|█████▍    | 204/375 [04:43<03:57,  1.39s/it] 55%|█████▍    | 205/375 [04:45<03:56,  1.39s/it] 55%|█████▍    | 206/375 [04:46<03:55,  1.39s/it] 55%|█████▌    | 207/375 [04:48<03:53,  1.39s/it] 55%|█████▌    | 208/375 [04:49<03:52,  1.39s/it] 56%|█████▌    | 209/375 [04:50<03:51,  1.39s/it] 56%|█████▌    | 210/375 [04:52<03:49,  1.39s/it] 56%|█████▋    | 211/375 [04:53<03:48,  1.39s/it] 57%|█████▋    | 212/375 [04:55<03:46,  1.39s/it] 57%|█████▋    | 213/375 [04:56<03:45,  1.39s/it] 57%|█████▋    | 214/375 [04:57<03:44,  1.39s/it] 57%|█████▋    | 215/375 [04:59<03:42,  1.39s/it] 58%|█████▊    | 216/375 [05:00<03:41,  1.39s/it] 58%|█████▊    | 217/375 [05:02<03:40,  1.39s/it] 58%|█████▊    | 218/375 [05:03<03:38,  1.39s/it] 58%|█████▊    | 219/375 [05:04<03:37,  1.39s/it] 59%|█████▊    | 220/375 [05:06<03:35,  1.39s/it] 59%|█████▉    | 221/375 [05:07<03:34,  1.39s/it] 59%|█████▉    | 222/375 [05:08<03:32,  1.39s/it] 59%|█████▉    | 223/375 [05:10<03:31,  1.39s/it] 60%|█████▉    | 224/375 [05:11<03:30,  1.39s/it] 60%|██████    | 225/375 [05:13<03:28,  1.39s/it] 60%|██████    | 226/375 [05:14<03:27,  1.39s/it] 61%|██████    | 227/375 [05:15<03:25,  1.39s/it] 61%|██████    | 228/375 [05:17<03:24,  1.39s/it] 61%|██████    | 229/375 [05:18<03:23,  1.39s/it] 61%|██████▏   | 230/375 [05:20<03:21,  1.39s/it] 62%|██████▏   | 231/375 [05:21<03:20,  1.39s/it] 62%|██████▏   | 232/375 [05:22<03:19,  1.39s/it] 62%|██████▏   | 233/375 [05:24<03:17,  1.39s/it] 62%|██████▏   | 234/375 [05:25<03:16,  1.39s/it] 63%|██████▎   | 235/375 [05:27<03:14,  1.39s/it] 63%|██████▎   | 236/375 [05:28<03:13,  1.39s/it] 63%|██████▎   | 237/375 [05:29<03:12,  1.39s/it] 63%|██████▎   | 238/375 [05:31<03:10,  1.39s/it] 64%|██████▎   | 239/375 [05:32<03:09,  1.39s/it] 64%|██████▍   | 240/375 [05:34<03:07,  1.39s/it] 64%|██████▍   | 241/375 [05:35<03:06,  1.39s/it] 65%|██████▍   | 242/375 [05:36<03:05,  1.39s/it] 65%|██████▍   | 243/375 [05:38<03:03,  1.39s/it] 65%|██████▌   | 244/375 [05:39<03:02,  1.39s/it] 65%|██████▌   | 245/375 [05:41<03:01,  1.39s/it] 66%|██████▌   | 246/375 [05:42<02:59,  1.39s/it] 66%|██████▌   | 247/375 [05:43<02:58,  1.39s/it] 66%|██████▌   | 248/375 [05:45<02:56,  1.39s/it] 66%|██████▋   | 249/375 [05:46<02:55,  1.39s/it] 67%|██████▋   | 250/375 [05:47<02:54,  1.39s/it] 67%|██████▋   | 251/375 [05:49<02:52,  1.39s/it] 67%|██████▋   | 252/375 [05:50<02:51,  1.39s/it] 67%|██████▋   | 253/375 [05:52<02:49,  1.39s/it] 68%|██████▊   | 254/375 [05:53<02:48,  1.39s/it] 68%|██████▊   | 255/375 [05:54<02:47,  1.39s/it] 68%|██████▊   | 256/375 [05:56<02:45,  1.39s/it] 69%|██████▊   | 257/375 [05:57<02:44,  1.40s/it] 69%|██████▉   | 258/375 [05:59<02:43,  1.39s/it] 69%|██████▉   | 259/375 [06:00<02:41,  1.39s/it] 69%|██████▉   | 260/375 [06:01<02:40,  1.39s/it] 70%|██████▉   | 261/375 [06:03<02:38,  1.39s/it] 70%|██████▉   | 262/375 [06:04<02:37,  1.39s/it] 70%|███████   | 263/375 [06:06<02:36,  1.39s/it] 70%|███████   | 264/375 [06:07<02:34,  1.39s/it] 71%|███████   | 265/375 [06:08<02:33,  1.39s/it] 71%|███████   | 266/375 [06:10<02:31,  1.39s/it] 71%|███████   | 267/375 [06:11<02:30,  1.39s/it] 71%|███████▏  | 268/375 [06:13<02:29,  1.39s/it] 72%|███████▏  | 269/375 [06:14<02:27,  1.40s/it] 72%|███████▏  | 270/375 [06:15<02:26,  1.39s/it] 72%|███████▏  | 271/375 [06:17<02:24,  1.39s/it] 73%|███████▎  | 272/375 [06:18<02:23,  1.39s/it] 73%|███████▎  | 273/375 [06:20<02:22,  1.39s/it] 73%|███████▎  | 274/375 [06:21<02:20,  1.39s/it] 73%|███████▎  | 275/375 [06:22<02:19,  1.39s/it] 74%|███████▎  | 276/375 [06:24<02:18,  1.39s/it] 74%|███████▍  | 277/375 [06:25<02:16,  1.39s/it] 74%|███████▍  | 278/375 [06:26<02:15,  1.39s/it] 74%|███████▍  | 279/375 [06:28<02:13,  1.39s/it] 75%|███████▍  | 280/375 [06:29<02:12,  1.39s/it] 75%|███████▍  | 281/375 [06:31<02:10,  1.39s/it] 75%|███████▌  | 282/375 [06:32<02:09,  1.39s/it] 75%|███████▌  | 283/375 [06:33<02:08,  1.39s/it] 76%|███████▌  | 284/375 [06:35<02:06,  1.39s/it] 76%|███████▌  | 285/375 [06:36<02:05,  1.39s/it] 76%|███████▋  | 286/375 [06:38<02:03,  1.39s/it] 77%|███████▋  | 287/375 [06:39<02:02,  1.39s/it] 77%|███████▋  | 288/375 [06:40<02:01,  1.39s/it] 77%|███████▋  | 289/375 [06:42<01:59,  1.39s/it] 77%|███████▋  | 290/375 [06:43<01:58,  1.39s/it] 78%|███████▊  | 291/375 [06:45<01:56,  1.39s/it] 78%|███████▊  | 292/375 [06:46<01:55,  1.39s/it] 78%|███████▊  | 293/375 [06:47<01:54,  1.39s/it] 78%|███████▊  | 294/375 [06:49<01:52,  1.39s/it] 79%|███████▊  | 295/375 [06:50<01:51,  1.39s/it] 79%|███████▉  | 296/375 [06:52<01:50,  1.39s/it] 79%|███████▉  | 297/375 [06:53<01:48,  1.39s/it] 79%|███████▉  | 298/375 [06:54<01:47,  1.39s/it] 80%|███████▉  | 299/375 [06:56<01:45,  1.39s/it] 80%|████████  | 300/375 [06:57<01:44,  1.39s/it] 80%|████████  | 301/375 [06:58<01:43,  1.39s/it] 81%|████████  | 302/375 [07:00<01:41,  1.39s/it] 81%|████████  | 303/375 [07:01<01:40,  1.39s/it] 81%|████████  | 304/375 [07:03<01:38,  1.39s/it] 81%|████████▏ | 305/375 [07:04<01:37,  1.39s/it] 82%|████████▏ | 306/375 [07:05<01:36,  1.39s/it] 82%|████████▏ | 307/375 [07:07<01:34,  1.39s/it] 82%|████████▏ | 308/375 [07:08<01:33,  1.39s/it] 82%|████████▏ | 309/375 [07:10<01:31,  1.39s/it] 83%|████████▎ | 310/375 [07:11<01:30,  1.40s/it] 83%|████████▎ | 311/375 [07:12<01:29,  1.39s/it] 83%|████████▎ | 312/375 [07:14<01:27,  1.39s/it] 83%|████████▎ | 313/375 [07:15<01:26,  1.39s/it] 84%|████████▎ | 314/375 [07:17<01:25,  1.39s/it] 84%|████████▍ | 315/375 [07:18<01:23,  1.39s/it] 84%|████████▍ | 316/375 [07:19<01:22,  1.39s/it] 85%|████████▍ | 317/375 [07:21<01:20,  1.39s/it] 85%|████████▍ | 318/375 [07:22<01:19,  1.40s/it] 85%|████████▌ | 319/375 [07:24<01:18,  1.39s/it] 85%|████████▌ | 320/375 [07:25<01:16,  1.39s/it] 86%|████████▌ | 321/375 [07:26<01:15,  1.39s/it] 86%|████████▌ | 322/375 [07:28<01:13,  1.39s/it] 86%|████████▌ | 323/375 [07:29<01:12,  1.39s/it] 86%|████████▋ | 324/375 [07:31<01:11,  1.39s/it] 87%|████████▋ | 325/375 [07:32<01:09,  1.39s/it] 87%|████████▋ | 326/375 [07:33<01:08,  1.39s/it] 87%|████████▋ | 327/375 [07:35<01:06,  1.39s/it] 87%|████████▋ | 328/375 [07:36<01:05,  1.39s/it] 88%|████████▊ | 329/375 [07:38<01:04,  1.39s/it] 88%|████████▊ | 330/375 [07:39<01:02,  1.39s/it] 88%|████████▊ | 331/375 [07:40<01:01,  1.39s/it] 89%|████████▊ | 332/375 [07:42<00:59,  1.39s/it] 89%|████████▉ | 333/375 [07:43<00:58,  1.39s/it] 89%|████████▉ | 334/375 [07:44<00:57,  1.39s/it] 89%|████████▉ | 335/375 [07:46<00:55,  1.39s/it] 90%|████████▉ | 336/375 [07:47<00:54,  1.39s/it] 90%|████████▉ | 337/375 [07:49<00:52,  1.39s/it] 90%|█████████ | 338/375 [07:50<00:51,  1.39s/it] 90%|█████████ | 339/375 [07:51<00:50,  1.39s/it] 91%|█████████ | 340/375 [07:53<00:48,  1.39s/it] 91%|█████████ | 341/375 [07:54<00:47,  1.39s/it] 91%|█████████ | 342/375 [07:56<00:45,  1.39s/it] 91%|█████████▏| 343/375 [07:57<00:44,  1.39s/it] 92%|█████████▏| 344/375 [07:58<00:43,  1.39s/it] 92%|█████████▏| 345/375 [08:00<00:41,  1.39s/it] 92%|█████████▏| 346/375 [08:01<00:40,  1.39s/it] 93%|█████████▎| 347/375 [08:03<00:38,  1.39s/it] 93%|█████████▎| 348/375 [08:04<00:37,  1.39s/it] 93%|█████████▎| 349/375 [08:05<00:36,  1.39s/it] 93%|█████████▎| 350/375 [08:07<00:34,  1.39s/it] 94%|█████████▎| 351/375 [08:08<00:33,  1.39s/it] 94%|█████████▍| 352/375 [08:10<00:31,  1.39s/it] 94%|█████████▍| 353/375 [08:11<00:30,  1.39s/it] 94%|█████████▍| 354/375 [08:12<00:29,  1.39s/it] 95%|█████████▍| 355/375 [08:14<00:27,  1.39s/it] 95%|█████████▍| 356/375 [08:15<00:26,  1.39s/it] 95%|█████████▌| 357/375 [08:16<00:25,  1.39s/it] 95%|█████████▌| 358/375 [08:18<00:23,  1.39s/it] 96%|█████████▌| 359/375 [08:19<00:22,  1.39s/it] 96%|█████████▌| 360/375 [08:21<00:20,  1.39s/it] 96%|█████████▋| 361/375 [08:22<00:19,  1.39s/it] 97%|█████████▋| 362/375 [08:23<00:18,  1.39s/it] 97%|█████████▋| 363/375 [08:25<00:16,  1.39s/it] 97%|█████████▋| 364/375 [08:26<00:15,  1.39s/it] 97%|█████████▋| 365/375 [08:28<00:13,  1.39s/it] 98%|█████████▊| 366/375 [08:29<00:12,  1.39s/it] 98%|█████████▊| 367/375 [08:30<00:11,  1.39s/it] 98%|█████████▊| 368/375 [08:32<00:09,  1.39s/it] 98%|█████████▊| 369/375 [08:33<00:08,  1.39s/it] 99%|█████████▊| 370/375 [08:35<00:06,  1.39s/it] 99%|█████████▉| 371/375 [08:36<00:05,  1.39s/it] 99%|█████████▉| 372/375 [08:37<00:04,  1.39s/it] 99%|█████████▉| 373/375 [08:39<00:02,  1.39s/it]100%|█████████▉| 374/375 [08:40<00:01,  1.39s/it]100%|██████████| 375/375 [08:42<00:00,  1.39s/it][INFO|trainer.py:1988] 2024-02-18 21:53:21,286 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 522.1551, 'train_samples_per_second': 5.745, 'train_steps_per_second': 0.718, 'train_loss': 0.7918170572916666, 'epoch': 3.0}
                                                 100%|██████████| 375/375 [08:42<00:00,  1.39s/it]100%|██████████| 375/375 [08:42<00:00,  1.39s/it]
[INFO|trainer.py:2985] 2024-02-18 21:53:21,421 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:53:21,424 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:53:38,217 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:53:38,219 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:53:38,221 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.7918
  train_runtime            = 0:08:42.15
  train_samples            =       1000
  train_samples_per_second =      5.745
  train_steps_per_second   =      0.718
02/18/2024 21:53:38 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:53:38,256 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:53:38,259 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:53:38,259 >>   Num examples = 277
[INFO|trainer.py:3296] 2024-02-18 21:53:38,259 >>   Batch size = 8
  0%|          | 0/35 [00:00<?, ?it/s]  6%|▌         | 2/35 [00:00<00:06,  4.72it/s]  9%|▊         | 3/35 [00:00<00:09,  3.34it/s] 11%|█▏        | 4/35 [00:01<00:10,  2.90it/s] 14%|█▍        | 5/35 [00:01<00:11,  2.69it/s] 17%|█▋        | 6/35 [00:02<00:11,  2.57it/s] 20%|██        | 7/35 [00:02<00:11,  2.50it/s] 23%|██▎       | 8/35 [00:02<00:10,  2.45it/s] 26%|██▌       | 9/35 [00:03<00:10,  2.42it/s] 29%|██▊       | 10/35 [00:03<00:10,  2.40it/s] 31%|███▏      | 11/35 [00:04<00:10,  2.39it/s] 34%|███▍      | 12/35 [00:04<00:09,  2.38it/s] 37%|███▋      | 13/35 [00:05<00:09,  2.38it/s] 40%|████      | 14/35 [00:05<00:08,  2.37it/s] 43%|████▎     | 15/35 [00:05<00:08,  2.37it/s] 46%|████▌     | 16/35 [00:06<00:08,  2.36it/s] 49%|████▊     | 17/35 [00:06<00:07,  2.36it/s] 51%|█████▏    | 18/35 [00:07<00:07,  2.36it/s] 54%|█████▍    | 19/35 [00:07<00:06,  2.36it/s] 57%|█████▋    | 20/35 [00:08<00:06,  2.35it/s] 60%|██████    | 21/35 [00:08<00:05,  2.35it/s] 63%|██████▎   | 22/35 [00:08<00:05,  2.35it/s] 66%|██████▌   | 23/35 [00:09<00:05,  2.36it/s] 69%|██████▊   | 24/35 [00:09<00:04,  2.36it/s] 71%|███████▏  | 25/35 [00:10<00:04,  2.36it/s] 74%|███████▍  | 26/35 [00:10<00:03,  2.36it/s] 77%|███████▋  | 27/35 [00:11<00:03,  2.36it/s] 80%|████████  | 28/35 [00:11<00:02,  2.36it/s] 83%|████████▎ | 29/35 [00:11<00:02,  2.36it/s] 86%|████████▌ | 30/35 [00:12<00:02,  2.36it/s] 89%|████████▊ | 31/35 [00:12<00:01,  2.36it/s] 91%|█████████▏| 32/35 [00:13<00:01,  2.36it/s] 94%|█████████▍| 33/35 [00:13<00:00,  2.36it/s] 97%|█████████▋| 34/35 [00:13<00:00,  2.36it/s]100%|██████████| 35/35 [00:14<00:00,  2.66it/s]100%|██████████| 35/35 [00:14<00:00,  2.45it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.4729
  eval_loss               =     0.7105
  eval_runtime            = 0:00:14.73
  eval_samples            =        277
  eval_samples_per_second =       18.8
  eval_steps_per_second   =      2.375
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 21:56:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 21:56:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/runs/Feb18_21-56-55_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 21:56:57 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:56:57 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 21:56:57 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 21:56:57 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 21:56:57,365 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 21:56:57,367 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "rte",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:56:57,401 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:56:57,401 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:56:57,401 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:56:57,401 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 21:56:57,401 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 21:56:57,459 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 21:56:57,519 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 21:57:00,605 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 21:57:00,606 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ed92b3eeb01c931f.arrow
02/18/2024 21:57:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ed92b3eeb01c931f.arrow
Running tokenizer on dataset:   0%|          | 0/277 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a194dbf2befaa5aa.arrow
02/18/2024 21:57:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a194dbf2befaa5aa.arrow
Running tokenizer on dataset: 100%|██████████| 277/277 [00:00<00:00, 2719.73 examples/s]
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c6c5e3c98889b02.arrow
02/18/2024 21:57:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7c6c5e3c98889b02.arrow
02/18/2024 21:57:00 - INFO - __main__ - Sample 81 of the training set: {'sentence1': "The plaintiffs in this most recent suit contend that Lee, Perlmutter, Arthur Lieberman and Avi Arad conspired in bad faith to conceal and misappropriate financial interests in Lee's creations assigned to Stan Lee Media in 1998. SLM's meltdown involved its former President Peter F. Paul fleeing to Brazil, contributions made to Bill and Hillary Clinton, Paul's extradition and more. In 2007, SLM filed a $5 billion lawsuit in which it claimed co-ownership of all of Stan Lee's creations for Marvel.", 'sentence2': "Hillary Clinton is Bill's sister.", 'label': 1, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 2174, 524, 28324, 297, 445, 1556, 7786, 14726, 640, 355, 393, 9371, 29892, 23907, 6149, 357, 29892, 11498, 7326, 495, 1171, 322, 319, 1403, 826, 328, 21588, 2859, 297, 4319, 10847, 304, 10628, 284, 322, 3984, 932, 6649, 403, 18161, 20017, 297, 9371, 29915, 29879, 907, 800, 9859, 304, 7813, 9371, 8213, 297, 29871, 29896, 29929, 29929, 29947, 29889, 317, 26369, 29915, 29879, 9232, 1594, 776, 9701, 967, 4642, 7178, 5310, 383, 29889, 3739, 9115, 22430, 304, 16078, 29892, 20706, 1754, 304, 6682, 322, 9143, 653, 2233, 16929, 29892, 3739, 29915, 29879, 17541, 328, 654, 322, 901, 29889, 512, 29871, 29906, 29900, 29900, 29955, 29892, 317, 26369, 934, 29881, 263, 395, 29945, 24464, 4307, 29658, 297, 607, 372, 17049, 1302, 29899, 776, 10475, 310, 599, 310, 7813, 9371, 29915, 29879, 907, 800, 363, 25791, 29889, 1, 9143, 653, 2233, 16929, 338, 6682, 29915, 29879, 9883, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:57:00 - INFO - __main__ - Sample 14 of the training set: {'sentence1': 'Brian Brohm, the Louisville quarterback, threw for 368 yards and five touchdowns as the Cardinals beat visiting Oregon State 63-27.', 'sentence2': 'The quarterback threw for 413 yards and three touchdowns, and then ran to the end zone two more times.', 'label': 1, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 15733, 4358, 7184, 29892, 278, 5899, 4909, 12616, 1627, 29892, 18318, 363, 29871, 29941, 29953, 29947, 17454, 322, 5320, 6023, 3204, 29879, 408, 278, 9160, 19016, 16646, 6493, 292, 25203, 4306, 29871, 29953, 29941, 29899, 29906, 29955, 29889, 1, 450, 12616, 1627, 18318, 363, 29871, 29946, 29896, 29941, 17454, 322, 2211, 6023, 3204, 29879, 29892, 322, 769, 6350, 304, 278, 1095, 10640, 1023, 901, 3064, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:57:00 - INFO - __main__ - Sample 3 of the training set: {'sentence1': 'Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.', 'sentence2': 'The previous name of Ho Chi Minh City was Saigon.', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 8660, 347, 26089, 713, 29892, 9087, 22760, 472, 1019, 19302, 983, 29892, 263, 16083, 2669, 5001, 393, 6911, 15075, 475, 278, 29871, 29906, 29899, 6360, 29899, 1025, 18444, 17778, 8907, 297, 8335, 18168, 3080, 29882, 4412, 313, 24784, 368, 5701, 335, 265, 511, 1497, 393, 577, 2215, 1048, 29871, 29896, 29892, 29945, 29900, 29900, 4344, 505, 4520, 14502, 29889, 1, 450, 3517, 1024, 310, 8335, 18168, 3080, 29882, 4412, 471, 5701, 335, 265, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 21:57:01 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 21:57:02,662 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 21:57:02,995 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 21:57:02,996 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 21:57:02,996 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 21:57:02,996 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 21:57:02,996 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 21:57:02,996 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 21:57:02,996 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 21:57:02,997 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:21,  2.14s/it]  5%|▌         | 2/39 [00:03<01:00,  1.64s/it]  8%|▊         | 3/39 [00:04<00:54,  1.52s/it] 10%|█         | 4/39 [00:06<00:51,  1.47s/it] 13%|█▎        | 5/39 [00:07<00:48,  1.43s/it] 15%|█▌        | 6/39 [00:08<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:44,  1.40s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.38s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.20s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.30s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:24<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:26,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.38s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.21s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.30s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.38s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:52<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.21s/it][INFO|trainer.py:1988] 2024-02-18 21:57:55,827 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.9617, 'train_samples_per_second': 5.664, 'train_steps_per_second': 0.736, 'train_loss': 0.5409269088353866, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.21s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 21:57:55,974 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 21:57:55,977 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 21:58:16,059 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 21:58:16,061 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 21:58:16,062 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/rte256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.5409
  train_runtime            = 0:00:52.96
  train_samples            =        100
  train_samples_per_second =      5.664
  train_steps_per_second   =      0.736
02/18/2024 21:58:16 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 21:58:16,110 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 21:58:16,112 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 21:58:16,112 >>   Num examples = 100
[INFO|trainer.py:3296] 2024-02-18 21:58:16,112 >>   Batch size = 8
  0%|          | 0/13 [00:00<?, ?it/s] 15%|█▌        | 2/13 [00:00<00:02,  4.62it/s] 23%|██▎       | 3/13 [00:00<00:03,  3.31it/s] 31%|███       | 4/13 [00:01<00:03,  2.89it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.68it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.57it/s] 54%|█████▍    | 7/13 [00:02<00:02,  2.50it/s] 62%|██████▏   | 8/13 [00:02<00:02,  2.46it/s] 69%|██████▉   | 9/13 [00:03<00:01,  2.43it/s] 77%|███████▋  | 10/13 [00:03<00:01,  2.41it/s] 85%|████████▍ | 11/13 [00:04<00:00,  2.39it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.38it/s]100%|██████████| 13/13 [00:04<00:00,  2.79it/s]100%|██████████| 13/13 [00:04<00:00,  2.66it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =       0.51
  eval_loss               =     1.6603
  eval_runtime            = 0:00:05.32
  eval_samples            =        100
  eval_samples_per_second =     18.764
  eval_steps_per_second   =      2.439
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/18/2024 22:02:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/18/2024 22:02:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1/runs/Feb18_22-02-14_v003.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/18/2024 22:02:15 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 22:02:15 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/18/2024 22:02:15 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/18/2024 22:02:15 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-18 22:02:15,853 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-18 22:02:15,855 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "wnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-18 22:02:16,314 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-18 22:02:16,314 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 22:02:16,314 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 22:02:16,314 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-18 22:02:16,314 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-18 22:02:16,375 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-18 22:02:16,433 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-18 22:02:19,532 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-18 22:02:19,532 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/635 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9adf066081c23c25.arrow
02/18/2024 22:02:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9adf066081c23c25.arrow
Running tokenizer on dataset: 100%|██████████| 635/635 [00:00<00:00, 4889.97 examples/s]Running tokenizer on dataset: 100%|██████████| 635/635 [00:00<00:00, 4672.14 examples/s]
Running tokenizer on dataset:   0%|          | 0/71 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1fa127e7bc81c97f.arrow
02/18/2024 22:02:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1fa127e7bc81c97f.arrow
Running tokenizer on dataset: 100%|██████████| 71/71 [00:00<00:00, 3099.33 examples/s]
Running tokenizer on dataset:   0%|          | 0/146 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-29ade6a6a5f0d79c.arrow
02/18/2024 22:02:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-29ade6a6a5f0d79c.arrow
Running tokenizer on dataset: 100%|██████████| 146/146 [00:00<00:00, 1113.69 examples/s]Running tokenizer on dataset: 100%|██████████| 146/146 [00:00<00:00, 1083.61 examples/s]
02/18/2024 22:02:20 - INFO - __main__ - Sample 81 of the training set: {'sentence1': 'The journalists interviewed the stars of the new movie. They were very cooperative, so the interview lasted for a long time.', 'sentence2': 'The journalists were very cooperative, so the interview lasted for a long time.', 'label': 0, 'idx': 81, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 8955, 2879, 15593, 287, 278, 10819, 310, 278, 716, 14064, 29889, 2688, 892, 1407, 1302, 3372, 1230, 29892, 577, 278, 15593, 1833, 287, 363, 263, 1472, 931, 29889, 1, 450, 8955, 2879, 892, 1407, 1302, 3372, 1230, 29892, 577, 278, 15593, 1833, 287, 363, 263, 1472, 931, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 22:02:20 - INFO - __main__ - Sample 14 of the training set: {'sentence1': 'Always before, Larry had helped Dad with his work. But he could not help him now, for Dad said that his boss at the railroad company would not want anyone but him to work in the office.', 'sentence2': 'Dad could not help him now.', 'label': 0, 'idx': 14, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 29849, 1434, 29892, 26977, 750, 9213, 360, 328, 411, 670, 664, 29889, 1205, 540, 1033, 451, 1371, 1075, 1286, 29892, 363, 360, 328, 1497, 393, 670, 289, 2209, 472, 278, 8367, 9972, 5001, 723, 451, 864, 5019, 541, 1075, 304, 664, 297, 278, 8034, 29889, 1, 360, 328, 1033, 451, 1371, 1075, 1286, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 22:02:20 - INFO - __main__ - Sample 3 of the training set: {'sentence1': "Steve follows Fred's example in everything. He influences him hugely.", 'sentence2': 'Steve influences him hugely.', 'label': 0, 'idx': 3, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 13981, 4477, 12001, 29915, 29879, 1342, 297, 4129, 29889, 940, 7112, 2063, 1075, 298, 688, 873, 29889, 1, 13981, 7112, 2063, 1075, 298, 688, 873, 29889], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/18/2024 22:02:20 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-18 22:02:21,827 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-18 22:02:22,148 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-18 22:02:22,148 >>   Num examples = 100
[INFO|trainer.py:1749] 2024-02-18 22:02:22,149 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-18 22:02:22,149 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-18 22:02:22,149 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-18 22:02:22,149 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-18 22:02:22,149 >>   Total optimization steps = 39
[INFO|trainer.py:1756] 2024-02-18 22:02:22,150 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/39 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  3%|▎         | 1/39 [00:02<01:25,  2.25s/it]  5%|▌         | 2/39 [00:03<01:02,  1.69s/it]  8%|▊         | 3/39 [00:04<00:55,  1.55s/it] 10%|█         | 4/39 [00:06<00:51,  1.48s/it] 13%|█▎        | 5/39 [00:07<00:49,  1.44s/it] 15%|█▌        | 6/39 [00:09<00:46,  1.42s/it] 18%|█▊        | 7/39 [00:10<00:45,  1.41s/it] 21%|██        | 8/39 [00:11<00:43,  1.40s/it] 23%|██▎       | 9/39 [00:13<00:41,  1.39s/it] 26%|██▌       | 10/39 [00:14<00:40,  1.39s/it] 28%|██▊       | 11/39 [00:15<00:38,  1.38s/it] 31%|███       | 12/39 [00:17<00:37,  1.38s/it] 33%|███▎      | 13/39 [00:18<00:31,  1.20s/it] 36%|███▌      | 14/39 [00:19<00:31,  1.26s/it] 38%|███▊      | 15/39 [00:20<00:31,  1.29s/it] 41%|████      | 16/39 [00:22<00:30,  1.32s/it] 44%|████▎     | 17/39 [00:23<00:29,  1.34s/it] 46%|████▌     | 18/39 [00:25<00:28,  1.35s/it] 49%|████▊     | 19/39 [00:26<00:27,  1.36s/it] 51%|█████▏    | 20/39 [00:27<00:25,  1.37s/it] 54%|█████▍    | 21/39 [00:29<00:24,  1.37s/it] 56%|█████▋    | 22/39 [00:30<00:23,  1.37s/it] 59%|█████▉    | 23/39 [00:31<00:22,  1.38s/it] 62%|██████▏   | 24/39 [00:33<00:20,  1.38s/it] 64%|██████▍   | 25/39 [00:34<00:19,  1.38s/it] 67%|██████▋   | 26/39 [00:35<00:15,  1.20s/it] 69%|██████▉   | 27/39 [00:36<00:15,  1.26s/it] 72%|███████▏  | 28/39 [00:38<00:14,  1.29s/it] 74%|███████▍  | 29/39 [00:39<00:13,  1.32s/it] 77%|███████▋  | 30/39 [00:40<00:12,  1.34s/it] 79%|███████▉  | 31/39 [00:42<00:10,  1.35s/it] 82%|████████▏ | 32/39 [00:43<00:09,  1.36s/it] 85%|████████▍ | 33/39 [00:45<00:08,  1.37s/it] 87%|████████▋ | 34/39 [00:46<00:06,  1.37s/it] 90%|████████▉ | 35/39 [00:47<00:05,  1.37s/it] 92%|█████████▏| 36/39 [00:49<00:04,  1.38s/it] 95%|█████████▍| 37/39 [00:50<00:02,  1.38s/it] 97%|█████████▋| 38/39 [00:52<00:01,  1.38s/it]100%|██████████| 39/39 [00:52<00:00,  1.20s/it][INFO|trainer.py:1988] 2024-02-18 22:03:14,988 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 52.9702, 'train_samples_per_second': 5.664, 'train_steps_per_second': 0.736, 'train_loss': 1.2020039680676582, 'epoch': 3.0}
                                               100%|██████████| 39/39 [00:52<00:00,  1.20s/it]100%|██████████| 39/39 [00:52<00:00,  1.36s/it]
[INFO|trainer.py:2985] 2024-02-18 22:03:15,123 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1
[INFO|configuration_utils.py:473] 2024-02-18 22:03:15,139 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-18 22:03:33,340 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-18 22:03:33,342 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-18 22:03:33,344 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/wnli256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =      1.202
  train_runtime            = 0:00:52.97
  train_samples            =        100
  train_samples_per_second =      5.664
  train_steps_per_second   =      0.736
02/18/2024 22:03:33 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-18 22:03:33,380 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-18 22:03:33,382 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-18 22:03:33,383 >>   Num examples = 71
[INFO|trainer.py:3296] 2024-02-18 22:03:33,383 >>   Batch size = 8
  0%|          | 0/9 [00:00<?, ?it/s] 22%|██▏       | 2/9 [00:00<00:01,  4.75it/s] 33%|███▎      | 3/9 [00:00<00:01,  3.35it/s] 44%|████▍     | 4/9 [00:01<00:01,  2.91it/s] 56%|█████▌    | 5/9 [00:01<00:01,  2.70it/s] 67%|██████▋   | 6/9 [00:02<00:01,  2.58it/s] 78%|███████▊  | 7/9 [00:02<00:00,  2.51it/s] 89%|████████▉ | 8/9 [00:02<00:00,  2.47it/s]100%|██████████| 9/9 [00:03<00:00,  2.56it/s]100%|██████████| 9/9 [00:03<00:00,  2.71it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.507
  eval_loss               =     0.7164
  eval_runtime            = 0:00:03.76
  eval_samples            =         71
  eval_samples_per_second =     18.839
  eval_steps_per_second   =      2.388
