WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 22:12:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 22:12:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/runs/Feb16_22-12-03_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
    main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
    cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
    self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Overwrite dataset info from restored data version if exists.
02/16/2024 22:12:05 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:12:05 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:12:05 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:12:05 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[2024-02-16 22:12:05,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 57915 closing signal SIGTERM
[2024-02-16 22:12:06,112] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 57916) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:12:05
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 57916)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
  File "run_glue.py", line 42
    Trainer,
    ^
SyntaxError: invalid syntax
  File "run_glue.py", line 42
    Trainer,
    ^
SyntaxError: invalid syntax
[2024-02-16 22:23:49,855] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 59223) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_22:23:49
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 59224)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:23:49
  host      : v014.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 59223)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue.py", line 33, in <module>
    from transformers import (
ImportError: cannot import name 'ParallelMode' from 'transformers' (/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/__init__.py)
Traceback (most recent call last):
  File "run_glue.py", line 33, in <module>
    from transformers import (
ImportError: cannot import name 'ParallelMode' from 'transformers' (/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/__init__.py)
[2024-02-16 22:24:27,584] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 59490) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_22:24:27
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 59491)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:24:27
  host      : v014.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 59490)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue.py", line 660, in <module>
    main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
    cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
    self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "run_glue.py", line 660, in <module>
    main()
  File "run_glue.py", line 251, in main
    training_args.parallel_mode = 'no' #To turn of distributed training
AttributeError: can't set attribute
[2024-02-16 22:27:23,145] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 60962) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-16_22:27:23
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 60963)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:27:23
  host      : v014.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 60962)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue.py", line 658, in <module>
    main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
    cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
    self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

02/16/2024 22:29:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/runs/Feb16_22-29-17_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/16/2024 22:29:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:29:18 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:29:18 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:29:18 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:29:19,193 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:29:19,196 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:29:19,257 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:29:19,257 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:29:19,257 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:29:19,258 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:29:19,258 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:29:19,375 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:29:19,940 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[2024-02-16 22:29:22,524] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 61664 closing signal SIGTERM
[2024-02-16 22:29:22,789] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 61665) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:29:22
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 61665)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "run_glue.py", line 658, in <module>
    main()
  File "run_glue.py", line 236, in main
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/hf_argparser.py", line 338, in parse_args_into_dataclasses
02/16/2024 22:30:43 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/runs/Feb16_22-30-43_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
    obj = dtype(**inputs)
  File "<string>", line 121, in __init__
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1495, in __post_init__
    and (self.device.type != "cuda")
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1939, in device
    return self._setup_devices
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/generic.py", line 56, in __get__
    cached = self.fget(obj)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/training_args.py", line 1875, in _setup_devices
    self.distributed_state = PartialState(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/state.py", line 236, in __init__
    torch.cuda.set_device(self.device)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/cuda/__init__.py", line 408, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Overwrite dataset info from restored data version if exists.
02/16/2024 22:30:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:30:45 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:30:45 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:30:45 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:30:45,262 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:30:45,264 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:30:45,300 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:30:45,300 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:30:45,301 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:30:45,301 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:30:45,301 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:30:45,362 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:30:45,418 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[2024-02-16 22:30:49,234] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 61955 closing signal SIGTERM
[2024-02-16 22:30:49,298] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 61956) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:30:49
  host      : v014.ib.bridges2.psc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 61956)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
  File "run_glue.py", line 269
    Log on each process the small summary:
        ^
SyntaxError: invalid syntax
[2024-02-16 22:33:27,792] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 62065) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:33:27
  host      : v014.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 62065)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 22:34:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 22:34:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/runs/Feb16_22-34-50_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/16/2024 22:34:52 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:34:52 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:34:52 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:34:52 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:34:52,522 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:34:52,524 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:34:52,556 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:34:52,557 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:34:52,557 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:34:52,557 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:34:52,557 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:34:52,616 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:34:53,109 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-16 22:35:33,834 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 22:35:33,834 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d6331b88904f300a.arrow
02/16/2024 22:35:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d6331b88904f300a.arrow
Running tokenizer on dataset:  27%|       | 1000/3668 [00:00<00:00, 4738.25 examples/s]Running tokenizer on dataset:  55%|    | 2000/3668 [00:00<00:00, 4863.86 examples/s]Running tokenizer on dataset:  82%| | 3000/3668 [00:00<00:00, 3681.09 examples/s]Running tokenizer on dataset: 100%|| 3668/3668 [00:00<00:00, 3847.04 examples/s]Running tokenizer on dataset: 100%|| 3668/3668 [00:00<00:00, 3968.50 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8e06d543cdb5435f.arrow
02/16/2024 22:35:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8e06d543cdb5435f.arrow
Running tokenizer on dataset: 100%|| 408/408 [00:00<00:00, 4593.24 examples/s]
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4410b96dc33b817a.arrow
02/16/2024 22:35:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4410b96dc33b817a.arrow
Running tokenizer on dataset:  58%|    | 1000/1725 [00:00<00:00, 4865.38 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 4840.19 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 4781.53 examples/s]
02/16/2024 22:35:35 - INFO - __main__ - Sample 654 of the training set: {'sentence1': 'The airline says only Robert Milton is taking a 15-per-cent reduction in pay .', 'sentence2': "The airline 's pilots , for example , are taking a 15-per-cent cut .", 'label': 0, 'idx': 736, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 4799, 1220, 4083, 871, 4755, 3833, 880, 338, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 20376, 297, 5146, 869, 1, 450, 4799, 1220, 525, 29879, 8230, 1862, 1919, 363, 1342, 1919, 526, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 5700, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:35:35 - INFO - __main__ - Sample 114 of the training set: {'sentence1': 'Police warned residents on Friday not to travel alone and to avoid convenience stores and service stations stores at night .', 'sentence2': 'Police advised residents Friday not to travel alone to convenience stores and to be watchful .', 'label': 0, 'idx': 131, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 18923, 1370, 9571, 24060, 373, 28728, 451, 304, 9850, 7432, 322, 304, 4772, 29703, 14422, 322, 2669, 16355, 14422, 472, 4646, 869, 1, 18923, 594, 11292, 24060, 28728, 451, 304, 9850, 7432, 304, 29703, 14422, 322, 304, 367, 6505, 1319, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:35:35 - INFO - __main__ - Sample 25 of the training set: {'sentence1': 'I wanted to bring the most beautiful people into the most beautiful building , he said Sunday inside the Grand Central concourse .', 'sentence2': '" I wanted to bring the most beautiful people into the most beautiful building , " Tunick said Sunday .', 'label': 1, 'idx': 28, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 540, 1497, 16340, 2768, 278, 6265, 8068, 3022, 10242, 869, 1, 376, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 376, 21072, 860, 1497, 16340, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:35:35 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 22:35:37,318 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 22:35:37,706 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 22:35:37,706 >>   Num examples = 1,000
[INFO|trainer.py:1749] 2024-02-16 22:35:37,706 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 22:35:37,706 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 22:35:37,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-16 22:35:37,707 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 22:35:37,707 >>   Total optimization steps = 375
[INFO|trainer.py:1756] 2024-02-16 22:35:37,708 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/375 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/375 [00:01<10:43,  1.72s/it]  1%|          | 2/375 [00:02<05:55,  1.05it/s]  1%|          | 3/375 [00:02<04:37,  1.34it/s]  1%|          | 4/375 [00:03<04:01,  1.54it/s]  1%|         | 5/375 [00:03<03:40,  1.68it/s]  2%|         | 6/375 [00:04<03:28,  1.77it/s]  2%|         | 7/375 [00:04<03:19,  1.84it/s]  2%|         | 8/375 [00:05<03:14,  1.89it/s]  2%|         | 9/375 [00:05<03:10,  1.92it/s]  3%|         | 10/375 [00:06<03:08,  1.94it/s]  3%|         | 11/375 [00:06<03:05,  1.96it/s]  3%|         | 12/375 [00:07<03:04,  1.97it/s]  3%|         | 13/375 [00:07<03:03,  1.97it/s]  4%|         | 14/375 [00:08<03:02,  1.98it/s]  4%|         | 15/375 [00:08<03:01,  1.98it/s]  4%|         | 16/375 [00:09<03:00,  1.99it/s]  5%|         | 17/375 [00:09<02:59,  1.99it/s]  5%|         | 18/375 [00:10<02:59,  1.99it/s]  5%|         | 19/375 [00:10<02:58,  1.99it/s]  5%|         | 20/375 [00:11<02:58,  1.99it/s]  6%|         | 21/375 [00:11<02:57,  1.99it/s]  6%|         | 22/375 [00:12<02:57,  1.99it/s]  6%|         | 23/375 [00:12<02:56,  1.99it/s]  6%|         | 24/375 [00:13<02:56,  1.99it/s]  7%|         | 25/375 [00:13<02:55,  1.99it/s]  7%|         | 26/375 [00:14<02:55,  1.99it/s]  7%|         | 27/375 [00:14<02:54,  1.99it/s]  7%|         | 28/375 [00:15<02:54,  1.99it/s]  8%|         | 29/375 [00:15<02:53,  1.99it/s]  8%|         | 30/375 [00:16<02:53,  1.99it/s]  8%|         | 31/375 [00:16<02:52,  1.99it/s]  9%|         | 32/375 [00:17<02:52,  1.99it/s]  9%|         | 33/375 [00:17<02:51,  1.99it/s]  9%|         | 34/375 [00:18<02:51,  1.99it/s]  9%|         | 35/375 [00:18<02:51,  1.99it/s] 10%|         | 36/375 [00:19<02:50,  1.99it/s] 10%|         | 37/375 [00:19<02:50,  1.99it/s] 10%|         | 38/375 [00:20<02:49,  1.99it/s] 10%|         | 39/375 [00:20<02:49,  1.99it/s] 11%|         | 40/375 [00:21<02:48,  1.99it/s] 11%|         | 41/375 [00:21<02:48,  1.99it/s] 11%|         | 42/375 [00:22<02:47,  1.99it/s] 11%|        | 43/375 [00:22<02:46,  1.99it/s] 12%|        | 44/375 [00:23<02:46,  1.99it/s] 12%|        | 45/375 [00:23<02:45,  1.99it/s] 12%|        | 46/375 [00:24<02:45,  1.99it/s] 13%|        | 47/375 [00:24<02:44,  1.99it/s] 13%|        | 48/375 [00:25<02:44,  1.99it/s] 13%|        | 49/375 [00:25<02:43,  1.99it/s] 13%|        | 50/375 [00:26<02:43,  1.99it/s] 14%|        | 51/375 [00:26<02:42,  1.99it/s] 14%|        | 52/375 [00:27<02:42,  1.99it/s] 14%|        | 53/375 [00:27<02:41,  1.99it/s] 14%|        | 54/375 [00:28<02:41,  1.99it/s] 15%|        | 55/375 [00:28<02:40,  1.99it/s] 15%|        | 56/375 [00:29<02:40,  1.99it/s] 15%|        | 57/375 [00:29<02:39,  1.99it/s] 15%|        | 58/375 [00:30<02:39,  1.99it/s] 16%|        | 59/375 [00:30<02:38,  1.99it/s] 16%|        | 60/375 [00:31<02:38,  1.99it/s] 16%|        | 61/375 [00:31<02:37,  1.99it/s] 17%|        | 62/375 [00:32<02:37,  1.99it/s] 17%|        | 63/375 [00:32<02:36,  1.99it/s] 17%|        | 64/375 [00:33<02:36,  1.99it/s] 17%|        | 65/375 [00:33<02:35,  1.99it/s] 18%|        | 66/375 [00:34<02:35,  1.99it/s] 18%|        | 67/375 [00:34<02:34,  1.99it/s] 18%|        | 68/375 [00:35<02:34,  1.99it/s] 18%|        | 69/375 [00:35<02:33,  1.99it/s] 19%|        | 70/375 [00:36<02:33,  1.99it/s] 19%|        | 71/375 [00:36<02:32,  1.99it/s] 19%|        | 72/375 [00:37<02:32,  1.99it/s] 19%|        | 73/375 [00:37<02:31,  1.99it/s] 20%|        | 74/375 [00:38<02:31,  1.99it/s] 20%|        | 75/375 [00:38<02:30,  1.99it/s] 20%|        | 76/375 [00:39<02:30,  1.99it/s] 21%|        | 77/375 [00:39<02:29,  1.99it/s] 21%|        | 78/375 [00:40<02:29,  1.99it/s] 21%|        | 79/375 [00:40<02:28,  1.99it/s] 21%|       | 80/375 [00:41<02:28,  1.99it/s] 22%|       | 81/375 [00:41<02:27,  1.99it/s] 22%|       | 82/375 [00:42<02:27,  1.99it/s] 22%|       | 83/375 [00:42<02:26,  1.99it/s] 22%|       | 84/375 [00:43<02:26,  1.99it/s] 23%|       | 85/375 [00:43<02:25,  1.99it/s] 23%|       | 86/375 [00:44<02:25,  1.99it/s] 23%|       | 87/375 [00:44<02:24,  1.99it/s] 23%|       | 88/375 [00:45<02:24,  1.99it/s] 24%|       | 89/375 [00:45<02:23,  1.99it/s] 24%|       | 90/375 [00:46<02:23,  1.99it/s] 24%|       | 91/375 [00:46<02:22,  1.99it/s] 25%|       | 92/375 [00:47<02:22,  1.99it/s] 25%|       | 93/375 [00:47<02:21,  1.99it/s] 25%|       | 94/375 [00:48<02:21,  1.99it/s] 25%|       | 95/375 [00:48<02:21,  1.99it/s] 26%|       | 96/375 [00:49<02:20,  1.99it/s] 26%|       | 97/375 [00:49<02:19,  1.99it/s] 26%|       | 98/375 [00:50<02:19,  1.99it/s] 26%|       | 99/375 [00:50<02:18,  1.99it/s] 27%|       | 100/375 [00:51<02:18,  1.99it/s] 27%|       | 101/375 [00:51<02:17,  1.99it/s] 27%|       | 102/375 [00:52<02:17,  1.99it/s] 27%|       | 103/375 [00:52<02:16,  1.99it/s] 28%|       | 104/375 [00:53<02:16,  1.99it/s] 28%|       | 105/375 [00:53<02:15,  1.99it/s] 28%|       | 106/375 [00:54<02:15,  1.99it/s] 29%|       | 107/375 [00:54<02:14,  1.99it/s] 29%|       | 108/375 [00:55<02:14,  1.99it/s] 29%|       | 109/375 [00:55<02:13,  1.99it/s] 29%|       | 110/375 [00:56<02:13,  1.99it/s] 30%|       | 111/375 [00:56<02:12,  1.99it/s] 30%|       | 112/375 [00:57<02:12,  1.99it/s] 30%|       | 113/375 [00:57<02:11,  1.99it/s] 30%|       | 114/375 [00:58<02:11,  1.99it/s] 31%|       | 115/375 [00:58<02:10,  1.99it/s] 31%|       | 116/375 [00:59<02:10,  1.99it/s] 31%|       | 117/375 [00:59<02:09,  1.99it/s] 31%|      | 118/375 [01:00<02:08,  1.99it/s] 32%|      | 119/375 [01:00<02:08,  1.99it/s] 32%|      | 120/375 [01:01<02:08,  1.99it/s] 32%|      | 121/375 [01:01<02:07,  1.99it/s] 33%|      | 122/375 [01:02<02:07,  1.99it/s] 33%|      | 123/375 [01:02<02:06,  1.99it/s] 33%|      | 124/375 [01:03<02:06,  1.99it/s] 33%|      | 125/375 [01:03<02:05,  1.99it/s] 34%|      | 126/375 [01:04<02:04,  1.99it/s] 34%|      | 127/375 [01:04<02:04,  1.99it/s] 34%|      | 128/375 [01:05<02:03,  1.99it/s] 34%|      | 129/375 [01:05<02:03,  1.99it/s] 35%|      | 130/375 [01:06<02:03,  1.99it/s] 35%|      | 131/375 [01:06<02:02,  1.99it/s] 35%|      | 132/375 [01:07<02:01,  1.99it/s] 35%|      | 133/375 [01:07<02:01,  1.99it/s] 36%|      | 134/375 [01:08<02:00,  1.99it/s] 36%|      | 135/375 [01:08<02:00,  1.99it/s] 36%|      | 136/375 [01:09<02:00,  1.99it/s] 37%|      | 137/375 [01:09<01:59,  1.99it/s] 37%|      | 138/375 [01:10<01:59,  1.99it/s] 37%|      | 139/375 [01:10<01:58,  1.99it/s] 37%|      | 140/375 [01:11<01:58,  1.99it/s] 38%|      | 141/375 [01:11<01:57,  1.99it/s] 38%|      | 142/375 [01:12<01:57,  1.99it/s] 38%|      | 143/375 [01:12<01:56,  1.99it/s] 38%|      | 144/375 [01:13<01:56,  1.99it/s] 39%|      | 145/375 [01:13<01:55,  1.99it/s] 39%|      | 146/375 [01:14<01:55,  1.99it/s] 39%|      | 147/375 [01:14<01:54,  1.99it/s] 39%|      | 148/375 [01:15<01:54,  1.99it/s] 40%|      | 149/375 [01:15<01:53,  1.99it/s] 40%|      | 150/375 [01:16<01:53,  1.99it/s] 40%|      | 151/375 [01:16<01:52,  1.99it/s] 41%|      | 152/375 [01:17<01:52,  1.99it/s] 41%|      | 153/375 [01:18<01:51,  1.99it/s] 41%|      | 154/375 [01:18<01:51,  1.99it/s] 41%|     | 155/375 [01:19<01:50,  1.99it/s] 42%|     | 156/375 [01:19<01:50,  1.99it/s] 42%|     | 157/375 [01:20<01:49,  1.99it/s] 42%|     | 158/375 [01:20<01:49,  1.99it/s] 42%|     | 159/375 [01:21<01:48,  1.99it/s] 43%|     | 160/375 [01:21<01:47,  1.99it/s] 43%|     | 161/375 [01:22<01:47,  1.99it/s] 43%|     | 162/375 [01:22<01:46,  1.99it/s] 43%|     | 163/375 [01:23<01:46,  1.99it/s] 44%|     | 164/375 [01:23<01:45,  1.99it/s] 44%|     | 165/375 [01:24<01:45,  1.99it/s] 44%|     | 166/375 [01:24<01:44,  1.99it/s] 45%|     | 167/375 [01:25<01:44,  1.99it/s] 45%|     | 168/375 [01:25<01:43,  1.99it/s] 45%|     | 169/375 [01:26<01:43,  2.00it/s] 45%|     | 170/375 [01:26<01:42,  1.99it/s] 46%|     | 171/375 [01:27<01:42,  1.99it/s] 46%|     | 172/375 [01:27<01:41,  1.99it/s] 46%|     | 173/375 [01:28<01:41,  1.99it/s] 46%|     | 174/375 [01:28<01:40,  2.00it/s] 47%|     | 175/375 [01:29<01:40,  2.00it/s] 47%|     | 176/375 [01:29<01:39,  1.99it/s] 47%|     | 177/375 [01:30<01:39,  2.00it/s] 47%|     | 178/375 [01:30<01:38,  2.00it/s] 48%|     | 179/375 [01:31<01:38,  2.00it/s] 48%|     | 180/375 [01:31<01:37,  1.99it/s] 48%|     | 181/375 [01:32<01:37,  2.00it/s] 49%|     | 182/375 [01:32<01:36,  1.99it/s] 49%|     | 183/375 [01:33<01:36,  1.99it/s] 49%|     | 184/375 [01:33<01:35,  1.99it/s] 49%|     | 185/375 [01:34<01:35,  1.99it/s] 50%|     | 186/375 [01:34<01:34,  1.99it/s] 50%|     | 187/375 [01:35<01:34,  2.00it/s] 50%|     | 188/375 [01:35<01:33,  1.99it/s] 50%|     | 189/375 [01:36<01:33,  2.00it/s] 51%|     | 190/375 [01:36<01:32,  2.00it/s] 51%|     | 191/375 [01:37<01:32,  2.00it/s] 51%|     | 192/375 [01:37<01:31,  2.00it/s] 51%|    | 193/375 [01:38<01:31,  2.00it/s] 52%|    | 194/375 [01:38<01:30,  2.00it/s] 52%|    | 195/375 [01:39<01:30,  1.99it/s] 52%|    | 196/375 [01:39<01:29,  1.99it/s] 53%|    | 197/375 [01:40<01:29,  1.99it/s] 53%|    | 198/375 [01:40<01:28,  1.99it/s] 53%|    | 199/375 [01:41<01:28,  1.99it/s] 53%|    | 200/375 [01:41<01:27,  1.99it/s] 54%|    | 201/375 [01:42<01:27,  1.99it/s] 54%|    | 202/375 [01:42<01:26,  1.99it/s] 54%|    | 203/375 [01:43<01:26,  1.99it/s] 54%|    | 204/375 [01:43<01:26,  1.99it/s] 55%|    | 205/375 [01:44<01:25,  1.99it/s] 55%|    | 206/375 [01:44<01:25,  1.98it/s] 55%|    | 207/375 [01:45<01:24,  1.98it/s] 55%|    | 208/375 [01:45<01:24,  1.98it/s] 56%|    | 209/375 [01:46<01:23,  1.99it/s] 56%|    | 210/375 [01:46<01:23,  1.99it/s] 56%|    | 211/375 [01:47<01:22,  1.99it/s] 57%|    | 212/375 [01:47<01:22,  1.98it/s] 57%|    | 213/375 [01:48<01:21,  1.98it/s] 57%|    | 214/375 [01:48<01:21,  1.99it/s] 57%|    | 215/375 [01:49<01:20,  1.99it/s] 58%|    | 216/375 [01:49<01:20,  1.99it/s] 58%|    | 217/375 [01:50<01:19,  1.99it/s] 58%|    | 218/375 [01:50<01:18,  1.99it/s] 58%|    | 219/375 [01:51<01:18,  1.99it/s] 59%|    | 220/375 [01:51<01:17,  1.99it/s] 59%|    | 221/375 [01:52<01:17,  1.99it/s] 59%|    | 222/375 [01:52<01:16,  1.99it/s] 59%|    | 223/375 [01:53<01:16,  1.99it/s] 60%|    | 224/375 [01:53<01:15,  1.99it/s] 60%|    | 225/375 [01:54<01:15,  1.99it/s] 60%|    | 226/375 [01:54<01:14,  1.99it/s] 61%|    | 227/375 [01:55<01:14,  1.99it/s] 61%|    | 228/375 [01:55<01:13,  1.99it/s] 61%|    | 229/375 [01:56<01:13,  1.99it/s] 61%|   | 230/375 [01:56<01:12,  1.99it/s] 62%|   | 231/375 [01:57<01:12,  1.99it/s] 62%|   | 232/375 [01:57<01:11,  1.99it/s] 62%|   | 233/375 [01:58<01:11,  1.99it/s] 62%|   | 234/375 [01:58<01:10,  1.99it/s] 63%|   | 235/375 [01:59<01:10,  1.99it/s] 63%|   | 236/375 [01:59<01:09,  1.99it/s] 63%|   | 237/375 [02:00<01:09,  1.99it/s] 63%|   | 238/375 [02:00<01:08,  1.99it/s] 64%|   | 239/375 [02:01<01:08,  1.99it/s] 64%|   | 240/375 [02:01<01:07,  1.99it/s] 64%|   | 241/375 [02:02<01:07,  1.99it/s] 65%|   | 242/375 [02:02<01:06,  1.99it/s] 65%|   | 243/375 [02:03<01:06,  1.99it/s] 65%|   | 244/375 [02:03<01:05,  1.99it/s] 65%|   | 245/375 [02:04<01:05,  1.99it/s] 66%|   | 246/375 [02:04<01:04,  1.99it/s] 66%|   | 247/375 [02:05<01:04,  1.99it/s] 66%|   | 248/375 [02:05<01:03,  1.99it/s] 66%|   | 249/375 [02:06<01:03,  1.99it/s] 67%|   | 250/375 [02:06<01:02,  1.99it/s] 67%|   | 251/375 [02:07<01:02,  1.99it/s] 67%|   | 252/375 [02:07<01:01,  1.99it/s] 67%|   | 253/375 [02:08<01:01,  1.99it/s] 68%|   | 254/375 [02:08<01:00,  1.99it/s] 68%|   | 255/375 [02:09<01:00,  1.99it/s] 68%|   | 256/375 [02:09<00:59,  1.99it/s] 69%|   | 257/375 [02:10<00:59,  1.99it/s] 69%|   | 258/375 [02:10<00:58,  1.99it/s] 69%|   | 259/375 [02:11<00:58,  1.99it/s] 69%|   | 260/375 [02:11<00:57,  1.99it/s] 70%|   | 261/375 [02:12<00:57,  1.99it/s] 70%|   | 262/375 [02:12<00:56,  1.99it/s] 70%|   | 263/375 [02:13<00:56,  1.99it/s] 70%|   | 264/375 [02:13<00:55,  1.99it/s] 71%|   | 265/375 [02:14<00:55,  1.99it/s] 71%|   | 266/375 [02:14<00:54,  1.99it/s] 71%|   | 267/375 [02:15<00:54,  1.99it/s] 71%|  | 268/375 [02:15<00:53,  1.99it/s] 72%|  | 269/375 [02:16<00:53,  1.99it/s] 72%|  | 270/375 [02:16<00:52,  1.99it/s] 72%|  | 271/375 [02:17<00:52,  1.99it/s] 73%|  | 272/375 [02:17<00:51,  1.99it/s] 73%|  | 273/375 [02:18<00:51,  1.99it/s] 73%|  | 274/375 [02:18<00:50,  1.99it/s] 73%|  | 275/375 [02:19<00:50,  1.99it/s] 74%|  | 276/375 [02:19<00:49,  1.99it/s] 74%|  | 277/375 [02:20<00:49,  1.99it/s] 74%|  | 278/375 [02:20<00:48,  1.99it/s] 74%|  | 279/375 [02:21<00:48,  1.99it/s] 75%|  | 280/375 [02:21<00:47,  1.99it/s] 75%|  | 281/375 [02:22<00:47,  1.99it/s] 75%|  | 282/375 [02:22<00:46,  1.99it/s] 75%|  | 283/375 [02:23<00:46,  1.99it/s] 76%|  | 284/375 [02:23<00:45,  1.99it/s] 76%|  | 285/375 [02:24<00:45,  1.99it/s] 76%|  | 286/375 [02:24<00:44,  1.99it/s] 77%|  | 287/375 [02:25<00:44,  1.99it/s] 77%|  | 288/375 [02:25<00:43,  1.99it/s] 77%|  | 289/375 [02:26<00:43,  1.99it/s] 77%|  | 290/375 [02:26<00:42,  1.98it/s] 78%|  | 291/375 [02:27<00:42,  1.98it/s] 78%|  | 292/375 [02:27<00:41,  1.98it/s] 78%|  | 293/375 [02:28<00:41,  1.99it/s] 78%|  | 294/375 [02:28<00:40,  1.99it/s] 79%|  | 295/375 [02:29<00:40,  1.99it/s] 79%|  | 296/375 [02:29<00:39,  1.99it/s] 79%|  | 297/375 [02:30<00:39,  1.99it/s] 79%|  | 298/375 [02:30<00:38,  1.98it/s] 80%|  | 299/375 [02:31<00:38,  1.98it/s] 80%|  | 300/375 [02:31<00:37,  1.98it/s] 80%|  | 301/375 [02:32<00:37,  1.98it/s] 81%|  | 302/375 [02:32<00:36,  1.98it/s] 81%|  | 303/375 [02:33<00:36,  1.98it/s] 81%|  | 304/375 [02:33<00:35,  1.98it/s] 81%| | 305/375 [02:34<00:35,  1.99it/s] 82%| | 306/375 [02:34<00:34,  1.99it/s] 82%| | 307/375 [02:35<00:34,  1.99it/s] 82%| | 308/375 [02:35<00:33,  1.99it/s] 82%| | 309/375 [02:36<00:33,  1.99it/s] 83%| | 310/375 [02:36<00:32,  1.99it/s] 83%| | 311/375 [02:37<00:32,  1.99it/s] 83%| | 312/375 [02:37<00:31,  1.99it/s] 83%| | 313/375 [02:38<00:31,  1.99it/s] 84%| | 314/375 [02:38<00:30,  1.99it/s] 84%| | 315/375 [02:39<00:30,  1.99it/s] 84%| | 316/375 [02:39<00:29,  1.99it/s] 85%| | 317/375 [02:40<00:29,  1.99it/s] 85%| | 318/375 [02:40<00:28,  1.99it/s] 85%| | 319/375 [02:41<00:28,  1.99it/s] 85%| | 320/375 [02:41<00:27,  1.98it/s] 86%| | 321/375 [02:42<00:27,  1.99it/s] 86%| | 322/375 [02:42<00:26,  1.99it/s] 86%| | 323/375 [02:43<00:26,  1.99it/s] 86%| | 324/375 [02:43<00:25,  1.98it/s] 87%| | 325/375 [02:44<00:25,  1.98it/s] 87%| | 326/375 [02:44<00:24,  1.98it/s] 87%| | 327/375 [02:45<00:24,  1.98it/s] 87%| | 328/375 [02:45<00:23,  1.98it/s] 88%| | 329/375 [02:46<00:23,  1.99it/s] 88%| | 330/375 [02:46<00:22,  1.99it/s] 88%| | 331/375 [02:47<00:22,  1.99it/s] 89%| | 332/375 [02:47<00:21,  1.99it/s] 89%| | 333/375 [02:48<00:21,  1.98it/s] 89%| | 334/375 [02:48<00:20,  1.99it/s] 89%| | 335/375 [02:49<00:20,  1.99it/s] 90%| | 336/375 [02:50<00:19,  1.99it/s] 90%| | 337/375 [02:50<00:19,  1.98it/s] 90%| | 338/375 [02:51<00:18,  1.98it/s] 90%| | 339/375 [02:51<00:18,  1.98it/s] 91%| | 340/375 [02:52<00:17,  1.98it/s] 91%| | 341/375 [02:52<00:17,  1.98it/s] 91%| | 342/375 [02:53<00:16,  1.99it/s] 91%|| 343/375 [02:53<00:16,  1.98it/s] 92%|| 344/375 [02:54<00:15,  1.98it/s] 92%|| 345/375 [02:54<00:15,  1.98it/s] 92%|| 346/375 [02:55<00:14,  1.98it/s] 93%|| 347/375 [02:55<00:14,  1.99it/s] 93%|| 348/375 [02:56<00:13,  1.99it/s] 93%|| 349/375 [02:56<00:13,  1.99it/s] 93%|| 350/375 [02:57<00:12,  1.99it/s] 94%|| 351/375 [02:57<00:12,  1.99it/s] 94%|| 352/375 [02:58<00:11,  1.99it/s] 94%|| 353/375 [02:58<00:11,  1.99it/s] 94%|| 354/375 [02:59<00:10,  1.99it/s] 95%|| 355/375 [02:59<00:10,  1.98it/s] 95%|| 356/375 [03:00<00:09,  1.98it/s] 95%|| 357/375 [03:00<00:09,  1.99it/s] 95%|| 358/375 [03:01<00:08,  1.98it/s] 96%|| 359/375 [03:01<00:08,  1.98it/s] 96%|| 360/375 [03:02<00:07,  1.98it/s] 96%|| 361/375 [03:02<00:07,  1.98it/s] 97%|| 362/375 [03:03<00:06,  1.98it/s] 97%|| 363/375 [03:03<00:06,  1.98it/s] 97%|| 364/375 [03:04<00:05,  1.99it/s] 97%|| 365/375 [03:04<00:05,  1.99it/s] 98%|| 366/375 [03:05<00:04,  1.99it/s] 98%|| 367/375 [03:05<00:04,  1.99it/s] 98%|| 368/375 [03:06<00:03,  1.99it/s] 98%|| 369/375 [03:06<00:03,  1.99it/s] 99%|| 370/375 [03:07<00:02,  1.98it/s] 99%|| 371/375 [03:07<00:02,  1.99it/s] 99%|| 372/375 [03:08<00:01,  1.98it/s] 99%|| 373/375 [03:08<00:01,  1.98it/s]100%|| 374/375 [03:09<00:00,  1.98it/s]100%|| 375/375 [03:09<00:00,  1.98it/s][INFO|trainer.py:1988] 2024-02-16 22:38:47,369 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 189.7927, 'train_samples_per_second': 15.807, 'train_steps_per_second': 1.976, 'train_loss': 0.7848043619791667, 'epoch': 3.0}
                                                 100%|| 375/375 [03:09<00:00,  1.98it/s]100%|| 375/375 [03:09<00:00,  1.98it/s]
[INFO|trainer.py:2985] 2024-02-16 22:38:47,505 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1
[INFO|configuration_utils.py:473] 2024-02-16 22:38:47,507 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-16 22:39:07,837 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-16 22:39:07,839 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-16 22:39:07,840 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc64GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.7848
  train_runtime            = 0:03:09.79
  train_samples            =       1000
  train_samples_per_second =     15.807
  train_steps_per_second   =      1.976
02/16/2024 22:39:07 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-16 22:39:07,914 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-16 22:39:07,917 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-16 22:39:07,917 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-16 22:39:07,917 >>   Batch size = 8
  0%|          | 0/51 [00:00<?, ?it/s]  4%|         | 2/51 [00:00<00:03, 16.09it/s]  8%|         | 4/51 [00:00<00:04, 10.31it/s] 12%|        | 6/51 [00:00<00:04,  9.23it/s] 14%|        | 7/51 [00:00<00:04,  8.95it/s] 16%|        | 8/51 [00:00<00:04,  8.76it/s] 18%|        | 9/51 [00:00<00:04,  8.60it/s] 20%|        | 10/51 [00:01<00:04,  8.50it/s] 22%|       | 11/51 [00:01<00:04,  8.44it/s] 24%|       | 12/51 [00:01<00:04,  8.40it/s] 25%|       | 13/51 [00:01<00:04,  8.38it/s] 27%|       | 14/51 [00:01<00:04,  8.35it/s] 29%|       | 15/51 [00:01<00:04,  8.30it/s] 31%|      | 16/51 [00:01<00:04,  8.29it/s] 33%|      | 17/51 [00:01<00:04,  8.30it/s] 35%|      | 18/51 [00:02<00:03,  8.28it/s] 37%|      | 19/51 [00:02<00:03,  8.26it/s] 39%|      | 20/51 [00:02<00:03,  8.24it/s] 41%|      | 21/51 [00:02<00:03,  8.24it/s] 43%|     | 22/51 [00:02<00:03,  8.21it/s] 45%|     | 23/51 [00:02<00:03,  8.21it/s] 47%|     | 24/51 [00:02<00:03,  8.20it/s] 49%|     | 25/51 [00:02<00:03,  8.20it/s] 51%|     | 26/51 [00:03<00:03,  8.18it/s] 53%|    | 27/51 [00:03<00:02,  8.18it/s] 55%|    | 28/51 [00:03<00:02,  8.17it/s] 57%|    | 29/51 [00:03<00:02,  8.17it/s] 59%|    | 30/51 [00:03<00:02,  8.18it/s] 61%|    | 31/51 [00:03<00:02,  8.18it/s] 63%|   | 32/51 [00:03<00:02,  8.16it/s] 65%|   | 33/51 [00:03<00:02,  8.17it/s] 67%|   | 34/51 [00:04<00:02,  8.18it/s] 69%|   | 35/51 [00:04<00:01,  8.17it/s] 71%|   | 36/51 [00:04<00:01,  8.18it/s] 73%|  | 37/51 [00:04<00:01,  8.17it/s] 75%|  | 38/51 [00:04<00:01,  8.19it/s] 76%|  | 39/51 [00:04<00:01,  8.20it/s] 78%|  | 40/51 [00:04<00:01,  8.22it/s] 80%|  | 41/51 [00:04<00:01,  8.21it/s] 82%| | 42/51 [00:04<00:01,  8.23it/s] 84%| | 43/51 [00:05<00:00,  8.21it/s] 86%| | 44/51 [00:05<00:00,  8.21it/s] 88%| | 45/51 [00:05<00:00,  8.21it/s] 90%| | 46/51 [00:05<00:00,  8.22it/s] 92%|| 47/51 [00:05<00:00,  8.21it/s] 94%|| 48/51 [00:05<00:00,  8.21it/s] 96%|| 49/51 [00:05<00:00,  8.21it/s] 98%|| 50/51 [00:05<00:00,  8.22it/s]100%|| 51/51 [00:06<00:00,  8.26it/s]100%|| 51/51 [00:06<00:00,  8.34it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.6789
  eval_combined_score     =      0.743
  eval_f1                 =     0.8071
  eval_loss               =     0.6244
  eval_runtime            = 0:00:06.25
  eval_samples            =        408
  eval_samples_per_second =      65.24
  eval_steps_per_second   =      8.155
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 22:41:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 22:41:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1/runs/Feb16_22-41-46_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/16/2024 22:41:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:41:47 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:41:47 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:41:47 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:41:47,697 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:41:48,124 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:41:48,159 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:41:48,159 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:41:48,159 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:41:48,159 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:41:48,159 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:41:48,216 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:41:48,271 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-16 22:41:51,220 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 22:41:51,220 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6858f2af472445e7.arrow
02/16/2024 22:41:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6858f2af472445e7.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-132e549c3441b137.arrow
02/16/2024 22:41:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-132e549c3441b137.arrow
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-39aaa766e06d1771.arrow
02/16/2024 22:41:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-39aaa766e06d1771.arrow
Running tokenizer on dataset:  58%|    | 1000/1725 [00:00<00:00, 5769.65 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 5878.80 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 5755.00 examples/s]
02/16/2024 22:41:52 - INFO - __main__ - Sample 654 of the training set: {'sentence1': 'The airline says only Robert Milton is taking a 15-per-cent reduction in pay .', 'sentence2': "The airline 's pilots , for example , are taking a 15-per-cent cut .", 'label': 0, 'idx': 736, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 4799, 1220, 4083, 871, 4755, 3833, 880, 338, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 20376, 297, 5146, 869, 1, 450, 4799, 1220, 525, 29879, 8230, 1862, 1919, 363, 1342, 1919, 526, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 5700, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:41:52 - INFO - __main__ - Sample 114 of the training set: {'sentence1': 'Police warned residents on Friday not to travel alone and to avoid convenience stores and service stations stores at night .', 'sentence2': 'Police advised residents Friday not to travel alone to convenience stores and to be watchful .', 'label': 0, 'idx': 131, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 18923, 1370, 9571, 24060, 373, 28728, 451, 304, 9850, 7432, 322, 304, 4772, 29703, 14422, 322, 2669, 16355, 14422, 472, 4646, 869, 1, 18923, 594, 11292, 24060, 28728, 451, 304, 9850, 7432, 304, 29703, 14422, 322, 304, 367, 6505, 1319, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:41:52 - INFO - __main__ - Sample 25 of the training set: {'sentence1': 'I wanted to bring the most beautiful people into the most beautiful building , he said Sunday inside the Grand Central concourse .', 'sentence2': '" I wanted to bring the most beautiful people into the most beautiful building , " Tunick said Sunday .', 'label': 1, 'idx': 28, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 540, 1497, 16340, 2768, 278, 6265, 8068, 3022, 10242, 869, 1, 376, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 376, 21072, 860, 1497, 16340, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:41:52 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 22:41:53,801 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 22:41:54,132 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 22:41:54,132 >>   Num examples = 1,000
[INFO|trainer.py:1749] 2024-02-16 22:41:54,133 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 22:41:54,133 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 22:41:54,133 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-16 22:41:54,133 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 22:41:54,133 >>   Total optimization steps = 375
[INFO|trainer.py:1756] 2024-02-16 22:41:54,134 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/375 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/375 [00:01<09:58,  1.60s/it]  1%|          | 2/375 [00:02<06:46,  1.09s/it]  1%|          | 3/375 [00:03<05:58,  1.04it/s]  1%|          | 4/375 [00:03<05:35,  1.10it/s]  1%|         | 5/375 [00:04<05:22,  1.15it/s]  2%|         | 6/375 [00:05<05:14,  1.17it/s]  2%|         | 7/375 [00:06<05:08,  1.19it/s]  2%|         | 8/375 [00:07<05:04,  1.20it/s]  2%|         | 9/375 [00:08<05:02,  1.21it/s]  3%|         | 10/375 [00:08<04:59,  1.22it/s]  3%|         | 11/375 [00:09<04:58,  1.22it/s]  3%|         | 12/375 [00:10<04:57,  1.22it/s]  3%|         | 13/375 [00:11<04:55,  1.22it/s]  4%|         | 14/375 [00:12<04:54,  1.22it/s]  4%|         | 15/375 [00:12<04:53,  1.23it/s]  4%|         | 16/375 [00:13<04:53,  1.23it/s]  5%|         | 17/375 [00:14<04:51,  1.23it/s]  5%|         | 18/375 [00:15<04:51,  1.23it/s]  5%|         | 19/375 [00:16<04:50,  1.23it/s]  5%|         | 20/375 [00:16<04:49,  1.23it/s]  6%|         | 21/375 [00:17<04:49,  1.22it/s]  6%|         | 22/375 [00:18<04:48,  1.22it/s]  6%|         | 23/375 [00:19<04:47,  1.23it/s]  6%|         | 24/375 [00:20<04:46,  1.23it/s]  7%|         | 25/375 [00:21<04:44,  1.23it/s]  7%|         | 26/375 [00:21<04:44,  1.23it/s]  7%|         | 27/375 [00:22<04:43,  1.23it/s]  7%|         | 28/375 [00:23<04:43,  1.22it/s]  8%|         | 29/375 [00:24<04:42,  1.22it/s]  8%|         | 30/375 [00:25<04:42,  1.22it/s]  8%|         | 31/375 [00:25<04:40,  1.23it/s]  9%|         | 32/375 [00:26<04:39,  1.23it/s]  9%|         | 33/375 [00:27<04:38,  1.23it/s]  9%|         | 34/375 [00:28<04:37,  1.23it/s]  9%|         | 35/375 [00:29<04:37,  1.23it/s] 10%|         | 36/375 [00:30<04:37,  1.22it/s] 10%|         | 37/375 [00:30<04:36,  1.22it/s] 10%|         | 38/375 [00:31<04:35,  1.22it/s] 10%|         | 39/375 [00:32<04:35,  1.22it/s] 11%|         | 40/375 [00:33<04:34,  1.22it/s] 11%|         | 41/375 [00:34<04:33,  1.22it/s] 11%|         | 42/375 [00:34<04:32,  1.22it/s] 11%|        | 43/375 [00:35<04:31,  1.22it/s] 12%|        | 44/375 [00:36<04:31,  1.22it/s] 12%|        | 45/375 [00:37<04:30,  1.22it/s] 12%|        | 46/375 [00:38<04:29,  1.22it/s] 13%|        | 47/375 [00:39<04:28,  1.22it/s] 13%|        | 48/375 [00:39<04:27,  1.22it/s] 13%|        | 49/375 [00:40<04:27,  1.22it/s] 13%|        | 50/375 [00:41<04:26,  1.22it/s] 14%|        | 51/375 [00:42<04:25,  1.22it/s] 14%|        | 52/375 [00:43<04:24,  1.22it/s] 14%|        | 53/375 [00:43<04:23,  1.22it/s] 14%|        | 54/375 [00:44<04:22,  1.22it/s] 15%|        | 55/375 [00:45<04:21,  1.22it/s] 15%|        | 56/375 [00:46<04:20,  1.23it/s] 15%|        | 57/375 [00:47<04:19,  1.23it/s] 15%|        | 58/375 [00:48<04:19,  1.22it/s] 16%|        | 59/375 [00:48<04:17,  1.23it/s] 16%|        | 60/375 [00:49<04:17,  1.22it/s] 16%|        | 61/375 [00:50<04:16,  1.22it/s] 17%|        | 62/375 [00:51<04:15,  1.22it/s] 17%|        | 63/375 [00:52<04:14,  1.22it/s] 17%|        | 64/375 [00:52<04:13,  1.23it/s] 17%|        | 65/375 [00:53<04:12,  1.23it/s] 18%|        | 66/375 [00:54<04:12,  1.23it/s] 18%|        | 67/375 [00:55<04:11,  1.22it/s] 18%|        | 68/375 [00:56<04:11,  1.22it/s] 18%|        | 69/375 [00:57<04:10,  1.22it/s] 19%|        | 70/375 [00:57<04:10,  1.22it/s] 19%|        | 71/375 [00:58<04:09,  1.22it/s] 19%|        | 72/375 [00:59<04:08,  1.22it/s] 19%|        | 73/375 [01:00<04:07,  1.22it/s] 20%|        | 74/375 [01:01<04:06,  1.22it/s] 20%|        | 75/375 [01:01<04:05,  1.22it/s] 20%|        | 76/375 [01:02<04:05,  1.22it/s] 21%|        | 77/375 [01:03<04:04,  1.22it/s] 21%|        | 78/375 [01:04<04:03,  1.22it/s] 21%|        | 79/375 [01:05<04:02,  1.22it/s] 21%|       | 80/375 [01:06<04:01,  1.22it/s] 22%|       | 81/375 [01:06<04:00,  1.22it/s] 22%|       | 82/375 [01:07<03:59,  1.22it/s] 22%|       | 83/375 [01:08<03:59,  1.22it/s] 22%|       | 84/375 [01:09<03:58,  1.22it/s] 23%|       | 85/375 [01:10<03:57,  1.22it/s] 23%|       | 86/375 [01:10<03:56,  1.22it/s] 23%|       | 87/375 [01:11<03:55,  1.22it/s] 23%|       | 88/375 [01:12<03:54,  1.22it/s] 24%|       | 89/375 [01:13<03:53,  1.22it/s] 24%|       | 90/375 [01:14<03:53,  1.22it/s] 24%|       | 91/375 [01:15<03:52,  1.22it/s] 25%|       | 92/375 [01:15<03:51,  1.22it/s] 25%|       | 93/375 [01:16<03:50,  1.22it/s] 25%|       | 94/375 [01:17<03:49,  1.22it/s] 25%|       | 95/375 [01:18<03:49,  1.22it/s] 26%|       | 96/375 [01:19<03:48,  1.22it/s] 26%|       | 97/375 [01:19<03:47,  1.22it/s] 26%|       | 98/375 [01:20<03:46,  1.22it/s] 26%|       | 99/375 [01:21<03:46,  1.22it/s] 27%|       | 100/375 [01:22<03:45,  1.22it/s] 27%|       | 101/375 [01:23<03:44,  1.22it/s] 27%|       | 102/375 [01:24<03:43,  1.22it/s] 27%|       | 103/375 [01:24<03:42,  1.22it/s] 28%|       | 104/375 [01:25<03:41,  1.22it/s] 28%|       | 105/375 [01:26<03:40,  1.22it/s] 28%|       | 106/375 [01:27<03:40,  1.22it/s] 29%|       | 107/375 [01:28<03:39,  1.22it/s] 29%|       | 108/375 [01:28<03:38,  1.22it/s] 29%|       | 109/375 [01:29<03:38,  1.22it/s] 29%|       | 110/375 [01:30<03:37,  1.22it/s] 30%|       | 111/375 [01:31<03:36,  1.22it/s] 30%|       | 112/375 [01:32<03:35,  1.22it/s] 30%|       | 113/375 [01:33<03:34,  1.22it/s] 30%|       | 114/375 [01:33<03:33,  1.22it/s] 31%|       | 115/375 [01:34<03:32,  1.22it/s] 31%|       | 116/375 [01:35<03:31,  1.22it/s] 31%|       | 117/375 [01:36<03:30,  1.22it/s] 31%|      | 118/375 [01:37<03:30,  1.22it/s] 32%|      | 119/375 [01:37<03:29,  1.22it/s] 32%|      | 120/375 [01:38<03:29,  1.22it/s] 32%|      | 121/375 [01:39<03:28,  1.22it/s] 33%|      | 122/375 [01:40<03:27,  1.22it/s] 33%|      | 123/375 [01:41<03:26,  1.22it/s] 33%|      | 124/375 [01:42<03:25,  1.22it/s] 33%|      | 125/375 [01:42<03:24,  1.22it/s] 34%|      | 126/375 [01:43<03:24,  1.22it/s] 34%|      | 127/375 [01:44<03:23,  1.22it/s] 34%|      | 128/375 [01:45<03:22,  1.22it/s] 34%|      | 129/375 [01:46<03:22,  1.22it/s] 35%|      | 130/375 [01:47<03:21,  1.22it/s] 35%|      | 131/375 [01:47<03:20,  1.22it/s] 35%|      | 132/375 [01:48<03:19,  1.22it/s] 35%|      | 133/375 [01:49<03:18,  1.22it/s] 36%|      | 134/375 [01:50<03:17,  1.22it/s] 36%|      | 135/375 [01:51<03:16,  1.22it/s] 36%|      | 136/375 [01:51<03:16,  1.22it/s] 37%|      | 137/375 [01:52<03:15,  1.22it/s] 37%|      | 138/375 [01:53<03:14,  1.22it/s] 37%|      | 139/375 [01:54<03:13,  1.22it/s] 37%|      | 140/375 [01:55<03:12,  1.22it/s] 38%|      | 141/375 [01:56<03:11,  1.22it/s] 38%|      | 142/375 [01:56<03:10,  1.22it/s] 38%|      | 143/375 [01:57<03:10,  1.22it/s] 38%|      | 144/375 [01:58<03:09,  1.22it/s] 39%|      | 145/375 [01:59<03:08,  1.22it/s] 39%|      | 146/375 [02:00<03:07,  1.22it/s] 39%|      | 147/375 [02:00<03:06,  1.22it/s] 39%|      | 148/375 [02:01<03:05,  1.22it/s] 40%|      | 149/375 [02:02<03:05,  1.22it/s] 40%|      | 150/375 [02:03<03:04,  1.22it/s] 40%|      | 151/375 [02:04<03:03,  1.22it/s] 41%|      | 152/375 [02:05<03:02,  1.22it/s] 41%|      | 153/375 [02:05<03:01,  1.22it/s] 41%|      | 154/375 [02:06<03:00,  1.22it/s] 41%|     | 155/375 [02:07<03:00,  1.22it/s] 42%|     | 156/375 [02:08<02:59,  1.22it/s] 42%|     | 157/375 [02:09<02:58,  1.22it/s] 42%|     | 158/375 [02:09<02:58,  1.22it/s] 42%|     | 159/375 [02:10<02:57,  1.22it/s] 43%|     | 160/375 [02:11<02:56,  1.22it/s] 43%|     | 161/375 [02:12<02:55,  1.22it/s] 43%|     | 162/375 [02:13<02:54,  1.22it/s] 43%|     | 163/375 [02:14<02:53,  1.22it/s] 44%|     | 164/375 [02:14<02:52,  1.22it/s] 44%|     | 165/375 [02:15<02:52,  1.22it/s] 44%|     | 166/375 [02:16<02:51,  1.22it/s] 45%|     | 167/375 [02:17<02:50,  1.22it/s] 45%|     | 168/375 [02:18<02:49,  1.22it/s] 45%|     | 169/375 [02:18<02:48,  1.22it/s] 45%|     | 170/375 [02:19<02:48,  1.22it/s] 46%|     | 171/375 [02:20<02:47,  1.22it/s] 46%|     | 172/375 [02:21<02:46,  1.22it/s] 46%|     | 173/375 [02:22<02:45,  1.22it/s] 46%|     | 174/375 [02:23<02:45,  1.22it/s] 47%|     | 175/375 [02:23<02:44,  1.22it/s] 47%|     | 176/375 [02:24<02:43,  1.22it/s] 47%|     | 177/375 [02:25<02:42,  1.22it/s] 47%|     | 178/375 [02:26<02:41,  1.22it/s] 48%|     | 179/375 [02:27<02:40,  1.22it/s] 48%|     | 180/375 [02:28<02:40,  1.22it/s] 48%|     | 181/375 [02:28<02:39,  1.22it/s] 49%|     | 182/375 [02:29<02:38,  1.22it/s] 49%|     | 183/375 [02:30<02:37,  1.22it/s] 49%|     | 184/375 [02:31<02:36,  1.22it/s] 49%|     | 185/375 [02:32<02:36,  1.22it/s] 50%|     | 186/375 [02:32<02:35,  1.22it/s] 50%|     | 187/375 [02:33<02:34,  1.21it/s] 50%|     | 188/375 [02:34<02:33,  1.22it/s] 50%|     | 189/375 [02:35<02:32,  1.22it/s] 51%|     | 190/375 [02:36<02:31,  1.22it/s] 51%|     | 191/375 [02:37<02:31,  1.22it/s] 51%|     | 192/375 [02:37<02:30,  1.22it/s] 51%|    | 193/375 [02:38<02:29,  1.22it/s] 52%|    | 194/375 [02:39<02:28,  1.22it/s] 52%|    | 195/375 [02:40<02:28,  1.22it/s] 52%|    | 196/375 [02:41<02:27,  1.22it/s] 53%|    | 197/375 [02:41<02:26,  1.22it/s] 53%|    | 198/375 [02:42<02:25,  1.22it/s] 53%|    | 199/375 [02:43<02:24,  1.22it/s] 53%|    | 200/375 [02:44<02:23,  1.22it/s] 54%|    | 201/375 [02:45<02:23,  1.21it/s] 54%|    | 202/375 [02:46<02:22,  1.21it/s] 54%|    | 203/375 [02:46<02:21,  1.21it/s] 54%|    | 204/375 [02:47<02:20,  1.21it/s] 55%|    | 205/375 [02:48<02:19,  1.21it/s] 55%|    | 206/375 [02:49<02:19,  1.22it/s] 55%|    | 207/375 [02:50<02:18,  1.21it/s] 55%|    | 208/375 [02:51<02:17,  1.21it/s] 56%|    | 209/375 [02:51<02:16,  1.21it/s] 56%|    | 210/375 [02:52<02:15,  1.21it/s] 56%|    | 211/375 [02:53<02:15,  1.21it/s] 57%|    | 212/375 [02:54<02:14,  1.21it/s] 57%|    | 213/375 [02:55<02:13,  1.21it/s] 57%|    | 214/375 [02:55<02:12,  1.21it/s] 57%|    | 215/375 [02:56<02:11,  1.21it/s] 58%|    | 216/375 [02:57<02:11,  1.21it/s] 58%|    | 217/375 [02:58<02:10,  1.21it/s] 58%|    | 218/375 [02:59<02:09,  1.21it/s] 58%|    | 219/375 [03:00<02:08,  1.21it/s] 59%|    | 220/375 [03:00<02:07,  1.21it/s] 59%|    | 221/375 [03:01<02:06,  1.21it/s] 59%|    | 222/375 [03:02<02:06,  1.21it/s] 59%|    | 223/375 [03:03<02:05,  1.21it/s] 60%|    | 224/375 [03:04<02:04,  1.21it/s] 60%|    | 225/375 [03:05<02:03,  1.21it/s] 60%|    | 226/375 [03:05<02:02,  1.22it/s] 61%|    | 227/375 [03:06<02:01,  1.22it/s] 61%|    | 228/375 [03:07<02:00,  1.22it/s] 61%|    | 229/375 [03:08<02:00,  1.22it/s] 61%|   | 230/375 [03:09<01:59,  1.22it/s] 62%|   | 231/375 [03:09<01:58,  1.22it/s] 62%|   | 232/375 [03:10<01:57,  1.21it/s] 62%|   | 233/375 [03:11<01:56,  1.21it/s] 62%|   | 234/375 [03:12<01:56,  1.22it/s] 63%|   | 235/375 [03:13<01:55,  1.21it/s] 63%|   | 236/375 [03:14<01:54,  1.21it/s] 63%|   | 237/375 [03:14<01:53,  1.21it/s] 63%|   | 238/375 [03:15<01:52,  1.21it/s] 64%|   | 239/375 [03:16<01:52,  1.21it/s] 64%|   | 240/375 [03:17<01:51,  1.21it/s] 64%|   | 241/375 [03:18<01:50,  1.21it/s] 65%|   | 242/375 [03:19<01:49,  1.21it/s] 65%|   | 243/375 [03:19<01:48,  1.21it/s] 65%|   | 244/375 [03:20<01:47,  1.21it/s] 65%|   | 245/375 [03:21<01:47,  1.21it/s] 66%|   | 246/375 [03:22<01:46,  1.21it/s] 66%|   | 247/375 [03:23<01:45,  1.21it/s] 66%|   | 248/375 [03:24<01:44,  1.21it/s] 66%|   | 249/375 [03:24<01:44,  1.21it/s] 67%|   | 250/375 [03:25<01:43,  1.21it/s] 67%|   | 251/375 [03:26<01:42,  1.21it/s] 67%|   | 252/375 [03:27<01:41,  1.21it/s] 67%|   | 253/375 [03:28<01:40,  1.21it/s] 68%|   | 254/375 [03:28<01:39,  1.21it/s] 68%|   | 255/375 [03:29<01:39,  1.21it/s] 68%|   | 256/375 [03:30<01:38,  1.21it/s] 69%|   | 257/375 [03:31<01:37,  1.21it/s] 69%|   | 258/375 [03:32<01:36,  1.21it/s] 69%|   | 259/375 [03:33<01:35,  1.21it/s] 69%|   | 260/375 [03:33<01:34,  1.21it/s] 70%|   | 261/375 [03:34<01:33,  1.21it/s] 70%|   | 262/375 [03:35<01:33,  1.21it/s] 70%|   | 263/375 [03:36<01:32,  1.21it/s] 70%|   | 264/375 [03:37<01:31,  1.22it/s] 71%|   | 265/375 [03:38<01:30,  1.21it/s] 71%|   | 266/375 [03:38<01:29,  1.22it/s] 71%|   | 267/375 [03:39<01:28,  1.21it/s] 71%|  | 268/375 [03:40<01:28,  1.21it/s] 72%|  | 269/375 [03:41<01:27,  1.21it/s] 72%|  | 270/375 [03:42<01:26,  1.21it/s] 72%|  | 271/375 [03:42<01:25,  1.21it/s] 73%|  | 272/375 [03:43<01:24,  1.21it/s] 73%|  | 273/375 [03:44<01:24,  1.21it/s] 73%|  | 274/375 [03:45<01:23,  1.21it/s] 73%|  | 275/375 [03:46<01:22,  1.21it/s] 74%|  | 276/375 [03:47<01:21,  1.21it/s] 74%|  | 277/375 [03:47<01:20,  1.21it/s] 74%|  | 278/375 [03:48<01:20,  1.21it/s] 74%|  | 279/375 [03:49<01:19,  1.21it/s] 75%|  | 280/375 [03:50<01:18,  1.21it/s] 75%|  | 281/375 [03:51<01:17,  1.21it/s] 75%|  | 282/375 [03:52<01:16,  1.21it/s] 75%|  | 283/375 [03:52<01:15,  1.21it/s] 76%|  | 284/375 [03:53<01:15,  1.21it/s] 76%|  | 285/375 [03:54<01:14,  1.21it/s] 76%|  | 286/375 [03:55<01:13,  1.21it/s] 77%|  | 287/375 [03:56<01:12,  1.21it/s] 77%|  | 288/375 [03:56<01:11,  1.21it/s] 77%|  | 289/375 [03:57<01:11,  1.21it/s] 77%|  | 290/375 [03:58<01:10,  1.21it/s] 78%|  | 291/375 [03:59<01:09,  1.21it/s] 78%|  | 292/375 [04:00<01:08,  1.21it/s] 78%|  | 293/375 [04:01<01:07,  1.21it/s] 78%|  | 294/375 [04:01<01:06,  1.21it/s] 79%|  | 295/375 [04:02<01:06,  1.21it/s] 79%|  | 296/375 [04:03<01:05,  1.21it/s] 79%|  | 297/375 [04:04<01:04,  1.21it/s] 79%|  | 298/375 [04:05<01:03,  1.21it/s] 80%|  | 299/375 [04:06<01:02,  1.21it/s] 80%|  | 300/375 [04:06<01:02,  1.21it/s] 80%|  | 301/375 [04:07<01:01,  1.21it/s] 81%|  | 302/375 [04:08<01:00,  1.21it/s] 81%|  | 303/375 [04:09<00:59,  1.21it/s] 81%|  | 304/375 [04:10<00:58,  1.21it/s] 81%| | 305/375 [04:11<00:57,  1.21it/s] 82%| | 306/375 [04:11<00:56,  1.21it/s] 82%| | 307/375 [04:12<00:56,  1.21it/s] 82%| | 308/375 [04:13<00:55,  1.21it/s] 82%| | 309/375 [04:14<00:54,  1.21it/s] 83%| | 310/375 [04:15<00:53,  1.21it/s] 83%| | 311/375 [04:15<00:52,  1.21it/s] 83%| | 312/375 [04:16<00:52,  1.21it/s] 83%| | 313/375 [04:17<00:51,  1.21it/s] 84%| | 314/375 [04:18<00:50,  1.21it/s] 84%| | 315/375 [04:19<00:49,  1.21it/s] 84%| | 316/375 [04:20<00:48,  1.21it/s] 85%| | 317/375 [04:20<00:47,  1.21it/s] 85%| | 318/375 [04:21<00:47,  1.21it/s] 85%| | 319/375 [04:22<00:46,  1.21it/s] 85%| | 320/375 [04:23<00:45,  1.21it/s] 86%| | 321/375 [04:24<00:44,  1.21it/s] 86%| | 322/375 [04:25<00:43,  1.21it/s] 86%| | 323/375 [04:25<00:42,  1.21it/s] 86%| | 324/375 [04:26<00:42,  1.21it/s] 87%| | 325/375 [04:27<00:41,  1.21it/s] 87%| | 326/375 [04:28<00:40,  1.21it/s] 87%| | 327/375 [04:29<00:39,  1.21it/s] 87%| | 328/375 [04:30<00:38,  1.21it/s] 88%| | 329/375 [04:30<00:38,  1.21it/s] 88%| | 330/375 [04:31<00:37,  1.21it/s] 88%| | 331/375 [04:32<00:36,  1.21it/s] 89%| | 332/375 [04:33<00:35,  1.21it/s] 89%| | 333/375 [04:34<00:34,  1.21it/s] 89%| | 334/375 [04:35<00:33,  1.21it/s] 89%| | 335/375 [04:35<00:33,  1.21it/s] 90%| | 336/375 [04:36<00:32,  1.21it/s] 90%| | 337/375 [04:37<00:31,  1.21it/s] 90%| | 338/375 [04:38<00:30,  1.21it/s] 90%| | 339/375 [04:39<00:29,  1.21it/s] 91%| | 340/375 [04:39<00:28,  1.21it/s] 91%| | 341/375 [04:40<00:28,  1.21it/s] 91%| | 342/375 [04:41<00:27,  1.21it/s] 91%|| 343/375 [04:42<00:26,  1.21it/s] 92%|| 344/375 [04:43<00:25,  1.21it/s] 92%|| 345/375 [04:44<00:24,  1.21it/s] 92%|| 346/375 [04:44<00:23,  1.21it/s] 93%|| 347/375 [04:45<00:23,  1.21it/s] 93%|| 348/375 [04:46<00:22,  1.21it/s] 93%|| 349/375 [04:47<00:21,  1.21it/s] 93%|| 350/375 [04:48<00:20,  1.21it/s] 94%|| 351/375 [04:49<00:19,  1.21it/s] 94%|| 352/375 [04:49<00:19,  1.21it/s] 94%|| 353/375 [04:50<00:18,  1.21it/s] 94%|| 354/375 [04:51<00:17,  1.21it/s] 95%|| 355/375 [04:52<00:16,  1.21it/s] 95%|| 356/375 [04:53<00:15,  1.21it/s] 95%|| 357/375 [04:54<00:14,  1.21it/s] 95%|| 358/375 [04:54<00:14,  1.21it/s] 96%|| 359/375 [04:55<00:13,  1.21it/s] 96%|| 360/375 [04:56<00:12,  1.21it/s] 96%|| 361/375 [04:57<00:11,  1.21it/s] 97%|| 362/375 [04:58<00:10,  1.21it/s] 97%|| 363/375 [04:58<00:09,  1.21it/s] 97%|| 364/375 [04:59<00:09,  1.21it/s] 97%|| 365/375 [05:00<00:08,  1.21it/s] 98%|| 366/375 [05:01<00:07,  1.21it/s] 98%|| 367/375 [05:02<00:06,  1.21it/s] 98%|| 368/375 [05:03<00:05,  1.21it/s] 98%|| 369/375 [05:03<00:04,  1.21it/s] 99%|| 370/375 [05:04<00:04,  1.21it/s] 99%|| 371/375 [05:05<00:03,  1.21it/s] 99%|| 372/375 [05:06<00:02,  1.21it/s] 99%|| 373/375 [05:07<00:01,  1.21it/s]100%|| 374/375 [05:08<00:00,  1.21it/s]100%|| 375/375 [05:08<00:00,  1.21it/s][INFO|trainer.py:1988] 2024-02-16 22:47:03,059 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 309.0568, 'train_samples_per_second': 9.707, 'train_steps_per_second': 1.213, 'train_loss': 0.7270558268229167, 'epoch': 3.0}
                                                 100%|| 375/375 [05:09<00:00,  1.21it/s]100%|| 375/375 [05:09<00:00,  1.21it/s]
[INFO|trainer.py:2985] 2024-02-16 22:47:03,194 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1
[INFO|configuration_utils.py:473] 2024-02-16 22:47:03,196 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-16 22:47:23,060 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-16 22:47:23,063 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-16 22:47:23,064 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc128GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.7271
  train_runtime            = 0:05:09.05
  train_samples            =       1000
  train_samples_per_second =      9.707
  train_steps_per_second   =      1.213
02/16/2024 22:47:23 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-16 22:47:23,100 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-16 22:47:23,103 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-16 22:47:23,103 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-16 22:47:23,103 >>   Batch size = 8
  0%|          | 0/51 [00:00<?, ?it/s]  4%|         | 2/51 [00:00<00:05,  8.71it/s]  6%|         | 3/51 [00:00<00:07,  6.18it/s]  8%|         | 4/51 [00:00<00:08,  5.36it/s] 10%|         | 5/51 [00:00<00:09,  4.97it/s] 12%|        | 6/51 [00:01<00:09,  4.76it/s] 14%|        | 7/51 [00:01<00:09,  4.63it/s] 16%|        | 8/51 [00:01<00:09,  4.54it/s] 18%|        | 9/51 [00:01<00:09,  4.49it/s] 20%|        | 10/51 [00:02<00:09,  4.46it/s] 22%|       | 11/51 [00:02<00:09,  4.43it/s] 24%|       | 12/51 [00:02<00:08,  4.42it/s] 25%|       | 13/51 [00:02<00:08,  4.40it/s] 27%|       | 14/51 [00:02<00:08,  4.39it/s] 29%|       | 15/51 [00:03<00:08,  4.39it/s] 31%|      | 16/51 [00:03<00:07,  4.39it/s] 33%|      | 17/51 [00:03<00:07,  4.38it/s] 35%|      | 18/51 [00:03<00:07,  4.37it/s] 37%|      | 19/51 [00:04<00:07,  4.37it/s] 39%|      | 20/51 [00:04<00:07,  4.37it/s] 41%|      | 21/51 [00:04<00:06,  4.37it/s] 43%|     | 22/51 [00:04<00:06,  4.36it/s] 45%|     | 23/51 [00:05<00:06,  4.35it/s] 47%|     | 24/51 [00:05<00:06,  4.35it/s] 49%|     | 25/51 [00:05<00:05,  4.35it/s] 51%|     | 26/51 [00:05<00:05,  4.36it/s] 53%|    | 27/51 [00:05<00:05,  4.36it/s] 55%|    | 28/51 [00:06<00:05,  4.36it/s] 57%|    | 29/51 [00:06<00:05,  4.36it/s] 59%|    | 30/51 [00:06<00:04,  4.36it/s] 61%|    | 31/51 [00:06<00:04,  4.36it/s] 63%|   | 32/51 [00:07<00:04,  4.36it/s] 65%|   | 33/51 [00:07<00:04,  4.36it/s] 67%|   | 34/51 [00:07<00:03,  4.36it/s] 69%|   | 35/51 [00:07<00:03,  4.28it/s] 71%|   | 36/51 [00:08<00:03,  4.31it/s] 73%|  | 37/51 [00:08<00:03,  4.32it/s] 75%|  | 38/51 [00:08<00:03,  4.33it/s] 76%|  | 39/51 [00:08<00:02,  4.34it/s] 78%|  | 40/51 [00:08<00:02,  4.35it/s] 80%|  | 41/51 [00:09<00:02,  4.35it/s] 82%| | 42/51 [00:09<00:02,  4.36it/s] 84%| | 43/51 [00:09<00:01,  4.36it/s] 86%| | 44/51 [00:09<00:01,  4.36it/s] 88%| | 45/51 [00:10<00:01,  4.36it/s] 90%| | 46/51 [00:10<00:01,  4.36it/s] 92%|| 47/51 [00:10<00:00,  4.37it/s] 94%|| 48/51 [00:10<00:00,  4.37it/s] 96%|| 49/51 [00:11<00:00,  4.37it/s] 98%|| 50/51 [00:11<00:00,  4.37it/s]100%|| 51/51 [00:11<00:00,  4.38it/s]100%|| 51/51 [00:11<00:00,  4.44it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.6887
  eval_combined_score     =     0.7394
  eval_f1                 =     0.7901
  eval_loss               =     0.6932
  eval_runtime            = 0:00:11.76
  eval_samples            =        408
  eval_samples_per_second =     34.669
  eval_steps_per_second   =      4.334
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 22:48:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 22:48:16 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/runs/Feb16_22-48-16_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/16/2024 22:48:17 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:48:17 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:48:17 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:48:17 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:48:17,967 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:48:17,969 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:48:18,030 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:48:18,030 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:48:18,031 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:48:18,031 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:48:18,031 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:48:18,084 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:48:18,140 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-16 22:48:21,103 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 22:48:21,104 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
02/16/2024 22:48:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5cddcbf5811fdd14.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0fe9f7c1b160ff74.arrow
02/16/2024 22:48:21 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0fe9f7c1b160ff74.arrow
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ab0d9d9940fc51bd.arrow
02/16/2024 22:48:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ab0d9d9940fc51bd.arrow
Running tokenizer on dataset:  58%|    | 1000/1725 [00:00<00:00, 4640.97 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 4707.47 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 4611.99 examples/s]
02/16/2024 22:48:21 - INFO - __main__ - Sample 654 of the training set: {'sentence1': 'The airline says only Robert Milton is taking a 15-per-cent reduction in pay .', 'sentence2': "The airline 's pilots , for example , are taking a 15-per-cent cut .", 'label': 0, 'idx': 736, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 4799, 1220, 4083, 871, 4755, 3833, 880, 338, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 20376, 297, 5146, 869, 1, 450, 4799, 1220, 525, 29879, 8230, 1862, 1919, 363, 1342, 1919, 526, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 5700, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:48:21 - INFO - __main__ - Sample 114 of the training set: {'sentence1': 'Police warned residents on Friday not to travel alone and to avoid convenience stores and service stations stores at night .', 'sentence2': 'Police advised residents Friday not to travel alone to convenience stores and to be watchful .', 'label': 0, 'idx': 131, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 18923, 1370, 9571, 24060, 373, 28728, 451, 304, 9850, 7432, 322, 304, 4772, 29703, 14422, 322, 2669, 16355, 14422, 472, 4646, 869, 1, 18923, 594, 11292, 24060, 28728, 451, 304, 9850, 7432, 304, 29703, 14422, 322, 304, 367, 6505, 1319, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:48:21 - INFO - __main__ - Sample 25 of the training set: {'sentence1': 'I wanted to bring the most beautiful people into the most beautiful building , he said Sunday inside the Grand Central concourse .', 'sentence2': '" I wanted to bring the most beautiful people into the most beautiful building , " Tunick said Sunday .', 'label': 1, 'idx': 28, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 540, 1497, 16340, 2768, 278, 6265, 8068, 3022, 10242, 869, 1, 376, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 376, 21072, 860, 1497, 16340, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:48:22 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 22:48:23,548 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 22:48:23,865 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 22:48:23,865 >>   Num examples = 1,000
[INFO|trainer.py:1749] 2024-02-16 22:48:23,866 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 22:48:23,866 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 22:48:23,866 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-16 22:48:23,866 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 22:48:23,866 >>   Total optimization steps = 375
[INFO|trainer.py:1756] 2024-02-16 22:48:23,867 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/375 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/375 [00:02<13:38,  2.19s/it]  1%|          | 2/375 [00:03<10:33,  1.70s/it]  1%|          | 3/375 [00:04<09:49,  1.58s/it]  1%|          | 4/375 [00:06<09:25,  1.53s/it]  1%|         | 5/375 [00:07<09:13,  1.50s/it]  2%|         | 6/375 [00:09<09:06,  1.48s/it]  2%|         | 7/375 [00:10<08:59,  1.47s/it]  2%|         | 8/375 [00:12<08:55,  1.46s/it]  2%|         | 9/375 [00:13<08:51,  1.45s/it]  3%|         | 10/375 [00:15<08:49,  1.45s/it]  3%|         | 11/375 [00:16<08:46,  1.45s/it]  3%|         | 12/375 [00:17<08:45,  1.45s/it]  3%|         | 13/375 [00:19<08:44,  1.45s/it]  4%|         | 14/375 [00:20<08:42,  1.45s/it]  4%|         | 15/375 [00:22<08:40,  1.45s/it]  4%|         | 16/375 [00:23<08:39,  1.45s/it]  5%|         | 17/375 [00:25<08:37,  1.45s/it]  5%|         | 18/375 [00:26<08:35,  1.45s/it]  5%|         | 19/375 [00:28<08:34,  1.44s/it]  5%|         | 20/375 [00:29<08:32,  1.44s/it]  6%|         | 21/375 [00:30<08:31,  1.45s/it]  6%|         | 22/375 [00:32<08:30,  1.45s/it]  6%|         | 23/375 [00:33<08:29,  1.45s/it]  6%|         | 24/375 [00:35<08:27,  1.45s/it]  7%|         | 25/375 [00:36<08:25,  1.44s/it]  7%|         | 26/375 [00:38<08:23,  1.44s/it]  7%|         | 27/375 [00:39<08:22,  1.44s/it]  7%|         | 28/375 [00:41<08:21,  1.45s/it]  8%|         | 29/375 [00:42<08:19,  1.44s/it]  8%|         | 30/375 [00:43<08:19,  1.45s/it]  8%|         | 31/375 [00:45<08:16,  1.44s/it]  9%|         | 32/375 [00:46<08:15,  1.44s/it]  9%|         | 33/375 [00:48<08:14,  1.44s/it]  9%|         | 34/375 [00:49<08:12,  1.44s/it]  9%|         | 35/375 [00:51<08:11,  1.45s/it] 10%|         | 36/375 [00:52<08:09,  1.44s/it] 10%|         | 37/375 [00:54<08:08,  1.45s/it] 10%|         | 38/375 [00:55<08:06,  1.44s/it] 10%|         | 39/375 [00:56<08:05,  1.45s/it] 11%|         | 40/375 [00:58<08:03,  1.44s/it] 11%|         | 41/375 [00:59<08:02,  1.45s/it] 11%|         | 42/375 [01:01<08:01,  1.45s/it] 11%|        | 43/375 [01:02<08:00,  1.45s/it] 12%|        | 44/375 [01:04<07:58,  1.45s/it] 12%|        | 45/375 [01:05<07:56,  1.45s/it] 12%|        | 46/375 [01:07<07:55,  1.44s/it] 13%|        | 47/375 [01:08<07:54,  1.45s/it] 13%|        | 48/375 [01:10<07:52,  1.44s/it] 13%|        | 49/375 [01:11<07:51,  1.45s/it] 13%|        | 50/375 [01:12<07:50,  1.45s/it] 14%|        | 51/375 [01:14<07:48,  1.45s/it] 14%|        | 52/375 [01:15<07:47,  1.45s/it] 14%|        | 53/375 [01:17<07:45,  1.45s/it] 14%|        | 54/375 [01:18<07:44,  1.45s/it] 15%|        | 55/375 [01:20<07:42,  1.45s/it] 15%|        | 56/375 [01:21<07:41,  1.45s/it] 15%|        | 57/375 [01:23<07:40,  1.45s/it] 15%|        | 58/375 [01:24<07:38,  1.45s/it] 16%|        | 59/375 [01:25<07:37,  1.45s/it] 16%|        | 60/375 [01:27<07:35,  1.45s/it] 16%|        | 61/375 [01:28<07:34,  1.45s/it] 17%|        | 62/375 [01:30<07:33,  1.45s/it] 17%|        | 63/375 [01:31<07:31,  1.45s/it] 17%|        | 64/375 [01:33<07:29,  1.44s/it] 17%|        | 65/375 [01:34<07:27,  1.44s/it] 18%|        | 66/375 [01:36<07:26,  1.45s/it] 18%|        | 67/375 [01:37<07:25,  1.45s/it] 18%|        | 68/375 [01:38<07:23,  1.45s/it] 18%|        | 69/375 [01:40<07:22,  1.45s/it] 19%|        | 70/375 [01:41<07:20,  1.45s/it] 19%|        | 71/375 [01:43<07:19,  1.45s/it] 19%|        | 72/375 [01:44<07:18,  1.45s/it] 19%|        | 73/375 [01:46<07:17,  1.45s/it] 20%|        | 74/375 [01:47<07:15,  1.45s/it] 20%|        | 75/375 [01:49<07:14,  1.45s/it] 20%|        | 76/375 [01:50<07:12,  1.45s/it] 21%|        | 77/375 [01:51<07:11,  1.45s/it] 21%|        | 78/375 [01:53<07:09,  1.45s/it] 21%|        | 79/375 [01:54<07:08,  1.45s/it] 21%|       | 80/375 [01:56<07:07,  1.45s/it] 22%|       | 81/375 [01:57<07:05,  1.45s/it] 22%|       | 82/375 [01:59<07:04,  1.45s/it] 22%|       | 83/375 [02:00<07:03,  1.45s/it] 22%|       | 84/375 [02:02<07:01,  1.45s/it] 23%|       | 85/375 [02:03<07:00,  1.45s/it] 23%|       | 86/375 [02:05<06:58,  1.45s/it] 23%|       | 87/375 [02:06<06:57,  1.45s/it] 23%|       | 88/375 [02:07<06:55,  1.45s/it] 24%|       | 89/375 [02:09<06:54,  1.45s/it] 24%|       | 90/375 [02:10<06:53,  1.45s/it] 24%|       | 91/375 [02:12<06:51,  1.45s/it] 25%|       | 92/375 [02:13<06:50,  1.45s/it] 25%|       | 93/375 [02:15<06:48,  1.45s/it] 25%|       | 94/375 [02:16<06:46,  1.45s/it] 25%|       | 95/375 [02:18<06:46,  1.45s/it] 26%|       | 96/375 [02:19<06:44,  1.45s/it] 26%|       | 97/375 [02:20<06:43,  1.45s/it] 26%|       | 98/375 [02:22<06:41,  1.45s/it] 26%|       | 99/375 [02:23<06:39,  1.45s/it] 27%|       | 100/375 [02:25<06:38,  1.45s/it] 27%|       | 101/375 [02:26<06:36,  1.45s/it] 27%|       | 102/375 [02:28<06:35,  1.45s/it] 27%|       | 103/375 [02:29<06:34,  1.45s/it] 28%|       | 104/375 [02:31<06:32,  1.45s/it] 28%|       | 105/375 [02:32<06:31,  1.45s/it] 28%|       | 106/375 [02:33<06:30,  1.45s/it] 29%|       | 107/375 [02:35<06:28,  1.45s/it] 29%|       | 108/375 [02:36<06:27,  1.45s/it] 29%|       | 109/375 [02:38<06:26,  1.45s/it] 29%|       | 110/375 [02:39<06:24,  1.45s/it] 30%|       | 111/375 [02:41<06:23,  1.45s/it] 30%|       | 112/375 [02:42<06:21,  1.45s/it] 30%|       | 113/375 [02:44<06:20,  1.45s/it] 30%|       | 114/375 [02:45<06:19,  1.45s/it] 31%|       | 115/375 [02:47<06:17,  1.45s/it] 31%|       | 116/375 [02:48<06:16,  1.45s/it] 31%|       | 117/375 [02:49<06:14,  1.45s/it] 31%|      | 118/375 [02:51<06:13,  1.45s/it] 32%|      | 119/375 [02:52<06:11,  1.45s/it] 32%|      | 120/375 [02:54<06:10,  1.45s/it] 32%|      | 121/375 [02:55<06:08,  1.45s/it] 33%|      | 122/375 [02:57<06:07,  1.45s/it] 33%|      | 123/375 [02:58<06:06,  1.45s/it] 33%|      | 124/375 [03:00<06:04,  1.45s/it] 33%|      | 125/375 [03:01<06:03,  1.45s/it] 34%|      | 126/375 [03:03<06:01,  1.45s/it] 34%|      | 127/375 [03:04<06:00,  1.45s/it] 34%|      | 128/375 [03:05<05:58,  1.45s/it] 34%|      | 129/375 [03:07<05:57,  1.45s/it] 35%|      | 130/375 [03:08<05:55,  1.45s/it] 35%|      | 131/375 [03:10<05:54,  1.45s/it] 35%|      | 132/375 [03:11<05:53,  1.45s/it] 35%|      | 133/375 [03:13<05:51,  1.45s/it] 36%|      | 134/375 [03:14<05:50,  1.45s/it] 36%|      | 135/375 [03:16<05:48,  1.45s/it] 36%|      | 136/375 [03:17<05:47,  1.45s/it] 37%|      | 137/375 [03:19<05:46,  1.46s/it] 37%|      | 138/375 [03:20<05:44,  1.45s/it] 37%|      | 139/375 [03:21<05:43,  1.45s/it] 37%|      | 140/375 [03:23<05:41,  1.45s/it] 38%|      | 141/375 [03:24<05:40,  1.45s/it] 38%|      | 142/375 [03:26<05:38,  1.45s/it] 38%|      | 143/375 [03:27<05:37,  1.46s/it] 38%|      | 144/375 [03:29<05:35,  1.45s/it] 39%|      | 145/375 [03:30<05:34,  1.45s/it] 39%|      | 146/375 [03:32<05:33,  1.46s/it] 39%|      | 147/375 [03:33<05:32,  1.46s/it] 39%|      | 148/375 [03:35<05:30,  1.46s/it] 40%|      | 149/375 [03:36<05:29,  1.46s/it] 40%|      | 150/375 [03:37<05:27,  1.46s/it] 40%|      | 151/375 [03:39<05:26,  1.46s/it] 41%|      | 152/375 [03:40<05:24,  1.46s/it] 41%|      | 153/375 [03:42<05:22,  1.45s/it] 41%|      | 154/375 [03:43<05:21,  1.46s/it] 41%|     | 155/375 [03:45<05:20,  1.46s/it] 42%|     | 156/375 [03:46<05:18,  1.45s/it] 42%|     | 157/375 [03:48<05:17,  1.46s/it] 42%|     | 158/375 [03:49<05:16,  1.46s/it] 42%|     | 159/375 [03:51<05:14,  1.46s/it] 43%|     | 160/375 [03:52<05:13,  1.46s/it] 43%|     | 161/375 [03:53<05:11,  1.46s/it] 43%|     | 162/375 [03:55<05:10,  1.46s/it] 43%|     | 163/375 [03:56<05:08,  1.46s/it] 44%|     | 164/375 [03:58<05:07,  1.46s/it] 44%|     | 165/375 [03:59<05:05,  1.46s/it] 44%|     | 166/375 [04:01<05:04,  1.46s/it] 45%|     | 167/375 [04:02<05:03,  1.46s/it] 45%|     | 168/375 [04:04<05:01,  1.46s/it] 45%|     | 169/375 [04:05<04:59,  1.46s/it] 45%|     | 170/375 [04:07<04:58,  1.46s/it] 46%|     | 171/375 [04:08<04:57,  1.46s/it] 46%|     | 172/375 [04:10<04:56,  1.46s/it] 46%|     | 173/375 [04:11<04:54,  1.46s/it] 46%|     | 174/375 [04:12<04:53,  1.46s/it] 47%|     | 175/375 [04:14<04:51,  1.46s/it] 47%|     | 176/375 [04:15<04:50,  1.46s/it] 47%|     | 177/375 [04:17<04:48,  1.46s/it] 47%|     | 178/375 [04:18<04:47,  1.46s/it] 48%|     | 179/375 [04:20<04:45,  1.46s/it] 48%|     | 180/375 [04:21<04:43,  1.46s/it] 48%|     | 181/375 [04:23<04:42,  1.46s/it] 49%|     | 182/375 [04:24<04:41,  1.46s/it] 49%|     | 183/375 [04:26<04:40,  1.46s/it] 49%|     | 184/375 [04:27<04:38,  1.46s/it] 49%|     | 185/375 [04:28<04:36,  1.46s/it] 50%|     | 186/375 [04:30<04:35,  1.46s/it] 50%|     | 187/375 [04:31<04:34,  1.46s/it] 50%|     | 188/375 [04:33<04:32,  1.46s/it] 50%|     | 189/375 [04:34<04:31,  1.46s/it] 51%|     | 190/375 [04:36<04:29,  1.46s/it] 51%|     | 191/375 [04:37<04:28,  1.46s/it] 51%|     | 192/375 [04:39<04:26,  1.46s/it] 51%|    | 193/375 [04:40<04:25,  1.46s/it] 52%|    | 194/375 [04:42<04:23,  1.46s/it] 52%|    | 195/375 [04:43<04:22,  1.46s/it] 52%|    | 196/375 [04:44<04:21,  1.46s/it] 53%|    | 197/375 [04:46<04:19,  1.46s/it] 53%|    | 198/375 [04:47<04:18,  1.46s/it] 53%|    | 199/375 [04:49<04:16,  1.46s/it] 53%|    | 200/375 [04:50<04:15,  1.46s/it] 54%|    | 201/375 [04:52<04:14,  1.46s/it] 54%|    | 202/375 [04:53<04:12,  1.46s/it] 54%|    | 203/375 [04:55<04:10,  1.46s/it] 54%|    | 204/375 [04:56<04:09,  1.46s/it] 55%|    | 205/375 [04:58<04:08,  1.46s/it] 55%|    | 206/375 [04:59<04:06,  1.46s/it] 55%|    | 207/375 [05:01<04:05,  1.46s/it] 55%|    | 208/375 [05:02<04:03,  1.46s/it] 56%|    | 209/375 [05:03<04:02,  1.46s/it] 56%|    | 210/375 [05:05<04:00,  1.46s/it] 56%|    | 211/375 [05:06<03:59,  1.46s/it] 57%|    | 212/375 [05:08<03:57,  1.46s/it] 57%|    | 213/375 [05:09<03:56,  1.46s/it] 57%|    | 214/375 [05:11<03:55,  1.46s/it] 57%|    | 215/375 [05:12<03:53,  1.46s/it] 58%|    | 216/375 [05:14<03:52,  1.46s/it] 58%|    | 217/375 [05:15<03:50,  1.46s/it] 58%|    | 218/375 [05:17<03:49,  1.46s/it] 58%|    | 219/375 [05:18<03:47,  1.46s/it] 59%|    | 220/375 [05:20<03:46,  1.46s/it] 59%|    | 221/375 [05:21<03:44,  1.46s/it] 59%|    | 222/375 [05:22<03:43,  1.46s/it] 59%|    | 223/375 [05:24<03:42,  1.46s/it] 60%|    | 224/375 [05:25<03:40,  1.46s/it] 60%|    | 225/375 [05:27<03:39,  1.46s/it] 60%|    | 226/375 [05:28<03:37,  1.46s/it] 61%|    | 227/375 [05:30<03:35,  1.46s/it] 61%|    | 228/375 [05:31<03:34,  1.46s/it] 61%|    | 229/375 [05:33<03:33,  1.46s/it] 61%|   | 230/375 [05:34<03:31,  1.46s/it] 62%|   | 231/375 [05:36<03:30,  1.46s/it] 62%|   | 232/375 [05:37<03:28,  1.46s/it] 62%|   | 233/375 [05:39<03:27,  1.46s/it] 62%|   | 234/375 [05:40<03:25,  1.46s/it] 63%|   | 235/375 [05:41<03:24,  1.46s/it] 63%|   | 236/375 [05:43<03:22,  1.46s/it] 63%|   | 237/375 [05:44<03:21,  1.46s/it] 63%|   | 238/375 [05:46<03:20,  1.46s/it] 64%|   | 239/375 [05:47<03:18,  1.46s/it] 64%|   | 240/375 [05:49<03:16,  1.46s/it] 64%|   | 241/375 [05:50<03:15,  1.46s/it] 65%|   | 242/375 [05:52<03:14,  1.46s/it] 65%|   | 243/375 [05:53<03:12,  1.46s/it] 65%|   | 244/375 [05:55<03:11,  1.46s/it] 65%|   | 245/375 [05:56<03:09,  1.46s/it] 66%|   | 246/375 [05:57<03:08,  1.46s/it] 66%|   | 247/375 [05:59<03:06,  1.46s/it] 66%|   | 248/375 [06:00<03:05,  1.46s/it] 66%|   | 249/375 [06:02<03:03,  1.46s/it] 67%|   | 250/375 [06:03<03:02,  1.46s/it] 67%|   | 251/375 [06:05<03:00,  1.46s/it] 67%|   | 252/375 [06:06<02:59,  1.46s/it] 67%|   | 253/375 [06:08<02:58,  1.46s/it] 68%|   | 254/375 [06:09<02:56,  1.46s/it] 68%|   | 255/375 [06:11<02:55,  1.46s/it] 68%|   | 256/375 [06:12<02:53,  1.46s/it] 69%|   | 257/375 [06:14<02:52,  1.46s/it] 69%|   | 258/375 [06:15<02:50,  1.46s/it] 69%|   | 259/375 [06:16<02:49,  1.46s/it] 69%|   | 260/375 [06:18<02:47,  1.46s/it] 70%|   | 261/375 [06:19<02:46,  1.46s/it] 70%|   | 262/375 [06:21<02:45,  1.46s/it] 70%|   | 263/375 [06:22<02:43,  1.46s/it] 70%|   | 264/375 [06:24<02:42,  1.46s/it] 71%|   | 265/375 [06:25<02:40,  1.46s/it] 71%|   | 266/375 [06:27<02:38,  1.46s/it] 71%|   | 267/375 [06:28<02:37,  1.46s/it] 71%|  | 268/375 [06:30<02:36,  1.46s/it] 72%|  | 269/375 [06:31<02:34,  1.46s/it] 72%|  | 270/375 [06:33<02:33,  1.46s/it] 72%|  | 271/375 [06:34<02:31,  1.46s/it] 73%|  | 272/375 [06:35<02:30,  1.46s/it] 73%|  | 273/375 [06:37<02:29,  1.46s/it] 73%|  | 274/375 [06:38<02:27,  1.46s/it] 73%|  | 275/375 [06:40<02:26,  1.46s/it] 74%|  | 276/375 [06:41<02:24,  1.46s/it] 74%|  | 277/375 [06:43<02:23,  1.46s/it] 74%|  | 278/375 [06:44<02:21,  1.46s/it] 74%|  | 279/375 [06:46<02:20,  1.46s/it] 75%|  | 280/375 [06:47<02:18,  1.46s/it] 75%|  | 281/375 [06:49<02:17,  1.46s/it] 75%|  | 282/375 [06:50<02:15,  1.46s/it] 75%|  | 283/375 [06:52<02:14,  1.46s/it] 76%|  | 284/375 [06:53<02:12,  1.46s/it] 76%|  | 285/375 [06:54<02:11,  1.46s/it] 76%|  | 286/375 [06:56<02:09,  1.46s/it] 77%|  | 287/375 [06:57<02:08,  1.46s/it] 77%|  | 288/375 [06:59<02:06,  1.46s/it] 77%|  | 289/375 [07:00<02:05,  1.46s/it] 77%|  | 290/375 [07:02<02:04,  1.46s/it] 78%|  | 291/375 [07:03<02:02,  1.46s/it] 78%|  | 292/375 [07:05<02:01,  1.46s/it] 78%|  | 293/375 [07:06<01:59,  1.46s/it] 78%|  | 294/375 [07:08<01:58,  1.46s/it] 79%|  | 295/375 [07:09<01:56,  1.46s/it] 79%|  | 296/375 [07:10<01:55,  1.46s/it] 79%|  | 297/375 [07:12<01:53,  1.46s/it] 79%|  | 298/375 [07:13<01:52,  1.46s/it] 80%|  | 299/375 [07:15<01:50,  1.46s/it] 80%|  | 300/375 [07:16<01:49,  1.46s/it] 80%|  | 301/375 [07:18<01:47,  1.46s/it] 81%|  | 302/375 [07:19<01:46,  1.46s/it] 81%|  | 303/375 [07:21<01:44,  1.46s/it] 81%|  | 304/375 [07:22<01:43,  1.46s/it] 81%| | 305/375 [07:24<01:42,  1.46s/it] 82%| | 306/375 [07:25<01:40,  1.46s/it] 82%| | 307/375 [07:27<01:39,  1.46s/it] 82%| | 308/375 [07:28<01:37,  1.46s/it] 82%| | 309/375 [07:29<01:36,  1.46s/it] 83%| | 310/375 [07:31<01:34,  1.46s/it] 83%| | 311/375 [07:32<01:33,  1.46s/it] 83%| | 312/375 [07:34<01:31,  1.46s/it] 83%| | 313/375 [07:35<01:30,  1.46s/it] 84%| | 314/375 [07:37<01:28,  1.46s/it] 84%| | 315/375 [07:38<01:27,  1.46s/it] 84%| | 316/375 [07:40<01:25,  1.46s/it] 85%| | 317/375 [07:41<01:24,  1.46s/it] 85%| | 318/375 [07:43<01:23,  1.46s/it] 85%| | 319/375 [07:44<01:21,  1.46s/it] 85%| | 320/375 [07:45<01:20,  1.46s/it] 86%| | 321/375 [07:47<01:18,  1.46s/it] 86%| | 322/375 [07:48<01:17,  1.46s/it] 86%| | 323/375 [07:50<01:15,  1.46s/it] 86%| | 324/375 [07:51<01:14,  1.46s/it] 87%| | 325/375 [07:53<01:13,  1.46s/it] 87%| | 326/375 [07:54<01:11,  1.46s/it] 87%| | 327/375 [07:56<01:10,  1.46s/it] 87%| | 328/375 [07:57<01:08,  1.46s/it] 88%| | 329/375 [07:59<01:07,  1.46s/it] 88%| | 330/375 [08:00<01:05,  1.46s/it] 88%| | 331/375 [08:02<01:04,  1.46s/it] 89%| | 332/375 [08:03<01:02,  1.46s/it] 89%| | 333/375 [08:04<01:01,  1.46s/it] 89%| | 334/375 [08:06<00:59,  1.46s/it] 89%| | 335/375 [08:07<00:58,  1.46s/it] 90%| | 336/375 [08:09<00:56,  1.46s/it] 90%| | 337/375 [08:10<00:55,  1.46s/it] 90%| | 338/375 [08:12<00:54,  1.46s/it] 90%| | 339/375 [08:13<00:52,  1.46s/it] 91%| | 340/375 [08:15<00:51,  1.46s/it] 91%| | 341/375 [08:16<00:49,  1.46s/it] 91%| | 342/375 [08:18<00:48,  1.46s/it] 91%|| 343/375 [08:19<00:46,  1.46s/it] 92%|| 344/375 [08:20<00:45,  1.46s/it] 92%|| 345/375 [08:22<00:43,  1.46s/it] 92%|| 346/375 [08:23<00:42,  1.46s/it] 93%|| 347/375 [08:25<00:40,  1.46s/it] 93%|| 348/375 [08:26<00:39,  1.46s/it] 93%|| 349/375 [08:28<00:37,  1.46s/it] 93%|| 350/375 [08:29<00:36,  1.46s/it] 94%|| 351/375 [08:31<00:34,  1.46s/it] 94%|| 352/375 [08:32<00:33,  1.46s/it] 94%|| 353/375 [08:34<00:32,  1.46s/it] 94%|| 354/375 [08:35<00:30,  1.46s/it] 95%|| 355/375 [08:37<00:29,  1.46s/it] 95%|| 356/375 [08:38<00:27,  1.46s/it] 95%|| 357/375 [08:39<00:26,  1.46s/it] 95%|| 358/375 [08:41<00:24,  1.46s/it] 96%|| 359/375 [08:42<00:23,  1.46s/it] 96%|| 360/375 [08:44<00:21,  1.46s/it] 96%|| 361/375 [08:45<00:20,  1.46s/it] 97%|| 362/375 [08:47<00:18,  1.46s/it] 97%|| 363/375 [08:48<00:17,  1.46s/it] 97%|| 364/375 [08:50<00:16,  1.46s/it] 97%|| 365/375 [08:51<00:14,  1.46s/it] 98%|| 366/375 [08:53<00:13,  1.46s/it] 98%|| 367/375 [08:54<00:11,  1.46s/it] 98%|| 368/375 [08:55<00:10,  1.46s/it] 98%|| 369/375 [08:57<00:08,  1.46s/it] 99%|| 370/375 [08:58<00:07,  1.46s/it] 99%|| 371/375 [09:00<00:05,  1.46s/it] 99%|| 372/375 [09:01<00:04,  1.46s/it] 99%|| 373/375 [09:03<00:02,  1.46s/it]100%|| 374/375 [09:04<00:01,  1.46s/it]100%|| 375/375 [09:06<00:00,  1.46s/it][INFO|trainer.py:1988] 2024-02-16 22:57:30,070 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 546.3362, 'train_samples_per_second': 5.491, 'train_steps_per_second': 0.686, 'train_loss': 0.6634263102213541, 'epoch': 3.0}
                                                 100%|| 375/375 [09:06<00:00,  1.46s/it]100%|| 375/375 [09:06<00:00,  1.46s/it]
[INFO|trainer.py:2985] 2024-02-16 22:57:30,206 >> Saving model checkpoint to /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1
[INFO|configuration_utils.py:473] 2024-02-16 22:57:30,208 >> Configuration saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/config.json
[INFO|modeling_utils.py:2462] 2024-02-16 22:57:50,651 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2435] 2024-02-16 22:57:50,654 >> tokenizer config file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2444] 2024-02-16 22:57:50,655 >> Special tokens file saved in /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc256GPU1/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     0.6634
  train_runtime            = 0:09:06.33
  train_samples            =       1000
  train_samples_per_second =      5.491
  train_steps_per_second   =      0.686
02/16/2024 22:57:50 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:737] 2024-02-16 22:57:50,687 >> The following columns in the evaluation set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3291] 2024-02-16 22:57:50,689 >> ***** Running Evaluation *****
[INFO|trainer.py:3293] 2024-02-16 22:57:50,689 >>   Num examples = 408
[INFO|trainer.py:3296] 2024-02-16 22:57:50,689 >>   Batch size = 8
  0%|          | 0/51 [00:00<?, ?it/s]  4%|         | 2/51 [00:00<00:10,  4.49it/s]  6%|         | 3/51 [00:00<00:15,  3.16it/s]  8%|         | 4/51 [00:01<00:17,  2.74it/s] 10%|         | 5/51 [00:01<00:18,  2.54it/s] 12%|        | 6/51 [00:02<00:18,  2.43it/s] 14%|        | 7/51 [00:02<00:18,  2.37it/s] 16%|        | 8/51 [00:03<00:18,  2.32it/s] 18%|        | 9/51 [00:03<00:18,  2.29it/s] 20%|        | 10/51 [00:04<00:18,  2.28it/s] 22%|       | 11/51 [00:04<00:17,  2.26it/s] 24%|       | 12/51 [00:04<00:17,  2.25it/s] 25%|       | 13/51 [00:05<00:16,  2.25it/s] 27%|       | 14/51 [00:05<00:16,  2.24it/s] 29%|       | 15/51 [00:06<00:16,  2.24it/s] 31%|      | 16/51 [00:06<00:15,  2.24it/s] 33%|      | 17/51 [00:07<00:15,  2.22it/s] 35%|      | 18/51 [00:07<00:14,  2.22it/s] 37%|      | 19/51 [00:08<00:14,  2.22it/s] 39%|      | 20/51 [00:08<00:13,  2.23it/s] 41%|      | 21/51 [00:08<00:13,  2.23it/s] 43%|     | 22/51 [00:09<00:13,  2.23it/s] 45%|     | 23/51 [00:09<00:12,  2.23it/s] 47%|     | 24/51 [00:10<00:12,  2.23it/s] 49%|     | 25/51 [00:10<00:11,  2.23it/s] 51%|     | 26/51 [00:11<00:11,  2.23it/s] 53%|    | 27/51 [00:11<00:10,  2.23it/s] 55%|    | 28/51 [00:12<00:10,  2.23it/s] 57%|    | 29/51 [00:12<00:09,  2.23it/s] 59%|    | 30/51 [00:12<00:09,  2.23it/s] 61%|    | 31/51 [00:13<00:08,  2.23it/s] 63%|   | 32/51 [00:13<00:08,  2.23it/s] 65%|   | 33/51 [00:14<00:08,  2.23it/s] 67%|   | 34/51 [00:14<00:07,  2.23it/s] 69%|   | 35/51 [00:15<00:07,  2.23it/s] 71%|   | 36/51 [00:15<00:06,  2.23it/s] 73%|  | 37/51 [00:16<00:06,  2.23it/s] 75%|  | 38/51 [00:16<00:05,  2.23it/s] 76%|  | 39/51 [00:17<00:05,  2.23it/s] 78%|  | 40/51 [00:17<00:04,  2.23it/s] 80%|  | 41/51 [00:17<00:04,  2.23it/s] 82%| | 42/51 [00:18<00:04,  2.23it/s] 84%| | 43/51 [00:18<00:03,  2.23it/s] 86%| | 44/51 [00:19<00:03,  2.22it/s] 88%| | 45/51 [00:19<00:02,  2.22it/s] 90%| | 46/51 [00:20<00:02,  2.22it/s] 92%|| 47/51 [00:20<00:01,  2.22it/s] 94%|| 48/51 [00:21<00:01,  2.23it/s] 96%|| 49/51 [00:21<00:00,  2.23it/s] 98%|| 50/51 [00:21<00:00,  2.23it/s]100%|| 51/51 [00:22<00:00,  2.23it/s]100%|| 51/51 [00:22<00:00,  2.27it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.6765
  eval_combined_score     =     0.7264
  eval_f1                 =     0.7763
  eval_loss               =     1.1254
  eval_runtime            = 0:00:22.94
  eval_samples            =        408
  eval_samples_per_second =     17.782
  eval_steps_per_second   =      2.223
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
02/16/2024 22:59:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
02/16/2024 22:59:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512GPU1/runs/Feb16_22-59-00_v014.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512GPU1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/experiments/mrpc512GPU1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Overwrite dataset info from restored data version if exists.
02/16/2024 22:59:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:59:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
02/16/2024 22:59:02 - INFO - datasets.builder - Found cached dataset glue (/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)
Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
02/16/2024 22:59:02 - INFO - datasets.info - Loading Dataset info from /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c
[INFO|configuration_utils.py:729] 2024-02-16 22:59:02,433 >> loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
[INFO|configuration_utils.py:792] 2024-02-16 22:59:02,748 >> Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mrpc",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:59:02,780 >> loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:59:02,780 >> loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:59:02,780 >> loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:59:02,781 >> loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
[INFO|tokenization_utils_base.py:2029] 2024-02-16 22:59:02,781 >> loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-16 22:59:02,837 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:3259] 2024-02-16 22:59:02,893 >> loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
[INFO|modeling_utils.py:3984] 2024-02-16 22:59:05,833 >> Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3996] 2024-02-16 22:59:05,833 >> Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-59334ae9b622ed60.arrow
02/16/2024 22:59:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-59334ae9b622ed60.arrow
Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8b6beb64f7364ca0.arrow
02/16/2024 22:59:06 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8b6beb64f7364ca0.arrow
Running tokenizer on dataset:   0%|          | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ccc77fe48f8e4be6.arrow
02/16/2024 22:59:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ccc77fe48f8e4be6.arrow
Running tokenizer on dataset:  58%|    | 1000/1725 [00:00<00:00, 3361.90 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 3393.14 examples/s]Running tokenizer on dataset: 100%|| 1725/1725 [00:00<00:00, 3332.50 examples/s]
02/16/2024 22:59:06 - INFO - __main__ - Sample 654 of the training set: {'sentence1': 'The airline says only Robert Milton is taking a 15-per-cent reduction in pay .', 'sentence2': "The airline 's pilots , for example , are taking a 15-per-cent cut .", 'label': 0, 'idx': 736, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 450, 4799, 1220, 4083, 871, 4755, 3833, 880, 338, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 20376, 297, 5146, 869, 1, 450, 4799, 1220, 525, 29879, 8230, 1862, 1919, 363, 1342, 1919, 526, 5622, 263, 29871, 29896, 29945, 29899, 546, 29899, 1760, 5700, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:59:06 - INFO - __main__ - Sample 114 of the training set: {'sentence1': 'Police warned residents on Friday not to travel alone and to avoid convenience stores and service stations stores at night .', 'sentence2': 'Police advised residents Friday not to travel alone to convenience stores and to be watchful .', 'label': 0, 'idx': 131, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 18923, 1370, 9571, 24060, 373, 28728, 451, 304, 9850, 7432, 322, 304, 4772, 29703, 14422, 322, 2669, 16355, 14422, 472, 4646, 869, 1, 18923, 594, 11292, 24060, 28728, 451, 304, 9850, 7432, 304, 29703, 14422, 322, 304, 367, 6505, 1319, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:59:06 - INFO - __main__ - Sample 25 of the training set: {'sentence1': 'I wanted to bring the most beautiful people into the most beautiful building , he said Sunday inside the Grand Central concourse .', 'sentence2': '" I wanted to bring the most beautiful people into the most beautiful building , " Tunick said Sunday .', 'label': 1, 'idx': 28, 'input_ids': [32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 32004, 1, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 540, 1497, 16340, 2768, 278, 6265, 8068, 3022, 10242, 869, 1, 376, 306, 5131, 304, 6963, 278, 1556, 9560, 2305, 964, 278, 1556, 9560, 5214, 1919, 376, 21072, 860, 1497, 16340, 869], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
02/16/2024 22:59:06 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:737] 2024-02-16 22:59:08,479 >> The following columns in the training set don't have a corresponding argument in `LlamaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `LlamaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1747] 2024-02-16 22:59:08,796 >> ***** Running training *****
[INFO|trainer.py:1748] 2024-02-16 22:59:08,796 >>   Num examples = 1,000
[INFO|trainer.py:1749] 2024-02-16 22:59:08,796 >>   Num Epochs = 3
[INFO|trainer.py:1750] 2024-02-16 22:59:08,796 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1753] 2024-02-16 22:59:08,796 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1754] 2024-02-16 22:59:08,796 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1755] 2024-02-16 22:59:08,797 >>   Total optimization steps = 375
[INFO|trainer.py:1756] 2024-02-16 22:59:08,797 >>   Number of trainable parameters = 1,280,153,600
  0%|          | 0/375 [00:00<?, ?it/s][rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/375 [00:03<22:43,  3.64s/it]Traceback (most recent call last):
  File "run_glue.py", line 656, in <module>
    main()
  File "run_glue.py", line 564, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1561, in train
    return inner_training_loop(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 1895, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2821, in training_step
    loss = self.compute_loss(model, inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/trainer.py", line 2844, in compute_loss
    outputs = model(**inputs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1314, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 985, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 746, in forward
    hidden_states = self.mlp(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 218, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 17.31 MiB is free. Including non-PyTorch memory, this process has 31.72 GiB memory in use. Of the allocated memory 29.89 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/375 [00:04<27:17,  4.38s/it]
[2024-02-16 22:59:16,350] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 67296) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-16_22:59:16
  host      : v014.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 67296)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v014: task 0: Exited with exit code 1
