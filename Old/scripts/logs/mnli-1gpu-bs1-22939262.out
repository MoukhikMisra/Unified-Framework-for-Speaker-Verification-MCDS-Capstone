Wed Mar 13 23:28:57 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0              41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
/ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts
START TIME: Wed Mar 13 23:28:58 EDT 2024
MASTER_ADDR=v021 MASTER_PORT=6013 NUM_PROCESSES=1 GPUS_PER_NODE=1 NNODES=1 $SLURM_PROCID
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 23:29:10 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "mnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/392702 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 1000/392702 [00:00<01:01, 6330.05 examples/s]Running tokenizer on dataset:   1%|          | 2000/392702 [00:00<00:55, 7049.38 examples/s]Running tokenizer on dataset:   1%|          | 3000/392702 [00:00<00:56, 6916.54 examples/s]Running tokenizer on dataset:   1%|          | 4000/392702 [00:00<00:51, 7498.63 examples/s]Running tokenizer on dataset:   1%|▏         | 5000/392702 [00:00<00:48, 7940.74 examples/s]Running tokenizer on dataset:   2%|▏         | 7000/392702 [00:00<00:44, 8756.39 examples/s]Running tokenizer on dataset:   2%|▏         | 9000/392702 [00:01<00:40, 9427.39 examples/s]Running tokenizer on dataset:   3%|▎         | 11000/392702 [00:01<00:38, 9884.42 examples/s]Running tokenizer on dataset:   3%|▎         | 13000/392702 [00:01<00:37, 10124.77 examples/s]Running tokenizer on dataset:   4%|▍         | 15000/392702 [00:01<00:36, 10365.73 examples/s]Running tokenizer on dataset:   4%|▍         | 17000/392702 [00:01<00:35, 10507.77 examples/s]Running tokenizer on dataset:   5%|▍         | 19000/392702 [00:01<00:35, 10568.53 examples/s]Running tokenizer on dataset:   5%|▌         | 21000/392702 [00:02<00:34, 10657.03 examples/s]Running tokenizer on dataset:   6%|▌         | 23000/392702 [00:02<00:34, 10659.54 examples/s]Running tokenizer on dataset:   6%|▋         | 25000/392702 [00:02<00:40, 8982.34 examples/s] Running tokenizer on dataset:   7%|▋         | 27000/392702 [00:02<00:39, 9305.03 examples/s]Running tokenizer on dataset:   7%|▋         | 29000/392702 [00:03<00:38, 9453.70 examples/s]Running tokenizer on dataset:   8%|▊         | 30000/392702 [00:03<00:38, 9499.82 examples/s]Running tokenizer on dataset:   8%|▊         | 31000/392702 [00:03<00:38, 9406.88 examples/s]Running tokenizer on dataset:   8%|▊         | 32000/392702 [00:03<00:38, 9373.98 examples/s]Running tokenizer on dataset:   8%|▊         | 33000/392702 [00:03<00:38, 9315.97 examples/s]Running tokenizer on dataset:   9%|▊         | 34000/392702 [00:03<00:38, 9206.14 examples/s]Running tokenizer on dataset:   9%|▉         | 35000/392702 [00:03<00:38, 9266.82 examples/s]Running tokenizer on dataset:   9%|▉         | 36000/392702 [00:03<00:38, 9197.58 examples/s]Running tokenizer on dataset:   9%|▉         | 37000/392702 [00:03<00:38, 9261.73 examples/s]Running tokenizer on dataset:  10%|▉         | 38000/392702 [00:04<00:38, 9255.27 examples/s]Running tokenizer on dataset:  10%|▉         | 39000/392702 [00:04<00:38, 9243.73 examples/s]Running tokenizer on dataset:  10%|█         | 40000/392702 [00:04<00:37, 9283.87 examples/s]Running tokenizer on dataset:  10%|█         | 41000/392702 [00:04<00:38, 9226.90 examples/s]Running tokenizer on dataset:  11%|█         | 42000/392702 [00:04<00:38, 9185.68 examples/s]Running tokenizer on dataset:  11%|█         | 43000/392702 [00:04<00:38, 9163.58 examples/s]Running tokenizer on dataset:  11%|█         | 44000/392702 [00:04<00:37, 9261.77 examples/s]Running tokenizer on dataset:  11%|█▏        | 45000/392702 [00:04<00:38, 9143.65 examples/s]Running tokenizer on dataset:  12%|█▏        | 46000/392702 [00:04<00:38, 9074.69 examples/s]Running tokenizer on dataset:  12%|█▏        | 47000/392702 [00:05<00:37, 9200.60 examples/s]Running tokenizer on dataset:  12%|█▏        | 48000/392702 [00:05<00:37, 9212.42 examples/s]Running tokenizer on dataset:  12%|█▏        | 49000/392702 [00:05<00:37, 9164.22 examples/s]Running tokenizer on dataset:  13%|█▎        | 50000/392702 [00:05<00:49, 6977.98 examples/s]Running tokenizer on dataset:  13%|█▎        | 51000/392702 [00:05<00:45, 7558.45 examples/s]Running tokenizer on dataset:  13%|█▎        | 52000/392702 [00:05<00:42, 8019.94 examples/s]Running tokenizer on dataset:  13%|█▎        | 53000/392702 [00:05<00:40, 8336.08 examples/s]Running tokenizer on dataset:  14%|█▍        | 54000/392702 [00:05<00:39, 8545.32 examples/s]Running tokenizer on dataset:  14%|█▍        | 55000/392702 [00:06<00:38, 8698.01 examples/s]Running tokenizer on dataset:  14%|█▍        | 56000/392702 [00:06<00:37, 8871.27 examples/s]Running tokenizer on dataset:  15%|█▍        | 57000/392702 [00:06<00:37, 8929.07 examples/s]Running tokenizer on dataset:  15%|█▍        | 58000/392702 [00:06<00:36, 9055.25 examples/s]Running tokenizer on dataset:  15%|█▌        | 60000/392702 [00:06<00:34, 9715.64 examples/s]Running tokenizer on dataset:  16%|█▌        | 62000/392702 [00:06<00:32, 10116.36 examples/s]Running tokenizer on dataset:  16%|█▋        | 64000/392702 [00:06<00:32, 10212.93 examples/s]Running tokenizer on dataset:  17%|█▋        | 66000/392702 [00:07<00:31, 10331.79 examples/s]Running tokenizer on dataset:  17%|█▋        | 68000/392702 [00:07<00:31, 10397.61 examples/s]Running tokenizer on dataset:  18%|█▊        | 70000/392702 [00:07<00:30, 10509.76 examples/s]Running tokenizer on dataset:  18%|█▊        | 72000/392702 [00:07<00:30, 10532.16 examples/s]Running tokenizer on dataset:  19%|█▉        | 74000/392702 [00:07<00:30, 10540.54 examples/s]Running tokenizer on dataset:  19%|█▉        | 76000/392702 [00:08<00:35, 8961.53 examples/s] Running tokenizer on dataset:  20%|█▉        | 78000/392702 [00:08<00:33, 9368.28 examples/s]Running tokenizer on dataset:  20%|██        | 80000/392702 [00:08<00:32, 9736.52 examples/s]Running tokenizer on dataset:  21%|██        | 82000/392702 [00:08<00:31, 9976.59 examples/s]Running tokenizer on dataset:  21%|██▏       | 84000/392702 [00:08<00:30, 10098.29 examples/s]Running tokenizer on dataset:  22%|██▏       | 86000/392702 [00:09<00:30, 10194.73 examples/s]Running tokenizer on dataset:  22%|██▏       | 88000/392702 [00:09<00:30, 9929.81 examples/s] Running tokenizer on dataset:  23%|██▎       | 90000/392702 [00:09<00:31, 9645.85 examples/s]Running tokenizer on dataset:  23%|██▎       | 91000/392702 [00:09<00:31, 9568.26 examples/s]Running tokenizer on dataset:  23%|██▎       | 92000/392702 [00:09<00:31, 9541.53 examples/s]Running tokenizer on dataset:  24%|██▎       | 93000/392702 [00:09<00:31, 9506.72 examples/s]Running tokenizer on dataset:  24%|██▍       | 94000/392702 [00:09<00:31, 9528.23 examples/s]Running tokenizer on dataset:  24%|██▍       | 95000/392702 [00:10<00:31, 9434.91 examples/s]Running tokenizer on dataset:  24%|██▍       | 96000/392702 [00:10<00:31, 9301.65 examples/s]Running tokenizer on dataset:  25%|██▍       | 97000/392702 [00:10<00:31, 9294.39 examples/s]Running tokenizer on dataset:  25%|██▍       | 98000/392702 [00:10<00:31, 9333.94 examples/s]Running tokenizer on dataset:  25%|██▌       | 99000/392702 [00:10<00:31, 9319.92 examples/s]Running tokenizer on dataset:  25%|██▌       | 100000/392702 [00:10<00:31, 9262.73 examples/s]Running tokenizer on dataset:  26%|██▌       | 101000/392702 [00:10<00:41, 7068.52 examples/s]Running tokenizer on dataset:  26%|██▌       | 102000/392702 [00:10<00:38, 7644.36 examples/s]Running tokenizer on dataset:  26%|██▌       | 103000/392702 [00:11<00:35, 8084.17 examples/s]Running tokenizer on dataset:  26%|██▋       | 104000/392702 [00:11<00:34, 8357.11 examples/s]Running tokenizer on dataset:  27%|██▋       | 105000/392702 [00:11<00:33, 8609.93 examples/s]Running tokenizer on dataset:  27%|██▋       | 106000/392702 [00:11<00:32, 8775.93 examples/s]Running tokenizer on dataset:  27%|██▋       | 107000/392702 [00:11<00:32, 8805.29 examples/s]Running tokenizer on dataset:  28%|██▊       | 108000/392702 [00:11<00:31, 8922.91 examples/s]Running tokenizer on dataset:  28%|██▊       | 109000/392702 [00:11<00:31, 9075.45 examples/s]Running tokenizer on dataset:  28%|██▊       | 110000/392702 [00:11<00:30, 9177.48 examples/s]Running tokenizer on dataset:  28%|██▊       | 111000/392702 [00:11<00:30, 9207.34 examples/s]Running tokenizer on dataset:  29%|██▊       | 112000/392702 [00:12<00:30, 9194.82 examples/s]Running tokenizer on dataset:  29%|██▉       | 113000/392702 [00:12<00:30, 9228.79 examples/s]Running tokenizer on dataset:  29%|██▉       | 114000/392702 [00:12<00:30, 9204.28 examples/s]Running tokenizer on dataset:  29%|██▉       | 115000/392702 [00:12<00:30, 9164.71 examples/s]Running tokenizer on dataset:  30%|██▉       | 116000/392702 [00:12<00:29, 9247.86 examples/s]Running tokenizer on dataset:  30%|██▉       | 117000/392702 [00:12<00:29, 9260.87 examples/s]Running tokenizer on dataset:  30%|███       | 119000/392702 [00:12<00:28, 9751.29 examples/s]Running tokenizer on dataset:  31%|███       | 121000/392702 [00:12<00:27, 10049.94 examples/s]Running tokenizer on dataset:  31%|███▏      | 123000/392702 [00:13<00:26, 10273.70 examples/s]Running tokenizer on dataset:  32%|███▏      | 125000/392702 [00:13<00:25, 10410.21 examples/s]Running tokenizer on dataset:  32%|███▏      | 127000/392702 [00:13<00:30, 8834.82 examples/s] Running tokenizer on dataset:  33%|███▎      | 129000/392702 [00:13<00:28, 9332.71 examples/s]Running tokenizer on dataset:  33%|███▎      | 131000/392702 [00:13<00:27, 9676.68 examples/s]Running tokenizer on dataset:  34%|███▍      | 133000/392702 [00:14<00:26, 9888.50 examples/s]Running tokenizer on dataset:  34%|███▍      | 135000/392702 [00:14<00:25, 10085.61 examples/s]Running tokenizer on dataset:  35%|███▍      | 137000/392702 [00:14<00:25, 10217.07 examples/s]Running tokenizer on dataset:  35%|███▌      | 139000/392702 [00:14<00:24, 10338.84 examples/s]Running tokenizer on dataset:  36%|███▌      | 141000/392702 [00:14<00:24, 10284.57 examples/s]Running tokenizer on dataset:  36%|███▋      | 143000/392702 [00:15<00:24, 10208.00 examples/s]Running tokenizer on dataset:  37%|███▋      | 145000/392702 [00:15<00:24, 10047.79 examples/s]Running tokenizer on dataset:  37%|███▋      | 147000/392702 [00:15<00:24, 9886.60 examples/s] Running tokenizer on dataset:  38%|███▊      | 148000/392702 [00:15<00:25, 9747.15 examples/s]Running tokenizer on dataset:  38%|███▊      | 149000/392702 [00:15<00:25, 9623.00 examples/s]Running tokenizer on dataset:  38%|███▊      | 150000/392702 [00:15<00:25, 9617.67 examples/s]Running tokenizer on dataset:  38%|███▊      | 151000/392702 [00:15<00:25, 9517.56 examples/s]Running tokenizer on dataset:  39%|███▊      | 152000/392702 [00:16<00:32, 7399.09 examples/s]Running tokenizer on dataset:  39%|███▉      | 153000/392702 [00:16<00:30, 7837.35 examples/s]Running tokenizer on dataset:  39%|███▉      | 154000/392702 [00:16<00:29, 8146.21 examples/s]Running tokenizer on dataset:  39%|███▉      | 155000/392702 [00:16<00:28, 8395.48 examples/s]Running tokenizer on dataset:  40%|███▉      | 156000/392702 [00:16<00:27, 8608.16 examples/s]Running tokenizer on dataset:  40%|███▉      | 157000/392702 [00:16<00:26, 8747.15 examples/s]Running tokenizer on dataset:  40%|████      | 158000/392702 [00:16<00:26, 8914.02 examples/s]Running tokenizer on dataset:  40%|████      | 159000/392702 [00:16<00:26, 8972.45 examples/s]Running tokenizer on dataset:  41%|████      | 160000/392702 [00:17<00:25, 9048.99 examples/s]Running tokenizer on dataset:  41%|████      | 161000/392702 [00:17<00:25, 9122.15 examples/s]Running tokenizer on dataset:  41%|████▏     | 162000/392702 [00:17<00:25, 9098.22 examples/s]Running tokenizer on dataset:  42%|████▏     | 163000/392702 [00:17<00:25, 9113.88 examples/s]Running tokenizer on dataset:  42%|████▏     | 164000/392702 [00:17<00:24, 9159.33 examples/s]Running tokenizer on dataset:  42%|████▏     | 165000/392702 [00:17<00:24, 9196.53 examples/s]Running tokenizer on dataset:  42%|████▏     | 166000/392702 [00:17<00:24, 9211.36 examples/s]Running tokenizer on dataset:  43%|████▎     | 167000/392702 [00:17<00:24, 9191.90 examples/s]Running tokenizer on dataset:  43%|████▎     | 168000/392702 [00:17<00:24, 9342.83 examples/s]Running tokenizer on dataset:  43%|████▎     | 169000/392702 [00:18<00:24, 9264.91 examples/s]Running tokenizer on dataset:  43%|████▎     | 170000/392702 [00:18<00:24, 9278.61 examples/s]Running tokenizer on dataset:  44%|████▎     | 171000/392702 [00:18<00:23, 9329.08 examples/s]Running tokenizer on dataset:  44%|████▍     | 172000/392702 [00:18<00:23, 9299.59 examples/s]Running tokenizer on dataset:  44%|████▍     | 173000/392702 [00:18<00:23, 9289.30 examples/s]Running tokenizer on dataset:  44%|████▍     | 174000/392702 [00:18<00:23, 9319.77 examples/s]Running tokenizer on dataset:  45%|████▍     | 175000/392702 [00:18<00:23, 9390.42 examples/s]Running tokenizer on dataset:  45%|████▌     | 177000/392702 [00:18<00:27, 7935.32 examples/s]Running tokenizer on dataset:  45%|████▌     | 178000/392702 [00:19<00:25, 8345.03 examples/s]Running tokenizer on dataset:  46%|████▌     | 180000/392702 [00:19<00:23, 9074.85 examples/s]Running tokenizer on dataset:  46%|████▋     | 182000/392702 [00:19<00:22, 9571.38 examples/s]Running tokenizer on dataset:  47%|████▋     | 184000/392702 [00:19<00:20, 9968.30 examples/s]Running tokenizer on dataset:  47%|████▋     | 186000/392702 [00:19<00:20, 10327.27 examples/s]Running tokenizer on dataset:  48%|████▊     | 188000/392702 [00:20<00:19, 10435.86 examples/s]Running tokenizer on dataset:  48%|████▊     | 190000/392702 [00:20<00:19, 10499.54 examples/s]Running tokenizer on dataset:  49%|████▉     | 192000/392702 [00:20<00:18, 10566.97 examples/s]Running tokenizer on dataset:  49%|████▉     | 194000/392702 [00:20<00:18, 10601.46 examples/s]Running tokenizer on dataset:  50%|████▉     | 196000/392702 [00:20<00:18, 10585.87 examples/s]Running tokenizer on dataset:  50%|█████     | 198000/392702 [00:20<00:18, 10442.00 examples/s]Running tokenizer on dataset:  51%|█████     | 200000/392702 [00:21<00:18, 10311.71 examples/s]Running tokenizer on dataset:  51%|█████▏    | 202000/392702 [00:21<00:19, 9977.66 examples/s] Running tokenizer on dataset:  52%|█████▏    | 204000/392702 [00:21<00:22, 8329.93 examples/s]Running tokenizer on dataset:  52%|█████▏    | 205000/392702 [00:21<00:22, 8473.00 examples/s]Running tokenizer on dataset:  52%|█████▏    | 206000/392702 [00:21<00:21, 8649.63 examples/s]Running tokenizer on dataset:  53%|█████▎    | 207000/392702 [00:22<00:21, 8763.00 examples/s]Running tokenizer on dataset:  53%|█████▎    | 208000/392702 [00:22<00:20, 8835.70 examples/s]Running tokenizer on dataset:  53%|█████▎    | 209000/392702 [00:22<00:20, 8936.10 examples/s]Running tokenizer on dataset:  53%|█████▎    | 210000/392702 [00:22<00:20, 8986.21 examples/s]Running tokenizer on dataset:  54%|█████▎    | 211000/392702 [00:22<00:20, 9037.24 examples/s]Running tokenizer on dataset:  54%|█████▍    | 212000/392702 [00:22<00:19, 9068.52 examples/s]Running tokenizer on dataset:  54%|█████▍    | 213000/392702 [00:22<00:19, 9138.90 examples/s]Running tokenizer on dataset:  54%|█████▍    | 214000/392702 [00:22<00:19, 9124.80 examples/s]Running tokenizer on dataset:  55%|█████▍    | 215000/392702 [00:22<00:19, 9141.13 examples/s]Running tokenizer on dataset:  55%|█████▌    | 216000/392702 [00:23<00:19, 9122.09 examples/s]Running tokenizer on dataset:  55%|█████▌    | 217000/392702 [00:23<00:19, 9119.19 examples/s]Running tokenizer on dataset:  56%|█████▌    | 218000/392702 [00:23<00:19, 9146.11 examples/s]Running tokenizer on dataset:  56%|█████▌    | 219000/392702 [00:23<00:18, 9181.02 examples/s]Running tokenizer on dataset:  56%|█████▌    | 220000/392702 [00:23<00:18, 9148.67 examples/s]Running tokenizer on dataset:  56%|█████▋    | 221000/392702 [00:23<00:18, 9157.96 examples/s]Running tokenizer on dataset:  57%|█████▋    | 222000/392702 [00:23<00:18, 9164.32 examples/s]Running tokenizer on dataset:  57%|█████▋    | 223000/392702 [00:23<00:18, 9153.09 examples/s]Running tokenizer on dataset:  57%|█████▋    | 224000/392702 [00:23<00:18, 9173.96 examples/s]Running tokenizer on dataset:  57%|█████▋    | 225000/392702 [00:24<00:18, 9200.85 examples/s]Running tokenizer on dataset:  58%|█████▊    | 226000/392702 [00:24<00:18, 9181.84 examples/s]Running tokenizer on dataset:  58%|█████▊    | 227000/392702 [00:24<00:18, 9124.97 examples/s]Running tokenizer on dataset:  58%|█████▊    | 228000/392702 [00:24<00:23, 6952.09 examples/s]Running tokenizer on dataset:  58%|█████▊    | 229000/392702 [00:24<00:21, 7464.01 examples/s]Running tokenizer on dataset:  59%|█████▊    | 230000/392702 [00:24<00:20, 7895.63 examples/s]Running tokenizer on dataset:  59%|█████▉    | 231000/392702 [00:24<00:19, 8423.29 examples/s]Running tokenizer on dataset:  59%|█████▉    | 233000/392702 [00:24<00:17, 9287.52 examples/s]Running tokenizer on dataset:  60%|█████▉    | 235000/392702 [00:25<00:15, 9867.73 examples/s]Running tokenizer on dataset:  60%|██████    | 237000/392702 [00:25<00:15, 10101.58 examples/s]Running tokenizer on dataset:  61%|██████    | 239000/392702 [00:25<00:14, 10315.37 examples/s]Running tokenizer on dataset:  61%|██████▏   | 241000/392702 [00:25<00:14, 10481.64 examples/s]Running tokenizer on dataset:  62%|██████▏   | 243000/392702 [00:25<00:14, 10572.10 examples/s]Running tokenizer on dataset:  62%|██████▏   | 245000/392702 [00:26<00:13, 10648.29 examples/s]Running tokenizer on dataset:  63%|██████▎   | 247000/392702 [00:26<00:13, 10645.68 examples/s]Running tokenizer on dataset:  63%|██████▎   | 249000/392702 [00:26<00:13, 10632.45 examples/s]Running tokenizer on dataset:  64%|██████▍   | 251000/392702 [00:26<00:13, 10557.43 examples/s]Running tokenizer on dataset:  64%|██████▍   | 253000/392702 [00:26<00:13, 10642.44 examples/s]Running tokenizer on dataset:  65%|██████▍   | 255000/392702 [00:27<00:15, 8984.20 examples/s] Running tokenizer on dataset:  65%|██████▌   | 257000/392702 [00:27<00:14, 9277.82 examples/s]Running tokenizer on dataset:  66%|██████▌   | 258000/392702 [00:27<00:14, 9302.83 examples/s]Running tokenizer on dataset:  66%|██████▌   | 259000/392702 [00:27<00:14, 9311.89 examples/s]Running tokenizer on dataset:  66%|██████▌   | 260000/392702 [00:27<00:14, 9220.43 examples/s]Running tokenizer on dataset:  66%|██████▋   | 261000/392702 [00:27<00:14, 9206.32 examples/s]Running tokenizer on dataset:  67%|██████▋   | 262000/392702 [00:27<00:14, 9074.67 examples/s]Running tokenizer on dataset:  67%|██████▋   | 263000/392702 [00:27<00:14, 9066.74 examples/s]Running tokenizer on dataset:  67%|██████▋   | 264000/392702 [00:28<00:14, 9101.34 examples/s]Running tokenizer on dataset:  67%|██████▋   | 265000/392702 [00:28<00:14, 9083.60 examples/s]Running tokenizer on dataset:  68%|██████▊   | 266000/392702 [00:28<00:13, 9139.16 examples/s]Running tokenizer on dataset:  68%|██████▊   | 267000/392702 [00:28<00:13, 9132.42 examples/s]Running tokenizer on dataset:  68%|██████▊   | 268000/392702 [00:28<00:13, 9179.73 examples/s]Running tokenizer on dataset:  68%|██████▊   | 269000/392702 [00:28<00:13, 9156.32 examples/s]Running tokenizer on dataset:  69%|██████▉   | 270000/392702 [00:28<00:13, 9204.37 examples/s]Running tokenizer on dataset:  69%|██████▉   | 271000/392702 [00:28<00:13, 9213.73 examples/s]Running tokenizer on dataset:  69%|██████▉   | 272000/392702 [00:28<00:12, 9355.77 examples/s]Running tokenizer on dataset:  70%|██████▉   | 273000/392702 [00:29<00:12, 9347.24 examples/s]Running tokenizer on dataset:  70%|██████▉   | 274000/392702 [00:29<00:12, 9388.57 examples/s]Running tokenizer on dataset:  70%|███████   | 275000/392702 [00:29<00:12, 9320.39 examples/s]Running tokenizer on dataset:  70%|███████   | 276000/392702 [00:29<00:12, 9244.39 examples/s]Running tokenizer on dataset:  71%|███████   | 277000/392702 [00:29<00:12, 9210.80 examples/s]Running tokenizer on dataset:  71%|███████   | 278000/392702 [00:29<00:12, 9210.54 examples/s]Running tokenizer on dataset:  71%|███████   | 279000/392702 [00:29<00:17, 6591.77 examples/s]Running tokenizer on dataset:  71%|███████▏  | 280000/392702 [00:29<00:15, 7219.25 examples/s]Running tokenizer on dataset:  72%|███████▏  | 281000/392702 [00:30<00:14, 7707.17 examples/s]Running tokenizer on dataset:  72%|███████▏  | 282000/392702 [00:30<00:13, 8052.13 examples/s]Running tokenizer on dataset:  72%|███████▏  | 283000/392702 [00:30<00:13, 8318.93 examples/s]Running tokenizer on dataset:  72%|███████▏  | 284000/392702 [00:30<00:12, 8604.27 examples/s]Running tokenizer on dataset:  73%|███████▎  | 285000/392702 [00:30<00:12, 8719.75 examples/s]Running tokenizer on dataset:  73%|███████▎  | 286000/392702 [00:30<00:12, 8863.97 examples/s]Running tokenizer on dataset:  73%|███████▎  | 287000/392702 [00:30<00:11, 8973.89 examples/s]Running tokenizer on dataset:  73%|███████▎  | 288000/392702 [00:30<00:11, 9114.51 examples/s]Running tokenizer on dataset:  74%|███████▎  | 289000/392702 [00:30<00:11, 9156.71 examples/s]Running tokenizer on dataset:  74%|███████▍  | 291000/392702 [00:31<00:10, 9766.67 examples/s]Running tokenizer on dataset:  75%|███████▍  | 293000/392702 [00:31<00:09, 10094.81 examples/s]Running tokenizer on dataset:  75%|███████▌  | 295000/392702 [00:31<00:09, 10292.08 examples/s]Running tokenizer on dataset:  76%|███████▌  | 297000/392702 [00:31<00:09, 10436.62 examples/s]Running tokenizer on dataset:  76%|███████▌  | 299000/392702 [00:31<00:08, 10493.14 examples/s]Running tokenizer on dataset:  77%|███████▋  | 301000/392702 [00:32<00:08, 10503.69 examples/s]Running tokenizer on dataset:  77%|███████▋  | 303000/392702 [00:32<00:08, 10538.20 examples/s]Running tokenizer on dataset:  78%|███████▊  | 305000/392702 [00:32<00:09, 8996.07 examples/s] Running tokenizer on dataset:  78%|███████▊  | 307000/392702 [00:32<00:09, 9320.73 examples/s]Running tokenizer on dataset:  79%|███████▊  | 309000/392702 [00:32<00:08, 9573.83 examples/s]Running tokenizer on dataset:  79%|███████▉  | 310000/392702 [00:33<00:08, 9638.35 examples/s]Running tokenizer on dataset:  79%|███████▉  | 312000/392702 [00:33<00:08, 9906.75 examples/s]Running tokenizer on dataset:  80%|███████▉  | 314000/392702 [00:33<00:08, 9770.19 examples/s]Running tokenizer on dataset:  80%|████████  | 315000/392702 [00:33<00:08, 9647.38 examples/s]Running tokenizer on dataset:  80%|████████  | 316000/392702 [00:33<00:08, 9550.97 examples/s]Running tokenizer on dataset:  81%|████████  | 317000/392702 [00:33<00:07, 9464.75 examples/s]Running tokenizer on dataset:  81%|████████  | 318000/392702 [00:33<00:08, 9270.37 examples/s]Running tokenizer on dataset:  81%|████████  | 319000/392702 [00:34<00:08, 9158.88 examples/s]Running tokenizer on dataset:  81%|████████▏ | 320000/392702 [00:34<00:07, 9161.35 examples/s]Running tokenizer on dataset:  82%|████████▏ | 321000/392702 [00:34<00:07, 9068.39 examples/s]Running tokenizer on dataset:  82%|████████▏ | 322000/392702 [00:34<00:07, 9092.91 examples/s]Running tokenizer on dataset:  82%|████████▏ | 323000/392702 [00:34<00:07, 9089.92 examples/s]Running tokenizer on dataset:  83%|████████▎ | 324000/392702 [00:34<00:07, 9098.25 examples/s]Running tokenizer on dataset:  83%|████████▎ | 325000/392702 [00:34<00:07, 9073.31 examples/s]Running tokenizer on dataset:  83%|████████▎ | 326000/392702 [00:34<00:07, 9149.94 examples/s]Running tokenizer on dataset:  83%|████████▎ | 327000/392702 [00:34<00:07, 9134.63 examples/s]Running tokenizer on dataset:  84%|████████▎ | 328000/392702 [00:35<00:07, 9178.66 examples/s]Running tokenizer on dataset:  84%|████████▍ | 329000/392702 [00:35<00:06, 9138.19 examples/s]Running tokenizer on dataset:  84%|████████▍ | 330000/392702 [00:35<00:08, 6974.47 examples/s]Running tokenizer on dataset:  84%|████████▍ | 331000/392702 [00:35<00:08, 7500.36 examples/s]Running tokenizer on dataset:  85%|████████▍ | 332000/392702 [00:35<00:07, 7894.84 examples/s]Running tokenizer on dataset:  85%|████████▍ | 333000/392702 [00:35<00:07, 8254.90 examples/s]Running tokenizer on dataset:  85%|████████▌ | 334000/392702 [00:35<00:06, 8411.70 examples/s]Running tokenizer on dataset:  85%|████████▌ | 335000/392702 [00:35<00:06, 8548.85 examples/s]Running tokenizer on dataset:  86%|████████▌ | 336000/392702 [00:36<00:06, 8760.72 examples/s]Running tokenizer on dataset:  86%|████████▌ | 337000/392702 [00:36<00:06, 8826.29 examples/s]Running tokenizer on dataset:  86%|████████▌ | 338000/392702 [00:36<00:06, 8985.50 examples/s]Running tokenizer on dataset:  86%|████████▋ | 339000/392702 [00:36<00:05, 8994.24 examples/s]Running tokenizer on dataset:  87%|████████▋ | 340000/392702 [00:36<00:05, 9009.71 examples/s]Running tokenizer on dataset:  87%|████████▋ | 341000/392702 [00:36<00:05, 9028.72 examples/s]Running tokenizer on dataset:  87%|████████▋ | 342000/392702 [00:36<00:05, 9125.79 examples/s]Running tokenizer on dataset:  87%|████████▋ | 343000/392702 [00:36<00:05, 9155.03 examples/s]Running tokenizer on dataset:  88%|████████▊ | 344000/392702 [00:36<00:05, 9198.06 examples/s]Running tokenizer on dataset:  88%|████████▊ | 345000/392702 [00:36<00:05, 9349.55 examples/s]Running tokenizer on dataset:  88%|████████▊ | 347000/392702 [00:37<00:04, 9942.98 examples/s]Running tokenizer on dataset:  89%|████████▉ | 349000/392702 [00:37<00:04, 10121.75 examples/s]Running tokenizer on dataset:  89%|████████▉ | 351000/392702 [00:37<00:04, 10234.90 examples/s]Running tokenizer on dataset:  90%|████████▉ | 353000/392702 [00:37<00:03, 10302.43 examples/s]Running tokenizer on dataset:  90%|█████████ | 355000/392702 [00:37<00:03, 10249.17 examples/s]Running tokenizer on dataset:  91%|█████████ | 357000/392702 [00:38<00:04, 8536.72 examples/s] Running tokenizer on dataset:  91%|█████████▏| 359000/392702 [00:38<00:03, 9115.25 examples/s]Running tokenizer on dataset:  92%|█████████▏| 361000/392702 [00:38<00:03, 9523.68 examples/s]Running tokenizer on dataset:  92%|█████████▏| 363000/392702 [00:38<00:03, 9778.67 examples/s]Running tokenizer on dataset:  93%|█████████▎| 365000/392702 [00:39<00:02, 9857.75 examples/s]Running tokenizer on dataset:  93%|█████████▎| 367000/392702 [00:39<00:02, 9818.10 examples/s]Running tokenizer on dataset:  94%|█████████▎| 368000/392702 [00:39<00:02, 9791.25 examples/s]Running tokenizer on dataset:  94%|█████████▍| 369000/392702 [00:39<00:02, 9672.22 examples/s]Running tokenizer on dataset:  94%|█████████▍| 370000/392702 [00:39<00:02, 9496.77 examples/s]Running tokenizer on dataset:  94%|█████████▍| 371000/392702 [00:39<00:02, 9450.34 examples/s]Running tokenizer on dataset:  95%|█████████▍| 372000/392702 [00:39<00:02, 9230.65 examples/s]Running tokenizer on dataset:  95%|█████████▍| 373000/392702 [00:39<00:02, 9153.44 examples/s]Running tokenizer on dataset:  95%|█████████▌| 374000/392702 [00:39<00:02, 9183.20 examples/s]Running tokenizer on dataset:  95%|█████████▌| 375000/392702 [00:40<00:01, 9212.39 examples/s]Running tokenizer on dataset:  96%|█████████▌| 376000/392702 [00:40<00:01, 9151.21 examples/s]Running tokenizer on dataset:  96%|█████████▌| 377000/392702 [00:40<00:01, 9231.01 examples/s]Running tokenizer on dataset:  96%|█████████▋| 378000/392702 [00:40<00:01, 9191.80 examples/s]Running tokenizer on dataset:  97%|█████████▋| 379000/392702 [00:40<00:01, 9157.11 examples/s]Running tokenizer on dataset:  97%|█████████▋| 380000/392702 [00:40<00:01, 9100.63 examples/s]Running tokenizer on dataset:  97%|█████████▋| 381000/392702 [00:40<00:01, 6817.31 examples/s]Running tokenizer on dataset:  97%|█████████▋| 382000/392702 [00:40<00:01, 7378.79 examples/s]Running tokenizer on dataset:  98%|█████████▊| 383000/392702 [00:41<00:01, 7865.94 examples/s]Running tokenizer on dataset:  98%|█████████▊| 384000/392702 [00:41<00:01, 8190.12 examples/s]Running tokenizer on dataset:  98%|█████████▊| 385000/392702 [00:41<00:00, 8466.23 examples/s]Running tokenizer on dataset:  98%|█████████▊| 386000/392702 [00:41<00:00, 8623.11 examples/s]Running tokenizer on dataset:  99%|█████████▊| 387000/392702 [00:41<00:00, 8759.19 examples/s]Running tokenizer on dataset:  99%|█████████▉| 388000/392702 [00:41<00:00, 8881.25 examples/s]Running tokenizer on dataset:  99%|█████████▉| 389000/392702 [00:41<00:00, 8961.72 examples/s]Running tokenizer on dataset:  99%|█████████▉| 390000/392702 [00:41<00:00, 8962.90 examples/s]Running tokenizer on dataset: 100%|█████████▉| 391000/392702 [00:41<00:00, 8947.35 examples/s]Running tokenizer on dataset: 100%|█████████▉| 392000/392702 [00:42<00:00, 9043.08 examples/s]Running tokenizer on dataset: 100%|██████████| 392702/392702 [00:42<00:00, 9310.60 examples/s]
Running tokenizer on dataset:   0%|          | 0/9815 [00:00<?, ? examples/s]Running tokenizer on dataset:  10%|█         | 1000/9815 [00:00<00:01, 6659.26 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9815 [00:00<00:00, 7902.37 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9815 [00:00<00:00, 8427.20 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9815 [00:00<00:00, 8219.85 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9815 [00:00<00:00, 6839.66 examples/s]Running tokenizer on dataset:  71%|███████▏  | 7000/9815 [00:00<00:00, 8177.43 examples/s]Running tokenizer on dataset:  92%|█████████▏| 9000/9815 [00:01<00:00, 9084.01 examples/s]Running tokenizer on dataset: 100%|██████████| 9815/9815 [00:01<00:00, 8538.30 examples/s]
Running tokenizer on dataset:   0%|          | 0/9832 [00:00<?, ? examples/s]Running tokenizer on dataset:  10%|█         | 1000/9832 [00:00<00:01, 6405.05 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9832 [00:00<00:00, 8930.11 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9832 [00:00<00:00, 8709.75 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9832 [00:00<00:00, 8790.73 examples/s]Running tokenizer on dataset:  71%|███████   | 7000/9832 [00:00<00:00, 7577.54 examples/s]Running tokenizer on dataset:  81%|████████▏ | 8000/9832 [00:00<00:00, 8074.40 examples/s]Running tokenizer on dataset: 100%|██████████| 9832/9832 [00:01<00:00, 8731.23 examples/s]Running tokenizer on dataset: 100%|██████████| 9832/9832 [00:01<00:00, 8379.67 examples/s]
Running tokenizer on dataset:   0%|          | 0/9796 [00:00<?, ? examples/s]Running tokenizer on dataset:  10%|█         | 1000/9796 [00:00<00:01, 5648.35 examples/s]Running tokenizer on dataset:  31%|███       | 3000/9796 [00:00<00:00, 8323.29 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9796 [00:00<00:00, 8278.02 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9796 [00:00<00:00, 7939.41 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9796 [00:00<00:00, 8289.00 examples/s]Running tokenizer on dataset:  71%|███████▏  | 7000/9796 [00:00<00:00, 6215.06 examples/s]Running tokenizer on dataset:  82%|████████▏ | 8000/9796 [00:01<00:00, 6901.75 examples/s]Running tokenizer on dataset:  92%|█████████▏| 9000/9796 [00:01<00:00, 7449.65 examples/s]Running tokenizer on dataset: 100%|██████████| 9796/9796 [00:01<00:00, 7472.90 examples/s]
Running tokenizer on dataset:   0%|          | 0/9847 [00:00<?, ? examples/s]Running tokenizer on dataset:  10%|█         | 1000/9847 [00:00<00:01, 6805.79 examples/s]Running tokenizer on dataset:  20%|██        | 2000/9847 [00:00<00:01, 7809.46 examples/s]Running tokenizer on dataset:  30%|███       | 3000/9847 [00:00<00:00, 8240.42 examples/s]Running tokenizer on dataset:  41%|████      | 4000/9847 [00:00<00:00, 8183.76 examples/s]Running tokenizer on dataset:  51%|█████     | 5000/9847 [00:00<00:00, 8343.00 examples/s]Running tokenizer on dataset:  61%|██████    | 6000/9847 [00:00<00:00, 8540.71 examples/s]Running tokenizer on dataset:  71%|███████   | 7000/9847 [00:00<00:00, 8677.90 examples/s]Running tokenizer on dataset:  81%|████████  | 8000/9847 [00:00<00:00, 8750.46 examples/s]Running tokenizer on dataset:  91%|█████████▏| 9000/9847 [00:01<00:00, 8695.48 examples/s]Running tokenizer on dataset: 100%|██████████| 9847/9847 [00:01<00:00, 8328.00 examples/s]
03/13/2024 23:31:26 - INFO - __main__ - Sample 362817 of the training set: {'input_ids': [1, 7133, 278, 937, 1629, 310, 278, 716, 3534, 292, 1824, 29892, 29871, 29929, 29929, 29995, 310, 278, 2758, 2925, 674, 367, 19591, 304, 15201, 382, 29954, 15922, 411, 385, 782, 428, 363, 278, 9886, 29871, 29896, 15543, 1, 450, 2030, 3534, 292, 1824, 6068, 29871, 29906, 29900, 29995, 310, 2758, 2925, 304, 367, 782, 428, 287, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 23:31:26 - INFO - __main__ - Sample 170372 of the training set: {'input_ids': [1, 940, 6091, 10569, 357, 5742, 1075, 408, 540, 17096, 29889, 1, 319, 2318, 310, 1757, 19090, 472, 1075, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 23:31:26 - INFO - __main__ - Sample 309549 of the training set: {'input_ids': [1, 1126, 393, 8095, 2996, 1623, 2898, 29889, 1, 450, 8095, 2996, 1623, 2289, 2898, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 23:31:29 - INFO - __main__ - ***** Running training *****
03/13/2024 23:31:29 - INFO - __main__ -   Num examples = 392702
03/13/2024 23:31:29 - INFO - __main__ -   Num Epochs = 3
03/13/2024 23:31:29 - INFO - __main__ -   Instantaneous batch size per device = 6
03/13/2024 23:31:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 6
03/13/2024 23:31:29 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 23:31:29 - INFO - __main__ -   Total optimization steps = 196353
  0%|          | 0/196353 [00:00<?, ?it/s]  0%|          | 1/196353 [00:01<83:33:25,  1.53s/it]  0%|          | 2/196353 [00:01<44:12:27,  1.23it/s]  0%|          | 3/196353 [00:02<34:55:44,  1.56it/s]  0%|          | 4/196353 [00:02<30:27:39,  1.79it/s]  0%|          | 5/196353 [00:03<29:29:18,  1.85it/s]  0%|          | 6/196353 [00:03<28:57:29,  1.88it/s]  0%|          | 7/196353 [00:04<26:53:09,  2.03it/s]  0%|          | 8/196353 [00:04<24:39:39,  2.21it/s]  0%|          | 9/196353 [00:04<24:50:54,  2.19it/s]  0%|          | 10/196353 [00:05<23:15:08,  2.35it/s]  0%|          | 11/196353 [00:05<22:48:36,  2.39it/s]  0%|          | 12/196353 [00:06<22:41:14,  2.40it/s]  0%|          | 13/196353 [00:06<22:22:01,  2.44it/s]  0%|          | 14/196353 [00:07<26:24:37,  2.07it/s]  0%|          | 15/196353 [00:07<26:09:59,  2.08it/s]  0%|          | 16/196353 [00:08<25:57:26,  2.10it/s]  0%|          | 17/196353 [00:08<30:36:02,  1.78it/s]  0%|          | 18/196353 [00:09<28:40:56,  1.90it/s]  0%|          | 19/196353 [00:09<27:20:11,  2.00it/s]  0%|          | 20/196353 [00:10<25:53:30,  2.11it/s]  0%|          | 21/196353 [00:10<26:30:24,  2.06it/s]  0%|          | 22/196353 [00:11<26:14:49,  2.08it/s]  0%|          | 23/196353 [00:11<24:54:34,  2.19it/s]  0%|          | 24/196353 [00:12<25:05:08,  2.17it/s]  0%|          | 25/196353 [00:12<25:28:07,  2.14it/s]  0%|          | 26/196353 [00:12<25:00:45,  2.18it/s]  0%|          | 27/196353 [00:13<24:36:04,  2.22it/s]  0%|          | 28/196353 [00:13<23:46:43,  2.29it/s]  0%|          | 29/196353 [00:14<26:08:06,  2.09it/s]  0%|          | 30/196353 [00:14<24:09:43,  2.26it/s]  0%|          | 31/196353 [00:15<23:38:55,  2.31it/s]  0%|          | 32/196353 [00:15<24:52:57,  2.19it/s]  0%|          | 33/196353 [00:16<25:06:54,  2.17it/s]  0%|          | 34/196353 [00:16<24:45:14,  2.20it/s]  0%|          | 35/196353 [00:16<23:16:47,  2.34it/s]  0%|          | 36/196353 [00:17<23:00:01,  2.37it/s]  0%|          | 37/196353 [00:17<23:40:41,  2.30it/s]  0%|          | 38/196353 [00:18<23:06:16,  2.36it/s]  0%|          | 39/196353 [00:18<24:56:39,  2.19it/s]  0%|          | 40/196353 [00:19<24:02:55,  2.27it/s]  0%|          | 41/196353 [00:19<24:00:06,  2.27it/s]  0%|          | 42/196353 [00:19<23:17:44,  2.34it/s]  0%|          | 43/196353 [00:20<23:30:44,  2.32it/s]  0%|          | 44/196353 [00:20<23:38:17,  2.31it/s]  0%|          | 45/196353 [00:21<23:07:38,  2.36it/s]  0%|          | 46/196353 [00:21<23:20:46,  2.34it/s]  0%|          | 47/196353 [00:22<23:05:55,  2.36it/s]  0%|          | 48/196353 [00:22<23:49:21,  2.29it/s]  0%|          | 49/196353 [00:23<24:21:13,  2.24it/s]  0%|          | 50/196353 [00:23<25:28:23,  2.14it/s]  0%|          | 51/196353 [00:23<23:49:56,  2.29it/s]  0%|          | 52/196353 [00:24<25:08:16,  2.17it/s]  0%|          | 53/196353 [00:24<24:25:09,  2.23it/s]  0%|          | 54/196353 [00:25<23:03:06,  2.37it/s]  0%|          | 55/196353 [00:25<22:04:27,  2.47it/s]  0%|          | 56/196353 [00:26<22:40:09,  2.41it/s]  0%|          | 57/196353 [00:26<23:32:59,  2.32it/s]  0%|          | 58/196353 [00:26<23:15:38,  2.34it/s]  0%|          | 59/196353 [00:27<24:51:15,  2.19it/s]  0%|          | 60/196353 [00:27<24:12:24,  2.25it/s]  0%|          | 61/196353 [00:28<23:28:56,  2.32it/s]  0%|          | 62/196353 [00:28<23:30:51,  2.32it/s]  0%|          | 63/196353 [00:29<23:11:10,  2.35it/s]  0%|          | 64/196353 [00:29<23:19:30,  2.34it/s]  0%|          | 65/196353 [00:30<25:15:48,  2.16it/s]  0%|          | 66/196353 [00:30<24:17:49,  2.24it/s]  0%|          | 67/196353 [00:30<23:42:41,  2.30it/s]  0%|          | 68/196353 [00:31<23:37:41,  2.31it/s]  0%|          | 69/196353 [00:31<23:40:08,  2.30it/s]  0%|          | 70/196353 [00:32<22:30:41,  2.42it/s]  0%|          | 71/196353 [00:32<22:30:09,  2.42it/s]  0%|          | 72/196353 [00:33<24:19:32,  2.24it/s]  0%|          | 73/196353 [00:33<24:42:54,  2.21it/s]  0%|          | 74/196353 [00:33<24:56:27,  2.19it/s]  0%|          | 75/196353 [00:34<24:34:34,  2.22it/s]  0%|          | 76/196353 [00:34<23:56:54,  2.28it/s]  0%|          | 77/196353 [00:35<23:30:08,  2.32it/s]  0%|          | 78/196353 [00:36<30:29:01,  1.79it/s]  0%|          | 79/196353 [00:36<29:57:39,  1.82it/s]  0%|          | 80/196353 [00:37<27:47:27,  1.96it/s]  0%|          | 81/196353 [00:37<27:49:44,  1.96it/s]  0%|          | 82/196353 [00:38<27:52:38,  1.96it/s]  0%|          | 83/196353 [00:38<27:54:39,  1.95it/s]  0%|          | 84/196353 [00:39<27:31:44,  1.98it/s]  0%|          | 85/196353 [00:39<25:10:18,  2.17it/s]  0%|          | 86/196353 [00:39<24:06:08,  2.26it/s]  0%|          | 87/196353 [00:40<24:48:10,  2.20it/s]  0%|          | 88/196353 [00:40<24:09:14,  2.26it/s]  0%|          | 89/196353 [00:41<24:32:30,  2.22it/s]  0%|          | 90/196353 [00:41<24:18:13,  2.24it/s]  0%|          | 91/196353 [00:41<22:48:58,  2.39it/s]  0%|          | 92/196353 [00:42<21:47:53,  2.50it/s]  0%|          | 93/196353 [00:42<22:49:45,  2.39it/s]  0%|          | 94/196353 [00:43<23:52:50,  2.28it/s]  0%|          | 95/196353 [00:43<23:28:53,  2.32it/s]  0%|          | 96/196353 [00:44<23:41:07,  2.30it/s]  0%|          | 97/196353 [00:44<25:07:37,  2.17it/s]  0%|          | 98/196353 [00:45<24:42:54,  2.21it/s]  0%|          | 99/196353 [00:45<24:00:51,  2.27it/s]  0%|          | 100/196353 [00:45<23:32:15,  2.32it/s]  0%|          | 101/196353 [00:46<25:16:32,  2.16it/s]  0%|          | 102/196353 [00:46<25:19:37,  2.15it/s]  0%|          | 103/196353 [00:47<24:16:58,  2.24it/s]  0%|          | 104/196353 [00:47<23:33:57,  2.31it/s]  0%|          | 105/196353 [00:48<24:04:42,  2.26it/s]  0%|          | 106/196353 [00:48<23:57:35,  2.28it/s]  0%|          | 107/196353 [00:49<25:07:40,  2.17it/s]  0%|          | 108/196353 [00:49<24:53:28,  2.19it/s]  0%|          | 109/196353 [00:50<24:37:45,  2.21it/s]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 552, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 755, in forward
    hidden_states = self.mlp(hidden_states)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 393, in forward
    return F.silu(input, inplace=self.inplace)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/functional.py", line 2075, in silu
    return torch._C._nn.silu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 7.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 109/196353 [00:51<25:31:44,  2.14it/s]
[2024-03-13 23:32:22,934] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 54171) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_23:32:22
  host      : v021.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 54171)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v021: task 0: Exited with exit code 1
END TIME: Wed Mar 13 23:32:23 EDT 2024
