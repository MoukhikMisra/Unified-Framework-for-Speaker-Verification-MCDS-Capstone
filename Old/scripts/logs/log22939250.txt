/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
03/13/2024 23:26:29 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

loading configuration file config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/config.json
Model config LlamaConfig {
  "_name_or_path": "datajuicer/LLaMA-1B-dj-refine-100B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "finetuning_task": "qnli",
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5504,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "pad_token_id": 32004,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.39.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

loading file tokenizer.model from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.model
loading file tokenizer.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer.json
loading file added_tokens.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/added_tokens.json
loading file special_tokens_map.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/special_tokens_map.json
loading file tokenizer_config.json from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /ocean/projects/cis220031p/mmisra/transformers/examples/pytorch/language-modeling/scripts/hg_cache/models--datajuicer--LLaMA-1B-dj-refine-100B/snapshots/1bd6974aad6057a3e17b69fa4f818c07aedeae51/pytorch_model.bin
Some weights of the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at datajuicer/LLaMA-1B-dj-refine-100B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/104743 [00:00<?, ? examples/s]Running tokenizer on dataset:   1%|          | 1000/104743 [00:00<00:19, 5206.82 examples/s]Running tokenizer on dataset:   2%|▏         | 2000/104743 [00:00<00:16, 6222.60 examples/s]Running tokenizer on dataset:   3%|▎         | 3000/104743 [00:00<00:15, 6566.85 examples/s]Running tokenizer on dataset:   4%|▍         | 4000/104743 [00:00<00:15, 6577.68 examples/s]Running tokenizer on dataset:   5%|▍         | 5000/104743 [00:00<00:14, 6780.40 examples/s]Running tokenizer on dataset:   6%|▌         | 6000/104743 [00:00<00:14, 6729.49 examples/s]Running tokenizer on dataset:   7%|▋         | 7000/104743 [00:01<00:14, 6965.78 examples/s]Running tokenizer on dataset:   8%|▊         | 8000/104743 [00:01<00:13, 7062.78 examples/s]Running tokenizer on dataset:   9%|▊         | 9000/104743 [00:01<00:13, 7153.13 examples/s]Running tokenizer on dataset:  10%|▉         | 10000/104743 [00:01<00:13, 7200.53 examples/s]Running tokenizer on dataset:  11%|█         | 11000/104743 [00:01<00:12, 7233.27 examples/s]Running tokenizer on dataset:  11%|█▏        | 12000/104743 [00:01<00:12, 7274.22 examples/s]Running tokenizer on dataset:  12%|█▏        | 13000/104743 [00:01<00:12, 7296.15 examples/s]Running tokenizer on dataset:  13%|█▎        | 14000/104743 [00:02<00:12, 7239.35 examples/s]Running tokenizer on dataset:  14%|█▍        | 15000/104743 [00:02<00:12, 7268.13 examples/s]Running tokenizer on dataset:  15%|█▌        | 16000/104743 [00:02<00:12, 7263.35 examples/s]Running tokenizer on dataset:  16%|█▌        | 17000/104743 [00:02<00:12, 7296.64 examples/s]Running tokenizer on dataset:  17%|█▋        | 18000/104743 [00:02<00:11, 7280.27 examples/s]Running tokenizer on dataset:  18%|█▊        | 19000/104743 [00:02<00:11, 7284.44 examples/s]Running tokenizer on dataset:  19%|█▉        | 20000/104743 [00:02<00:11, 7281.40 examples/s]Running tokenizer on dataset:  20%|██        | 21000/104743 [00:02<00:11, 7293.61 examples/s]Running tokenizer on dataset:  21%|██        | 22000/104743 [00:03<00:11, 7321.97 examples/s]Running tokenizer on dataset:  22%|██▏       | 23000/104743 [00:03<00:11, 7329.89 examples/s]Running tokenizer on dataset:  23%|██▎       | 24000/104743 [00:03<00:10, 7344.18 examples/s]Running tokenizer on dataset:  24%|██▍       | 25000/104743 [00:03<00:10, 7337.76 examples/s]Running tokenizer on dataset:  25%|██▍       | 26000/104743 [00:03<00:10, 7328.29 examples/s]Running tokenizer on dataset:  26%|██▌       | 27000/104743 [00:03<00:10, 7263.37 examples/s]Running tokenizer on dataset:  27%|██▋       | 28000/104743 [00:03<00:10, 7038.40 examples/s]Running tokenizer on dataset:  28%|██▊       | 29000/104743 [00:04<00:10, 6919.04 examples/s]Running tokenizer on dataset:  29%|██▊       | 30000/104743 [00:04<00:10, 6812.75 examples/s]Running tokenizer on dataset:  30%|██▉       | 31000/104743 [00:04<00:10, 6784.46 examples/s]Running tokenizer on dataset:  31%|███       | 32000/104743 [00:04<00:10, 6858.01 examples/s]Running tokenizer on dataset:  32%|███▏      | 33000/104743 [00:04<00:10, 6737.96 examples/s]Running tokenizer on dataset:  32%|███▏      | 34000/104743 [00:04<00:10, 6682.48 examples/s]Running tokenizer on dataset:  33%|███▎      | 35000/104743 [00:04<00:10, 6663.83 examples/s]Running tokenizer on dataset:  34%|███▍      | 36000/104743 [00:05<00:10, 6690.77 examples/s]Running tokenizer on dataset:  35%|███▌      | 37000/104743 [00:05<00:10, 6693.83 examples/s]Running tokenizer on dataset:  36%|███▋      | 38000/104743 [00:05<00:09, 6681.11 examples/s]Running tokenizer on dataset:  37%|███▋      | 39000/104743 [00:05<00:09, 6656.57 examples/s]Running tokenizer on dataset:  38%|███▊      | 40000/104743 [00:05<00:09, 6635.64 examples/s]Running tokenizer on dataset:  39%|███▉      | 41000/104743 [00:05<00:09, 6601.01 examples/s]Running tokenizer on dataset:  40%|████      | 42000/104743 [00:06<00:09, 6666.80 examples/s]Running tokenizer on dataset:  41%|████      | 43000/104743 [00:06<00:09, 6630.87 examples/s]Running tokenizer on dataset:  42%|████▏     | 44000/104743 [00:06<00:08, 6758.36 examples/s]Running tokenizer on dataset:  43%|████▎     | 45000/104743 [00:06<00:08, 6784.93 examples/s]Running tokenizer on dataset:  44%|████▍     | 46000/104743 [00:06<00:08, 6787.71 examples/s]Running tokenizer on dataset:  45%|████▍     | 47000/104743 [00:06<00:08, 6743.25 examples/s]Running tokenizer on dataset:  46%|████▌     | 48000/104743 [00:07<00:10, 5372.40 examples/s]Running tokenizer on dataset:  47%|████▋     | 49000/104743 [00:07<00:09, 5702.18 examples/s]Running tokenizer on dataset:  48%|████▊     | 50000/104743 [00:07<00:09, 5987.66 examples/s]Running tokenizer on dataset:  49%|████▊     | 51000/104743 [00:07<00:08, 6308.72 examples/s]Running tokenizer on dataset:  50%|████▉     | 52000/104743 [00:07<00:08, 6514.72 examples/s]Running tokenizer on dataset:  51%|█████     | 53000/104743 [00:07<00:07, 6719.04 examples/s]Running tokenizer on dataset:  52%|█████▏    | 54000/104743 [00:07<00:07, 6817.75 examples/s]Running tokenizer on dataset:  53%|█████▎    | 55000/104743 [00:08<00:07, 6876.42 examples/s]Running tokenizer on dataset:  53%|█████▎    | 56000/104743 [00:08<00:07, 6946.52 examples/s]Running tokenizer on dataset:  54%|█████▍    | 57000/104743 [00:08<00:06, 6982.35 examples/s]Running tokenizer on dataset:  55%|█████▌    | 58000/104743 [00:08<00:06, 7085.39 examples/s]Running tokenizer on dataset:  56%|█████▋    | 59000/104743 [00:08<00:06, 7115.90 examples/s]Running tokenizer on dataset:  57%|█████▋    | 60000/104743 [00:08<00:06, 7171.30 examples/s]Running tokenizer on dataset:  58%|█████▊    | 61000/104743 [00:08<00:06, 7234.99 examples/s]Running tokenizer on dataset:  59%|█████▉    | 62000/104743 [00:09<00:05, 7263.01 examples/s]Running tokenizer on dataset:  60%|██████    | 63000/104743 [00:09<00:05, 7261.10 examples/s]Running tokenizer on dataset:  61%|██████    | 64000/104743 [00:09<00:05, 7288.36 examples/s]Running tokenizer on dataset:  62%|██████▏   | 65000/104743 [00:09<00:05, 7325.52 examples/s]Running tokenizer on dataset:  63%|██████▎   | 66000/104743 [00:09<00:05, 7323.96 examples/s]Running tokenizer on dataset:  64%|██████▍   | 67000/104743 [00:09<00:05, 7383.39 examples/s]Running tokenizer on dataset:  65%|██████▍   | 68000/104743 [00:09<00:05, 7343.87 examples/s]Running tokenizer on dataset:  66%|██████▌   | 69000/104743 [00:09<00:04, 7333.25 examples/s]Running tokenizer on dataset:  67%|██████▋   | 70000/104743 [00:10<00:04, 7358.42 examples/s]Running tokenizer on dataset:  68%|██████▊   | 71000/104743 [00:10<00:04, 7325.18 examples/s]Running tokenizer on dataset:  69%|██████▊   | 72000/104743 [00:10<00:04, 7301.77 examples/s]Running tokenizer on dataset:  70%|██████▉   | 73000/104743 [00:10<00:04, 7294.61 examples/s]Running tokenizer on dataset:  71%|███████   | 74000/104743 [00:10<00:04, 7308.19 examples/s]Running tokenizer on dataset:  72%|███████▏  | 75000/104743 [00:10<00:04, 7420.69 examples/s]Running tokenizer on dataset:  73%|███████▎  | 76000/104743 [00:10<00:03, 7403.61 examples/s]Running tokenizer on dataset:  74%|███████▎  | 77000/104743 [00:11<00:03, 7321.72 examples/s]Running tokenizer on dataset:  74%|███████▍  | 78000/104743 [00:11<00:03, 7240.94 examples/s]Running tokenizer on dataset:  75%|███████▌  | 79000/104743 [00:11<00:03, 7188.27 examples/s]Running tokenizer on dataset:  76%|███████▋  | 80000/104743 [00:11<00:03, 7112.43 examples/s]Running tokenizer on dataset:  77%|███████▋  | 81000/104743 [00:11<00:03, 7044.40 examples/s]Running tokenizer on dataset:  78%|███████▊  | 82000/104743 [00:11<00:03, 7037.56 examples/s]Running tokenizer on dataset:  79%|███████▉  | 83000/104743 [00:11<00:03, 7008.91 examples/s]Running tokenizer on dataset:  80%|████████  | 84000/104743 [00:12<00:02, 7011.26 examples/s]Running tokenizer on dataset:  81%|████████  | 85000/104743 [00:12<00:02, 7095.39 examples/s]Running tokenizer on dataset:  82%|████████▏ | 86000/104743 [00:12<00:03, 5679.12 examples/s]Running tokenizer on dataset:  83%|████████▎ | 87000/104743 [00:12<00:02, 6030.25 examples/s]Running tokenizer on dataset:  84%|████████▍ | 88000/104743 [00:12<00:02, 6323.40 examples/s]Running tokenizer on dataset:  85%|████████▍ | 89000/104743 [00:12<00:02, 6552.32 examples/s]Running tokenizer on dataset:  86%|████████▌ | 90000/104743 [00:13<00:02, 6746.36 examples/s]Running tokenizer on dataset:  87%|████████▋ | 91000/104743 [00:13<00:01, 6903.95 examples/s]Running tokenizer on dataset:  88%|████████▊ | 92000/104743 [00:13<00:01, 6978.01 examples/s]Running tokenizer on dataset:  89%|████████▉ | 93000/104743 [00:13<00:01, 7052.53 examples/s]Running tokenizer on dataset:  90%|████████▉ | 94000/104743 [00:13<00:01, 7099.56 examples/s]Running tokenizer on dataset:  91%|█████████ | 95000/104743 [00:13<00:01, 7052.76 examples/s]Running tokenizer on dataset:  92%|█████████▏| 96000/104743 [00:13<00:01, 7109.03 examples/s]Running tokenizer on dataset:  93%|█████████▎| 97000/104743 [00:13<00:01, 7073.98 examples/s]Running tokenizer on dataset:  94%|█████████▎| 98000/104743 [00:14<00:00, 7042.05 examples/s]Running tokenizer on dataset:  95%|█████████▍| 99000/104743 [00:14<00:00, 7027.85 examples/s]Running tokenizer on dataset:  95%|█████████▌| 100000/104743 [00:14<00:00, 7052.70 examples/s]Running tokenizer on dataset:  96%|█████████▋| 101000/104743 [00:14<00:00, 7118.67 examples/s]Running tokenizer on dataset:  97%|█████████▋| 102000/104743 [00:14<00:00, 7187.65 examples/s]Running tokenizer on dataset:  98%|█████████▊| 103000/104743 [00:14<00:00, 7283.17 examples/s]Running tokenizer on dataset:  99%|█████████▉| 104000/104743 [00:14<00:00, 7290.32 examples/s]Running tokenizer on dataset: 100%|██████████| 104743/104743 [00:15<00:00, 7213.26 examples/s]Running tokenizer on dataset: 100%|██████████| 104743/104743 [00:15<00:00, 6944.92 examples/s]
Running tokenizer on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]Running tokenizer on dataset:  18%|█▊        | 1000/5463 [00:00<00:01, 3617.08 examples/s]Running tokenizer on dataset:  37%|███▋      | 2000/5463 [00:00<00:00, 4708.53 examples/s]Running tokenizer on dataset:  55%|█████▍    | 3000/5463 [00:00<00:00, 5189.55 examples/s]Running tokenizer on dataset:  73%|███████▎  | 4000/5463 [00:00<00:00, 4905.89 examples/s]Running tokenizer on dataset:  92%|█████████▏| 5000/5463 [00:00<00:00, 5447.60 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:01<00:00, 5163.62 examples/s]
Running tokenizer on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]Running tokenizer on dataset:  18%|█▊        | 1000/5463 [00:00<00:00, 5376.02 examples/s]Running tokenizer on dataset:  37%|███▋      | 2000/5463 [00:00<00:00, 5284.44 examples/s]Running tokenizer on dataset:  55%|█████▍    | 3000/5463 [00:00<00:00, 5580.99 examples/s]Running tokenizer on dataset:  73%|███████▎  | 4000/5463 [00:00<00:00, 5676.46 examples/s]Running tokenizer on dataset:  92%|█████████▏| 5000/5463 [00:00<00:00, 6110.32 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:00<00:00, 5846.22 examples/s]
03/13/2024 23:29:39 - INFO - __main__ - Sample 91001 of the training set: {'input_ids': [1, 1932, 471, 278, 4799, 637, 3806, 304, 367, 8072, 13303, 29973, 1, 1222, 6021, 304, 367, 8072, 1751, 1288, 4688, 29871, 29906, 29900, 29896, 29953, 29892, 372, 338, 3806, 304, 1371, 278, 11359, 7113, 1583, 29899, 2146, 2416, 13396, 322, 13731, 6617, 17407, 5849, 29892, 27668, 26307, 373, 4908, 5874, 16226, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
03/13/2024 23:29:39 - INFO - __main__ - Sample 46891 of the training set: {'input_ids': [1, 1724, 947, 9766, 561, 2361, 275, 2737, 304, 29973, 1, 4525, 25828, 4835, 526, 5517, 20974, 491, 263, 20364, 1190, 479, 310, 274, 403, 305, 324, 314, 1475, 515, 278, 17520, 7492, 23547, 681, 1788, 29892, 607, 10008, 297, 2933, 304, 6788, 322, 278, 10416, 4972, 633, 8945, 1907, 393, 1121, 515, 270, 952, 2220, 310, 278, 5192, 2301, 2841, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
03/13/2024 23:29:39 - INFO - __main__ - Sample 13992 of the training set: {'input_ids': [1, 11644, 471, 349, 19771, 5100, 2105, 297, 278, 278, 1008, 5381, 29973, 1, 450, 278, 10412, 540, 4240, 363, 349, 19771, 297, 27689, 505, 1063, 2845, 27745, 3276, 470, 11543, 304, 916, 3913, 29892, 541, 1784, 916, 278, 10412, 10503, 573, 297, 916, 14368, 310, 278, 501, 29889, 29903, 1696, 4049, 11551, 292, 278, 349, 19771, 1024, 29936, 27689, 29915, 29879, 10503, 4357, 12662, 792, 15521, 29892, 373, 607, 540, 11465, 630, 29892, 471, 451, 263, 349, 19771, 278, 1008, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/accelerate/accelerator.py:538: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
03/13/2024 23:29:41 - INFO - __main__ - ***** Running training *****
03/13/2024 23:29:41 - INFO - __main__ -   Num examples = 104743
03/13/2024 23:29:41 - INFO - __main__ -   Num Epochs = 3
03/13/2024 23:29:41 - INFO - __main__ -   Instantaneous batch size per device = 6
03/13/2024 23:29:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 6
03/13/2024 23:29:41 - INFO - __main__ -   Gradient Accumulation steps = 1
03/13/2024 23:29:41 - INFO - __main__ -   Total optimization steps = 52374
  0%|          | 0/52374 [00:00<?, ?it/s]  0%|          | 1/52374 [00:01<24:24:50,  1.68s/it]  0%|          | 2/52374 [00:02<13:00:13,  1.12it/s]  0%|          | 3/52374 [00:02<10:10:50,  1.43it/s]  0%|          | 4/52374 [00:02<8:59:16,  1.62it/s]   0%|          | 5/52374 [00:03<8:40:33,  1.68it/s]  0%|          | 6/52374 [00:03<7:47:58,  1.87it/s]  0%|          | 7/52374 [00:04<8:06:23,  1.79it/s]  0%|          | 8/52374 [00:04<7:24:37,  1.96it/s]  0%|          | 9/52374 [00:05<9:11:05,  1.58it/s]  0%|          | 10/52374 [00:06<8:24:42,  1.73it/s]  0%|          | 11/52374 [00:06<7:47:34,  1.87it/s]  0%|          | 12/52374 [00:07<7:12:32,  2.02it/s]  0%|          | 13/52374 [00:07<7:06:48,  2.04it/s]  0%|          | 14/52374 [00:08<7:24:41,  1.96it/s]  0%|          | 15/52374 [00:08<7:28:09,  1.95it/s]  0%|          | 16/52374 [00:09<7:17:05,  2.00it/s]  0%|          | 17/52374 [00:09<7:27:06,  1.95it/s]  0%|          | 18/52374 [00:10<7:08:56,  2.03it/s]  0%|          | 19/52374 [00:10<7:02:44,  2.06it/s]  0%|          | 20/52374 [00:11<7:14:36,  2.01it/s]  0%|          | 21/52374 [00:11<7:09:00,  2.03it/s]  0%|          | 22/52374 [00:12<7:21:17,  1.98it/s]  0%|          | 23/52374 [00:12<7:06:51,  2.04it/s]  0%|          | 24/52374 [00:13<6:44:56,  2.15it/s]  0%|          | 25/52374 [00:13<7:01:03,  2.07it/s]  0%|          | 26/52374 [00:14<6:52:27,  2.12it/s]  0%|          | 27/52374 [00:14<7:02:32,  2.06it/s]  0%|          | 28/52374 [00:15<7:10:11,  2.03it/s]  0%|          | 29/52374 [00:15<7:16:00,  2.00it/s]  0%|          | 30/52374 [00:16<7:23:52,  1.97it/s]  0%|          | 31/52374 [00:16<7:15:38,  2.00it/s]  0%|          | 32/52374 [00:17<7:02:44,  2.06it/s]  0%|          | 33/52374 [00:17<7:04:43,  2.05it/s]  0%|          | 34/52374 [00:18<7:20:00,  1.98it/s]  0%|          | 35/52374 [00:18<7:16:50,  2.00it/s]  0%|          | 36/52374 [00:19<7:23:01,  1.97it/s]  0%|          | 37/52374 [00:19<7:28:38,  1.94it/s]  0%|          | 38/52374 [00:20<7:31:47,  1.93it/s]  0%|          | 39/52374 [00:20<7:12:52,  2.02it/s]  0%|          | 40/52374 [00:21<7:20:35,  1.98it/s]  0%|          | 41/52374 [00:21<7:30:32,  1.94it/s]  0%|          | 42/52374 [00:22<7:30:06,  1.94it/s]  0%|          | 43/52374 [00:22<7:11:21,  2.02it/s]  0%|          | 44/52374 [00:23<8:10:12,  1.78it/s]  0%|          | 45/52374 [00:23<7:54:06,  1.84it/s]  0%|          | 46/52374 [00:24<7:35:04,  1.92it/s]  0%|          | 47/52374 [00:24<7:12:44,  2.02it/s]  0%|          | 48/52374 [00:25<7:17:27,  1.99it/s]  0%|          | 49/52374 [00:25<7:33:01,  1.92it/s]  0%|          | 50/52374 [00:26<8:26:38,  1.72it/s]  0%|          | 51/52374 [00:27<8:51:51,  1.64it/s]  0%|          | 52/52374 [00:27<8:10:37,  1.78it/s]  0%|          | 53/52374 [00:28<7:38:33,  1.90it/s]  0%|          | 54/52374 [00:28<7:37:25,  1.91it/s]  0%|          | 55/52374 [00:29<8:29:06,  1.71it/s]  0%|          | 56/52374 [00:30<9:06:32,  1.60it/s]  0%|          | 57/52374 [00:30<8:40:50,  1.67it/s]  0%|          | 58/52374 [00:31<7:51:18,  1.85it/s]  0%|          | 59/52374 [00:31<7:44:47,  1.88it/s]  0%|          | 60/52374 [00:31<7:22:29,  1.97it/s]  0%|          | 61/52374 [00:32<7:11:52,  2.02it/s]  0%|          | 62/52374 [00:32<6:48:59,  2.13it/s]  0%|          | 63/52374 [00:33<7:07:18,  2.04it/s]  0%|          | 64/52374 [00:33<7:14:52,  2.00it/s]  0%|          | 65/52374 [00:34<7:19:05,  1.99it/s]  0%|          | 66/52374 [00:34<7:25:13,  1.96it/s]  0%|          | 67/52374 [00:35<7:49:48,  1.86it/s]  0%|          | 68/52374 [00:36<7:44:56,  1.88it/s]  0%|          | 69/52374 [00:36<7:33:56,  1.92it/s]  0%|          | 70/52374 [00:37<7:25:57,  1.95it/s]  0%|          | 71/52374 [00:37<7:06:58,  2.04it/s]  0%|          | 72/52374 [00:38<7:31:50,  1.93it/s]  0%|          | 73/52374 [00:38<7:14:05,  2.01it/s]  0%|          | 74/52374 [00:39<7:07:31,  2.04it/s]  0%|          | 75/52374 [00:40<9:20:28,  1.56it/s]  0%|          | 76/52374 [00:40<7:59:46,  1.82it/s]  0%|          | 77/52374 [00:41<8:31:10,  1.71it/s]  0%|          | 78/52374 [00:41<8:32:51,  1.70it/s]  0%|          | 79/52374 [00:42<8:44:51,  1.66it/s]  0%|          | 80/52374 [00:42<9:05:13,  1.60it/s]  0%|          | 81/52374 [00:43<9:09:30,  1.59it/s]  0%|          | 82/52374 [00:43<8:15:51,  1.76it/s]  0%|          | 83/52374 [00:44<7:49:13,  1.86it/s]  0%|          | 84/52374 [00:44<7:37:37,  1.90it/s]  0%|          | 85/52374 [00:45<7:37:17,  1.91it/s]  0%|          | 86/52374 [00:46<7:46:24,  1.87it/s]  0%|          | 87/52374 [00:46<7:36:27,  1.91it/s]  0%|          | 88/52374 [00:47<7:42:25,  1.88it/s]  0%|          | 89/52374 [00:47<7:13:54,  2.01it/s]  0%|          | 90/52374 [00:47<6:58:05,  2.08it/s]  0%|          | 91/52374 [00:48<7:25:52,  1.95it/s]  0%|          | 92/52374 [00:49<7:27:46,  1.95it/s]  0%|          | 93/52374 [00:49<6:59:47,  2.08it/s]  0%|          | 94/52374 [00:49<7:10:39,  2.02it/s]  0%|          | 95/52374 [00:50<7:48:12,  1.86it/s]  0%|          | 96/52374 [00:51<7:45:57,  1.87it/s]  0%|          | 97/52374 [00:51<7:30:54,  1.93it/s]  0%|          | 98/52374 [00:52<7:19:53,  1.98it/s]  0%|          | 99/52374 [00:52<7:32:47,  1.92it/s]  0%|          | 100/52374 [00:53<6:54:19,  2.10it/s]  0%|          | 101/52374 [00:53<6:52:00,  2.11it/s]  0%|          | 102/52374 [00:53<6:43:50,  2.16it/s]  0%|          | 103/52374 [00:54<6:37:13,  2.19it/s]  0%|          | 104/52374 [00:54<6:54:36,  2.10it/s]  0%|          | 105/52374 [00:55<7:09:23,  2.03it/s]  0%|          | 106/52374 [00:55<6:57:08,  2.09it/s]  0%|          | 107/52374 [00:56<7:01:29,  2.07it/s]  0%|          | 108/52374 [00:56<6:44:51,  2.15it/s]  0%|          | 109/52374 [00:57<6:47:19,  2.14it/s]  0%|          | 110/52374 [00:57<7:03:38,  2.06it/s]  0%|          | 111/52374 [00:58<6:54:09,  2.10it/s]  0%|          | 112/52374 [00:58<7:11:14,  2.02it/s]  0%|          | 113/52374 [00:59<7:05:12,  2.05it/s]  0%|          | 114/52374 [00:59<7:23:12,  1.97it/s]  0%|          | 115/52374 [01:00<7:05:47,  2.05it/s]  0%|          | 116/52374 [01:00<7:23:29,  1.96it/s]  0%|          | 117/52374 [01:01<7:14:02,  2.01it/s]  0%|          | 118/52374 [01:01<7:20:55,  1.98it/s]  0%|          | 119/52374 [01:02<7:18:46,  1.98it/s]  0%|          | 120/52374 [01:02<7:54:50,  1.83it/s]  0%|          | 121/52374 [01:03<7:20:15,  1.98it/s]  0%|          | 122/52374 [01:03<7:02:43,  2.06it/s]  0%|          | 123/52374 [01:04<7:41:33,  1.89it/s]  0%|          | 124/52374 [01:05<8:02:11,  1.81it/s]  0%|          | 125/52374 [01:05<7:47:51,  1.86it/s]  0%|          | 126/52374 [01:06<7:41:45,  1.89it/s]  0%|          | 127/52374 [01:06<7:39:37,  1.89it/s]  0%|          | 128/52374 [01:07<7:25:43,  1.95it/s]Traceback (most recent call last):
  File "run_glue_no_trainer.py", line 668, in <module>
    main()
  File "run_glue_no_trainer.py", line 552, in main
    outputs = model(**batch)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1382, in forward
    transformer_outputs = self.model(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1025, in forward
    layer_outputs = decoder_layer(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 740, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 671, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 3.31 MiB is free. Including non-PyTorch memory, this process has 31.73 GiB memory in use. Of the allocated memory 28.71 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 128/52374 [01:07<7:38:50,  1.90it/s]
[2024-03-13 23:30:52,628] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 69018) of binary: /jet/home/mmisra/miniconda3/envs/benchmark/bin/python
Traceback (most recent call last):
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/jet/home/mmisra/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_glue_no_trainer.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-13_23:30:52
  host      : v010.ib.bridges2.psc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 69018)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: v010: task 0: Exited with exit code 1
